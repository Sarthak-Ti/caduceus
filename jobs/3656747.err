Seed set to 2222
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
wandb: Currently logged in as: sarthak-ti. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.2
wandb: Run data is saved locally in ./wandb/run-20240217_154404-etz909z6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scintillating-dragon-21
wandb: ⭐️ View project at https://wandb.ai/sarthak-ti/dna
wandb: 🚀 View run at https://wandb.ai/sarthak-ti/dna/runs/etz909z6
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name               | Type              | Params
---------------------------------------------------------
0 | model              | DNAEmbeddingModel | 456 K 
1 | encoder            | Identity          | 0     
2 | decoder            | SequenceDecoder   | 129   
3 | train_torchmetrics | MetricCollection  | 0     
4 | val_torchmetrics   | MetricCollection  | 0     
5 | test_torchmetrics  | MetricCollection  | 0     
---------------------------------------------------------
456 K     Trainable params
0         Non-trainable params
456 K     Total params
1.825     Total estimated model params size (MB)
Epoch 0, global step 67299: 'val/mse' reached 3.93574 (best 3.93574), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/00-val_loss=3.93574.ckpt' as top 1
Epoch 1, global step 134598: 'val/mse' reached 3.84707 (best 3.84707), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/01-val_loss=3.84707.ckpt' as top 2
Epoch 2, global step 201897: 'val/mse' reached 3.81969 (best 3.81969), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/02-val_loss=3.81969.ckpt' as top 3
Epoch 3, global step 269196: 'val/mse' reached 3.80256 (best 3.80256), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/03-val_loss=3.80256.ckpt' as top 4
Epoch 4, global step 336495: 'val/mse' reached 3.76850 (best 3.76850), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/04-val_loss=3.76850.ckpt' as top 5
Epoch 5, global step 403794: 'val/mse' reached 3.77192 (best 3.76850), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/05-val_loss=3.77192.ckpt' as top 6
Epoch 6, global step 471093: 'val/mse' reached 3.73603 (best 3.73603), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/06-val_loss=3.73603.ckpt' as top 7
Epoch 7, global step 538392: 'val/mse' reached 3.75147 (best 3.73603), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/07-val_loss=3.75147.ckpt' as top 8
Epoch 8, global step 605691: 'val/mse' reached 3.72797 (best 3.72797), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/08-val_loss=3.72797.ckpt' as top 9
Epoch 9, global step 672990: 'val/mse' reached 3.72707 (best 3.72707), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/09-val_loss=3.72707.ckpt' as top 10
Epoch 10, global step 740289: 'val/mse' reached 3.72192 (best 3.72192), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/10-val_loss=3.72192.ckpt' as top 11
Epoch 11, global step 807588: 'val/mse' reached 3.72003 (best 3.72003), saving model to '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-02-17/15-43-26-621154/checkpoints/11-val_loss=3.72003.ckpt' as top 12
`Trainer.fit` stopped: `max_epochs=12` reached.
wandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job
wandb: - 0.007 MB of 0.007 MB uploadedwandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb:           test/loss █▅▄▄▃▃▂▂▁▁▁▁
wandb:            test/mse █▅▄▄▃▃▂▂▁▁▁▁
wandb:         timer/epoch ▂▁▁▁▂█▂▃▅▆▃▃
wandb:          timer/step ▅▄▆▅▅▆▄▃▇▆█▃▂▅▄▇▆▄█▄▆▄▅▃█▁▅█▄▆▁█▃▅▅▃▆▆▅█
wandb:    timer/validation ▅▃▂▂▁█▁▂▅▆▂▁
wandb:          train/loss █▅▃▃▂▂▂▁▁▁▁▁
wandb:           train/mse █▅▃▃▂▂▂▁▁▁▁▁
wandb:       trainer/epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███
wandb: trainer/global_step ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:        trainer/loss █▆▆▅▇█▇▄▅▅▇▇▄▇▅▆▅▆█▆▂▃▇▆▁▅▅▁▄▃▅▅▅▆▃▆▅▅▄▃
wandb:      trainer/lr/pg1 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      trainer/lr/pg2 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            val/loss █▅▄▄▃▃▂▂▁▁▁▁
wandb:             val/mse █▅▄▄▃▃▂▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:               epoch 11
wandb:           test/loss 3.71997
wandb:            test/mse 3.71997
wandb:         timer/epoch 41289.33771
wandb:          timer/step 0.54468
wandb:    timer/validation 4516.25076
wandb:          train/loss 3.66922
wandb:           train/mse 3.66922
wandb:       trainer/epoch 11.0
wandb: trainer/global_step 807587
wandb:        trainer/loss 3.64458
wandb:      trainer/lr/pg1 1e-05
wandb:      trainer/lr/pg2 1e-05
wandb:            val/loss 3.72003
wandb:             val/mse 3.72003
wandb: 
wandb: 🚀 View run scintillating-dragon-21 at: https://wandb.ai/sarthak-ti/dna/runs/etz909z6
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240217_154404-etz909z6/logs
