{'train': {'seed': 2222, 'interval': 'step', 'monitor': 'val/mse', 'mode': 'min', 'ema': 0.0, 'test': False, 'debug': False, 'ignore_warnings': False, 'state': {'mode': None, 'n_context': 0, 'n_context_eval': 0}, 'ckpt': None, 'disable_dataset': False, 'validate_at_start': False, 'pretrained_model_path': '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-29/17-36-53-758146/checkpoints/last.ckpt', 'pretrained_model_strict_load': False, 'pretrained_model_state_hook': {'_name_': 'load_backbone', 'freeze_backbone': False, 'ignore_head': True, 'add_embeddings': False, 'ignore_embeddings': True}, 'post_init_hook': {'_name_': None}, 'layer_decay': {'_name_': None, 'decay': 0.7}, 'gpu_mem': 82, 'global_batch_size': 2048}, 'tolerance': {'logdir': './resume', 'id': None}, 'wandb': None, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'devices': 2, 'accelerator': 'gpu', 'accumulate_grad_batches': 1, 'max_epochs': 30, 'gradient_clip_val': 1.0, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'num_sanity_val_steps': 0, 'num_nodes': 2, 'precision': 'bf16', 'strategy': 'ddp'}, 'loader': {'batch_size': 50, 'num_workers': 4, 'pin_memory': True, 'drop_last': True}, 'dataset': {'_name_': 'DNase', 'bed_file': None, 'fasta_file': None, 'dataset_name': 'DNase', 'tokenizer_name': 'char', 'cache_dir': None, 'max_length': 1024, 'add_eos': True, 'batch_size': 2048, 'batch_size_eval': 4096, 'num_workers': 4, 'shuffle': True, 'pin_memory': True, 'filter': True, 'max_length_val': 1024, 'max_length_test': 1024, 'pad_max_length': None, 'rc_aug': True, 'd_output': 1, 'use_fixed_len_val': False, 'replace_N_token': False, 'pad_interval': False}, 'optimizer': {'_name_': 'adamw', 'lr': 8e-05, 'weight_decay': 0.1, 'betas': [0.9, 0.999]}, 'scheduler': {'_name_': 'cosine_warmup_timm', 't_in_epochs': False, 't_initial': 14310, 'lr_min': 8.000000000000001e-06, 'warmup_lr_init': 1e-06, 'warmup_t': 143.1}, 'callbacks': {'learning_rate_monitor': {'logging_interval': 'step'}, 'timer': {'step': True, 'inter_step': False, 'epoch': True, 'val': True, '_name_': 'timer'}, 'params': {'total': True, 'trainable': True, 'fixed': True, '_name_': 'params'}, 'model_checkpoint': {'monitor': 'val/mse', 'mode': 'min', 'save_top_k': -1, 'save_last': True, 'dirpath': 'checkpoints/', 'filename': '{epoch:02d}-val_loss={val/loss:.5f}', 'auto_insert_metric_name': False, 'verbose': True, 'every_n_epochs': 1, '_name_': 'model_checkpoint'}}, 'task': {'loss': 'mse', 'metrics': ['mse'], '_name_': 'regression'}, 'encoder': 'id', 'decoder': {'mode': 'pool', '_name_': 'sequence'}, 'model': {'d_model': 128, 'n_layer': 2, 'd_inner': 512, 'vocab_size': 20, 'resid_dropout': 0.0, 'embed_dropout': 0.1, 'fused_mlp': False, 'fused_dropout_add_ln': False, 'checkpoint_mixer': False, 'checkpoint_mlp': False, 'residual_in_fp32': True, 'pad_vocab_size_multiple': 1, 'layer': {'emb_dim': 5, 'filter_order': 64, 'short_filter_order': 3, 'l_max': 1026, 'modulate': True, 'w': 10, 'lr': 8e-05, 'wd': 0.0, 'lr_pos_emb': 0.0, '_name_': 'hyena'}, '_name_': 'dna_embedding'}}