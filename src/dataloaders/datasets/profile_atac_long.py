
import pandas as pd
import torch
from random import randrange, random, sample
import numpy as np
import sys
sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')
from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer
import pyBigWig
import pyfaidx


"""

Dataset for getting the profiles for an atac seq peak, based off of the data generated by the chrombpnet package
Requires saving out the peak data as a np array prior to becoming a dataset loader
Values are the ones that are default, based off of testing_cbpnet.ipynb

"""
chrom_info= {'chr1': [10000, 248946422],
 'chr2': [10000, 242183529],
 'chr3': [10000, 198235559],
 'chr4': [10000, 190204555],
 'chr5': [10000, 181478259],
 'chr6': [60000, 170745979],
 'chr7': [10000, 159335973],
 'chr8': [60000, 145078636],
 'chr9': [10000, 138334717],
 'chr10': [10000, 133787422],
 'chr11': [60000, 135076622],
 'chr12': [10000, 133265309],
 'chr13': [16000000, 114354328],
 'chr14': [16022637, 106883718],
 'chr15': [17000000, 101981189],
 'chr16': [10000, 90228345],
 'chr17': [60000, 83247441],
 'chr18': [10000, 80263285],
 'chr19': [60000, 58607616],
 'chr20': [60000, 64334167],
 'chr21': [5010000, 46699983],
 'chr22': [10510000, 50808468],
 'chrX': [10000, 156030895],
 'chrY': [2781479, 56887902],}


# helper functions

def exists(val):
    return val is not None

def coin_flip():
    return random() > 0.5

def split_integers(start, end, max_length):
    result = [i for i in range(start, end, max_length)]
    return np.array(result)[:-1]

'''
there's a few key differences between this and the profile_atac dataset
First of all, we are not using the pyfaidx or pybigwig packages, we are using numpy arrays
Second, we are much faster
Third, we don't do any of the random cropping, just do profile over the whole input and output, but use the jitter option to add some randomness
So even if it centers around a peak, there's many other peaks too!
Finally, we also don't center around the peak at all, we just sweep through the genome. to center around peak, look at the
profile_atac_long_old.py file
'''
class ProfileATACLong():
    def __init__(
        self,
        split,
        max_length,
        pad_max_length=None,
        tokenizer=None,
        tokenizer_name=None,
        add_eos=False,
        use_padding = True,
        # return_seq_indices=False,
        # shift_augs=None,
        rc_aug=False,
        jitter=0, #adds this much jitter to either side!
        return_augs=False,
        replace_N_token=False,  # replace N token with pad token
        pad_interval = False,  # options for different padding
        uppercase = True,
        d_output = None,
        output_len = None,
        input_len = None,
        single_cell_type = False,
        train_bias = False, #if we are training a bias model
        data_path = None, #the path for the numpy arrays of the data, should contain chroomosome and summit locations
        cts_bw_file = None, #the path to an npz file converted from the bigwig file
        one_hot = False, #if you want to return one_hot or set it to None
    ):
        
        self.max_length = max_length
        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length
        self.tokenizer_name = tokenizer_name
        self.tokenizer = tokenizer #defined the proper one in the data loader
        if self.tokenizer is None:
            self.tokenizer = CharacterTokenizer(
                characters=['A', 'C', 'G', 'T', 'N'],
                model_max_length=1026,  # add 2 since default adds eos/eos tokens, crop later
                add_special_tokens=False,
            )
        self.d_output = d_output
        self.return_augs = return_augs
        self.add_eos = add_eos
        self.replace_N_token = replace_N_token  
        self.pad_interval = pad_interval
        self.rc_aug = rc_aug
        self.uppercase = uppercase
        self.jitter = jitter
        if self.jitter > 10000:
            raise ValueError('Jitter should be less than 10,000, else you could try to access regions outside of the genome \
                this results in a smaller dataset than expected! and will lead to collate issues with the dataloader')
        self.train_bias = train_bias
        self.one_hot = one_hot
        # self.tokenizer_mapping = {'A': 7,'C': 8,'G': 9,'T': 10,'N': 11}
        # self.RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}

        
        #first load in the bigwig and genome fasta files
        if cts_bw_file is None:
            cts_bw_file = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.npz'
        # genome_fasta = '/data/leslie/sarthak/data/chrombpnet_test/hg38.fa'
        # genome_fasta_rc = '/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa'
        genome_np = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz'
        genome_np_rc = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_rc.npz'
        with np.load(genome_np) as data:
            self.genome = {key: np.array(data[key]) for key in data}
        with np.load(genome_np_rc) as data:
            self.genome_rc = {key: np.array(data[key]) for key in data}

        with np.load(cts_bw_file) as data:
            self.cts = {key: np.array(data[key]) for key in data}
        
        # self.cts_bw = pyBigWig.open(cts_bw_file)
        # self.genome = pyfaidx.Fasta(genome_fasta)
        # self.genome_rc = pyfaidx.Fasta(genome_fasta_rc)
        # self.cell_types = cell_types

        #we load in based on the split
        if split == 'val':
            split = 'valid' #chrombpnets way of calling the data for some reason???
        
        #what we do is load in the data for sweeps
        midpoints = []
        chroms = []
        for chrom in chrom_info.keys():
            start,end = chrom_info[chrom]
            splits = split_integers(start,end,self.max_length)
            midpoints.extend(splits + self.max_length//2)
            chroms.extend([chrom]*len(splits))
        dtype = [('chromosome', 'U10'), ('start', 'i4')]
        self.data = np.array(list(zip(chroms, midpoints)), dtype=dtype)
        
        
        
    def __len__(self):
        return self.data.shape[0]

    def replace_value(self, x, old_value, new_value):
        return torch.where(x == old_value, new_value, x)

    def __getitem__(self, idx):
        """Returns a sequence of specified len"""
        if idx < 0:
            idx = len(self) + idx # negative indexing
        
        #get the peak coordinates
        peak = self.data[idx]
        chrom = peak[0]
        center = int(peak[1])
        offset = self.max_length//2 + self.jitter
        cts = self.cts[chrom][center-offset:center+offset]
        # np.nan_to_num(self.cts_bw.values(chrom, center-offset, center+offset))
        #first we see if we do RC, if it is RC get it from the other file
        if self.rc_aug and coin_flip():
            # seq = [self.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]
            lenchrom = len(self.genome[chrom])
            seq = self.genome_rc[chrom][lenchrom-(center+offset):lenchrom-(center-offset)]
            cts = cts[::-1]
        else:
            seq = self.genome[chrom][center-offset:center+offset]
        
        
        
        
        if self.jitter:
            start = sample(range(self.jitter*2+1),1)[0] #because range excludes the last one
            # start = np.random.choice(range(self.jitter*2+1)) #because range excludes the last one
            seq = seq[start:start+self.max_length]
            cts = cts[start:start+self.max_length]
        
        if self.one_hot:
            raise NotImplementedError('Not implemented yet, we have it tokenized, how to go from tokenized to that?')
            one_hot_seq = dna_to_one_hot([seq])[0]
        else:
            one_hot_seq = torch.zeros(seq.shape, dtype=torch.float32) #I think there's an issue calling this None
            #and turn it to a float tensor
            
        # offset = self.max_length//2 + self.jitter
        #now we need to add our cell type tokens
        #we will add them to the left
        # seq = seq
        
        #just do a manual mapping
        # seq = [self.tokenizer_mapping.get(nuc, 11) for nuc in seq]
        #RC after tokenization
        

        # if self.tokenizer_name == 'char': #will stick with this for sure
        #     seq = self.tokenizer(seq,
        #         add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos
        #         padding="max_length",
        #         max_length=self.max_length,
        #         truncation=True,
        #     )
        #     seq = seq["input_ids"]  # get input_ids
            

        # elif self.tokenizer_name == 'bpe':
        #     seq = self.tokenizer(seq, 
        #         # add_special_tokens=False, 
        #         padding="max_length",
        #         max_length=self.pad_max_length,
        #         truncation=True,
        #     ) 
            # get input_ids
            # if self.add_eos:
            #     seq = seq["input_ids"][1:]  # remove the bos, keep the eos token
            # else:
            #     seq = seq["input_ids"][1:-1]  # remove both special tokens

        #now we simply return the sequence and targets
        #make sure we turn them to the proper torch tensors first
        # print(seq)
        counts = np.log(1+np.sum(cts))
        seq = torch.LongTensor(seq)
        # print(counts)
        counts = torch.FloatTensor([counts])
        cts = torch.FloatTensor(cts.copy().astype(np.int32))
        if self.one_hot:
            one_hot_seq = torch.FloatTensor(one_hot_seq.copy()) #is a numpy array so can do .copy()
        #now let's store it smartly, let's append counts to the end of cts
        # print(cts.shape, counts.shape)
        # combined = torch.cat((cts, counts))
        return (seq, one_hot_seq), (cts,counts) #coutns is literally just the sum of the cts + 1 then logged

def main():
    # test the dataset
    dataset = ProfileATACLong('train', 1000000, tokenizer_name = 'char', rc_aug = True, jitter = 100_000)
    print(len(dataset)) #this is correct, 1.1*num_peaks
    print(dataset.seqs.shape)
    print(dataset.cts.shape)
    print(dataset[0])
'''
Can run in the terminal using these commands
cd /data/leslie/sarthak/hyena/hyena-dna/
python
import src.dataloaders.datasets.profile_atac_long as profile_atac_long
dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 1_000)
out = dataset[0]
out[0] #the input data tokenized
'''

if __name__ == '__main__':
    main()