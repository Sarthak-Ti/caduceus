
import pandas as pd
import torch
from random import randrange, random, sample
import numpy as np
import sys
sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')
from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer
import pyBigWig
import pyfaidx


"""

Dataset for getting the profiles for an atac seq peak, based off of the data generated by the chrombpnet package
Requires saving out the peak data as a np array prior to becoming a dataset loader
Values are the ones that are default, based off of testing_cbpnet.ipynb

"""



# helper functions

def exists(val):
    return val is not None

def coin_flip():
    return random() > 0.5

def one_hot_to_dna(one_hot):
    """
    Converts a one-hot encoding into a list of DNA ("ACGT") sequences, where the
    position of 1s is ordered alphabetically by "ACGT". `one_hot` must be an
    N x L x 4 array of one-hot encodings. Returns a lits of N "ACGT" strings,
    each of length L, in the same order as the input array. The returned
    sequences will only consist of letters "A", "C", "G", "T", or "N" (all
    upper-case). Any encodings that are all 0s will be translated to "N".
    """
    bases = np.array(["A", "C", "G", "T", "N"])
    # Create N x L array of all 5s
    one_hot_inds = np.tile(one_hot.shape[2], one_hot.shape[:2])

    # Get indices of where the 1s are
    batch_inds, seq_inds, base_inds = np.where(one_hot)

    # In each of the locations in the N x L array, fill in the location of the 1
    one_hot_inds[batch_inds, seq_inds] = base_inds

    # Fetch the corresponding base for each position using indexing
    seq_array = bases[one_hot_inds]
    return ["".join(seq) for seq in seq_array]

def dna_to_one_hot(seqs):
    """
    Converts a list of DNA ("ACGT") sequences to one-hot encodings, where the
    position of 1s is ordered alphabetically by "ACGT". `seqs` must be a list
    of N strings, where every string is the same length L. Returns an N x L x 4
    NumPy array of one-hot encodings, in the same order as the input sequences.
    All bases will be converted to upper-case prior to performing the encoding.
    Any bases that are not "ACGT" will be given an encoding of all 0s.
    """
    seq_len = len(seqs[0])
    assert np.all(np.array([len(s) for s in seqs]) == seq_len)

    # Join all sequences together into one long string, all uppercase
    seq_concat = "".join(seqs).upper() + "ACGT"
    # Add one example of each base, so np.unique doesn't miss indices later

    one_hot_map = np.identity(5)[:, :-1].astype(np.int8)

    # Convert string into array of ASCII character codes;
    base_vals = np.frombuffer(bytearray(seq_concat, "utf8"), dtype=np.int8)

    # Anything that's not an A, C, G, or T gets assigned a higher code
    base_vals[~np.isin(base_vals, np.array([65, 67, 71, 84]))] = 85

    # Convert the codes into indices in [0, 4], in ascending order by code
    _, base_inds = np.unique(base_vals, return_inverse=True)

    # Get the one-hot encoding for those indices, and reshape back to separate
    return one_hot_map[base_inds[:-4]].reshape((len(seqs), seq_len, 4))

def take_per_row(A, indx, num_elem):
    """
    Matrix A, indx is a vector for each row which specifies 
    slice beginning for that row. Each has width num_elem.
    """

    all_indx = indx[:,None] + np.arange(num_elem)
    return A[np.arange(all_indx.shape[0])[:,None], all_indx]

def random_crop(seqs, labels, seq_crop_width, label_crop_width, coords):
    """
    Takes sequences and corresponding counts labels. They should have the same
    #examples. The widths would correspond to inputlen and outputlen respectively,
    and any additional flanking width for jittering which should be the same
    for seqs and labels. Each example is cropped starting at a random offset. 

    seq_crop_width - label_crop_width should be equal to seqs width - labels width,
    essentially implying they should have the same flanking width.
    """

    assert(seqs.shape[1]>=seq_crop_width)
    assert(labels.shape[1]>=label_crop_width)
    assert(seqs.shape[1] - seq_crop_width == labels.shape[1] - label_crop_width)

    max_start = seqs.shape[1] - seq_crop_width # This should be the same for both input and output

    starts = np.random.choice(range(max_start+1), size=seqs.shape[0], replace=True)

    new_coords = coords.copy()
    new_coords[:,1] = new_coords[:,1].astype(int) - (seqs.shape[1]//2) + starts

    return take_per_row(seqs, starts, seq_crop_width), take_per_row(labels, starts, label_crop_width), new_coords

def random_rev_comp(seqs, labels, coords, frac=0.5):
    """
    Data augmentation: applies reverse complement randomly to a fraction of 
    sequences and labels.

    Assumes seqs are arranged in ACGT. Then ::-1 gives TGCA which is revcomp.

    NOTE: Performs in-place modification.
    """
    
    pos_to_rc = np.random.choice(range(seqs.shape[0]), 
            size=int(seqs.shape[0]*frac),
            replace=False)

    seqs[pos_to_rc] = seqs[pos_to_rc, ::-1, ::-1]
    labels[pos_to_rc] = labels[pos_to_rc, ::-1]
    coords[pos_to_rc,2] =  "r"
	
    return seqs, labels, coords

def reverse_complement_one_hot(seq):
    """
    Reverse complements a one-hot encoded DNA sequence.
    
    Parameters:
        seq (np.ndarray): A one-hot encoded DNA sequence of shape (4, 1024).
    
    Returns:
        np.ndarray: The reverse complemented one-hot encoded DNA sequence.
    """
    # Reverse the sequence along the length axis (1024) and swap the first axis (4) for complement
    return seq[::-1, ::-1]
    

# def get_middle_N(seq, N):
#     """
#     Get the middle N bases from a sequence
#     """
#     start = len(seq)//2 - N//2
#     return seq[start:start+N]

# augmentations

string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}

def string_reverse_complement(seq):
    rev_comp = ''
    for base in seq[::-1]:
        if base in string_complement_map:
            rev_comp += string_complement_map[base]
        # if bp not complement map, use the same bp
        else:
            rev_comp += base
    return rev_comp





'''
there's a few key differences between this and the profile_atac dataset
First of all, we are not using the pyfaidx or pybigwig packages, we are using numpy arrays
Second, we are much faster
Third, we don't do any of the random cropping, just do profile over the whole input and output, but use the jitter option to add some randomness
So even if it centers around a peak, there's many other peaks too!
'''
class ProfileATACLong():
    def __init__(
        self,
        split,
        max_length,
        pad_max_length=None,
        tokenizer=None,
        tokenizer_name=None,
        add_eos=False,
        use_padding = True,
        # return_seq_indices=False,
        # shift_augs=None,
        rc_aug=False,
        jitter=0, #adds this much jitter to either side!
        return_augs=False,
        replace_N_token=False,  # replace N token with pad token
        pad_interval = False,  # options for different padding
        uppercase = True,
        d_output = None,
        output_len = None,
        input_len = None,
        single_cell_type = False,
        train_bias = False, #if we are training a bias model
        data_path = None, #the path for the numpy arrays of the data, should contain chroomosome and summit locations
        cts_bw_file = None, #the path to an npz file converted from the bigwig file
        one_hot = False, #if you want to return one_hot or set it to None
    ):
        
        self.max_length = max_length
        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length
        self.tokenizer_name = tokenizer_name
        self.tokenizer = tokenizer #defined the proper one in the data loader
        if self.tokenizer is None:
            self.tokenizer = CharacterTokenizer(
                characters=['A', 'C', 'G', 'T', 'N'],
                model_max_length=1026,  # add 2 since default adds eos/eos tokens, crop later
                add_special_tokens=False,
            )
        self.d_output = d_output
        self.return_augs = return_augs
        self.add_eos = add_eos
        self.replace_N_token = replace_N_token  
        self.pad_interval = pad_interval
        self.rc_aug = rc_aug
        self.uppercase = uppercase
        self.jitter = jitter
        self.train_bias = train_bias
        self.one_hot = one_hot
        # self.tokenizer_mapping = {'A': 7,'C': 8,'G': 9,'T': 10,'N': 11}
        # self.RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}

        
        #first load in the bigwig and genome fasta files
        if cts_bw_file is None:
            cts_bw_file = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.npz'
        # genome_fasta = '/data/leslie/sarthak/data/chrombpnet_test/hg38.fa'
        # genome_fasta_rc = '/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa'
        genome_np = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz'
        genome_np_rc = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_rc.npz'
        with np.load(genome_np) as data:
            self.genome = {key: np.array(data[key]) for key in data}
        with np.load(genome_np_rc) as data:
            self.genome_rc = {key: np.array(data[key]) for key in data}

        with np.load(cts_bw_file) as data:
            self.cts = {key: np.array(data[key]) for key in data}
        
        # self.cts_bw = pyBigWig.open(cts_bw_file)
        # self.genome = pyfaidx.Fasta(genome_fasta)
        # self.genome_rc = pyfaidx.Fasta(genome_fasta_rc)
        # self.cell_types = cell_types

        #we load in based on the split
        if split == 'val':
            split = 'valid' #chrombpnets way of calling the data for some reason???
        if data_path is None:
            data_path = f'/data/leslie/sarthak/data/chrombpnet_test/saved_data_1000/{split}/'
        else:
            data_path = data_path+f'{split}/'
        #what we do is load in the numpy arrays
        self.peak_coords = np.load(data_path+'peak_coords.npy')
        #now we need to filter out the peaks that are too close to the edge
        remove_array=np.ones(len(self.peak_coords))
        for i,row in enumerate(self.peak_coords):
            start = int(row[1])
            if start + max_length+jitter > len(self.genome[row[0]]) or start < max_length+jitter:
                remove_array[i] = 0
        self.peak_coords = self.peak_coords[remove_array==1]
        #now we gather sequences based on the length
        #do almost all of it in the get item function in case we do like 1 million bp       
        #TODO
        #add a check to ensure that the coordinates are not too close to the edge of the chromosome based on max length
        
        
        
    def __len__(self):
        return self.peak_coords.shape[0]

    def replace_value(self, x, old_value, new_value):
        return torch.where(x == old_value, new_value, x)

    def __getitem__(self, idx):
        """Returns a sequence of specified len"""
        if idx < 0:
            idx = len(self) + idx # negative indexing
        
        #get the peak coordinates
        peak = self.peak_coords[idx]
        chrom = peak[0]
        center = int(peak[1])
        offset = self.max_length//2 + self.jitter
        cts = self.cts[chrom][center-offset:center+offset]
        # np.nan_to_num(self.cts_bw.values(chrom, center-offset, center+offset))
        #first we see if we do RC, if it is RC get it from the other file
        if self.rc_aug and coin_flip():
            # seq = [self.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]
            lenchrom = len(self.genome[chrom])
            seq = self.genome_rc[chrom][lenchrom-(center+offset):lenchrom-(center-offset)]
            cts = cts[::-1]
        else:
            seq = self.genome[chrom][center-offset:center+offset]
        
        
        
        
        if self.jitter:
            start = sample(range(self.jitter*2+1),1)[0] #because range excludes the last one
            # start = np.random.choice(range(self.jitter*2+1)) #because range excludes the last one
            seq = seq[start:start+self.max_length]
            cts = cts[start:start+self.max_length]
        
        if self.one_hot:
            raise NotImplementedError('Not implemented yet, we have it tokenized, how to go from tokenized to that?')
            one_hot_seq = dna_to_one_hot([seq])[0]
        else:
            one_hot_seq = torch.zeros(seq.shape, dtype=torch.float32) #I think there's an issue calling this None
            #and turn it to a float tensor
            
        # offset = self.max_length//2 + self.jitter
        #now we need to add our cell type tokens
        #we will add them to the left
        # seq = seq
        
        #just do a manual mapping
        # seq = [self.tokenizer_mapping.get(nuc, 11) for nuc in seq]
        #RC after tokenization
        

        # if self.tokenizer_name == 'char': #will stick with this for sure
        #     seq = self.tokenizer(seq,
        #         add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos
        #         padding="max_length",
        #         max_length=self.max_length,
        #         truncation=True,
        #     )
        #     seq = seq["input_ids"]  # get input_ids
            

        # elif self.tokenizer_name == 'bpe':
        #     seq = self.tokenizer(seq, 
        #         # add_special_tokens=False, 
        #         padding="max_length",
        #         max_length=self.pad_max_length,
        #         truncation=True,
        #     ) 
            # get input_ids
            # if self.add_eos:
            #     seq = seq["input_ids"][1:]  # remove the bos, keep the eos token
            # else:
            #     seq = seq["input_ids"][1:-1]  # remove both special tokens

        #now we simply return the sequence and targets
        #make sure we turn them to the proper torch tensors first
        # print(seq)
        counts = np.log(1+np.sum(cts))
        seq = torch.LongTensor(seq)
        # print(counts)
        counts = torch.FloatTensor([counts])
        cts = torch.FloatTensor(cts.copy().astype(np.int32))
        if self.one_hot:
            one_hot_seq = torch.FloatTensor(one_hot_seq.copy()) #is a numpy array so can do .copy()
        #now let's store it smartly, let's append counts to the end of cts
        # print(cts.shape, counts.shape)
        # combined = torch.cat((cts, counts))
        return (seq, one_hot_seq), (cts,counts) #coutns is literally just the sum of the cts + 1 then logged

def main():
    # test the dataset
    dataset = ProfileATACLong('train', 1000000, tokenizer_name = 'char', rc_aug = True, jitter = 100_000)
    print(len(dataset)) #this is correct, 1.1*num_peaks
    print(dataset.seqs.shape)
    print(dataset.cts.shape)
    print(dataset[0])
'''
Can run in the terminal using these commands
cd /data/leslie/sarthak/hyena/hyena-dna/
python
import src.dataloaders.datasets.profile_atac_long as profile_atac_long
dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 10_000)
out = dataset[0]
out[0] #the input data tokenized
'''

if __name__ == '__main__':
    main()