
import pandas as pd
import torch
from random import randrange, random
import numpy as np
import sys
sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')
from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer


"""

Dataset for getting the profiles for an atac seq peak, based off of the data generated by the chrombpnet package
Requires saving out the peak data as a np array prior to becoming a dataset loader
Values are the ones that are default, based off of testing_cbpnet.ipynb

"""



# helper functions

def exists(val):
    return val is not None

def coin_flip():
    return random() > 0.5

def one_hot_to_dna(one_hot):
    """
    Converts a one-hot encoding into a list of DNA ("ACGT") sequences, where the
    position of 1s is ordered alphabetically by "ACGT". `one_hot` must be an
    N x L x 4 array of one-hot encodings. Returns a lits of N "ACGT" strings,
    each of length L, in the same order as the input array. The returned
    sequences will only consist of letters "A", "C", "G", "T", or "N" (all
    upper-case). Any encodings that are all 0s will be translated to "N".
    """
    bases = np.array(["A", "C", "G", "T", "N"])
    # Create N x L array of all 5s
    one_hot_inds = np.tile(one_hot.shape[2], one_hot.shape[:2])

    # Get indices of where the 1s are
    batch_inds, seq_inds, base_inds = np.where(one_hot)

    # In each of the locations in the N x L array, fill in the location of the 1
    one_hot_inds[batch_inds, seq_inds] = base_inds

    # Fetch the corresponding base for each position using indexing
    seq_array = bases[one_hot_inds]
    return ["".join(seq) for seq in seq_array]

def take_per_row(A, indx, num_elem):
    """
    Matrix A, indx is a vector for each row which specifies 
    slice beginning for that row. Each has width num_elem.
    """

    all_indx = indx[:,None] + np.arange(num_elem)
    return A[np.arange(all_indx.shape[0])[:,None], all_indx]

def random_crop(seqs, labels, seq_crop_width, label_crop_width, coords):
    """
    Takes sequences and corresponding counts labels. They should have the same
    #examples. The widths would correspond to inputlen and outputlen respectively,
    and any additional flanking width for jittering which should be the same
    for seqs and labels. Each example is cropped starting at a random offset. 

    seq_crop_width - label_crop_width should be equal to seqs width - labels width,
    essentially implying they should have the same flanking width.
    """

    assert(seqs.shape[1]>=seq_crop_width)
    assert(labels.shape[1]>=label_crop_width)
    assert(seqs.shape[1] - seq_crop_width == labels.shape[1] - label_crop_width)

    max_start = seqs.shape[1] - seq_crop_width # This should be the same for both input and output

    starts = np.random.choice(range(max_start+1), size=seqs.shape[0], replace=True)

    new_coords = coords.copy()
    new_coords[:,1] = new_coords[:,1].astype(int) - (seqs.shape[1]//2) + starts

    return take_per_row(seqs, starts, seq_crop_width), take_per_row(labels, starts, label_crop_width), new_coords

def random_rev_comp(seqs, labels, coords, frac=0.5):
    """
    Data augmentation: applies reverse complement randomly to a fraction of 
    sequences and labels.

    Assumes seqs are arranged in ACGT. Then ::-1 gives TGCA which is revcomp.

    NOTE: Performs in-place modification.
    """
    
    pos_to_rc = np.random.choice(range(seqs.shape[0]), 
            size=int(seqs.shape[0]*frac),
            replace=False)

    seqs[pos_to_rc] = seqs[pos_to_rc, ::-1, ::-1]
    labels[pos_to_rc] = labels[pos_to_rc, ::-1]
    coords[pos_to_rc,2] =  "r"
	
    return seqs, labels, coords

def reverse_complement_one_hot(seq):
    """
    Reverse complements a one-hot encoded DNA sequence.
    
    Parameters:
        seq (np.ndarray): A one-hot encoded DNA sequence of shape (4, 1024).
    
    Returns:
        np.ndarray: The reverse complemented one-hot encoded DNA sequence.
    """
    # Reverse the sequence along the length axis (1024) and swap the first axis (4) for complement
    return seq[::-1, ::-1]
    

# def get_middle_N(seq, N):
#     """
#     Get the middle N bases from a sequence
#     """
#     start = len(seq)//2 - N//2
#     return seq[start:start+N]

# augmentations

string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}

def string_reverse_complement(seq):
    rev_comp = ''
    for base in seq[::-1]:
        if base in string_complement_map:
            rev_comp += string_complement_map[base]
        # if bp not complement map, use the same bp
        else:
            rev_comp += base
    return rev_comp



class ProfileATAC(): #when we have unique cell type specific tokens
    def __init__(
        self,
        split,
        max_length,
        pad_max_length=None,
        tokenizer=None,
        tokenizer_name=None,
        add_eos=False,
        use_padding = True,
        # return_seq_indices=False,
        # shift_augs=None,
        rc_aug=False,
        jitter=False,
        return_augs=False,
        replace_N_token=False,  # replace N token with pad token
        pad_interval = False,  # options for different padding
        uppercase = True,
        d_output = None,
        output_len = None,
        input_len = None,
        single_cell_type = False,
    ):
        
        self.max_length = max_length
        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length
        self.tokenizer_name = tokenizer_name
        self.tokenizer = tokenizer #defined the proper one in the data loader
        if self.tokenizer is None:
            self.tokenizer = CharacterTokenizer(
                characters=['A', 'C', 'G', 'T', 'N'],
                model_max_length=1026,  # add 2 since default adds eos/eos tokens, crop later
                add_special_tokens=False,
            )
        self.d_output = d_output
        self.return_augs = return_augs
        self.add_eos = add_eos
        self.replace_N_token = replace_N_token  
        self.pad_interval = pad_interval
        self.rc_aug = rc_aug
        self.uppercase = uppercase
        self.jitter = jitter
        # self.cell_types = cell_types

        #we load in based on the split
        if split == 'val':
            split = 'valid' #chrombpnets way of calling the data for some reason???
        data_path = f'/data/leslie/sarthak/data/chrombpnet_test/saved_data_1000/{split}/'
        #what we do is load in the numpy arrays
        self.peak_seqs = np.load(data_path+'peak_seqs.npy')
        self.peak_cts = np.load(data_path+'peak_cts.npy')
        self.peak_coords = np.load(data_path+'peak_coords.npy')

        if split == 'train' or split == 'valid':
            self.nonpeak_seqs = np.load(data_path+'nonpeak_seqs.npy') #nonpeaks only true for train and valid?
            self.nonpeak_cts = np.load(data_path+'nonpeak_cts.npy')
            self.nonpeak_coords = np.load(data_path+'nonpeak_coords.npy')

            #now we subsample the nonpeaks. Probably done by chrombpnet for val but doesn't really hurt to check
            num_nonpeak_samples = int(0.1 * self.peak_seqs.shape[0]) #now a 1:10 ratio
            nonpeak_indices_to_keep = np.random.choice(len(self.nonpeak_seqs), size=num_nonpeak_samples, replace=False)
            self.nonpeak_seqs = self.nonpeak_seqs[nonpeak_indices_to_keep]
            self.nonpeak_cts = self.nonpeak_cts[nonpeak_indices_to_keep]
            self.nonpeak_coords = self.nonpeak_coords[nonpeak_indices_to_keep]
            # print(self.nonpeak_seqs.shape)
            #and now we create a joint seqs, cts and coords
            #now we get the middle center 1524 of the peaks data and random crop it, but only if it's training
            middle_seqs = self.peak_seqs.shape[1]//2
            self.peak_seqs = self.peak_seqs[:,middle_seqs-762:middle_seqs+762,:]
            #for the cts don't want the middle 1524 but subtract 24 from it because then we can predict the middle 1000
            middle_cts = self.peak_cts.shape[1]//2
            self.peak_cts = self.peak_cts[:,middle_cts-650:middle_cts+650]
            #now random crop
            self.peak_seqs, self.peak_cts, self.peak_coords = random_crop(self.peak_seqs, self.peak_cts, self.max_length, 800, self.peak_coords)

            #and get the middle 1024 of the nonpeaks data
            middle_seqs = self.nonpeak_seqs.shape[1]//2
            self.nonpeak_seqs = self.nonpeak_seqs[:,middle_seqs-512:middle_seqs+512,:]
            middle_cts = self.nonpeak_cts.shape[1]//2
            self.nonpeak_cts = self.nonpeak_cts[:,middle_cts-400:middle_cts+400] #this gives th emiddle 800
        elif split == 'test':
            self.nonpeak_seqs = np.empty((0,) + self.peak_seqs.shape[1:], dtype=self.peak_seqs.dtype)
            self.nonpeak_cts = np.empty((0,) + self.peak_cts.shape[1:], dtype=self.peak_cts.dtype)
            self.nonpeak_coords = np.empty((0,) + self.peak_coords.shape[1:], dtype=self.peak_coords.dtype)
        # print(self.nonpeak_cts.shape)
        # print(middle_cts)
        # self.nonpeak_cts = self.nonpeak_cts[:,middle_cts-511:middle_cts+512]
        # print(self.nonpeak_cts.shape)
        

        self.seqs = np.concatenate([self.peak_seqs, self.nonpeak_seqs], axis=0)
        self.cts = np.concatenate([self.peak_cts, self.nonpeak_cts], axis=0)
        self.coords = np.concatenate([self.peak_coords, self.nonpeak_coords], axis=0)
        #now we do the random croppint of our data
        #first let's get the center 1524 of our data using indexing
        # middle_seqs = self.seqs.shape[1]//2
        # self.seqs = self.seqs[:,middle_seqs-762:middle_seqs+762,:]
        # middle_cts = self.cts.shape[1]//2
        # self.cts = self.cts[:,middle_cts-762:middle_cts+762]
        #now we do the random cropping
        # self.seqs, self.cts, self.coords = random_crop(self.seqs, self.cts, self.max_length-1, self.max_length-1, self.coords)
        self.counts = np.log(1+self.cts.sum(-1, keepdims=True))
        #and we are done!
        #finally let's just unonehot the sequences
        self.one_hot_seq = self.seqs
        self.seqs = np.array(one_hot_to_dna(self.seqs))
        
        
        
    def __len__(self):
        return self.seqs.shape[0]

    def replace_value(self, x, old_value, new_value):
        return torch.where(x == old_value, new_value, x)

    def __getitem__(self, idx):
        """Returns a sequence of specified len"""
        if idx < 0:
            idx = len(self) + idx # negative indexing
        
        seq = self.seqs[idx]
        one_hot_seq = self.one_hot_seq[idx]
        cts = self.cts[idx]
        
        if self.rc_aug and coin_flip():
            seq = string_reverse_complement(seq)
            one_hot_seq = reverse_complement_one_hot(one_hot_seq)
            cts = cts[::-1]
        #let the tokenizer pad things
        
        #now we need to add our cell type tokens
        #we will add them to the left
        # seq = seq

        if self.tokenizer_name == 'char': #will stick with this for sure
            seq = self.tokenizer(seq,
                add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos
                padding="max_length",
                max_length=self.max_length,
                truncation=True,
            )
            seq = seq["input_ids"]  # get input_ids
            

        elif self.tokenizer_name == 'bpe':
            seq = self.tokenizer(seq, 
                # add_special_tokens=False, 
                padding="max_length",
                max_length=self.pad_max_length,
                truncation=True,
            ) 
            # get input_ids
            if self.add_eos:
                seq = seq["input_ids"][1:]  # remove the bos, keep the eos token
            else:
                seq = seq["input_ids"][1:-1]  # remove both special tokens

        #now we simply return the sequence and targets
        #make sure we turn them to the proper torch tensors first
        seq = torch.LongTensor(seq.copy())
        counts = torch.FloatTensor(self.counts[idx])
        cts = torch.FloatTensor(cts.copy())
        one_hot_seq = torch.FloatTensor(one_hot_seq.transpose(1,0).copy())
        return (seq, one_hot_seq), (cts, counts) #coutns is literally just the sum of the cts + 1 then logged

def main():
    # test the dataset
    dataset = ProfileATAC('train', 1024, tokenizer_name = 'char', rc_aug = True, jitter = True)
    print(len(dataset)) #this is correct, 1.1*num_peaks
    print(dataset.seqs.shape)
    print(dataset.cts.shape)
    print(dataset[0])
    '''
    Can run in the terminal using these commands
    cd /data/leslie/sarthak/hyena/hyena-dna/
    python
    import src.dataloaders.datasets.profile_atac as profile_atac
    dataset = profile_atac.ProfileATAC('train', 1024, tokenizer_name = 'char', rc_aug = True, jitter = True)
    out = dataset[0]
    out[0] #the input data tokenized
    '''

if __name__ == '__main__':
    main()