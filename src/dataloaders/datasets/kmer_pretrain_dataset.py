
import torch
from random import randrange, random, sample
import numpy as np
import sys
sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')
import json
from src.dataloaders.utils.mlm import mlm_getitem
from caduceus.tokenization_caduceus_custom import CaduceusTokenizer

"""

Dataset for getting the profiles for an atac seq peak, based off of the data generated by the chrombpnet package
Requires saving out the peak data as a np array prior to becoming a dataset loader
Values are the ones that are default, based off of testing_cbpnet.ipynb

"""
chrom_info= {'chr1': [10000, 248946422],
 'chr2': [10000, 242183529],
 'chr3': [10000, 198235559],
 'chr4': [10000, 190204555],
 'chr5': [10000, 181478259],
 'chr6': [60000, 170745979],
 'chr7': [10000, 159335973],
 'chr8': [60000, 145078636],
 'chr9': [10000, 138334717],
 'chr10': [10000, 133787422],
 'chr11': [60000, 135076622],
 'chr12': [10000, 133265309],
 'chr13': [16000000, 114354328],
 'chr14': [16022637, 106883718],
 'chr15': [17000000, 101981189],
 'chr16': [10000, 90228345],
 'chr17': [60000, 83247441],
 'chr18': [10000, 80263285],
 'chr19': [60000, 58607616],
 'chr20': [60000, 64334167],
 'chr21': [5010000, 46699983],
 'chr22': [10510000, 50808468],
 'chrX': [10000, 156030895],
 'chrY': [2781479, 56887902],}

trainsplits = {'train':['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr9', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY'],
          'val':['chr10', 'chr8'],
          'valid': ['chr10', 'chr8'],
          'test':['chr11', 'chr12']}

def exists(val):
    return val is not None

def coin_flip():
    return random() > 0.5

def split_integers(start, end, max_length):
    result = [i for i in range(start, end, max_length)]
    return np.array(result)[:-1]

'''
Class that sweeps through the genome by chromosome and returns the sequence for pretraining
'''
class KmerPretrain():
    def __init__(
        self,
        split,
        max_length,
        pad_max_length=None,
        tokenizer=None,
        tokenizer_name=None,
        rc_aug=False,
        d_output = None,
        kmer_len = 6,
        clean_data = True, #should really be true...
        mlm_probability=0.15,
        eligible_replacements=None,
    ):
        
        self.max_length = max_length
        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length
        self.tokenizer_name = tokenizer_name
        self.tokenizer = tokenizer #defined the proper one in the data loader
        self.d_output = d_output
        self.rc_aug = rc_aug
        self.mlm_probability = mlm_probability
        if tokenizer is None:
            self.tokenizer = CaduceusTokenizer(max_length, kmer_len=kmer_len)
        self.eligible_replacements = eligible_replacements

        #first load in the bigwig and genome fasta files
        genome_np = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz'
        if kmer_len is not None:
            genome_np = f'/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_kmer_{kmer_len}.npz'
            print(f'Using kmer genome with length {kmer_len}')
        with np.load(genome_np) as data:
            self.genome = {key: np.array(data[key]) for key in data}
        
        #RC will be implemented by loading the json file and then mapping every element to the reverse complement and reversing order
        with open(f'/data/leslie/sarthak/data/enformer/data/complement_map_{kmer_len}mer.json', 'r') as f:
            complement_map = json.load(f)
        max_key = int(list(complement_map.keys())[-1])
        self.complement_array = np.zeros(max_key + 1, dtype=int)
        for k, v in complement_map.items():
            self.complement_array[int(k)] = v
        
        #we load in based on the split
        if split == 'val':
            split = 'valid' #chrombpnets way of calling the data for some reason???
        
        #what we do is load in the data for sweeps
        midpoints = []
        chroms = []
        for chrom in chrom_info.keys():
            if chrom not in trainsplits[split]:
                continue
            start,end = chrom_info[chrom]
            splits = split_integers(start,end,self.max_length)
            midpoints.extend(splits + self.max_length//2)
            chroms.extend([chrom]*len(splits))
        dtype = [('chromosome', 'U10'), ('midpoints', 'i4')]
        data = np.array(list(zip(chroms, midpoints)), dtype=dtype)
        
        if clean_data:
            self.data = self.clean_data(data)
        else:
            self.data = data
        
    def clean_data(self, data):
        #it removes peaks that have too many Ns (usually centromeres)
        # data = self.all_data
        good_data = []
        self.num_bad = 0
        for i in range(data.shape[0]):
            peak = data[i]
            chrom = peak[0]
            center = int(peak[1])
            seq = self.genome[chrom][center-self.max_length//2:center+self.max_length//2] 
            #find how many 11 are in there
            n_count = np.sum(seq == 15624) #the N token equivalent
            if n_count > 1000:
                # print(f'Chrom {chrom} has {n_count} Ns in {i}')
                self.num_bad += 1
            else:
                good_data.append(i)
        return data[np.array(good_data)] #remove the ones that have more than 1000 Ns
        
    def __len__(self):
        return self.data.shape[0]

    def replace_value(self, x, old_value, new_value):
        return torch.where(x == old_value, new_value, x)

    def __getitem__(self, idx):
        """Returns a sequence of specified len"""
        if idx < 0:
            idx = len(self) + idx # negative indexing
        
        #get the peak coordinates
        peak = self.data[idx]
        chrom = peak[0]
        center = int(peak[1])
        seq = self.genome[chrom][center-self.max_length//2:center+self.max_length//2] #should be nonoverlapping and contain all the data
        if self.rc_aug and coin_flip():
            raise ValueError('RC augmentation is not to be used, this uses rcps')
            # seq = seq[::-1]
            # seq = np.array([self.complement_map[str(x)] for x in seq])
            seq = self.complement_array[seq[::-1]]
        
        seq = torch.LongTensor(seq)
        seq = self.replace_value(seq, self.tokenizer.max_char_len, self.tokenizer.pad_token_id)
        
        # seq = torch.LongTensor(np.concatenate(([self.BOS], seq, [self.EOS])))
        data, target = mlm_getitem(
                seq,
                mlm_probability=self.mlm_probability,
                contains_eos=False,
                tokenizer=self.tokenizer,
                eligible_replacements=self.eligible_replacements,
            )
        return data, target

        # return seq[:-1].clone(), seq[1:].clone() #this matches predicting the next nucleotide

def main():
    # test the dataset
    dataset = KmerPretrain('train', 196608, rc_aug = True)
    print(len(dataset)) #this is correct, 1.1*num_peaks
    print(dataset.seqs.shape)
    print(dataset[0])
'''
Can run in the terminal using these commands
cd /data/leslie/sarthak/caduceus/
python
import src.dataloaders.datasets.kmer_pretrain_dataset as kmer_pretrain_dataset
dataset = kmer_pretrain_dataset.KmerPretrain('train', 196608)
out = dataset[0]
out[0] #the input data tokenized
'''

if __name__ == '__main__':
    main()