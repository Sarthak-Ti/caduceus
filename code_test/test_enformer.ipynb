{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 15:34:12.261916: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-02 15:34:12.413880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-02 15:34:12.414501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-02 15:34:12.415063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:83:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-02 15:34:12.415634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:84:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-02 15:34:12.415684: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-02 15:34:12.424612: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-02 15:34:12.424673: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-02 15:34:12.477509: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-02 15:34:12.478165: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-02 15:34:12.478998: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-02 15:34:12.480482: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-02 15:34:12.480664: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-02 15:34:12.480701: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-02 15:34:12.483742: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-02 15:34:12.494217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-02 15:34:12.494237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'enformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Numpy array [batch_size, SEQ_LENGTH, 4] one hot encoded in order 'ACGT'. The\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# `one_hot_encode` function is available in `enformer.py` and outputs can be\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# stacked to form a batch.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, SEQ_LENGTH, \u001b[38;5;241m4\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43menformer\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_on_batch(inputs)\n\u001b[1;32m     15\u001b[0m predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enformer' is not defined"
     ]
    }
   ],
   "source": [
    "#we have the environment, let's test to see if enformer is here\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "enformer_model = hub.load(\"https://kaggle.com/models/deepmind/enformer/frameworks/TensorFlow2/variations/enformer/versions/1\").model\n",
    "\n",
    "SEQ_LENGTH = 393_216\n",
    "\n",
    "# Numpy array [batch_size, SEQ_LENGTH, 4] one hot encoded in order 'ACGT'. The\n",
    "# `one_hot_encode` function is available in `enformer.py` and outputs can be\n",
    "# stacked to form a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x7f077819fc10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 15:34:42.594829: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-02 15:34:42.766977: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299875000 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 896, 5313])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.zeros((1, SEQ_LENGTH, 4), dtype=tf.float32)\n",
    "predictions = enformer_model.predict_on_batch(inputs)\n",
    "predictions['human'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predictions['human'][0,:,27] #for the hepg2 cell line, not real data tho, all N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([896])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject at 0x7f077819fc10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check out the attributes\n",
    "enformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_checkpoint_dependencies',\n",
       " '_crop_final',\n",
       " '_crop_input',\n",
       " '_deferred_dependencies',\n",
       " '_delete_tracking',\n",
       " '_downres',\n",
       " '_final_pointwise',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_handle_deferred_dependencies',\n",
       " '_heads',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_map_resources',\n",
       " '_maybe_initialize_trackable',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_no_dependency',\n",
       " '_object_identifier',\n",
       " '_preload_simple_restoration',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_self_name_based_restores',\n",
       " '_self_saveable_object_factories',\n",
       " '_self_setattr_tracking',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_update_uid',\n",
       " '_setattr_tracking',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_stem',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_track_trackable',\n",
       " '_tracking_metadata',\n",
       " '_transformer',\n",
       " '_trunk',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " 'predict_on_batch']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(enformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__name__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__original_wrapped__',\n",
       " '__qualname__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '__wrapped__',\n",
       " '_autograph',\n",
       " '_call',\n",
       " '_clone',\n",
       " '_concrete_stateful_fn',\n",
       " '_create_implements_attribute',\n",
       " '_created_variables',\n",
       " '_decorate',\n",
       " '_defun',\n",
       " '_defun_with_scope',\n",
       " '_descriptor_cache',\n",
       " '_experimental_autograph_options',\n",
       " '_experimental_follow_type_hints',\n",
       " '_experimental_relax_shapes',\n",
       " '_function_spec',\n",
       " '_get_concrete_function_garbage_collected',\n",
       " '_get_key_for_call_stats',\n",
       " '_get_tracing_count',\n",
       " '_graph_deleter',\n",
       " '_implements',\n",
       " '_initialize',\n",
       " '_initialize_uninitialized_variables',\n",
       " '_input_signature',\n",
       " '_jit_compile',\n",
       " '_key_for_call_stats',\n",
       " '_lifted_initializer_graph',\n",
       " '_list_all_concrete_functions',\n",
       " '_list_all_concrete_functions_for_serialization',\n",
       " '_lock',\n",
       " '_name',\n",
       " '_omit_frequent_tracing_warning',\n",
       " '_python_function',\n",
       " '_run_functions_eagerly',\n",
       " '_self_saveable_object_factories',\n",
       " '_shared_rendezvous',\n",
       " '_stateful_fn',\n",
       " '_stateless_fn',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_tf_decorator',\n",
       " 'concrete_functions',\n",
       " 'experimental_get_compiler_ir',\n",
       " 'experimental_get_tracing_count',\n",
       " 'function_spec',\n",
       " 'get_concrete_function',\n",
       " 'get_initialization_function',\n",
       " 'input_signature',\n",
       " 'pretty_printed_concrete_signatures',\n",
       " 'python_function']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(enformer_model.predict_on_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eh it's fine, the point is that we can use it to predict just fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next step is to convert the data, we have all these TFR files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 18:57:02.467801: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def organism_path(organism):\n",
    "  return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism)\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8):\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genome</th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>clip</th>\n",
       "      <th>scale</th>\n",
       "      <th>sum_stat</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF833POA</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:cerebellum male adult (27 years) and mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF110QGM</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:frontal cortex male adult (27 years) and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF880MKD</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:chorion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF463ZLQ</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:Ishikawa treated with 0.02% dimethyl sul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF890OGQ</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:GM03348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  genome   identifier  \\\n",
       "0      0       0  ENCFF833POA   \n",
       "1      1       0  ENCFF110QGM   \n",
       "2      2       0  ENCFF880MKD   \n",
       "3      3       0  ENCFF463ZLQ   \n",
       "4      4       0  ENCFF890OGQ   \n",
       "\n",
       "                                                file  clip  scale sum_stat  \\\n",
       "0  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "1  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "2  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "3  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "4  /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "\n",
       "                                         description  \n",
       "0  DNASE:cerebellum male adult (27 years) and mal...  \n",
       "1  DNASE:frontal cortex male adult (27 years) and...  \n",
       "2                                      DNASE:chorion  \n",
       "3  DNASE:Ishikawa treated with 0.02% dimethyl sul...  \n",
       "4                                      DNASE:GM03348  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_targets_human = get_targets('human')\n",
    "df_targets_human.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 18:58:16.077749: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-03 18:58:16.171477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-03 18:58:16.171965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-03 18:58:16.172536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:83:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-03 18:58:16.173026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:84:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-03 18:58:16.173085: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-03 18:58:16.180497: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-03 18:58:16.180543: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-03 18:58:16.232275: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-03 18:58:16.232904: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-03 18:58:16.233723: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-03 18:58:16.234990: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-03 18:58:16.235186: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-03 18:58:16.235222: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-03 18:58:16.257277: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-03 18:58:16.263782: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-03 18:58:16.263800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    }
   ],
   "source": [
    "human_dataset = get_dataset('human', 'train').batch(1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: {sequence: (None, 131072, 4), target: (None, 896, 5313)}, types: {sequence: tf.float32, target: tf.float32}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-03 19:00:02.388551: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-03 19:00:02.390178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2299880000 Hz\n"
     ]
    }
   ],
   "source": [
    "it = iter(human_dataset)\n",
    "example = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
       " array([[[0., 0., 0., 1.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 1.]]], dtype=float32)>,\n",
       " 'target': <tf.Tensor: shape=(1, 896, 5313), dtype=float32, numpy=\n",
       " array([[[0.10839844, 0.10760498, 0.04425049, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.10162354, 0.09332275, 0.0094986 , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.10272217, 0.15600586, 0.        , ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.07714844, 0.07678223, 0.03509521, ..., 0.        ,\n",
       "          0.01934814, 0.        ],\n",
       "         [0.07666016, 0.03826904, 0.05648804, ..., 0.        ,\n",
       "          0.        , 0.        ],\n",
       "         [0.08319092, 0.06051636, 0.02156067, ..., 0.        ,\n",
       "          0.        , 0.        ]]], dtype=float32)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['sequence'] #it's the tf tensor, so we have officially accessed one of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(1, 896, 5313), dtype=float32, numpy=\n",
      "array([[[0.07519531, 0.01112366, 0.01609802, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.0881958 , 0.01392365, 0.03829956, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.05108643, 0.01622009, 0.01737976, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.00142574, 0.001544  , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.00675964, 0.        , 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.05609131, 0.0559082 , 0.06762695, ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "example2 = next(it)\n",
    "print(example2) #wait this is the next example, still part of the same TFR file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5284736\n"
     ]
    }
   ],
   "source": [
    "#the next question is whether this gives us a single TFR file or goes through all of them?\n",
    "print(896*5313+131072*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021138944"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_array=np.zeros((896*5313+131072*4)).astype(np.float32)\n",
    "#see the size in GB\n",
    "temp_array.nbytes/1e9\n",
    "#yeah so the storage is the fact that it's many batches!\n",
    "#we have to figure out what the function is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: {sequence: (None, 131072, 4), target: (None, 896, 5313)}, types: {sequence: tf.float32, target: tf.float32}>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dataset length is infinite.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(human_dataset)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhuman_dataset\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:516\u001b[0m, in \u001b[0;36mDatasetV2.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcardinality()\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m INFINITE:\n\u001b[0;32m--> 516\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is infinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m UNKNOWN:\n\u001b[1;32m    518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is unknown.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: dataset length is infinite."
     ]
    }
   ],
   "source": [
    "print(human_dataset)\n",
    "print(len(human_dataset)) #when it was defined with .repeat() it's infinite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dataset length is unknown.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:518\u001b[0m, in \u001b[0;36mDatasetV2.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is infinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m UNKNOWN:\n\u001b[0;32m--> 518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is unknown.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "\u001b[0;31mTypeError\u001b[0m: dataset length is unknown."
     ]
    }
   ],
   "source": [
    "test_dataset = get_dataset('human', 'test').batch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to dos\n",
    "\n",
    "Figure out how to get all the separate elements of the dataset, and then make it so that we can store this as some sort of np array!\n",
    "\n",
    "Test the loss and see how we can recreate this in pytorch with some known values! \n",
    " \n",
    "loss = tf.reduce_mean(\n",
    "          tf.keras.losses.poisson(batch['target'], outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 09:57:24.418832: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def organism_path(organism):\n",
    "  return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism) #just leads to the organism path, quite easy\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8): #this gets the data and deserializes it\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset): #this is the function that gets the TFR files, gets all of them and sorts them\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 09:59:38.511412: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-04 09:59:39.036379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:17:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 09:59:39.040411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:65:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 09:59:39.042482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:ca:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 09:59:39.046056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:e3:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 09:59:39.046100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-04 09:59:39.084386: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-04 09:59:39.084465: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-04 09:59:39.125907: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-04 09:59:39.130939: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-04 09:59:39.134760: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-04 09:59:39.144985: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-04 09:59:39.145115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-04 09:59:39.145142: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-04 09:59:39.288386: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 09:59:39.295279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-04 09:59:39.295295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dataset length is unknown.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m human \u001b[38;5;241m=\u001b[39m get_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, num_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#let's see how many batches there are\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhuman\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m#5313\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:518\u001b[0m, in \u001b[0;36mDatasetV2.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is infinite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m UNKNOWN:\n\u001b[0;32m--> 518\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset length is unknown.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m length\n",
      "\u001b[0;31mTypeError\u001b[0m: dataset length is unknown."
     ]
    }
   ],
   "source": [
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "#let's see how many batches there are\n",
    "print(len(human)) #5313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 10:00:42.543156: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-04 10:00:42.544037: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 34021\n"
     ]
    }
   ],
   "source": [
    "num_batches = sum(1 for _ in human)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expect this to be quite large! 34000 is indeed large, let's see the total space\n",
    "a = next(iter(human))\n",
    "a1 = a['sequence'].numpy()\n",
    "a2 = a['target'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 131072, 4)\n",
      "(1, 896, 5313)\n",
      "0.002097152\n",
      "0.019041792\n"
     ]
    }
   ],
   "source": [
    "#now print the size of a1 and a2\n",
    "print(a1.shape)\n",
    "print(a2.shape)\n",
    "#and how much space they take\n",
    "print(a1.nbytes/1e9)\n",
    "print(a2.nbytes/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021138944\n",
      "719.168013824\n"
     ]
    }
   ],
   "source": [
    "space = a1.nbytes+a2.nbytes\n",
    "print(space/1e9)\n",
    "print(space*34021/1e9) #yeah 720 GB is way too much lmfao! But if we tokenize it should be a lot less!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.0\n"
     ]
    }
   ],
   "source": [
    "#so what we will do is tokenize one sequendce to see how long the sequence is when we store all of them! Can store it as int 8!\n",
    "b = np.zeros(131072).astype(np.int8)\n",
    "print((b.nbytes/a1.nbytes)*720) #only 6% as much!! 45 GB for the entire dataset just the sequences!!\n",
    "#and let's see the targets which will likely be much larger..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see the type\n",
    "a2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  7 ... 10  7 10]\n"
     ]
    }
   ],
   "source": [
    "#so we need to iterate over the dataset, and then we need to tokenize the sequence, and then save it\n",
    "#we can do this with a for loop, and then we can save it as a numpy array\n",
    "#so let's create a function that turns the one hot encoded to ACGTN\n",
    "one_hot_to_base = {\n",
    "    (1, 0, 0, 0): 7,\n",
    "    (0, 1, 0, 0): 8,\n",
    "    (0, 0, 1, 0): 9,\n",
    "    (0, 0, 0, 1): 10,\n",
    "    (0, 0, 0, 0): 11  # all zeros map to 'N'\n",
    "}\n",
    "\n",
    "# Function to convert one-hot encoded data to tokenized data\n",
    "def one_hot_to_token(one_hot_data):\n",
    "    # Reshape data if needed (assume input shape is (1, 131072, 4))\n",
    "    one_hot_data = one_hot_data.reshape(-1, 4)\n",
    "    \n",
    "    # Convert each one-hot vector to a tuple to use as a dictionary key\n",
    "    tokens = [one_hot_to_base[tuple(vector)] for vector in one_hot_data]\n",
    "    \n",
    "    return np.array(tokens, dtype=np.int8)\n",
    "\n",
    "# Example one-hot encoded data\n",
    "one_hot_data = a1\n",
    "# Shape: (1, 5, 4) for demonstration\n",
    "\n",
    "# Convert the data\n",
    "tokenized_data = one_hot_to_token(one_hot_data)\n",
    "\n",
    "print(tokenized_data)  # Output: \"ACNGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131072,)\n",
      "[10  9  7 10 10  8 10 10  7  7  9  7 10 10 10]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data.shape) #yeah it all seems good!\n",
    "print(tokenized_data[:15])\n",
    "print(one_hot_data[0,:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data.nbytes/a1.nbytes) #yeah it's the same thing! Each element is a byte!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.6875"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but a2 is more complex\n",
    "a2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.459200512\n"
     ]
    }
   ],
   "source": [
    "#now let's loop it and save out the data\n",
    "\n",
    "#we're gonna use HDF5! It's the best way to store large numpy arrays and we can store it paired\n",
    "#let's test it tho\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def organism_path(organism):\n",
    "  return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism) #just leads to the organism path, quite easy\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8): #this gets the data and deserializes it\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset): #this is the function that gets the TFR files, gets all of them and sorts them\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "\n",
    "one_hot_to_base = {\n",
    "    (1, 0, 0, 0): 7,\n",
    "    (0, 1, 0, 0): 8,\n",
    "    (0, 0, 1, 0): 9,\n",
    "    (0, 0, 0, 1): 10,\n",
    "    (0, 0, 0, 0): 11  # all zeros map to 'N'\n",
    "}\n",
    "\n",
    "# Function to convert one-hot encoded data to tokenized data\n",
    "def one_hot_to_token(one_hot_data):\n",
    "    # Reshape data if needed (assume input shape is (1, 131072, 4))\n",
    "    one_hot_data = one_hot_data.reshape(-1, 4)\n",
    "    \n",
    "    # Convert each one-hot vector to a tuple to use as a dictionary key\n",
    "    tokens = [one_hot_to_base[tuple(vector)] for vector in one_hot_data]\n",
    "    \n",
    "    return np.array(tokens, dtype=np.int8)\n",
    "\n",
    "#we need at least 50GB of ram to save it tho, so need to request a lot then we can do it!\n",
    "sequence_array = np.zeros((34021,131072), dtype=np.int8)\n",
    "print(sequence_array.nbytes/1e9) #oh only 4.5GB? That's quite good! in that case the other one at float32 should only be 4.5*8!\n",
    "label_array = np.zeros((34021, 896, 513), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.550738432\n"
     ]
    }
   ],
   "source": [
    "test_array = np.zeros((34021,896,513), dtype=np.float32)\n",
    "print(test_array.nbytes/1e9) #oh only 60 GB? That's quite good! in that case the other one at float32 should only be 4.5*8!\n",
    "#lmao no it's 600 GB!!\n",
    "#so we will save total_array as a int8 and then a numpy array, even .npy so no need to nonlazy load it, but then need to save the other one as h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 93/34021 [02:04<12:37:59,  1.34s/it]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sequence_array = np.zeros((34021,131072), dtype=np.int8)\n",
    "for i, batch in tqdm(enumerate(human), total=34021):\n",
    "    sequence = batch['sequence'].numpy()\n",
    "    # target = batch['target'].numpy()\n",
    "    tokenized_sequence = one_hot_to_token(sequence)\n",
    "    sequence_array[i] = tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 16:48:47.569517: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-04 16:49:28.548766: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-04 16:49:28.984000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:17:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 16:49:28.987647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:65:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 16:49:28.995177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:ca:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 16:49:28.999178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:e3:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.35GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2024-07-04 16:49:28.999233: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-04 16:49:29.005481: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-04 16:49:29.005553: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-04 16:49:29.035785: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-04 16:49:29.036243: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-04 16:49:29.036773: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-04 16:49:29.037670: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-04 16:49:29.037831: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-04 16:49:29.037858: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-04 16:49:29.091398: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 16:49:29.100199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-04 16:49:29.100227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.459200512\n"
     ]
    }
   ],
   "source": [
    "#let's test to see how the human dataset works\n",
    "\n",
    "#now let's loop it and save out the data\n",
    "\n",
    "#we're gonna use HDF5! It's the best way to store large numpy arrays and we can store it paired\n",
    "#let's test it tho\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def organism_path(organism):\n",
    "  return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism) #just leads to the organism path, quite easy\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8): #this gets the data and deserializes it\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset): #this is the function that gets the TFR files, gets all of them and sorts them\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "\n",
    "one_hot_to_base = {\n",
    "    (1, 0, 0, 0): 7,\n",
    "    (0, 1, 0, 0): 8,\n",
    "    (0, 0, 1, 0): 9,\n",
    "    (0, 0, 0, 1): 10,\n",
    "    (0, 0, 0, 0): 11  # all zeros map to 'N'\n",
    "}\n",
    "\n",
    "# Function to convert one-hot encoded data to tokenized data\n",
    "def one_hot_to_token(one_hot_data):\n",
    "    # Reshape data if needed (assume input shape is (1, 131072, 4))\n",
    "    one_hot_data = one_hot_data.reshape(-1, 4)\n",
    "    \n",
    "    # Convert each one-hot vector to a tuple to use as a dictionary key\n",
    "    tokens = [one_hot_to_base[tuple(vector)] for vector in one_hot_data]\n",
    "    \n",
    "    return np.array(tokens, dtype=np.int8)\n",
    "\n",
    "#we need at least 50GB of ram to save it tho, so need to request a lot then we can do it!\n",
    "sequence_array = np.zeros((34021,131072), dtype=np.int8)\n",
    "print(sequence_array.nbytes/1e9) #oh only 4.5GB? That's quite good! in that case the other one at float32 should only be 4.5*8!\n",
    "# label_array = np.zeros((34021, 896, 513), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 12:02:29.288040: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-04 12:02:29.289686: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2300115000 Hz\n"
     ]
    }
   ],
   "source": [
    "it = iter(human)\n",
    "small_array = np.zeros((5,896,5313))\n",
    "for i in range(5):\n",
    "    batch = next(it)\n",
    "    label = batch['target'].numpy()\n",
    "    small_array[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 896, 5313)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_array.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.10839844, 0.10760498, 0.04425049, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.10162354, 0.09332275, 0.0094986 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.10272217, 0.15600586, 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.07714844, 0.07678223, 0.03509521, ..., 0.        ,\n",
       "         0.01934814, 0.        ],\n",
       "        [0.07666016, 0.03826904, 0.05648804, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.08319092, 0.06051636, 0.02156067, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.04019165, 0.06155396, 0.        , ..., 0.        ,\n",
       "         0.        , 0.48095703],\n",
       "        [0.13452148, 0.15014648, 0.        , ..., 0.        ,\n",
       "         0.984375  , 0.        ],\n",
       "        [0.17260742, 0.38916016, 0.02510071, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.09771729, 0.07757568, 0.04217529, ..., 0.        ,\n",
       "         0.48291016, 0.        ],\n",
       "        [0.08557129, 0.04833984, 0.01190948, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.0609436 , 0.08276367, 0.11486816, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.07098389, 0.08081055, 0.03701782, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.11895752, 0.11425781, 0.02832031, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.08062744, 0.09338379, 0.03622437, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.0770874 , 0.05877686, 0.04119873, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.08978271, 0.05175781, 0.00675964, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.09411621, 0.11566162, 0.04425049, ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.0541687 , 0.13928223, 0.06115723, ..., 0.02940369,\n",
       "         0.        , 0.        ],\n",
       "        [0.00959015, 0.02078247, 0.01351929, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.01797485, 0.01933289, 0.01222992, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.22631836, 0.21362305, 0.08758545, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.38989258, 0.47094727, 0.48022461, ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.74707031, 0.81640625, 1.0078125 , ..., 0.        ,\n",
       "         0.        , 0.        ]],\n",
       "\n",
       "       [[0.01335907, 0.00794983, 0.        , ..., 0.        ,\n",
       "         0.984375  , 0.        ],\n",
       "        [0.00754929, 0.00812531, 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.00970459, 0.00933075, 0.        , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.10101318, 0.06243896, 0.3190918 , ..., 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.05557251, 0.00617599, 0.05374146, ..., 0.        ,\n",
       "         0.        , 0.01580811],\n",
       "        [0.07263184, 0.02262878, 0.02156067, ..., 0.        ,\n",
       "         0.        , 0.        ]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "#let's save this to test it\n",
    "with h5py.File('/data/leslie/sarthak/data/enformer/data/test.h5', 'w') as f:\n",
    "    f.create_dataset('label', data=small_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['label']>\n",
      "(5, 896, 5313)\n",
      "[[[0.10839844 0.10760498 0.04425049 ... 0.         0.         0.        ]\n",
      "  [0.10162354 0.09332275 0.0094986  ... 0.         0.         0.        ]\n",
      "  [0.10272217 0.15600586 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.07714844 0.07678223 0.03509521 ... 0.         0.01934814 0.        ]\n",
      "  [0.07666016 0.03826904 0.05648804 ... 0.         0.         0.        ]\n",
      "  [0.08319092 0.06051636 0.02156067 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.04019165 0.06155396 0.         ... 0.         0.         0.48095703]\n",
      "  [0.13452148 0.15014648 0.         ... 0.         0.984375   0.        ]\n",
      "  [0.17260742 0.38916016 0.02510071 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.09771729 0.07757568 0.04217529 ... 0.         0.48291016 0.        ]\n",
      "  [0.08557129 0.04833984 0.01190948 ... 0.         0.         0.        ]\n",
      "  [0.0609436  0.08276367 0.11486816 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.07098389 0.08081055 0.03701782 ... 0.         0.         0.        ]\n",
      "  [0.11895752 0.11425781 0.02832031 ... 0.         0.         0.        ]\n",
      "  [0.08062744 0.09338379 0.03622437 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.0770874  0.05877686 0.04119873 ... 0.         0.         0.        ]\n",
      "  [0.08978271 0.05175781 0.00675964 ... 0.         0.         0.        ]\n",
      "  [0.09411621 0.11566162 0.04425049 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.0541687  0.13928223 0.06115723 ... 0.02940369 0.         0.        ]\n",
      "  [0.00959015 0.02078247 0.01351929 ... 0.         0.         0.        ]\n",
      "  [0.01797485 0.01933289 0.01222992 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.22631836 0.21362305 0.08758545 ... 0.         0.         0.        ]\n",
      "  [0.38989258 0.47094727 0.48022461 ... 0.         0.         0.        ]\n",
      "  [0.74707031 0.81640625 1.0078125  ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.01335907 0.00794983 0.         ... 0.         0.984375   0.        ]\n",
      "  [0.00754929 0.00812531 0.         ... 0.         0.         0.        ]\n",
      "  [0.00970459 0.00933075 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.10101318 0.06243896 0.3190918  ... 0.         0.         0.        ]\n",
      "  [0.05557251 0.00617599 0.05374146 ... 0.         0.         0.01580811]\n",
      "  [0.07263184 0.02262878 0.02156067 ... 0.         0.         0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "#now load it in\n",
    "with h5py.File('/data/leslie/sarthak/data/enformer/data/test.h5', 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['label'].shape)\n",
    "    print(f['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['label']>\n"
     ]
    }
   ],
   "source": [
    "loaded_h5 = h5py.File('/data/leslie/sarthak/data/enformer/data/test.h5', 'r')\n",
    "print(loaded_h5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 896, 5313)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_h5['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647.820805632\n"
     ]
    }
   ],
   "source": [
    "#let's calculate how large the full thing would be\n",
    "size = 34021*896*5313 #then each of these is int32 so each one is 4 bytes\n",
    "total = size*4\n",
    "print(total/1e9) #so 650 GB, that's quite a lot, but it's doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataset\n",
      "(34021, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 16:25:43.244074: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-04 16:25:43.245135: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2300115000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n",
      "loaded batch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "batch_size = 2\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(batch_size)\n",
    "# Example data dimensions\n",
    "filename = '/data/leslie/sarthak/data/enformer/data/test_large.h5'\n",
    "# Function to create and save data incrementally\n",
    "def create_hdf5_file(dataset, filename, chunk_size, compression='lzf'):\n",
    "    # Create an HDF5 file\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        float32_data = f.create_dataset(\n",
    "            'labels',\n",
    "            shape=(34021, 896, 5313),\n",
    "            maxshape=(34021, 896,5313),\n",
    "            dtype='f4',  # float32 data type\n",
    "            chunks=(chunk_size, 896, 5313),\n",
    "            compression=compression\n",
    "        )\n",
    "        print('created dataset')\n",
    "        print(float32_data.shape)\n",
    "        idx=0\n",
    "        for batch in dataset:\n",
    "            label = batch['target'].numpy()\n",
    "            print('loaded batch')\n",
    "            float32_data[idx:idx+label.shape[0]] = label\n",
    "            idx += label.shape[0]\n",
    "            if idx>2:\n",
    "                return idx\n",
    "\n",
    "\n",
    "\n",
    "# Create HDF5 file with chunking and compression\n",
    "create_hdf5_file(human, filename, batch_size)\n",
    "#writing this is EXTREMELY slow!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['labels']>\n",
      "(34021, 896, 5313)\n",
      "[[[0.10839844 0.10760498 0.04425049 ... 0.         0.         0.        ]\n",
      "  [0.10162354 0.09332275 0.0094986  ... 0.         0.         0.        ]\n",
      "  [0.10272217 0.15600586 0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.07714844 0.07678223 0.03509521 ... 0.         0.01934814 0.        ]\n",
      "  [0.07666016 0.03826904 0.05648804 ... 0.         0.         0.        ]\n",
      "  [0.08319092 0.06051636 0.02156067 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.04019165 0.06155396 0.         ... 0.         0.         0.48095703]\n",
      "  [0.13452148 0.15014648 0.         ... 0.         0.984375   0.        ]\n",
      "  [0.17260742 0.38916016 0.02510071 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.09771729 0.07757568 0.04217529 ... 0.         0.48291016 0.        ]\n",
      "  [0.08557129 0.04833984 0.01190948 ... 0.         0.         0.        ]\n",
      "  [0.0609436  0.08276367 0.11486816 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.07098389 0.08081055 0.03701782 ... 0.         0.         0.        ]\n",
      "  [0.11895752 0.11425781 0.02832031 ... 0.         0.         0.        ]\n",
      "  [0.08062744 0.09338379 0.03622437 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.0770874  0.05877686 0.04119873 ... 0.         0.         0.        ]\n",
      "  [0.08978271 0.05175781 0.00675964 ... 0.         0.         0.        ]\n",
      "  [0.09411621 0.11566162 0.04425049 ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.0541687  0.13928223 0.06115723 ... 0.02940369 0.         0.        ]\n",
      "  [0.00959015 0.02078247 0.01351929 ... 0.         0.         0.        ]\n",
      "  [0.01797485 0.01933289 0.01222992 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.22631836 0.21362305 0.08758545 ... 0.         0.         0.        ]\n",
      "  [0.38989258 0.47094727 0.4802246  ... 0.         0.         0.        ]\n",
      "  [0.7470703  0.81640625 1.0078125  ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "647.820805632\n"
     ]
    }
   ],
   "source": [
    "#ran it for a bit, let's see it\n",
    "import h5py\n",
    "with h5py.File('/data/leslie/sarthak/data/enformer/data/test_large.h5', 'r') as f:\n",
    "    print(f.keys())\n",
    "    print(f['labels'].shape)\n",
    "    print(f['labels'][:5])\n",
    "    print(f['labels'].nbytes/1e9) #yeah it's quite quick!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created dataset\n",
      "(34021, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/265 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/265 [00:48<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "batch_size = 128\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(batch_size)\n",
    "# Example data dimensions\n",
    "filename = '/data/leslie/sarthak/data/enformer/data/train_label.h5'\n",
    "# Function to create and save data incrementally\n",
    "def create_hdf5_file(dataset, filename, chunk_size, compression='lzf'):\n",
    "    # Create an HDF5 file\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        float32_data = f.create_dataset(\n",
    "            'labels',\n",
    "            shape=(34021, 896, 5313),\n",
    "            maxshape=(34021, 896,5313),\n",
    "            dtype='f4',  # float32 data type\n",
    "            chunks=(chunk_size, 896, 5313),\n",
    "            compression=compression\n",
    "        )\n",
    "        # print('created dataset')\n",
    "        # print(float32_data.shape)\n",
    "        idx=0\n",
    "        for i,batch in tqdm(enumerate(dataset), total = 34021//batch_size):\n",
    "            label = batch['target'].numpy()\n",
    "            print('loaded batch')\n",
    "            float32_data[idx:idx+label.shape[0]] = label\n",
    "            idx += label.shape[0]\n",
    "            if idx>2:\n",
    "                return idx\n",
    "#it's indeed quite slow, loading took a while and then actually doing it also took a while, \n",
    "\n",
    "\n",
    "# Create HDF5 file with chunking and compression\n",
    "create_hdf5_file(human, filename, batch_size)\n",
    "#writing this is EXTREMELY slow!\n",
    "#1.5 minutes out of the 265... yeah it's fine tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.437349376\n"
     ]
    }
   ],
   "source": [
    "#see the space 128 takes\n",
    "print(128*896*5313*4/1e9) #so 2.5 GB, so 2.5 GB per batch, so 2.5*34021/1e9\n",
    "#we can easily quadruple it and add more cores, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100/2.5\n",
    "\n",
    "#so let's do 32*128, maybe 16 to be safe... nah let's try 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*128 #batch size 4096, with 100 GB of ram, across 4 workers, so request 25 total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_val = get_dataset('human', 'valid', num_threads=1).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 131072, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(human_val))['sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212\n"
     ]
    }
   ],
   "source": [
    "#now iterate through it\n",
    "for i, batch in enumerate(human_val):\n",
    "    2\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936\n"
     ]
    }
   ],
   "source": [
    "#number of elements is that + 1, because index is 1 lower\n",
    "#so is 2213\n",
    "human_test = get_dataset('human', 'test', num_threads=1).batch(1)\n",
    "for i, batch in enumerate(human_test):\n",
    "    2\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now let's compare some data from both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 11:18:37.033308: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-05 11:18:56.825376: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-05 11:18:56.956668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 11:18:56.959075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 11:18:56.959638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:83:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 11:18:56.960188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:84:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 11:18:56.960227: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-05 11:18:56.970093: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-05 11:18:56.970145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-05 11:18:57.023583: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-05 11:18:57.030350: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-05 11:18:57.031188: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-05 11:18:57.032569: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-05 11:18:57.032784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-05 11:18:57.032819: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-05 11:18:57.035119: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 11:18:57.044603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-05 11:18:57.044646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n"
     ]
    }
   ],
   "source": [
    "#let's test to see how the human dataset works\n",
    "\n",
    "#now let's loop it and save out the data\n",
    "\n",
    "#we're gonna use HDF5! It's the best way to store large numpy arrays and we can store it paired\n",
    "#let's test it tho\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import functools\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def organism_path(organism):\n",
    "  return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism) #just leads to the organism path, quite easy\n",
    "\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8): #this gets the data and deserializes it\n",
    "  metadata = get_metadata(organism)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                    compression_type='ZLIB',\n",
    "                                    num_parallel_reads=num_threads)\n",
    "  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                        num_parallel_calls=num_threads)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def get_metadata(organism):\n",
    "  # Keys:\n",
    "  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n",
    "  # pool_width, crop_bp, target_length\n",
    "  path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "  with tf.io.gfile.GFile(path, 'r') as f:\n",
    "    return json.load(f)\n",
    "\n",
    "\n",
    "def tfrecord_files(organism, subset): #this is the function that gets the TFR files, gets all of them and sorts them\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n",
    "  feature_map = {\n",
    "      'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "      'target': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_example(serialized_example, feature_map)\n",
    "  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "  sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "  target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "  target = tf.reshape(target,\n",
    "                      (metadata['target_length'], metadata['num_targets']))\n",
    "  target = tf.cast(target, tf.float32)\n",
    "\n",
    "  return {'sequence': sequence,\n",
    "          'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "  return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "\n",
    "one_hot_to_base = {\n",
    "    (1, 0, 0, 0): 7,\n",
    "    (0, 1, 0, 0): 8,\n",
    "    (0, 0, 1, 0): 9,\n",
    "    (0, 0, 0, 1): 10,\n",
    "    (0, 0, 0, 0): 11  # all zeros map to 'N'\n",
    "}\n",
    "\n",
    "# Function to convert one-hot encoded data to tokenized data\n",
    "def one_hot_to_token(one_hot_data):\n",
    "    # Reshape data if needed (assume input shape is (1, 131072, 4))\n",
    "    one_hot_data = one_hot_data.reshape(-1, 4)\n",
    "    \n",
    "    # Convert each one-hot vector to a tuple to use as a dictionary key\n",
    "    tokens = [one_hot_to_base[tuple(vector)] for vector in one_hot_data]\n",
    "    \n",
    "    return np.array(tokens, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "train_label = h5py.File('/data/leslie/sarthak/data/enformer/data/train_label.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34021, 896, 5313)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the keys\n",
    "train_label['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 11:05:12.494884: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-05 11:05:12.496298: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2300040000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(1, 896, 5313), dtype=float32, numpy=\n",
      "array([[[0.10839844, 0.10760498, 0.04425049, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.10162354, 0.09332275, 0.0094986 , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.10272217, 0.15600586, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.07714844, 0.07678223, 0.03509521, ..., 0.        ,\n",
      "         0.01934814, 0.        ],\n",
      "        [0.07666016, 0.03826904, 0.05648804, ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.08319092, 0.06051636, 0.02156067, ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "it = iter(human)\n",
    "ex1 = next(it)\n",
    "print(ex1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 896, 5313])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(ex1['target'].numpy(), train_label['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_label['labels'][0] #wait loading it is quite slow??\n",
    "#so per epoch that adds 16 seconds... really??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(32, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]],\n",
      "\n",
      "       [[0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]],\n",
      "\n",
      "       [[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(32, 896, 5313), dtype=float32, numpy=\n",
      "array([[[1.08398438e-01, 1.07604980e-01, 4.42504883e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.01623535e-01, 9.33227539e-02, 9.49859619e-03, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.02722168e-01, 1.56005859e-01, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [7.71484375e-02, 7.67822266e-02, 3.50952148e-02, ...,\n",
      "         0.00000000e+00, 1.93481445e-02, 0.00000000e+00],\n",
      "        [7.66601562e-02, 3.82690430e-02, 5.64880371e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [8.31909180e-02, 6.05163574e-02, 2.15606689e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
      "\n",
      "       [[4.01916504e-02, 6.15539551e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 4.80957031e-01],\n",
      "        [1.34521484e-01, 1.50146484e-01, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 9.84375000e-01, 0.00000000e+00],\n",
      "        [1.72607422e-01, 3.89160156e-01, 2.51007080e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [9.77172852e-02, 7.75756836e-02, 4.21752930e-02, ...,\n",
      "         0.00000000e+00, 4.82910156e-01, 0.00000000e+00],\n",
      "        [8.55712891e-02, 4.83398438e-02, 1.19094849e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [6.09436035e-02, 8.27636719e-02, 1.14868164e-01, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
      "\n",
      "       [[7.09838867e-02, 8.08105469e-02, 3.70178223e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [1.18957520e-01, 1.14257812e-01, 2.83203125e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [8.06274414e-02, 9.33837891e-02, 3.62243652e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [7.70874023e-02, 5.87768555e-02, 4.11987305e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [8.97827148e-02, 5.17578125e-02, 6.75964355e-03, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [9.41162109e-02, 1.15661621e-01, 4.42504883e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2.32849121e-02, 2.38952637e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [2.93273926e-02, 5.50537109e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [3.82080078e-02, 2.30102539e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [7.07031250e-01, 8.88671875e-01, 7.05078125e-01, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [4.99023438e-01, 6.71875000e-01, 9.15039062e-01, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [4.65332031e-01, 7.03125000e-01, 4.12109375e-01, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 9.82421875e-01]],\n",
      "\n",
      "       [[3.17077637e-02, 9.13238525e-03, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 9.83886719e-01, 2.77832031e-01],\n",
      "        [4.42199707e-02, 3.10363770e-02, 6.62841797e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [5.36441803e-04, 5.36918640e-04, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [8.00781250e-02, 9.48486328e-02, 5.56945801e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [9.26513672e-02, 4.08630371e-02, 1.08337402e-01, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [6.65893555e-02, 5.01403809e-02, 6.14624023e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]],\n",
      "\n",
      "       [[3.43322754e-02, 3.66821289e-02, 2.83203125e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [5.15441895e-02, 2.20184326e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [8.01391602e-02, 5.28259277e-02, 0.00000000e+00, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        ...,\n",
      "        [1.17431641e-01, 6.45141602e-02, 6.17980957e-02, ...,\n",
      "         0.00000000e+00, 9.82910156e-01, 0.00000000e+00],\n",
      "        [6.89086914e-02, 4.91638184e-02, 2.83203125e-02, ...,\n",
      "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "        [5.73120117e-02, 5.66711426e-02, 0.00000000e+00, ...,\n",
      "         1.41723633e-01, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "#let's try a few more examples\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(32)\n",
    "it = iter(human)\n",
    "ex2 = next(it)\n",
    "print(ex2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see this grabs it rather quick\n",
    "data = train_label['labels'][0] #wait loading it is quite slow??\n",
    "#now compare them\n",
    "np.allclose(ex2['target'][0].numpy(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_label['labels'][1] #wait loading it is quite slow??\n",
    "np.allclose(ex2['target'][1].numpy(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.04019165 0.06155396 0.         ... 0.         0.         0.48095703]\n",
      " [0.13452148 0.15014648 0.         ... 0.         0.984375   0.        ]\n",
      " [0.17260742 0.38916016 0.02510071 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.09771729 0.07757568 0.04217529 ... 0.         0.48291016 0.        ]\n",
      " [0.08557129 0.04833984 0.01190948 ... 0.         0.         0.        ]\n",
      " [0.0609436  0.08276367 0.11486816 ... 0.         0.         0.        ]], shape=(896, 5313), dtype=float32)\n",
      "[[0.07519531 0.01112366 0.01609802 ... 0.         0.         0.        ]\n",
      " [0.0881958  0.01392365 0.03829956 ... 0.         0.         0.        ]\n",
      " [0.05108643 0.01622009 0.01737976 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.00142574 0.001544   0.         ... 0.         0.         0.        ]\n",
      " [0.00675964 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.05609131 0.0559082  0.06762695 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(ex2['target'][1])\n",
    "print(data)\n",
    "#oh it's actually quite different... why??\n",
    "\n",
    "#some issue with the way it loads the batch being inconsistent??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]], shape=(131072, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(ex2['sequence'][31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of examples: 34021\n",
      "Example at index 0: {'sequence': <tf.Tensor: shape=(131072, 4), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.]], dtype=float32)>, 'target': <tf.Tensor: shape=(896, 5313), dtype=float32, numpy=\n",
      "array([[0.10839844, 0.10760498, 0.04425049, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.10162354, 0.09332275, 0.0094986 , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.10272217, 0.15600586, 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.07714844, 0.07678223, 0.03509521, ..., 0.        , 0.01934814,\n",
      "        0.        ],\n",
      "       [0.07666016, 0.03826904, 0.05648804, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.08319092, 0.06051636, 0.02156067, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)>}\n",
      "Sequence shape: (131072, 4), Target shape: (896, 5313)\n"
     ]
    }
   ],
   "source": [
    "#let's try just loading the TF data with a mapping\n",
    "\n",
    "def organism_path(organism):\n",
    "    return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism)\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8):\n",
    "    metadata = get_metadata(organism)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                      compression_type='ZLIB',\n",
    "                                      num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                          num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n",
    "def get_metadata(organism):\n",
    "    path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "    with tf.io.gfile.GFile(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "    return sorted(tf.io.gfile.glob(os.path.join(\n",
    "        organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "    )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    feature_map = {\n",
    "        'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "        'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                        (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence, 'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "    targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "    return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "def get_example_by_index(organism, subset, index, num_threads=8):\n",
    "    metadata = get_metadata(organism)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                      compression_type='ZLIB',\n",
    "                                      num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                          num_parallel_calls=num_threads)\n",
    "    iterator = iter(dataset.skip(index).take(1))\n",
    "    example = next(iterator)\n",
    "    return example\n",
    "\n",
    "# Example usage:\n",
    "organism = 'human'\n",
    "subset = 'train'\n",
    "\n",
    "# Count the number of examples\n",
    "dataset = get_dataset(organism, subset, num_threads=1)\n",
    "num_examples = 34021\n",
    "print(f'Total number of examples: {num_examples}')\n",
    "\n",
    "# Access the 32nd example (index 31)\n",
    "example_index = 0\n",
    "example = get_example_by_index(organism, subset, example_index)\n",
    "print(f'Example at index {example_index}: {example}')\n",
    "\n",
    "# Convert the example to numpy arrays\n",
    "sequence_np = example['sequence'].numpy()\n",
    "target_np = example['target'].numpy()\n",
    "print(f'Sequence shape: {sequence_np.shape}, Target shape: {target_np.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]], shape=(131072, 4), dtype=float32)\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#let's compare\n",
    "print(ex2['sequence'][0])\n",
    "print(sequence_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]], shape=(131072, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]], shape=(131072, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#that seems correct, now check out ex2 1\n",
    "print(ex2['sequence'][1])\n",
    "example = get_example_by_index(organism, subset, 1)\n",
    "print(example['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 131072, 4)\n",
      "(1, 131072, 4)\n",
      "(1, 131072, 4)\n",
      "(1, 131072, 4)\n",
      "(1, 131072, 4)\n"
     ]
    }
   ],
   "source": [
    "# I figured out the issue, iti seems when calling.batch, tensorflow shuffles the data, so it's not in the same order as the h5 file\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "it = iter(human)\n",
    "#now go through 5 iterations\n",
    "for i in range(5):\n",
    "    ex = next(it)\n",
    "    print(ex['sequence'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(1, 131072, 4), dtype=float32, numpy=\n",
      "array([[[0., 0., 1., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        ...,\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.]]], dtype=float32)>, 'target': <tf.Tensor: shape=(1, 896, 5313), dtype=float32, numpy=\n",
      "array([[[0.01335907, 0.00794983, 0.        , ..., 0.        ,\n",
      "         0.984375  , 0.        ],\n",
      "        [0.00754929, 0.00812531, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.00970459, 0.00933075, 0.        , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.10101318, 0.06243896, 0.3190918 , ..., 0.        ,\n",
      "         0.        , 0.        ],\n",
      "        [0.05557251, 0.00617599, 0.05374146, ..., 0.        ,\n",
      "         0.        , 0.01580811],\n",
      "        [0.07263184, 0.02262878, 0.02156067, ..., 0.        ,\n",
      "         0.        , 0.        ]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(131072, 4), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.]], dtype=float32)>, 'target': <tf.Tensor: shape=(896, 5313), dtype=float32, numpy=\n",
      "array([[0.06286621, 0.03756714, 0.00804901, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.09143066, 0.0609436 , 0.04440308, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.09613037, 0.05172729, 0.01834106, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.03494263, 0.07391357, 0.00627518, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.02326965, 0.0553894 , 0.09417725, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.02053833, 0.04690552, 0.22131348, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "example = get_example_by_index(organism, subset, 4)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we have the index 4 data, let's compare this\n",
    "data = train_label['labels'][4]\n",
    "np.allclose(example['target'].numpy(), data)\n",
    "np.allclose(ex['target'].numpy(), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]], shape=(1, 131072, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#hmmmmm, it could be that it happens when we load the dataset, let's use the same dataset\n",
    "human_dataset = get_dataset('human', 'train', num_threads=1)\n",
    "human_1 = human_dataset.batch(1)\n",
    "human_2 = human_dataset.batch(2)\n",
    "it = iter(human_1)\n",
    "#let's get the second example\n",
    "ex = next(it)\n",
    "ex2 = next(it)\n",
    "print(ex2['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]], shape=(131072, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#now try this human 2\n",
    "it = iter(human_2)\n",
    "ex = next(it)\n",
    "print(ex['sequence'][1]) #ok so the batching is identical, the issue seems to be if we call the dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]], shape=(1, 131072, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "human_dataset = get_dataset('human', 'train', num_threads=1)\n",
    "human_1 = human_dataset.batch(1)\n",
    "human_2 = human_dataset.batch(2)\n",
    "it = iter(human_1)\n",
    "#let's get the second example\n",
    "ex = next(it)\n",
    "ex2 = next(it)\n",
    "print(ex2['sequence'])\n",
    "#this is consistent, and it seems to be exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no, again this is exactly the same???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(131072, 4), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.]], dtype=float32)>, 'target': <tf.Tensor: shape=(896, 5313), dtype=float32, numpy=\n",
      "array([[0.07519531, 0.01112366, 0.01609802, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.0881958 , 0.01392365, 0.03829956, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.05108643, 0.01622009, 0.01737976, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.00142574, 0.001544  , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.00675964, 0.        , 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.05609131, 0.0559082 , 0.06762695, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "#So I think the issue just has to do with batching the data for some reason\n",
    "example = get_example_by_index(organism, subset, 1)\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# redo functions and define it with a set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:00:04.382955: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-05 12:00:22.872394: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-07-05 12:00:23.029781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 12:00:23.033005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 12:00:23.033572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:83:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 12:00:23.034132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:84:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2024-07-05 12:00:23.034173: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-07-05 12:00:23.043791: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-07-05 12:00:23.043851: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-07-05 12:00:23.097982: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-07-05 12:00:23.098624: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-07-05 12:00:23.099464: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-07-05 12:00:23.100901: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-07-05 12:00:23.101090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /admin/lsflilac/lsf/10.1/linux3.10-glibc2.17-x86_64/lib\n",
      "2024-07-05 12:00:23.101117: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1766] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-07-05 12:00:23.101905: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 12:00:23.109435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-07-05 12:00:23.109464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      \n",
      "2024-07-05 12:00:23.262738: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-07-05 12:00:23.263428: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2300040000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example at index 1: {'sequence': <tf.Tensor: shape=(131072, 4), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       ...,\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0.]], dtype=float32)>, 'target': <tf.Tensor: shape=(896, 5313), dtype=float32, numpy=\n",
      "array([[0.04019165, 0.06155396, 0.        , ..., 0.        , 0.        ,\n",
      "        0.48095703],\n",
      "       [0.13452148, 0.15014648, 0.        , ..., 0.        , 0.984375  ,\n",
      "        0.        ],\n",
      "       [0.17260742, 0.38916016, 0.02510071, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.09771729, 0.07757568, 0.04217529, ..., 0.        , 0.48291016,\n",
      "        0.        ],\n",
      "       [0.08557129, 0.04833984, 0.01190948, ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.0609436 , 0.08276367, 0.11486816, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "#let's try just loading the TF data with a mapping\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def organism_path(organism):\n",
    "    return os.path.join('/data/leslie/sarthak/data/enformer/data/', organism)\n",
    "\n",
    "def get_dataset(organism, subset, num_threads=8):\n",
    "    metadata = get_metadata(organism)\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "                                      compression_type='ZLIB',\n",
    "                                      num_parallel_reads=num_threads)\n",
    "    dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "                          num_parallel_calls=num_threads)\n",
    "    return dataset\n",
    "\n",
    "def get_metadata(organism):\n",
    "    path = os.path.join(organism_path(organism), 'statistics.json')\n",
    "    with tf.io.gfile.GFile(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def tfrecord_files(organism, subset):\n",
    "    return sorted(tf.io.gfile.glob(os.path.join(\n",
    "        organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "    )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "\n",
    "def deserialize(serialized_example, metadata):\n",
    "    feature_map = {\n",
    "        'sequence': tf.io.FixedLenFeature([], tf.string),\n",
    "        'target': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_example(serialized_example, feature_map)\n",
    "    sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n",
    "    sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n",
    "    sequence = tf.cast(sequence, tf.float32)\n",
    "\n",
    "    target = tf.io.decode_raw(example['target'], tf.float16)\n",
    "    target = tf.reshape(target,\n",
    "                        (metadata['target_length'], metadata['num_targets']))\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    return {'sequence': sequence, 'target': target}\n",
    "\n",
    "def get_targets(organism):\n",
    "    targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n",
    "    return pd.read_csv(targets_txt, sep='\\t')\n",
    "\n",
    "def get_example_by_index(dataset, organism, subset, index, num_threads=8):\n",
    "    metadata = get_metadata(organism)\n",
    "    # dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n",
    "    #                                   compression_type='ZLIB',\n",
    "    #                                   num_parallel_reads=num_threads)\n",
    "    # dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n",
    "    #                       num_parallel_calls=num_threads)\n",
    "    iterator = iter(dataset.skip(index).take(1))\n",
    "    example = next(iterator)\n",
    "    return example\n",
    "\n",
    "# Example usage:\n",
    "organism = 'human'\n",
    "subset = 'train'\n",
    "\n",
    "# Count the number of examples\n",
    "dataset = get_dataset(organism, subset, num_threads=1)\n",
    "num_examples = 34021\n",
    "# print(f'Total number of examples: {num_examples}')\n",
    "\n",
    "# Access the 2nd example (index 1)\n",
    "example_index = 1\n",
    "example = get_example_by_index(dataset, organism, subset, example_index)\n",
    "print(f'Example at index {example_index}: {example}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  ...\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0.]]], shape=(1, 131072, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.04019165 0.06155396 0.         ... 0.         0.         0.48095703]\n",
      "  [0.13452148 0.15014648 0.         ... 0.         0.984375   0.        ]\n",
      "  [0.17260742 0.38916016 0.02510071 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.09771729 0.07757568 0.04217529 ... 0.         0.48291016 0.        ]\n",
      "  [0.08557129 0.04833984 0.01190948 ... 0.         0.         0.        ]\n",
      "  [0.0609436  0.08276367 0.11486816 ... 0.         0.         0.        ]]], shape=(1, 896, 5313), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "human1 = dataset.batch(1)\n",
    "it = iter(human1)\n",
    "ex1 = next(it)\n",
    "ex2 = next(it)\n",
    "print(ex2['sequence'])\n",
    "print(ex2['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " ...\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]], shape=(131072, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.04019165 0.06155396 0.         ... 0.         0.         0.48095703]\n",
      " [0.13452148 0.15014648 0.         ... 0.         0.984375   0.        ]\n",
      " [0.17260742 0.38916016 0.02510071 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.09771729 0.07757568 0.04217529 ... 0.         0.48291016 0.        ]\n",
      " [0.08557129 0.04833984 0.01190948 ... 0.         0.         0.        ]\n",
      " [0.0609436  0.08276367 0.11486816 ... 0.         0.         0.        ]], shape=(896, 5313), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "human2 = dataset.batch(2)\n",
    "it = iter(human2)\n",
    "ex1 = next(it)\n",
    "print(ex1['sequence'][1])\n",
    "print(ex1['target'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': <tf.Tensor: shape=(131072, 4), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.]], dtype=float32)>, 'target': <tf.Tensor: shape=(896, 5313), dtype=float32, numpy=\n",
      "array([[0.07965088, 0.07824707, 0.05599976, ..., 0.        , 0.        ,\n",
      "        0.00358963],\n",
      "       [0.08392334, 0.15319824, 0.10559082, ..., 0.        , 0.        ,\n",
      "        0.30615234],\n",
      "       [0.0760498 , 0.07672119, 0.09527588, ..., 0.        , 0.01780701,\n",
      "        0.        ],\n",
      "       ...,\n",
      "       [0.03396606, 0.03102112, 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.0748291 , 0.02720642, 0.        , ..., 0.        , 0.        ,\n",
      "        0.        ],\n",
      "       [0.07141113, 0.0362854 , 0.09655762, ..., 0.        , 0.        ,\n",
      "        0.        ]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "#these look identical! So as long as we use the same dataset, it should be the same, let's test one more\n",
    "dataset = get_dataset(organism, subset, num_threads=1)\n",
    "example = get_example_by_index(dataset, organism, subset, 10) #oof, it's actually really slow...\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09265137 0.19274902 0.265625   ... 0.         0.         0.        ]\n",
      " [0.12792969 0.16992188 0.4387207  ... 0.         0.         0.        ]\n",
      " [0.12878418 0.14343262 0.44189453 ... 0.         0.         0.12255859]\n",
      " ...\n",
      " [0.06341553 0.08654785 0.27294922 ... 0.         0.         0.        ]\n",
      " [0.15966797 0.12335205 0.45141602 ... 0.9736328  0.         0.        ]\n",
      " [0.14868164 0.10693359 0.27368164 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "train_label = h5py.File('/data/leslie/sarthak/data/enformer/data/train_label.h5', 'r')\n",
    "data = train_label['labels'][10]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n"
     ]
    }
   ],
   "source": [
    "#let's do some further testing\n",
    "num_threads = 1\n",
    "batch_size = 2\n",
    "num_elems = 34021\n",
    "chunk_size = 2\n",
    "compression = 'lzf'\n",
    "import numpy as np\n",
    "import h5py\n",
    "human = get_dataset('human', 'train', num_threads=num_threads).batch(batch_size)\n",
    "# Example data dimensions\n",
    "filename = '/data/leslie/sarthak/data/enformer/data/test.h5'\n",
    "# Function to create and save data incrementally\n",
    "batch = next(iter(human))\n",
    "print(batch['target'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17010 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/17010 [03:27<982:42:13, 207.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/17010 [07:12<1028:41:16, 217.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/17010 [10:54<1038:18:18, 219.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/17010 [14:33<1036:34:54, 219.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/17010 [18:10<1032:26:31, 218.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/17010 [21:39<1227:32:49, 259.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5376\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    float32_data = f.create_dataset(\n",
    "        'labels',\n",
    "        shape=(num_elems, 896, 5313),\n",
    "        maxshape=(num_elems, 896,5313),\n",
    "        dtype='f4',  # float32 data type\n",
    "        chunks=(chunk_size, 896, 5313),\n",
    "        compression=compression,\n",
    "    )\n",
    "    # print('created dataset')\n",
    "    # print(float32_data.shape)\n",
    "    idx=0\n",
    "    for i,batch in tqdm(enumerate(dataset), total=num_elems//batch_size):\n",
    "        label = batch['target'].numpy()\n",
    "        print('loaded batch')\n",
    "        float32_data[idx:idx+label.shape[0]] = label\n",
    "        idx += label.shape[0]\n",
    "        if i==5:\n",
    "            break\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([896, 5313])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's view it!\n",
    "batch['target'].shape #oh it was messed up because used dataset which was not batched, assigned iti to 896 different ones lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/17010 [00:00<1:30:44,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/17010 [00:00<59:47,  4.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/17010 [00:00<49:44,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/17010 [00:01<44:31,  6.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/17010 [00:01<41:30,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/17010 [00:01<38:54,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/17010 [00:02<40:43,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/17010 [00:02<42:49,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/17010 [00:02<41:25,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 896, 5313)\n",
      "(2, 896, 5313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/17010 [00:02<46:59,  6.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(human), total\u001b[38;5;241m=\u001b[39mnum_elems\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size):\n\u001b[1;32m      2\u001b[0m     label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(label\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:761\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    760\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 744\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/enformer2/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2723\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   2722\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2723\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2724\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIteratorGetNext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2725\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2726\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2727\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i,batch in tqdm(enumerate(human), total=num_elems//batch_size):\n",
    "    label = batch['target'].numpy()\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07965088 0.07824707 0.05599976 ... 0.         0.         0.00358963]\n",
      " [0.08392334 0.15319824 0.10559082 ... 0.         0.         0.30615234]\n",
      " [0.0760498  0.07672119 0.09527588 ... 0.         0.01780701 0.        ]\n",
      " ...\n",
      " [0.03396606 0.03102112 0.         ... 0.         0.         0.        ]\n",
      " [0.0748291  0.02720642 0.         ... 0.         0.         0.        ]\n",
      " [0.07141113 0.0362854  0.09655762 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "load_data = h5py.File(filename, 'r')\n",
    "data = load_data['labels'][10]\n",
    "print(data)\n",
    "load_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07965088 0.07824707 0.05599976 ... 0.         0.         0.00358963]\n",
      " [0.08392334 0.15319824 0.10559082 ... 0.         0.         0.30615234]\n",
      " [0.0760498  0.07672119 0.09527588 ... 0.         0.01780701 0.        ]\n",
      " ...\n",
      " [0.03396606 0.03102112 0.         ... 0.         0.         0.        ]\n",
      " [0.0748291  0.02720642 0.         ... 0.         0.         0.        ]\n",
      " [0.07141113 0.0362854  0.09655762 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(batch['target'].numpy()[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09265137 0.19274902 0.265625   ... 0.         0.         0.        ]\n",
      " [0.12792969 0.16992188 0.4387207  ... 0.         0.         0.        ]\n",
      " [0.12878418 0.14343262 0.44189453 ... 0.         0.         0.12255859]\n",
      " ...\n",
      " [0.06341553 0.08654785 0.27294922 ... 0.         0.         0.        ]\n",
      " [0.15966797 0.12335205 0.45141602 ... 0.9736328  0.         0.        ]\n",
      " [0.14868164 0.10693359 0.27368164 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#but let's compare thaht with the full dataset\n",
    "load_data2 = h5py.File('/data/leslie/sarthak/data/enformer/data/train_label.h5', 'r')\n",
    "data2 = load_data2['labels'][10]\n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34021, 896, 5313)\n"
     ]
    }
   ],
   "source": [
    "#finally let's just load in the real one and see if the bottom row is zero or not\n",
    "loaded = h5py.File('/data/leslie/sarthak/data/enformer/data/train_label.h5', 'r')\n",
    "print(loaded['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01947021, 0.03265381, 0.01818848, ..., 0.        , 0.04885864,\n",
       "        0.        ],\n",
       "       [0.01475525, 0.03619385, 0.00531006, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.02165222, 0.03610229, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.06347656, 0.03421021, 0.10302734, ..., 0.14416504, 1.2871094 ,\n",
       "        0.58691406],\n",
       "       [0.05636597, 0.03067017, 0.01673889, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0335083 , 0.05386353, 0.01303864, ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded['labels'][-1] #has values, let's just assume it's correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see if the sequences are correct\n",
    "seq = np.load('/data/leslie/sarthak/data/enformer/data/train_sequence.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34021, 131072)\n"
     ]
    }
   ],
   "source": [
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  9  7 10 10  8 10 10  7  7  9  7 10 10 10 10  8  8  7  9]\n",
      "[ 8  7  9 10  8 10  8  8  8 10  9 10  8 10  7  8  9  7  7 10]\n"
     ]
    }
   ],
   "source": [
    "print(seq[0,:20])\n",
    "print(seq[10,:20])\n",
    "#TGATTCTTAAGATTTTCCAG\n",
    "#CAGTCTCCCTGTCTACGAAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]], shape=(20, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#now let's see if the sequences are correct\n",
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "it = iter(human)\n",
    "ex1 = next(it)\n",
    "print(ex1['sequence'][0,:20])\n",
    "#TGATTCTTAAGATTTTCCAG\n",
    "#perfectly aligns!! at least for the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]], shape=(20, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "human = get_dataset('human', 'train', num_threads=1).batch(10)\n",
    "it = iter(human)\n",
    "ex1 = next(it)\n",
    "print(ex1['sequence'][9,:20])\n",
    "#AGGAAAGATTATTCCACGAT\n",
    "#a fundamentally different sequence, this isn't even RC..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 131072, 4)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]], shape=(20, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "human = get_dataset('human', 'train', num_threads=1).batch(1)\n",
    "it = iter(human)\n",
    "for i in range(10):\n",
    "    ex = next(it)\n",
    "print(ex['sequence'].shape)\n",
    "print(ex['sequence'][0,:20])\n",
    "#so thisi s the same as that dataset, huh I guess maybe the order does shift around??\n",
    "#the other big difference might have to do with the workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.3193359375"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "896*5313/131072\n",
    "#so 36x as much data in the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-0.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-1.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-2.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-3.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-4.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-5.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-6.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-7.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-8.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-9.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-10.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-11.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-12.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-13.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-14.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-15.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-16.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-17.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-18.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-19.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-20.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-21.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-22.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-23.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-24.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-25.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-26.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-27.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-28.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-29.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-30.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-31.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-32.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-33.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-34.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-35.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-36.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-37.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-38.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-39.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-40.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-41.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-42.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-43.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-44.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-45.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-46.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-47.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-48.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-49.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-50.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-51.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-52.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-53.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-54.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-55.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-56.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-57.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-58.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-59.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-60.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-61.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-62.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-63.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-64.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-65.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-66.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-67.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-68.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-69.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-70.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-71.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-72.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-73.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-74.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-75.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-76.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-77.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-78.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-79.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-80.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-81.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-82.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-83.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-84.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-85.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-86.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-87.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-88.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-89.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-90.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-91.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-92.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-93.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-94.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-95.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-96.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-97.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-98.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-99.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-100.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-101.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-102.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-103.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-104.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-105.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-106.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-107.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-108.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-109.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-110.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-111.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-112.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-113.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-114.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-115.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-116.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-117.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-118.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-119.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-120.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-121.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-122.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-123.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-124.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-125.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-126.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-127.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-128.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-129.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-130.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-131.tfr', '/data/leslie/sarthak/data/enformer/data/human/tfrecords/train-0-132.tfr']\n"
     ]
    }
   ],
   "source": [
    "def tfrecord_files(organism, subset): #this is the function that gets the TFR files, gets all of them and sorts them\n",
    "  # Sort the values by int(*).\n",
    "  return sorted(tf.io.gfile.glob(os.path.join(\n",
    "      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n",
    "  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n",
    "a = tfrecord_files('human', 'train')\n",
    "print(a)\n",
    "#so this is always loaded identically with the sorted\n",
    "#I'm quite confident it's due to the multiple workers and how the dataset handles that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def poisson(yt, yp, epsilon: float = 1e-7):\n",
    "    \"\"\"Poisson loss, without mean reduction.\"\"\"\n",
    "    return yp - yt * tf.math.log(yp + epsilon)\n",
    "\n",
    "\n",
    "def poisson_multinomial(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    total_weight: float = 1,\n",
    "    weight_range: float = 1,\n",
    "    weight_exp: int = 4,\n",
    "    epsilon: float = 1e-7,\n",
    "    rescale: bool = False,\n",
    "):\n",
    "    \"\"\"Possion decomposition with multinomial specificity term.\n",
    "\n",
    "    Args:\n",
    "        total_weight (float): Weight of the Poisson total term.\n",
    "        epsilon (float): Added small value to avoid log(0).\n",
    "        rescale (bool): Rescale loss after re-weighting.\n",
    "    \"\"\"\n",
    "    seq_len = y_true.shape[1]\n",
    "    \n",
    "    if weight_range < 1:\n",
    "        raise ValueError(\"Poisson Multinomial weight_range must be >=1\")\n",
    "    elif weight_range == 1:\n",
    "        position_weights = tf.ones((1, seq_len, 1))\n",
    "    else:\n",
    "        pos_start = -(seq_len / 2 - 0.5)\n",
    "        pos_end = seq_len / 2 + 0.5\n",
    "        positions = tf.range(pos_start, pos_end, dtype=tf.float32)\n",
    "        sigma = -pos_start / (np.log(weight_range)) ** (1 / weight_exp)\n",
    "        position_weights = tf.exp(-((positions / sigma) ** weight_exp))\n",
    "        position_weights /= tf.reduce_max(position_weights)\n",
    "        position_weights = tf.expand_dims(position_weights, axis=0)\n",
    "        position_weights = tf.expand_dims(position_weights, axis=-1)\n",
    "\n",
    "    y_true = tf.math.multiply(y_true, position_weights)\n",
    "    y_pred = tf.math.multiply(y_pred, position_weights)\n",
    "\n",
    "    # sum across lengths\n",
    "    s_true = tf.math.reduce_sum(y_true, axis=-2) # B x T\n",
    "    s_pred = tf.math.reduce_sum(y_pred, axis=-2) # B x T\n",
    "\n",
    "    # total count poisson loss, mean across targets\n",
    "    poisson_term = poisson(s_true, s_pred)  # B x T\n",
    "    poisson_term /= tf.reduce_sum(position_weights)\n",
    "\n",
    "    # add epsilon to protect against tiny values\n",
    "    y_true += epsilon\n",
    "    y_pred += epsilon\n",
    "\n",
    "    # normalize to sum to one\n",
    "    p_pred = y_pred / tf.expand_dims(s_pred, axis=-2) # B x L x T\n",
    "\n",
    "    # multinomial loss\n",
    "    pl_pred = tf.math.log(p_pred)  # B x L x T\n",
    "    multinomial_dot = -tf.math.multiply(y_true, pl_pred)  # B x L x T\n",
    "    multinomial_term = tf.math.reduce_sum(multinomial_dot, axis=-2)  # B x T\n",
    "    multinomial_term /= tf.reduce_sum(position_weights)\n",
    "\n",
    "    # normalize to scale of 1:1 term ratio\n",
    "    loss_raw = multinomial_term + total_weight * poisson_term\n",
    "    if rescale:\n",
    "        loss_rescale = loss_raw * 2 / (1 + total_weight)\n",
    "    else:\n",
    "        loss_rescale = loss_raw\n",
    "\n",
    "    return loss_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10, 1)\n",
      "tf.Tensor(\n",
      "[[[0.6667078]\n",
      "  [0.8621566]\n",
      "  [0.9621714]\n",
      "  [0.9950683]\n",
      "  [1.       ]\n",
      "  [1.       ]\n",
      "  [0.9950683]\n",
      "  [0.9621714]\n",
      "  [0.8621566]\n",
      "  [0.6667078]]], shape=(1, 10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#let's test this position weight stuff\n",
    "weight_range = 1.5\n",
    "seq_len = 10\n",
    "weight_exp =4\n",
    "\n",
    "pos_start = -(seq_len / 2 - 0.5)\n",
    "pos_end = seq_len / 2 + 0.5\n",
    "positions = tf.range(pos_start, pos_end, dtype=tf.float32)\n",
    "sigma = -pos_start / (np.log(weight_range)) ** (1 / weight_exp)\n",
    "position_weights = tf.exp(-((positions / sigma) ** weight_exp))\n",
    "position_weights /= tf.reduce_max(position_weights)\n",
    "position_weights = tf.expand_dims(position_weights, axis=0)\n",
    "position_weights = tf.expand_dims(position_weights, axis=-1)\n",
    "\n",
    "print(position_weights.shape)\n",
    "print(position_weights) #so weights the center more... that does make a lot of sense\n",
    "#and up to 1.5* as much in the middle as compared to the ends... hmmm I think we'll just go with the same as the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.5488135  0.71518937 0.60276338 ... 0.63212081 0.14788355 0.95568239]\n",
      "  [0.12785748 0.77676417 0.31861386 ... 0.30325158 0.26912988 0.70332816]\n",
      "  [0.15640513 0.84344823 0.86062903 ... 0.03735818 0.22248574 0.85637566]\n",
      "  ...\n",
      "  [0.89456813 0.59438585 0.66791604 ... 0.05151961 0.91356269 0.55993623]\n",
      "  [0.43623324 0.95413652 0.44569275 ... 0.63626726 0.32920214 0.24306976]\n",
      "  [0.36453648 0.68097476 0.07672555 ... 0.62856675 0.25360279 0.65144222]]]\n"
     ]
    }
   ],
   "source": [
    "#the thing is I don't believe that's used for the loss, I think they just use poisson\n",
    "loss_fn = tf.keras.losses.Poisson(reduction=tf.keras.losses.Reduction.NONE)\n",
    "#loss = y_pred - y_true * log(y_pred)\n",
    "#let's run a test data through the loss function\n",
    "#set the numpy random seed\n",
    "np.random.seed(0)\n",
    "yt = np.random.rand(1, 896, 5313)\n",
    "yp = np.random.rand(1, 896, 5313)\n",
    "print(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.00022292 1.00757444 1.00415909 1.00359941 1.00718081 1.00416183\n",
      "  0.99344254 0.99909258 1.00300491 0.99306333 0.99845058 0.99022168\n",
      "  1.00071287 0.99707371 0.98563379 1.00084507 0.9928121  1.00180829\n",
      "  1.00850582 1.00211418 1.00247204 0.99590069 1.00041306 1.00224841\n",
      "  1.01315427 0.9951458  1.00442684 1.00265813 1.00241566 0.99757701\n",
      "  0.99688488 1.00116313 0.99127781 0.9946245  0.99067163 0.99931455\n",
      "  0.99310327 1.00716162 1.00698757 0.99767929 0.9996357  0.99766952\n",
      "  1.01239705 0.99424928 1.00188434 0.99451548 1.00510573 0.99444735\n",
      "  1.00790846 0.9991039  1.00330055 0.99709892 0.99819225 1.00282311\n",
      "  1.00023794 1.00487578 0.99372268 0.99318743 1.00604045 1.00781834\n",
      "  1.01526725 0.99130601 1.00461531 1.00587463 1.00496805 0.99951851\n",
      "  0.99620724 1.00721371 0.99886042 1.00199795 0.98976231 0.9955222\n",
      "  1.00958121 1.00602722 0.98738235 0.99414504 1.00160146 0.9888891\n",
      "  0.99438679 1.0021646  0.98880106 1.00092995 0.9960779  1.00324845\n",
      "  0.99835968 1.00442517 1.00486374 0.99958879 1.00351012 1.00139475\n",
      "  0.99102652 1.00127411 1.00252485 0.97760385 1.00902724 0.99009389\n",
      "  1.00761378 0.99190491 0.99126583 1.0048995  0.9993031  0.99714661\n",
      "  1.00838339 1.00199199 1.00118101 1.00195551 1.00965214 0.99026781\n",
      "  1.00705206 1.00423765 0.99404383 1.00576472 0.99294591 0.99837881\n",
      "  0.99955785 1.00138104 0.99721831 0.99534386 0.98707134 1.00146914\n",
      "  0.99160945 1.00024486 0.99206519 1.00999808 1.00049877 0.99651718\n",
      "  0.9873448  0.99288464 1.00512016 0.99311835 1.00510538 1.00163782\n",
      "  0.99850225 1.00019157 0.99755484 0.99206454 1.01019824 0.99639064\n",
      "  0.99767405 0.99740463 0.99313134 0.99363381 1.00993073 0.99915099\n",
      "  0.99872738 1.00183797 1.00626922 0.99957961 0.99946922 0.99643284\n",
      "  1.00083447 1.00755262 0.98129755 1.00715446 1.00566947 1.0034163\n",
      "  0.99589539 0.99914795 0.99658251 1.00058246 0.99868864 1.00148809\n",
      "  1.00172758 1.00313437 1.00509608 0.98978174 1.00744855 0.98965299\n",
      "  0.99287015 0.9993062  1.00194895 1.00475311 1.00181925 0.99927241\n",
      "  0.99922878 0.98936778 1.00509953 1.00197673 1.00308192 0.98925245\n",
      "  0.98977935 0.99544322 1.00394559 0.98732752 0.9900943  1.00263822\n",
      "  0.99964756 0.99331808 0.98691225 1.00557792 0.99699193 0.99952883\n",
      "  1.01304579 0.98286438 0.99700898 1.00298584 1.01650953 0.9929744\n",
      "  0.99609655 1.00013196 0.99965221 0.99702698 0.99928552 0.99447566\n",
      "  0.99618447 1.00623548 1.00837183 0.99606681 0.99287069 0.99795395\n",
      "  1.0049454  1.00806463 0.99496436 1.00470901 1.00754213 0.98834199\n",
      "  1.00208342 0.99152994 0.99006134 1.00417972 1.00161254 0.99716455\n",
      "  0.99797457 0.99398631 1.00617731 0.99291497 0.99896234 0.99749994\n",
      "  0.99862039 0.9973892  0.99411511 1.00933707 0.99837434 0.99466991\n",
      "  0.99617028 1.00046623 0.99105108 1.00689483 0.99360818 1.00328755\n",
      "  0.98974079 0.99678063 1.01797032 0.99120885 1.00290048 0.99368215\n",
      "  1.01455092 1.00323558 0.99894375 0.99823958 0.99670267 1.00106955\n",
      "  1.02273667 1.0082711  0.99580675 1.00363088 0.9916634  0.99350697\n",
      "  1.00916338 1.00007617 1.00246668 0.99753469 0.9868241  1.00227988\n",
      "  0.99855143 1.01007402 0.99706757 0.99805355 1.00242782 0.9914974\n",
      "  0.99381757 0.99828988 1.0011965  0.9952057  1.00115299 0.99240839\n",
      "  1.00668383 1.00329757 1.00163591 1.00303984 1.00847471 0.99975592\n",
      "  1.00231707 1.01004446 1.00985575 1.00949538 1.00253081 1.01257014\n",
      "  0.97989106 1.00292408 1.01564717 1.00140297 1.00105858 1.00988197\n",
      "  0.99989724 1.00792682 0.99736643 1.00727069 0.99176288 0.9974159\n",
      "  1.00225925 0.99644679 1.00456548 1.01357794 0.98546737 1.00187898\n",
      "  1.00237501 0.9938134  1.00287724 1.00314879 1.00331926 0.997621\n",
      "  0.9878462  1.01247692 1.0050106  0.99593115 0.99972928 1.0042541\n",
      "  0.98491925 0.99594986 1.00302219 1.00974238 0.99992806 0.9973551\n",
      "  0.9993946  1.00976884 1.00022006 0.99913031 0.99913269 1.00385666\n",
      "  0.98438603 0.99976391 1.01164281 1.00883126 0.9872058  0.99377733\n",
      "  1.00252378 1.0029453  1.00560451 0.99977601 1.00782049 0.98733789\n",
      "  0.99530005 0.99256891 1.00408924 1.0069896  1.00158918 1.00629318\n",
      "  0.99639899 0.99786961 0.99197477 0.99248278 1.01115477 0.9950527\n",
      "  0.99748468 1.00531733 1.00050724 1.00482619 1.00769866 1.0011245\n",
      "  1.00346005 1.00036824 0.99688125 1.00631309 0.99188048 1.00182354\n",
      "  0.9998588  0.99913245 1.00822711 1.00130224 1.00941694 1.00221372\n",
      "  1.01167619 1.00468814 0.99960905 0.99709123 0.99739605 1.00517607\n",
      "  1.00020933 1.01552761 1.00292456 0.99350291 1.00746751 0.99609131\n",
      "  1.00046229 1.01070452 0.99385381 1.01739204 1.00267267 1.00453496\n",
      "  0.99816954 1.01392531 1.01526546 1.00386512 1.00142682 0.9963572\n",
      "  1.00516999 0.99326152 0.99249583 0.99596936 1.00398993 1.00335598\n",
      "  1.00204325 0.99869835 1.00719774 1.00278366 1.00518179 0.99919415\n",
      "  1.00964236 1.00387299 1.01571655 1.0095917  1.00023818 1.00577343\n",
      "  0.99808609 0.99139649 1.00180709 0.99832523 0.99378449 0.99487668\n",
      "  0.9855116  1.00298011 0.993595   0.99383479 1.01684225 0.99043339\n",
      "  0.99952799 0.99635488 1.00492966 1.00890195 1.00230074 0.99493617\n",
      "  0.99713606 0.9915098  1.00420547 1.00279188 1.00402331 0.99528885\n",
      "  0.99486125 0.99579024 1.00976562 0.99796915 0.99555415 0.99508995\n",
      "  1.00784981 0.99663019 0.994807   1.00296032 1.00266159 1.007236\n",
      "  1.00748181 0.99483103 1.00282478 1.00976145 1.00402927 0.98640186\n",
      "  1.00600231 0.99426007 0.98395288 0.99757785 1.00299954 0.99756068\n",
      "  1.01048934 0.99586892 0.99589509 0.99613595 0.99950916 0.99605519\n",
      "  0.99524981 0.99754256 1.00661707 1.00078237 0.99908453 1.00070417\n",
      "  1.00342631 1.00358522 0.99790603 0.99403197 1.00388968 1.00012016\n",
      "  1.0060066  0.99260783 1.0003233  1.00200117 0.99169731 0.99252772\n",
      "  1.00108445 1.00629199 1.00054848 0.99958473 1.00034583 1.00856519\n",
      "  0.99674135 0.99074703 1.00652373 1.00307381 0.9928295  1.00126803\n",
      "  0.996566   1.00737917 1.00253379 0.99647248 0.98858052 1.00254416\n",
      "  0.99702358 0.98688221 0.99470448 1.00145829 1.00394225 0.99174738\n",
      "  1.01001751 0.99970663 0.99590904 1.00469756 0.99960631 1.00308287\n",
      "  0.999596   1.0016979  0.99923563 1.01009798 0.9997828  1.00106549\n",
      "  0.99505371 0.9956637  0.99434924 1.00427318 0.99496198 0.99427646\n",
      "  1.00302458 0.99271715 0.98343623 1.00615156 0.99688441 0.99259335\n",
      "  0.99814355 0.99454284 1.0083673  0.99317658 0.99432385 0.99526268\n",
      "  0.996804   1.00317478 0.99602723 0.98960072 0.99613261 0.98895884\n",
      "  0.98861527 0.9966498  0.99917692 1.00190544 1.00523055 1.00126648\n",
      "  1.00993848 0.99312097 0.99158305 1.00287378 1.01050675 1.00576925\n",
      "  0.99534643 1.00569546 1.00392783 1.00143087 0.99747878 0.99843156\n",
      "  0.99223334 1.00195432 1.00663257 1.01318836 0.9834699  0.99959266\n",
      "  1.00374389 0.98727548 0.99544871 0.98661327 1.01080954 1.00601888\n",
      "  1.00120044 0.99787498 0.99721915 0.99332774 1.00739396 1.00725102\n",
      "  1.00049961 1.00220573 0.99107766 1.00492811 0.99741739 1.00227404\n",
      "  0.99682856 1.0073911  1.01142776 1.00801778 1.00219131 1.00054085\n",
      "  0.99747473 0.99812812 0.99344552 0.99424094 0.99898154 0.99038225\n",
      "  0.98899931 1.00164831 1.00062919 0.99929422 1.01577783 1.00960505\n",
      "  0.99793208 1.0014075  0.99458593 0.99687505 1.00508773 0.99859369\n",
      "  1.00414622 1.00786316 1.00617158 1.00447917 1.00329363 0.98687023\n",
      "  1.00145507 1.0029124  1.01907992 0.99024659 0.99870396 0.99503553\n",
      "  1.00536191 1.00677323 0.99754393 1.00718415 0.99215585 1.00083971\n",
      "  0.99826443 0.99488133 0.99237901 0.99422497 0.99170917 0.99456799\n",
      "  0.99882144 1.00866628 0.99681586 0.99283546 0.99684817 1.00049353\n",
      "  1.00052452 0.99297792 0.99619758 1.00044608 1.01047707 0.99683481\n",
      "  0.99989688 1.00175905 0.99758971 1.00105524 1.00930333 0.99307823\n",
      "  0.99618673 0.98166806 0.99772394 0.98585123 1.0065186  0.99838692\n",
      "  1.00697565 0.99060935 0.99488378 0.99369693 1.00257087 0.98531801\n",
      "  0.99472553 0.99056727 1.00404572 1.00832236 1.00075305 0.99332291\n",
      "  0.99499291 0.99932486 0.99363756 1.00521576 0.98955238 1.00645006\n",
      "  0.99925828 0.98483795 1.01203585 1.00169802 0.9917329  0.98337287\n",
      "  1.00253499 1.00263882 1.00016057 0.99854767 1.00522542 0.99348134\n",
      "  1.01089263 0.99759048 1.00050569 0.99491477 1.02825499 0.98858845\n",
      "  0.99430811 0.98674524 1.00730991 1.00103426 1.00188291 0.99870211\n",
      "  1.00790548 1.00096238 1.00407171 1.00812757 0.99372452 1.00247014\n",
      "  0.99799389 1.00472236 0.99887472 0.99537784 1.00939751 1.00478077\n",
      "  0.98926854 0.99371934 1.00669801 0.99724442 0.99635184 1.00612259\n",
      "  1.00230229 1.00838363 0.99916631 0.99598801 1.00767684 1.00924397\n",
      "  1.01164222 0.99432105 0.99968219 1.01117849 1.00638413 1.00440133\n",
      "  1.01199222 0.99473864 0.9907068  0.99890876 0.99924666 0.9948312\n",
      "  1.00548816 1.00081873 1.0058074  0.99191284 1.00247467 0.99912399\n",
      "  1.00489259 0.99806184 0.99894947 1.00030506 0.99178809 0.989604\n",
      "  1.00972569 1.00032806 0.98626119 1.00200307 0.9949168  1.00520134\n",
      "  0.99377376 0.99995345 1.01321399 0.99871653 0.99191839 1.00248528\n",
      "  0.99678892 1.00272083 0.99353945 1.00122297 1.00593817 1.00034666\n",
      "  1.00807607 1.00890648 0.98039865 0.99617606 0.9945761  1.00510502\n",
      "  0.99251866 0.99064142 1.01098192 1.00138831 0.99914062 0.99549717\n",
      "  1.0132879  1.01604569 0.99669379 1.00779128 0.99946713 1.0155015\n",
      "  1.00148368 1.00715375 1.00201821 1.00245535 1.00423133 0.99565935\n",
      "  0.99533379 1.00785649 1.00452363 0.99556214 1.00312698 1.00270414\n",
      "  1.00675666 1.00003958 0.9850589  0.99730504 1.00482011 1.01048315\n",
      "  1.00373018 1.00487649 0.98727179 1.0063765  0.99814439 1.00376141\n",
      "  0.99321443 1.00433624 1.00382042 1.00207913 1.00988734 1.00282586\n",
      "  0.99772537 1.0045979  1.00289977 0.99996954 1.00028276 1.00302327\n",
      "  1.0041759  0.99684316 0.99899817 0.99764788 1.00098002 1.01013267\n",
      "  0.99942571 0.98029429 0.99108118 1.00053513 1.0014838  1.00663185\n",
      "  0.99583733 0.99505132 0.99888855 1.00773394 1.0009954  1.00684369\n",
      "  1.00496817 1.00628388 0.99896288 0.9948166  1.00248528 1.00808191\n",
      "  0.99459755 1.00659227 0.99968654 0.99734229 0.99362296 1.00058377\n",
      "  0.99486387 1.00392282 0.99737108 1.00648308 1.01315498 1.00414765\n",
      "  1.0016185  0.99557787 1.00725174 1.00753772 1.00363696 1.00676847\n",
      "  0.99341917 0.99004036 1.00472212 0.99748737 1.0115453  0.9996413\n",
      "  0.98694283 1.00312102 1.01240933 1.01753199 1.00056255 1.00664663\n",
      "  1.00534379 0.99793923 0.99639916 0.99552035 0.99230975 0.99281847\n",
      "  0.99552763 0.99456745 0.99957424 0.99907571 0.99823219 1.00364649\n",
      "  0.99145567 0.99556911 0.9970721  1.00996816 0.99695343 0.99780369\n",
      "  0.99305266 0.99449563]], shape=(1, 896), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "#now print it out\n",
    "print(a:=loss_fn(yp, yt)) #the shape is perfect, as that is the loss per class, so we need to keep them separate for the multitasking, then later sum, but can see how it performs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000541438348591\n"
     ]
    }
   ],
   "source": [
    "#find mean of a\n",
    "print(np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class PoissonLoss(nn.Module):\n",
    "    def __init__(self, reduction='none'):\n",
    "        super(PoissonLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = y_pred - y_true * torch.log(y_pred)\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "#let's test this\n",
    "yt = torch.tensor(yt)\n",
    "yp = torch.tensor(yp)\n",
    "\n",
    "loss_fn = PoissonLoss(reduction='none')\n",
    "loss = loss_fn(yp,yt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see it for a specific output\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "enformer_model = hub.load(\"https://kaggle.com/models/deepmind/enformer/frameworks/TensorFlow2/variations/enformer/versions/1\").model\n",
    "\n",
    "SEQ_LENGTH = 393_216\n",
    "\n",
    "# Numpy array [batch_size, SEQ_LENGTH, 4] one hot encoded in order 'ACGT'. The\n",
    "# `one_hot_encode` function is available in `enformer.py` and outputs can be\n",
    "# stacked to form a batch.\n",
    "enformer_model\n",
    "inputs = tf.zeros((1, SEQ_LENGTH, 4), dtype=tf.float32)\n",
    "predictions = enformer_model.predict_on_batch(inputs)\n",
    "predictions['human'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
