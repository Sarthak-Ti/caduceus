{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimating cell type specific tokens\n",
    "#we will test the tokenizer and data loader using the ctst\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.datasets.DNase_ctst_dataset import DNaseCtstDataset\n",
    "\n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "# from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "from src.models.sequence.dna_embedding import DNAEmbeddingModel\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "\n",
    "# d_output = 161\n",
    "\n",
    "tokenizer = CharacterTokenizer( #make sure to fix the tokenizer too\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = DNaseCtstDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True', filter = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS]': 0, '[SEP]': 1, '[BOS]': 2, '[MASK]': 3, '[PAD]': 4, '[RESERVED]': 5, '[UNK]': 6, 'A': 7, 'C': 8, 'G': 9, 'T': 10, 'N': 11}\n",
      "tensor([12,  8,  7,  ...,  9,  7,  8])\n"
     ]
    }
   ],
   "source": [
    "a, b = ccre[0]\n",
    "print(ccre.tokenizer._vocab_str_to_int)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([172,   8,   7,  ...,   9,   7,   8])\n",
      "tensor([12, 10,  8,  ...,  7, 10, 10])\n",
      "tensor([171,  10,   8,  ...,   7,  10,  10])\n",
      "tensor([172,  10,   8,  ...,   7,  10,  10])\n",
      "tensor([12, 10,  8,  ...,  8,  7,  8])\n"
     ]
    }
   ],
   "source": [
    "a,b = ccre[160] #rest of the sequence is exactly the same!!\n",
    "print(a)\n",
    "a,b = ccre[161]\n",
    "print(a) #back to 12!\n",
    "a,b = ccre[320]\n",
    "print(a)\n",
    "a,b = ccre[321]\n",
    "print(a)\n",
    "a,b = ccre[322] #should be a new thing, and indeed it is!\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(a)) #now o1024 cuz we appended this on, no eos token still"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
