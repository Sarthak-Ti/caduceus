{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n",
      "**Using Char-level tokenizer**\n",
      "72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 195.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 218.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#use logits and dataloader to evaluate our models\n",
    "\n",
    "#we can also load via the logits\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# https://github.com/openai/gpt-2/issues/131#issuecomment-492786058\n",
    "# def preprocess(text):\n",
    "#     text = text.replace(\"“\", '\"')\n",
    "#     text = text.replace(\"”\", '\"')\n",
    "#     return '\\n'+text.strip()\n",
    "\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "            \n",
    "        results = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        for seq in tqdm(seqs):\n",
    "            \n",
    "            if isinstance(self.tokenizer, Tokenizer):\n",
    "                tokenized_seq = self.tokenizer.encode(seq).ids\n",
    "            else:\n",
    "                tokenized_seq = self.tokenizer.encode(seq)\n",
    "            \n",
    "            # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "            logits, __ = self.model(torch.tensor([tokenized_seq]).to(device=self.device))\n",
    "\n",
    "            # Using head, so just have logits\n",
    "            results.append(logits)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "        else:\n",
    "            raise NotImplementedError(\"You need to provide a custom tokenizer!\")\n",
    "\n",
    "        return model, tokenizer\n",
    "        \n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# SAFARI_PATH = os.getenv('SAFARI_PATH', '.')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"--model_cfg\",\n",
    "#     default=f\"{SAFARI_PATH}/configs/evals/hyena_small_150b.yaml\",\n",
    "# )\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_path\",\n",
    "#     default=f\"\",\n",
    "#     help=\"Path to model state dict checkpoint\"\n",
    "# )\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "    \n",
    "# task = HG38Encoder(args.model_cfg, args.ckpt_path, max_seq_len=1024)\n",
    "\n",
    "trained = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "original = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', max_seq_len=1024)\n",
    "# # sample sequence, can pass a list of seqs (themselves a list of chars)\n",
    "seqs = [\"ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\"]\n",
    "print(len(seqs[0]))\n",
    "logits = trained.encode(seqs)\n",
    "# print(logits)\n",
    "print(logits[0].logits.shape)\n",
    "logits2 = original.encode(seqs)\n",
    "print(logits2[0].logits.shape)\n",
    "# # breakpoint()\n",
    "\n",
    "# predict 73 when th elength is 72 because they also predict that last one that could be eos token\n",
    "# predict through the input but remember we have an output for every single input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 216.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73, 16])\n",
      "None\n",
      "1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#need to modify this class to also get the y prediction sequence and make sure it makes sense\n",
    "\n",
    "#use logits and dataloader to evaluate our models\n",
    "\n",
    "#we can also load via the logits\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# https://github.com/openai/gpt-2/issues/131#issuecomment-492786058\n",
    "# def preprocess(text):\n",
    "#     text = text.replace(\"“\", '\"')\n",
    "#     text = text.replace(\"”\", '\"')\n",
    "#     return '\\n'+text.strip()\n",
    "\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "            \n",
    "        results = []\n",
    "        # outputs = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        for seq in tqdm(seqs):\n",
    "            \n",
    "            if isinstance(self.tokenizer, Tokenizer):\n",
    "                tokenized_seq = self.tokenizer.encode(seq).ids\n",
    "            else:\n",
    "                tokenized_seq = self.tokenizer.encode(seq)\n",
    "            \n",
    "            # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "            logits, _ = self.model(torch.tensor([tokenized_seq]).to(device=self.device))\n",
    "\n",
    "            # Using head, so just have logits\n",
    "            results.append(logits)\n",
    "            # outputs.append(output)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cuda:0')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "trained = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "# original = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', max_seq_len=1024)\n",
    "# # sample sequence, can pass a list of seqs (themselves a list of chars)\n",
    "seqs = [\"ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\"]\n",
    "y_hat = trained.encode(seqs)\n",
    "print(y_hat[0].logits.shape)\n",
    "print(y[0])\n",
    "print(trained.max_seq_len) #it is the 1024, but it's not padding here\n",
    "# print(y_hat[0].logits[0,16,:])\n",
    "# print(y[0][0,16,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(trained.tokenizer, Tokenizer)) #so doesn't do .ids, just tokenizes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[7, 1]\n",
      "[7, 1]\n"
     ]
    }
   ],
   "source": [
    "a = trained.tokenizer('A')['input_ids'] #completely identical tho\n",
    "b = trained.tokenizer.encode('A')\n",
    "print(a == b)\n",
    "print(a)\n",
    "print(b)\n",
    "#just all 4 and then 7 and eos token of 1\n",
    "#we need to do this padding, as this is what the model sees since it's always the same length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]}\n",
      "1026\n"
     ]
    }
   ],
   "source": [
    "print(a:=trained.tokenizer('A', padding=\"max_length\"))\n",
    "#it's 1024 length\n",
    "print(len(a['input_ids'])) #1024 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]}\n",
      "1026\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=trained.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding = 'max_length',\n",
    "                padding_side='left'\n",
    "            )\n",
    "print(a:=tokenizer('A', padding=\"max_length\"))\n",
    "print(len(a['input_ids'])) #1024 + 2\n",
    "#see how many 4s there are in input ids\n",
    "print(a['input_ids'].count(4)) #1024\n",
    "#so makes it go to 1026, but we need to cut off so total length is 1023 actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(trained.device) #eyo we got cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with the padding, almost finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1023, 16])\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#let's tokenize it with this option\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "            \n",
    "        results = []\n",
    "        # outputs = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        for seq in tqdm(seqs):\n",
    "            \n",
    "            if isinstance(self.tokenizer, Tokenizer):\n",
    "                tokenized_seq = self.tokenizer.encode(seq, ).ids\n",
    "            else:\n",
    "                tokenized_seq = self.tokenizer(seq, padding=\"max_length\")['input_ids']\n",
    "            tokenized_seq = tokenized_seq[3:] #make it so the length is 1023 as that's what we expect\n",
    "            # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "            logits, _ = self.model(torch.tensor([tokenized_seq]).to(device=self.device))\n",
    "\n",
    "            # Using head, so just have logits\n",
    "            results.append(logits)\n",
    "            # outputs.append(output)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cuda:0')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "trained = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "# original = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', max_seq_len=1024)\n",
    "# # sample sequence, can pass a list of seqs (themselves a list of chars)\n",
    "seqs = [\"ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\"]\n",
    "y_hat = trained.encode(seqs)\n",
    "print(y_hat[0].logits.shape)\n",
    "print(y[0]) #wait this is none, ignore this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1023])\n",
      "(tensor([ 4,  4,  4,  ...,  8, 10,  9]), tensor([ 4,  4,  4,  ..., 10,  9,  1]))\n"
     ]
    }
   ],
   "source": [
    "from src.dataloaders.datasets.ccre_dataset import CcreDataset\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "#make sure this is correct from the yaml files\n",
    "tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = CcreDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True')\n",
    "print(ccre[0][0].shape) #perfect, the length we expected, and it's already tokenized\n",
    "print(ccre[0])\n",
    "#add eos makes it so that it returns the right sequence length, adds the eos and bos and pads to make the right length, then remove box\n",
    "#eos token only added to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7,\n",
      "        7])\n",
      "[SEP]\n",
      "{'[CLS]': 0, '[SEP]': 1, '[BOS]': 2, '[MASK]': 3, '[PAD]': 4, '[RESERVED]': 5, '[UNK]': 6, 'A': 7, 'C': 8, 'G': 9, 'T': 10, 'N': 11}\n"
     ]
    }
   ],
   "source": [
    "#let's make sure that the number of 4s matches what we expect\n",
    "print(ccre[0][0][0:25]) #1024\n",
    "#yes exactly 23 4s, so we're good, that's the same as in the data\n",
    "print(tokenizer.eos_token)\n",
    "#now print the full vocab\n",
    "print(tokenizer._vocab_str_to_int)\n",
    "#so we can see that the sep is the eos token for some reason, and that everything looks as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105252\n"
     ]
    }
   ],
   "source": [
    "#now we actually load some data in batches\n",
    "print(len(ccre))\n",
    "#a decent number of items, but small enough that evaluation will be easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n",
      "torch.Size([1, 1023])\n",
      "torch.Size([1, 1023, 16])\n"
     ]
    }
   ],
   "source": [
    "#now put thorugh model, get logits and calculate ce loss\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "        # seqs = seqs.to(device=self.device)\n",
    "        results = []\n",
    "        # outputs = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        \n",
    "        # if isinstance(self.tokenizer, Tokenizer):\n",
    "        #     tokenized_seq = self.tokenizer.encode(seq, ).ids\n",
    "        # else:\n",
    "        #     tokenized_seq = self.tokenizer(seq, padding=\"max_length\")['input_ids']\n",
    "        # tokenized_seq = tokenized_seq[3:] #make it so the length is 1023 as that's what we expect\n",
    "        # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "        logits = self.model(seqs)[0]\n",
    "\n",
    "        # Using head, so just have logits\n",
    "        results.append(logits)\n",
    "        # outputs.append(output)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cuda:0')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "trained = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "\n",
    "seq, target = ccre[0]\n",
    "#now get the logits\n",
    "# print(seq.shape)\n",
    "#unsqueeze to make it a batch\n",
    "seq = seq.unsqueeze(0)\n",
    "seq = seq.to(device=trained.device)\n",
    "print(seq.shape)\n",
    "y_hat = trained.encode(seq)\n",
    "print(y_hat[0].logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1023])\n",
      "tensor(4)\n",
      "tensor([ 0.8840, -0.4067,  0.7564,  0.7946,  9.7755,  0.8988,  0.8157,  0.2324,\n",
      "        -0.2248, -0.4690, -0.5704,  0.4473,  0.7938,  0.8818,  0.8177,  0.8402],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#compare the targets\n",
    "print(target.shape)\n",
    "print(target[16])\n",
    "print(y_hat[0].logits[0,16,:])\n",
    "#understands the pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n",
      "tensor([-10.4342,  -5.4222, -10.2867, -10.5126, -10.5325, -10.2794, -10.3038,\n",
      "          0.5174,   0.6755,   1.0137,   0.2429,  -9.4974, -10.2678, -10.0635,\n",
      "        -10.2559, -10.3255], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'G'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now test a different one\n",
    "print(target[200])\n",
    "print(y_hat[0].logits[0,200,:])\n",
    "#yeah doing that well too\n",
    "tokenizer.decode(9) #it's a G!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1023])\n"
     ]
    }
   ],
   "source": [
    "#finally let's calculation ce loss\n",
    "import torch.nn.functional as F\n",
    "a=F.cross_entropy(y_hat[0].logits[0,:,:], target.to(trained.device), reduction='none')\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ab937c68c90>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#now we plot a\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(a.detach().cpu().numpy())\n",
    "\n",
    "#0 loss on the pad, then its high on the rest of the sequence, which makes sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(452)\n"
     ]
    }
   ],
   "source": [
    "#now find 'accuracy' using the argmax\n",
    "b = torch.argmax(y_hat[0].logits[0,:,:], dim=1).to('cpu')\n",
    "#find the number that match target\n",
    "print((b==target).sum())\n",
    "#452 out of 1023, not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# full loop calculating loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n",
      "**Using Char-level tokenizer**\n"
     ]
    }
   ],
   "source": [
    "#now put thorugh model, get logits and calculate ce loss\n",
    "\n",
    "from src.dataloaders.datasets.ccre_dataset import CcreDataset\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "#make sure this is correct from the yaml files\n",
    "tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = CcreDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True')\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "        # seqs = seqs.to(device=self.device)\n",
    "        results = []\n",
    "        # outputs = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        \n",
    "        # if isinstance(self.tokenizer, Tokenizer):\n",
    "        #     tokenized_seq = self.tokenizer.encode(seq, ).ids\n",
    "        # else:\n",
    "        #     tokenized_seq = self.tokenizer(seq, padding=\"max_length\")['input_ids']\n",
    "        # tokenized_seq = tokenized_seq[3:] #make it so the length is 1023 as that's what we expect\n",
    "        # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "        logits = self.model(seqs)[0]\n",
    "\n",
    "        # Using head, so just have logits\n",
    "        results.append(logits)\n",
    "        # outputs.append(output)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cuda:0')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "trained = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "original = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', max_seq_len=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 105252/105252 [16:02<00:00, 109.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2147)\n",
      "tensor(0.4176)\n"
     ]
    }
   ],
   "source": [
    "full_loss = torch.zeros((len(ccre), 1023))\n",
    "full_acc = torch.zeros((len(ccre), 1023))\n",
    "for idx, (seq, target) in tqdm(enumerate(ccre), total=len(ccre)):\n",
    "    target = target.to(trained.device)\n",
    "    seq = seq.unsqueeze(0)\n",
    "    seq = seq.to(device=trained.device)\n",
    "    y_hat = trained.encode(seq)\n",
    "    a=F.cross_entropy(y_hat[0].logits[0,:,:], target, reduction='none')\n",
    "    full_loss[idx,:] = a.detach().cpu()\n",
    "    b = torch.argmax(y_hat[0].logits[0,:,:], dim=1)\n",
    "    full_acc[idx,:] = (b==target).detach().cpu()\n",
    "    \n",
    "#print the full mean of both loss and acc\n",
    "print(full_loss.mean())\n",
    "print(full_acc.mean())\n",
    "\n",
    "#I reran it, but original values were 1.2147 and 0.4177 respectively for loss and acc\n",
    "#actually is .4176 the second time, but I doubt it matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 105252/105252 [16:01<00:00, 109.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4388)\n",
      "tensor(0.3931)\n"
     ]
    }
   ],
   "source": [
    "#repeat for the original model\n",
    "\n",
    "full_loss2 = torch.zeros((len(ccre), 1023))\n",
    "full_acc2 = torch.zeros((len(ccre), 1023))\n",
    "for idx, (seq, target) in tqdm(enumerate(ccre), total=len(ccre)):\n",
    "    target = target.to(trained.device)\n",
    "    seq = seq.unsqueeze(0)\n",
    "    seq = seq.to(device=trained.device)\n",
    "    y_hat = original.encode(seq)\n",
    "    a=F.cross_entropy(y_hat[0].logits[0,:,:], target, reduction='none')\n",
    "    full_loss2[idx,:] = a.detach().cpu()\n",
    "    b = torch.argmax(y_hat[0].logits[0,:,:], dim=1)\n",
    "    full_acc2[idx,:] = (b==target).detach().cpu()\n",
    "    \n",
    "#print the full mean of both loss and acc\n",
    "print(full_loss2.mean())\n",
    "print(full_acc2.mean())\n",
    "\n",
    "#Again I reran it, but original values were 1.4388 and .3931 for the loss and accuracy respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4388)\n",
      "tensor(0.3931)\n"
     ]
    }
   ],
   "source": [
    "#i didn't change what I was appending... so these values are for the original model\n",
    "print(full_loss.mean())\n",
    "print(full_acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3290/3290 [03:37<00:00, 15.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2147)\n",
      "tensor(0.4176)\n"
     ]
    }
   ],
   "source": [
    "#let's test this batched approach\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming `ccre_dataset` is your dataset\n",
    "ccre_loader = DataLoader(ccre, batch_size=32, shuffle=False) #results are identical even if you shuffle, obviously since it's just the mean\n",
    "loader_loss = torch.zeros((len(ccre), 1023))\n",
    "loader_acc = torch.zeros((len(ccre), 1023))\n",
    "startidx = 0\n",
    "for idx, (seq, target) in tqdm(enumerate(ccre_loader), total=len(ccre_loader)):\n",
    "    b_size = seq.shape[0]\n",
    "    target = target.to(trained.device)\n",
    "    seq = seq.to(device=trained.device)\n",
    "    y_hat = trained.encode(seq)\n",
    "    # print(target.shape) #32, 1023\n",
    "    # print(seq.shape) #32, 1023\n",
    "    # print(len(y_hat)) #1\n",
    "    # print(y_hat[0].logits.shape) #32, 1023, 16 as expected\n",
    "    # break\n",
    "    #now fix the accuracy and loss to work with batches\n",
    "    logits_reshaped = y_hat[0].logits.view(-1, 16)\n",
    "    target_reshaped = target.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits_reshaped, target_reshaped, reduction='none')\n",
    "    \n",
    "    losses = losses.view(b_size, 1023)\n",
    "    # print(losses.shape) #yeah as expected\n",
    "    # break\n",
    "    \n",
    "    loader_loss[startidx:startidx+b_size,:] = losses.detach().cpu()\n",
    "    \n",
    "    # a=F.cross_entropy(y_hat[0].logits[0,:,:], target, reduction='none')\n",
    "    # full_loss2[idx,:] = a.detach().cpu()\n",
    "    \n",
    "    b = torch.argmax(y_hat[0].logits, dim=2)\n",
    "    loader_acc[startidx:startidx+b_size,:] = (b==target).detach().cpu()\n",
    "    startidx += b_size\n",
    "    # full_acc2[idx,:] = (b==target).detach().cpu()\n",
    "print(loader_loss.mean())\n",
    "print(loader_acc.mean())\n",
    "#the real way to do it if we want to do it in batches\n",
    "#can increase batch size, but had a 11GB gpu for now, if had A100, could do 512\n",
    "#it does indeed work much faster, and the results are completely identical! So this is the new function we can use to evaluate our models!\n",
    "#and again this is without RC\n",
    "\n",
    "#also note this approach we use is near identical to what they do in terms of calculating loss and evlauating, so we just verified it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3290/3290 [03:53<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1938)\n",
      "tensor(0.4314)\n"
     ]
    }
   ],
   "source": [
    "#wait we had one with a lot more epochs, let's compare that too\n",
    "trained_100 = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/20-02-31-396880/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "\n",
    "e100loss = torch.zeros((len(ccre), 1023))\n",
    "e100acc = torch.zeros((len(ccre), 1023))\n",
    "startidx = 0\n",
    "for idx, (seq, target) in tqdm(enumerate(ccre_loader), total=len(ccre_loader)):\n",
    "    b_size = seq.shape[0]\n",
    "    target = target.to(trained.device)\n",
    "    seq = seq.to(device=trained.device)\n",
    "    y_hat = trained_100.encode(seq)\n",
    "    logits_reshaped = y_hat[0].logits.view(-1, 16)\n",
    "    target_reshaped = target.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits_reshaped, target_reshaped, reduction='none')\n",
    "    \n",
    "    losses = losses.view(b_size, 1023)\n",
    "    \n",
    "    e100loss[startidx:startidx+b_size,:] = losses.detach().cpu()\n",
    "    \n",
    "    b = torch.argmax(y_hat[0].logits, dim=2)\n",
    "    e100acc[startidx:startidx+b_size,:] = (b==target).detach().cpu()\n",
    "    startidx += b_size\n",
    "    # full_acc2[idx,:] = (b==target).detach().cpu()\n",
    "print(e100loss.mean())\n",
    "print(e100acc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original loss after 24: 1.2463696002960205\n",
      "trained loss after 24: 1.2398368120193481\n",
      "trained 100 epochs loss after 24: 1.2197294235229492\n",
      "original accuracy after 24: 0.4019705355167389\n",
      "trained accuracy after 24: 0.40536975860595703\n",
      "trained 100 epochs accuracy after 24: 0.419145792722702\n"
     ]
    }
   ],
   "source": [
    "#since it might be the pad let's calculate the mean from 24 onwards\n",
    "print('original loss after 24:', float(full_loss2[:,24:].mean()))\n",
    "print('trained loss after 24:', float(full_loss[:,24:].mean()))\n",
    "print('trained 100 epochs loss after 24:', float(e100loss[:,24:].mean()))\n",
    "print('original accuracy after 24:', float(full_acc2[:,24:].mean()))\n",
    "print('trained accuracy after 24:', float(full_acc[:,24:].mean()))\n",
    "print('trained 100 epochs accuracy after 24:', float(e100acc[:,24:].mean()))\n",
    "\n",
    "#yeah... it might just straight up be the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate it with the longer ones, see if the sequence matches what is expected\n",
    "so we will load the tokenizer, make sure all 1023 are translated exactly as expected, then the full length should be 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1023]) torch.Size([1023])\n"
     ]
    }
   ],
   "source": [
    "from src.dataloaders.datasets.ccre_dataset import CcreDataset\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "#make sure this is correct from the yaml files\n",
    "tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = CcreDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True')\n",
    "\n",
    "a,b = ccre[0]\n",
    "print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 7, 9,  ..., 9, 7, 8])\n",
      "tensor([7, 9, 7,  ..., 7, 8, 1])\n",
      "CAGAAGTTGGGCAAAAGCCTGATTTGAGGAAGTTTTGGGCTTCAAGAGTCAGCCACGAGGCAGGCACTAGGCCTGGAAATGGCCTCACAGTCATGAGTTGGGCCTAAATGGGCCACTGTGAGGGAGGAGCTGTGCCTGTTGAGGCTGCTGGCAGGCAGGCAGAAATTTGGCCTGGGGCAGCTGCCATGAGGCAAGAGCTGGGCCTGGAAAAAGCCCCTGGGAGGCAAGAGCAGGGCCTGCAGAGGCTGTTCTCAAGTCAAAGCTGGGCCTGTTGATGCCACCGGGAAGCAGAAGGTGGGCCTGGAGAGTTTGACTTGAGGAAGTTTTGGGCCTACATTGGCCGCCATGAGCTGGACAGGAACTGGGCCAAAAAAGGCTGTTGTGAGGCAGCAGTTGTGCCTGTAGACCCAGCCAAGAGGAAGAGGTGGGTCTGGAGAAGCCCCCATGAGGCAGAGGTTGGGCCTGTAGACGCTGACAGGAGGCAGGAGCTGGGCCTGGACAGGTCAACTTGAGGAGATTTTGGGCCTTCATAGGCCACCAGGAGGCAGTAGTTGGGACTAGAGAGTCTGACTTGAGTAAGTTTTGGGCCCGGAGATGACGTCCTGGGACAGGAGTTGGGCGTGGAGAGGCCACCGTGAGGCATAAGCTGGATGTAGAGAGGCCAGTGTGAGGCAAGACCTGGGCCTGTCTAGGCTGCTGGGAGACAGGCAGGAATCTGGCCAGGGAAGGTTGCCATGAGACAAAAGTTGGGCCTGGAAAGGCCCTTGTGAAGCATGAGCTTGGCCTAAAGAGGCCACTGGGTGGCAGGAGCTGGGTGTGTAGAAGCTGCTGAAAGGTTGGGAGCTTGGCTTGGGGGGTCCACAGTGAGGTAGATGCTGGGCGTGAAGAATCTGCTGTGAGGCAGACGTTGGGACTGTAGAGGCTGACGGGAGGCAGAGGCTGGGCCTGGAGGGGCCACCAAGATGCAGGAGCTGGGCCTGGAGAGGCTGCAAAGAAGCATGACCTGGGCCTGGTGAGGTCGAC\n",
      "AGAAGTTGGGCAAAAGCCTGATTTGAGGAAGTTTTGGGCTTCAAGAGTCAGCCACGAGGCAGGCACTAGGCCTGGAAATGGCCTCACAGTCATGAGTTGGGCCTAAATGGGCCACTGTGAGGGAGGAGCTGTGCCTGTTGAGGCTGCTGGCAGGCAGGCAGAAATTTGGCCTGGGGCAGCTGCCATGAGGCAAGAGCTGGGCCTGGAAAAAGCCCCTGGGAGGCAAGAGCAGGGCCTGCAGAGGCTGTTCTCAAGTCAAAGCTGGGCCTGTTGATGCCACCGGGAAGCAGAAGGTGGGCCTGGAGAGTTTGACTTGAGGAAGTTTTGGGCCTACATTGGCCGCCATGAGCTGGACAGGAACTGGGCCAAAAAAGGCTGTTGTGAGGCAGCAGTTGTGCCTGTAGACCCAGCCAAGAGGAAGAGGTGGGTCTGGAGAAGCCCCCATGAGGCAGAGGTTGGGCCTGTAGACGCTGACAGGAGGCAGGAGCTGGGCCTGGACAGGTCAACTTGAGGAGATTTTGGGCCTTCATAGGCCACCAGGAGGCAGTAGTTGGGACTAGAGAGTCTGACTTGAGTAAGTTTTGGGCCCGGAGATGACGTCCTGGGACAGGAGTTGGGCGTGGAGAGGCCACCGTGAGGCATAAGCTGGATGTAGAGAGGCCAGTGTGAGGCAAGACCTGGGCCTGTCTAGGCTGCTGGGAGACAGGCAGGAATCTGGCCAGGGAAGGTTGCCATGAGACAAAAGTTGGGCCTGGAAAGGCCCTTGTGAAGCATGAGCTTGGCCTAAAGAGGCCACTGGGTGGCAGGAGCTGGGTGTGTAGAAGCTGCTGAAAGGTTGGGAGCTTGGCTTGGGGGGTCCACAGTGAGGTAGATGCTGGGCGTGAAGAATCTGCTGTGAGGCAGACGTTGGGACTGTAGAGGCTGACGGGAGGCAGAGGCTGGGCCTGGAGGGGCCACCAAGATGCAGGAGCTGGGCCTGGAGAGGCTGCAAAGAAGCATGACCTGGGCCTGGTGAGGTCGAC[SEP]\n"
     ]
    }
   ],
   "source": [
    "#the actual sequence should be\n",
    "#CAGAAGTTGGGC\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(tokenizer.decode(a))\n",
    "print(tokenizer.decode(b))\n",
    "\n",
    "#it shouldu end with GACT but it seems the last token is cut off, eh who really cares, it's because we added bos token at first then we removed it\n",
    "#which is fine, maybe it's there and it's 1025 actual length, or it's just 1023, doesn't really matter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the old head model\n",
    "most probably fine since it still predicts decently and can take this data as an input but doesn't hurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n"
     ]
    }
   ],
   "source": [
    "#now put thorugh model, get logits and calculate ce loss\n",
    "\n",
    "from src.dataloaders.datasets.ccre_dataset import CcreDataset\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "#make sure this is correct from the yaml files\n",
    "tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = CcreDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True')\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "        # seqs = seqs.to(device=self.device)\n",
    "        results = []\n",
    "        # outputs = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        \n",
    "        # if isinstance(self.tokenizer, Tokenizer):\n",
    "        #     tokenized_seq = self.tokenizer.encode(seq, ).ids\n",
    "        # else:\n",
    "        #     tokenized_seq = self.tokenizer(seq, padding=\"max_length\")['input_ids']\n",
    "        # tokenized_seq = tokenized_seq[3:] #make it so the length is 1023 as that's what we expect\n",
    "        # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "        logits = self.model(seqs)[0]\n",
    "\n",
    "        # Using head, so just have logits\n",
    "        results.append(logits)\n",
    "        # outputs.append(output)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cuda:0')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "            # print(tokenizer._vocab_str_to_int)\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "epoch = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-29/16-27-39-527212/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 206/206 [01:14<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2348)\n",
      "tensor(0.4076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "ccre_loader = DataLoader(ccre, batch_size=512, shuffle=False) #results are identical even if you shuffle, obviously since it's just the mean\n",
    "e100loss = torch.zeros((len(ccre), 1023))\n",
    "e100acc = torch.zeros((len(ccre), 1023))\n",
    "startidx = 0\n",
    "for idx, (seq, target) in tqdm(enumerate(ccre_loader), total=len(ccre_loader)):\n",
    "    b_size = seq.shape[0]\n",
    "    target = target.to(epoch.device)\n",
    "    seq = seq.to(device=epoch.device)\n",
    "    y_hat = epoch.encode(seq)\n",
    "    logits_reshaped = y_hat[0].logits.view(-1, 16)\n",
    "    target_reshaped = target.view(-1)\n",
    "    \n",
    "    losses = F.cross_entropy(logits_reshaped, target_reshaped, reduction='none')\n",
    "    \n",
    "    losses = losses.view(b_size, 1023)\n",
    "    \n",
    "    e100loss[startidx:startidx+b_size,:] = losses.detach().cpu()\n",
    "    \n",
    "    b = torch.argmax(y_hat[0].logits, dim=2)\n",
    "    e100acc[startidx:startidx+b_size,:] = (b==target).detach().cpu()\n",
    "    startidx += b_size\n",
    "    # full_acc2[idx,:] = (b==target).detach().cpu()\n",
    "print(e100loss.mean())\n",
    "print(e100acc.mean())\n",
    "\n",
    "#seems fine and reasonable, so let's just reuse the head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
