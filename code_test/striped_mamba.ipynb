{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47066c1",
   "metadata": {},
   "source": [
    "# we will make a striped version of mamba\n",
    "\n",
    "to do this we need to understand what the model is currently doing and see if we can change or update it at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bacde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see what we have so far\n",
    "# _name_: dna_embedding_caduceus\n",
    "#then we can have some config parameters for the ssm\n",
    "#then bidirectional add, and weight tie\n",
    "\n",
    "#also let's see if we have mamba 2 and can make that an option\n",
    "#indeed I have mamba 2, so we can make that an option too!\n",
    "\n",
    "#it initializes a nn.model thing, then takes in a caduceus confdig as well as device, dtype and whether it's conjoined (I think the rcps) for train/test\n",
    "\n",
    "#if it's conjoined, then complicated, but I'm not conjoining it here, that's only when I'm doing RCPS\n",
    "#then just runs self.caduceus with return_dict false\n",
    "#caduceus is defined by the caduceus config and device/dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f41bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the modeling caduceus file, it is not too complicated.\n",
    "#inherits the initialization and function form hugging faces PretrainedModel\n",
    "#the way it initializes weights should work for transformers or SSMs? unclear\n",
    "\n",
    "#then the actual model is built on caduceusmixermodel class\n",
    "#this is the actual architecture, we can ignore a lot of it because we ignore things like the embeddings\n",
    "#so if we alter this and ensure that it works with the initialization, then we are good?\n",
    "#we can compare with the transformer class and see how it initializes and loads its backbone\n",
    "#this is th elongconvlm script\n",
    "#so combine ieas from the modeling caduceus and long conv lm to properly model it\n",
    "\n",
    "#key ideas are we can ignore embeddings\n",
    "#we might need to do the thing where we upsample and downsample from mamba to transformers\n",
    "#but initialize it in a way that makes sense, and make sure the blocks work for either one\n",
    "\n",
    "#could also use rope and base it on the striped hyena framework?\n",
    "#I think better is jamba, since it does MOE too potentially? No need for ROPE, claim is mamba does that!\n",
    "#it does sliding window attention and stuff too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing, we can steal form jamba\n",
    "\n",
    "# def _init_weights(self, module):\n",
    "#         std = self.config.initializer_range\n",
    "#         if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "#             module.weight.data.normal_(mean=0.0, std=std)\n",
    "#             if module.bias is not None:\n",
    "#                 module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also use Hydra instead of bimamba, so then we can do closer to jamba!\n",
    "#make it adaptable so can do mamba2, mixed, or pure transformer\n",
    "#let's us find a baesline to test it...\n",
    "#no MOE for now, for now just make sure you get general idea of how it uses mamba and attention\n",
    "#can implement MOE late if needed!\n",
    "\n",
    "#bilby is quite complex tho lmao!\n",
    "#basically, it requires a bunch of steps, but seems like the best approach for how to do the striped mamba! Let's use this to base it off of\n",
    "# But understanding individual elements will be hard, so will have to manually run some elements myself to see the default what they are doing!\n",
    "#But we use hydra and things like that instead of their implementations of mamba/transformer\n",
    "#we should use ROPE for positional encoding despite what JAMBA says because of genomics and the pooling part\n",
    "\n",
    "#we can test with no pooling and with pooling for transformer, but issue is memory limitations\n",
    "\n",
    "#Still I think this is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a474348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the first step is to get bilby working, and then to do it with hydra\n",
    "\n",
    "#the nice part about hydra is that it uses mamba2 and no separate installation, so we can simply downbload the folder in and then import form there...\n",
    "#but we can get the general structure and arguments and ideas about how to construct it from bilby\n",
    "#but then we'll do our own thing with hydra and attention with ROPE!!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
