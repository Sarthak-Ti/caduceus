{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have had some serious issues with the collate function in pytorch\n",
    "IT seems that with our loader we have these collate features, we also get this deffault collate, let's see if we can figure out why my collate is so weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.genomics import HG38\n",
    "from src.dataloaders.datasets.profile_atac_long import ProfileATACLong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Union\n",
    "from torch.utils.data.dataloader import DataLoader, Dataset\n",
    "class ProfileATACLongLoader(HG38): #for unique cell type tokens\n",
    "    _name_ = \"ProfileATACLongLoader\"\n",
    "    l_output = 0  # need to set this for decoder to work correctly\n",
    "    #global in the context of the class or its instances. potentially used by hydra? I am unsure of what this does...\n",
    "\n",
    "    def __init__(self, dataset_name, dest_path=None, tokenizer_name='char', d_output=None, rc_aug=False,\n",
    "                max_length=1024, use_padding=True, max_length_val=None, max_length_test=None,\n",
    "                padding_side='left', return_mask=False, val_ratio=0.0005, val_split_seed=2357, add_eos=False, \n",
    "                detokenize=False, val_only=False, batch_size=32, batch_size_eval=None, num_workers=1,\n",
    "                shuffle=True, pin_memory=False, drop_last=False, fault_tolerant=False, ddp=False,\n",
    "                fast_forward_epochs=None, fast_forward_batches=None, single_cell_type = None,\n",
    "                train_bias=False, data_path=None,jitter=0, *args, **kwargs):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dest_path = dest_path\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.d_output = d_output\n",
    "        self.rc_aug = rc_aug\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.max_length_val = max_length_val if max_length_val is not None else max_length\n",
    "        self.max_length_test = max_length_test if max_length_test is not None else max_length\n",
    "        self.padding_side = padding_side\n",
    "        self.return_mask = return_mask\n",
    "        self.val_ratio = val_ratio\n",
    "        self.val_split_seed = val_split_seed\n",
    "        self.val_only = val_only\n",
    "        self.add_eos = add_eos\n",
    "        self.detokenize = detokenize\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_size_eval = batch_size_eval if batch_size_eval is not None else self.batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.pin_memory = pin_memory\n",
    "        self.drop_last = drop_last\n",
    "        self.single_cell_type = single_cell_type\n",
    "        self.train_bias = train_bias\n",
    "        self.data_path = data_path\n",
    "        self.jitter=jitter\n",
    "\n",
    "        # if self.dest_path is None:\n",
    "        #     self.dest_path = default_data_path / self._name_\n",
    "\n",
    "        if fault_tolerant:\n",
    "            assert self.shuffle\n",
    "        self.fault_tolerant = fault_tolerant\n",
    "        if ddp:\n",
    "            assert fault_tolerant\n",
    "        self.ddp = ddp\n",
    "        self.fast_forward_epochs = fast_forward_epochs\n",
    "        self.fast_forward_batches = fast_forward_batches\n",
    "        if self.fast_forward_epochs is not None or self.fast_forward_batches is not None:\n",
    "            assert ddp and fault_tolerant\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # TODO instantiate with registry\n",
    "        #what we need to do is have characters be the list of cell indices 0-161\n",
    "        characters = ['A', 'C', 'G', 'T', 'N']\n",
    "\n",
    "        # Combine the two lists to form the final list of tokens\n",
    "        # characters = number_tokens + nucleotide_tokens\n",
    "        # if self.tokenizer_name == 'char':\n",
    "        #     print(\"**Using Char-level tokenizer**\")\n",
    "        #     self.tokenizer = CharacterTokenizer(\n",
    "        #         characters=characters,\n",
    "        #         model_max_length=self.max_length + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "        #         add_special_tokens=False,\n",
    "        #         padding_side=self.padding_side,\n",
    "        #     )\n",
    "        self.tokenizer=None\n",
    "        \n",
    "        # Create all splits: torch datasets (only train/test in this benchmark)\n",
    "        self.dataset_train, self.dataset_val = [\n",
    "            ProfileATACLong(split=split,\n",
    "                                max_length=max_len,\n",
    "                                # dataset_name=self.dataset_name,\n",
    "                                tokenizer=self.tokenizer,  # pass the tokenize wrapper\n",
    "                                tokenizer_name=self.tokenizer_name,\n",
    "                                use_padding=self.use_padding,\n",
    "                                d_output=self.d_output, #we manually defined it in the dataset\n",
    "                                add_eos=self.add_eos,\n",
    "                                # dest_path=self.dest_path,\n",
    "                                rc_aug=self.rc_aug,\n",
    "                                return_augs=False,\n",
    "                                single_cell_type = self.single_cell_type,\n",
    "                                data_path=self.data_path,\n",
    "                                train_bias=self.train_bias,\n",
    "                                jitter = self.jitter,\n",
    "                                # return_mask=self.return_mask,\n",
    "            )\n",
    "            for split, max_len in zip(['train', 'val'], [self.max_length, self.max_length_val])\n",
    "        ] #uses dataset class and makes a train and validation using the basic loader\n",
    "        \n",
    "    def test_dataloader(self, *args: Any, **kwargs: Any) -> Union[DataLoader, List[DataLoader]]:\n",
    "        \"\"\" The test dataloader, it's a dummy loader just to make the trainer happy, we don't use it.\"\"\"\n",
    "        return self._data_loader(self.dataset_val, batch_size=self.batch_size_eval)\n",
    "    \n",
    "    #need a new collate fn\n",
    "    # @classmethod\n",
    "    # def _collate_fn(cls, batch, *args, **kwargs): #my custom collate function that is used since it's better and works for this custom class\n",
    "    #     \"\"\"\n",
    "    #     Custom collate function to handle nested tuples of tensors.\n",
    "    #     \"\"\"\n",
    "    #     print(\"Using custom collate function\")\n",
    "    #     # Unzip the batch into separate components\n",
    "    #     (seqs, one_hot_seqs), (cts, counts), *z = zip(*batch)\n",
    "        \n",
    "    #     # Collate each component separately\n",
    "    #     seqs = cls._collate(seqs, *args, **kwargs)\n",
    "    #     one_hot_seqs = cls._collate(one_hot_seqs, *args, **kwargs)\n",
    "    #     cts = cls._collate(cts, *args, **kwargs)\n",
    "    #     counts = cls._collate(counts, *args, **kwargs)\n",
    "        \n",
    "    #     # Combine the collated components back into the original structure\n",
    "    #     x = (seqs, one_hot_seqs)\n",
    "    #     y = (cts, counts)\n",
    "        \n",
    "    #     return_value = (x, y, *z)\n",
    "    #     return cls._return_callback(return_value, *args, **kwargs)\n",
    "    @classmethod\n",
    "    def _collate_fn(cls, batch, *args, **kwargs): #my custom collate function that is used since it's better and works for this custom class\n",
    "        #we will literally just return it as is\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's create a dataloader instance\n",
    "loader = ProfileATACLongLoader(dataset_name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([ 8, 10,  7,  ...,  8,  7, 10]), []),\n",
       " (tensor([1., 0., 3.,  ..., 0., 1., 0.]), tensor([6.7081])))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can test the output\n",
    "loader.setup() #simply instantiates the datasets\n",
    "loader.dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[tensor([[10,  8, 10,  ..., 10,  7,  8],\n",
      "        [ 9,  8, 10,  ...,  8,  8,  7],\n",
      "        [ 7,  9,  7,  ...,  8, 10,  7],\n",
      "        ...,\n",
      "        [ 9,  8,  8,  ...,  8,  7,  9],\n",
      "        [ 8,  8,  7,  ...,  8,  7, 10],\n",
      "        [ 7, 10, 10,  ..., 10, 10,  8]]), []], [tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 2.,  ..., 0., 2., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 4.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[6.2729],\n",
      "        [5.6937],\n",
      "        [6.2766],\n",
      "        [5.7557],\n",
      "        [6.9441],\n",
      "        [6.0137],\n",
      "        [5.4848],\n",
      "        [6.5889],\n",
      "        [6.3919],\n",
      "        [7.2506],\n",
      "        [5.9814],\n",
      "        [6.3351],\n",
      "        [5.6419],\n",
      "        [6.7286],\n",
      "        [7.3421],\n",
      "        [9.5280],\n",
      "        [6.1399],\n",
      "        [6.3630],\n",
      "        [8.2602],\n",
      "        [7.5110],\n",
      "        [6.8427],\n",
      "        [7.0148],\n",
      "        [7.6183],\n",
      "        [6.3456],\n",
      "        [5.7589],\n",
      "        [6.6846],\n",
      "        [7.4657],\n",
      "        [6.5482],\n",
      "        [9.0037],\n",
      "        [6.4457],\n",
      "        [6.3368],\n",
      "        [5.8021]])]]\n",
      "1\n",
      "[[tensor([[ 7, 10,  8,  ..., 10,  9, 10],\n",
      "        [ 9,  8, 10,  ...,  9,  7, 10],\n",
      "        [ 7, 10,  8,  ..., 10,  9,  8],\n",
      "        ...,\n",
      "        [ 9,  7,  7,  ...,  7, 10, 10],\n",
      "        [ 7, 10,  9,  ...,  7,  9,  7],\n",
      "        [ 9,  9,  7,  ..., 10,  7,  7]]), []], [tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 5.,  ..., 0., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [4., 1., 1.,  ..., 9., 0., 1.]]), tensor([[5.1648],\n",
      "        [7.1831],\n",
      "        [6.9088],\n",
      "        [6.8178],\n",
      "        [6.3869],\n",
      "        [5.5984],\n",
      "        [7.2882],\n",
      "        [7.8034],\n",
      "        [6.5073],\n",
      "        [7.7407],\n",
      "        [6.7616],\n",
      "        [6.3835],\n",
      "        [7.5153],\n",
      "        [8.2855],\n",
      "        [6.9745],\n",
      "        [7.6089],\n",
      "        [6.0331],\n",
      "        [9.3765],\n",
      "        [6.9726],\n",
      "        [6.2206],\n",
      "        [6.6093],\n",
      "        [7.3512],\n",
      "        [5.6058],\n",
      "        [6.9866],\n",
      "        [7.1115],\n",
      "        [8.4655],\n",
      "        [6.3835],\n",
      "        [7.0876],\n",
      "        [6.5582],\n",
      "        [7.1261],\n",
      "        [6.1964],\n",
      "        [8.3197]])]]\n",
      "2\n",
      "[[tensor([[10,  9,  9,  ...,  9,  9,  8],\n",
      "        [ 7,  7,  8,  ...,  8, 10, 10],\n",
      "        [ 7,  7,  7,  ...,  9, 10,  9],\n",
      "        ...,\n",
      "        [ 9,  9,  7,  ..., 10,  7, 10],\n",
      "        [10,  8,  9,  ..., 10,  9,  9],\n",
      "        [ 9, 10,  9,  ..., 10,  7,  9]]), []], [tensor([[0., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 2., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 1., 0.]]), tensor([[7.1647],\n",
      "        [6.4441],\n",
      "        [6.1506],\n",
      "        [6.4983],\n",
      "        [6.3733],\n",
      "        [5.7333],\n",
      "        [7.3626],\n",
      "        [6.8134],\n",
      "        [6.8596],\n",
      "        [6.8459],\n",
      "        [6.8363],\n",
      "        [6.0210],\n",
      "        [9.2349],\n",
      "        [6.6067],\n",
      "        [8.3615],\n",
      "        [7.9406],\n",
      "        [5.6560],\n",
      "        [6.4167],\n",
      "        [6.6896],\n",
      "        [8.4325],\n",
      "        [5.7589],\n",
      "        [7.3969],\n",
      "        [5.8749],\n",
      "        [8.2011],\n",
      "        [5.8464],\n",
      "        [7.7075],\n",
      "        [5.7900],\n",
      "        [5.9558],\n",
      "        [6.7154],\n",
      "        [7.1647],\n",
      "        [6.1570],\n",
      "        [5.7900]])]]\n",
      "3\n",
      "[[tensor([[ 8, 10,  9,  ...,  7,  7, 10],\n",
      "        [ 7, 10, 10,  ..., 10,  8,  7],\n",
      "        [ 7, 10,  8,  ...,  7,  7,  7],\n",
      "        ...,\n",
      "        [ 7,  7,  7,  ...,  7,  7, 10],\n",
      "        [10,  7,  9,  ..., 10, 10,  7],\n",
      "        [ 9,  9,  9,  ..., 10,  9,  7]]), []], [tensor([[ 0.,  5., 38.,  ...,  3.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]), tensor([[8.8232],\n",
      "        [5.2364],\n",
      "        [5.2627],\n",
      "        [7.5132],\n",
      "        [9.4043],\n",
      "        [5.8171],\n",
      "        [6.1048],\n",
      "        [6.6846],\n",
      "        [6.3208],\n",
      "        [5.6802],\n",
      "        [5.9764],\n",
      "        [7.4419],\n",
      "        [6.5779],\n",
      "        [6.1717],\n",
      "        [7.9058],\n",
      "        [7.7545],\n",
      "        [8.1371],\n",
      "        [6.2246],\n",
      "        [6.1612],\n",
      "        [6.9508],\n",
      "        [6.7991],\n",
      "        [6.3578],\n",
      "        [5.8665],\n",
      "        [7.1639],\n",
      "        [6.3801],\n",
      "        [7.7003],\n",
      "        [7.7367],\n",
      "        [7.4401],\n",
      "        [5.0304],\n",
      "        [5.7777],\n",
      "        [6.0638],\n",
      "        [6.4583]])]]\n"
     ]
    }
   ],
   "source": [
    "#now iterate over the loader\n",
    "for i, batch in enumerate(loader.train_dataloader()):\n",
    "    print(i)\n",
    "    print(batch)\n",
    "    if i>2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(len(batch))\n",
    "print(len(batch[0]))\n",
    "print(batch[0][0].shape) #this is the seq data in tensor format!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 10,  9,  ...,  7,  7, 10],\n",
       "        [ 7, 10, 10,  ..., 10,  8,  7],\n",
       "        [ 7, 10,  8,  ...,  7,  7,  7],\n",
       "        ...,\n",
       "        [ 7,  7,  7,  ...,  7,  7, 10],\n",
       "        [10,  7,  9,  ..., 10, 10,  7],\n",
       "        [ 9,  9,  9,  ..., 10,  9,  7]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.fault_tolerant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataloader from pytorch\n",
    "from torch.utils.data import DataLoader\n",
    "manual_loader = DataLoader(loader.dataset_train, batch_size=32, shuffle=True, num_workers=1, collate_fn=loader._collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(manual_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(a))\n",
    "#see so this returns it very differently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the dataset to ensure it actually works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.genomics import HG38\n",
    "from src.dataloaders.datasets.profile_atac_long import ProfileATACLong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProfileATACLong(split='train', max_length=32768, tokenizer=None, tokenizer_name='char', use_padding=True, d_output=None, add_eos=False, rc_aug=False, return_augs=False, jitter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([ 7,  8,  7,  ...,  7, 10,  7]), tensor([0., 0., 0.,  ..., 0., 0., 0.])), (tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([10.1966])))\n"
     ]
    }
   ],
   "source": [
    "out = dataset[0]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32768])\n",
      "torch.Size([32768])\n",
      "torch.Size([32768])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "x,y = out\n",
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(y[0].shape)\n",
    "print(y[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape[0] == 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 42917/220311 [00:08<00:34, 5183.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#now let's go through the dataset\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    out = dataset[i]\n",
    "    x,y = out\n",
    "    if x[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if x[1].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[1].shape[0] != 1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([26787])\n",
      "torch.Size([26787])\n",
      "torch.Size([26787])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(y[0].shape)\n",
    "print(y[1].shape)\n",
    "\n",
    "#are we at the end of the genome? let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chr15', '101980786', 'f', '1'], dtype='<U21')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.peak_coords[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101991189\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.genome['chr15']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10403"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.genome['chr15']) - int(dataset.peak_coords[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220311"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.peak_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 10k to the right! We need to do some sort of filtering\n",
    "import numpy as np\n",
    "remove_array=np.ones(len(dataset.peak_coords))\n",
    "for i,row in enumerate(dataset.peak_coords):\n",
    "    start = int(row[1])\n",
    "    if start + 32768 > len(dataset.genome[row[0]]) or start < 32768:\n",
    "        remove_array[i]=0\n",
    "#this is very quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 20280,  20281,  20282,  20283,  20284,  32025,  32026,  32027,\n",
       "         32028,  32029,  42917,  51375,  53014,  53015,  53016,  85967,\n",
       "         88788,  95373,  95374, 118312, 142040, 147674, 195135, 195136,\n",
       "        196790, 196894, 197016, 197017, 214822, 215779, 216887, 216888,\n",
       "        216889, 216890, 217089, 217090, 217091, 217092, 217093]),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(remove_array==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove those arrays\n",
    "dataset.peak_coords = dataset.peak_coords[remove_array==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220272\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.peak_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220272/220272 [00:40<00:00, 5448.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#now let's go through the dataset\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    out = dataset[i]\n",
    "    x,y = out\n",
    "    if x[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if x[1].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[1].shape[0] != 1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helllllllll yeah! Now let's add this to the thing and test it\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.genomics import HG38\n",
    "from src.dataloaders.datasets.profile_atac_long import ProfileATACLong\n",
    "\n",
    "dataset = ProfileATACLong(split='train', max_length=32768, tokenizer=None, tokenizer_name='char', use_padding=True, d_output=None, add_eos=False, rc_aug=False, return_augs=False, jitter=100_000)\n",
    "#huge jitter just because we're trying to test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219854/219854 [00:39<00:00, 5500.60it/s]\n"
     ]
    }
   ],
   "source": [
    "#now let's go through the dataset\n",
    "#see that it's a decent amount shorter\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    out = dataset[i]\n",
    "    x,y = out\n",
    "    if x[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if x[1].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[0].shape[0] != 32768:\n",
    "        print(i)\n",
    "        break\n",
    "    if y[1].shape[0] != 1:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok finally seems good! Let's run the actual thing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
