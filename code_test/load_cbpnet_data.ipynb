{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_cbpnet_data.ipynb\n",
    "\n",
    "The goal is to see if we can use the coordinates to load 32kb data from the coordinates instead of using the whole\n",
    "\n",
    "We will just use peak regions for now and not focus on nonpeak regions. no bias model either, but first let's load the data\n",
    "\n",
    "The peak regions are fine,b ut nonpeak region sneed to be GC matched across the 32kb so we can adapt their code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220311, 2024, 4) (220311, 4) (220311, 1800)\n"
     ]
    }
   ],
   "source": [
    "#first load in the data of coordinates and seq\n",
    "import numpy as np\n",
    "arraypath = '/data/leslie/sarthak/data/chrombpnet_test/saved_data_1000/train/'\n",
    "peak_seqs = np.load(arraypath + 'peak_seqs.npy')\n",
    "peak_coords = np.load(arraypath + 'peak_coords.npy')\n",
    "peak_cts = np.load(arraypath + 'peak_cts.npy')\n",
    "print(peak_seqs.shape, peak_coords.shape, peak_cts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr1' '100029207' 'f' '1']\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "[[0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " ...\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(peak_coords[0])\n",
    "print(peak_cts[0]) #the idea is we want to get something that matches this\n",
    "print(peak_seqs[0]) #and something that matches this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get the bigwig file and use what they use to laod the data\n",
    "import pyBigWig\n",
    "import pyfaidx\n",
    "#it is indeed centered around this r summit + r start, where summit is how far into the peak the summit is\n",
    "cts_bw_file = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.bw'\n",
    "genome_fasta = '/data/leslie/sarthak/data/chrombpnet_test/hg38.fa'\n",
    "cts_bw = pyBigWig.open(cts_bw_file)\n",
    "genome = pyfaidx.Fasta(genome_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800,)\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "width=1800\n",
    "test_cts = np.nan_to_num(cts_bw.values('chr1', 100029207-width//2, 100029207+width//2))\n",
    "print(test_cts.shape)\n",
    "print(test_cts[:20])\n",
    "print(peak_cts[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(test_cts, peak_cts[0]))\n",
    "#yeah it's quite easy to extract these sequences!\n",
    "#and it is indeed centered around the summit not the start or the middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCAAAGTGCTGGGTTTACAGGCGTGAGCCGCTGCGCCCGGCCAATACTGCCTTTCAAAAGAGACTGATAGTGACCAGCATTAGTAATCATACTGGTGGGGTTTTTTGGGGTGTTTTTCTGTTTTGTTTTTTGTTTTTTGTTTTTGAGACAAGGTCTTCCTCCCGTTGCCTAGGCTGGAGTGCGGTGACACAATCTTGGCTCACTGCAGCCTTGATAGAGCAAGCTCAAGTGATCCTCCCAGGAGCCTCGGCCCCCAAGCAGCTGGGAATACAGGTGCGCGCCACCATGCCCAGCTGATTTTTGTATTTTTTTGTAGAGATGGGGTTTTGCCATGTTGCCCGGATTGGTCTCAAACTCCTGAGCTTCAGTGATCTGCCTGCCTTGGCCTCCCAAAGTGTTGGGATTACAGGCGTGAGCGACCACACCCAGCCCATATTGGTCTTTCTTACTGTTCTTAAAAAGAGAATTCCTTTAAGGCAGGACCGATTACATATACACTCTAGAAAAGAAAATAGCAAGGAAGAAATAAATTGCCTTCAATTACCAAAGATTTGAGCTTCTGCTATGGCTGAGAGTGTTTTGGTCATTGCAAATTCAGGGGTTTCCCAAGTTCACCCTCAGTTCTGGCTAGAAAGAGAAAACTTACTAAAAGCTATTATACTCACAGTCATATTTATTACAGAGAAAGGAAATACAAATTAAAACCAGCCAAAGGAAGTGACACATAAAACAGAGTCTAGGAGTGGTCCAAACTTGAGGCTTTCGGTGTCCTTTTCTTGTAGCGTCATGGAAGGTGTTATCTACTCCTGACCACAATGTTTGACAGTACACACATAGTATTGCCATTCAGGGAAGCTCACCTAAGCTTTGGTGTCCAGATTTTTATTGAGAGAGGCTCTATTAGTTGGCATGGTTGGTTGATTTTTTTGCCCATGTAGTTGTCCTCTCTTTCCAACATCTGCCCCTCCCTGGAGATCTGGTTGACATCAAGACTTCAGGGCCTCACCATAGGTTATCTCGTTAGCATAAACTGTCAAGTGTTGTCTAAGGAACCCACAATGAATAATAAAGACATTCCTATCAGTGAGAACTCCCAAAGACTTACACCAGAACTTTCTTTGGATGGGCCAAATTTCTTACTACACAAAGACCATTCATCTCTATACACTTCCTTCTGAATTGATGAGGATGATACAAGCAACGACAATTCTTCTTTTCAGAGACTTTTAATTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTCTCGCTCTGTCACTCAGGCTGGAGTTCAGCGGCACGATCTCTGCTCACTGCAACCTCCGCCTCCTGGGTTTGAGCAATTCTCCTACCTCAGCCTCCCAAGTAGCTGGGATTACTGGTGCTCACCACCACGCCCAACTAATTTTTGTATTTTTGGTAGAGACGGGTTTTCACCATGTTGGCCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCTGCCGGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATCACACCCAGCCCAGAGACTTTTAATTTATACATTGTCATTTTTAATTTTAGAATATCTGTCAAATGATTCTGAACATAACATGAATCTAGTGTGGAAAAATGTTTATAATCAGATATTGTGTTAAGAACATATATATATATATAGAGAGAGAGAGAGCATAGTATTGTCATTTAGTTTTCACTTCTTATGTTTAATGACAACTTTATAAATGCTGGACTATTTTAAACTACAAATTTAAAACATGGTTTATAAACCTTTTACCTGTACTGCATATTAATAAACAATTTTGAACTAATTTTAAATTAGGCATTCCTATCACCATTATAAGGAAGAGCTAAAACCTGATACCAGTTGTTATTTTGGCTCACAGTAGGGCTTCACTAGTGTTAGGATCACATTTTCCTCCCTTGGACCTGAGAATCAGGTGTTTTTCGTTTAATGCATTTATTTTCATTCATTCATTTAGGATTTCTAAAATGTTAGAGGTAATTTGTTTAAA\n"
     ]
    }
   ],
   "source": [
    "#let's test with the sequence too\n",
    "width=2024\n",
    "test_seq = genome['chr1'][100029207-width//2:100029207+width//2].seq\n",
    "# print(test_seq.shape)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCAAAGTGCTGGGTTTACAGGCGTGAGCCGCTGCGCCCGGCCAATACTGCCTTTCAAAAGAGACTGATAGTGACCAGCATTAGTAATCATACTGGTGGGGTTTTTTGGGGTGTTTTTCTGTTTTGTTTTTTGTTTTTTGTTTTTGAGACAAGGTCTTCCTCCCGTTGCCTAGGCTGGAGTGCGGTGACACAATCTTGGCTCACTGCAGCCTTGATAGAGCAAGCTCAAGTGATCCTCCCAGGAGCCTCGGCCCCCAAGCAGCTGGGAATACAGGTGCGCGCCACCATGCCCAGCTGATTTTTGTATTTTTTTGTAGAGATGGGGTTTTGCCATGTTGCCCGGATTGGTCTCAAACTCCTGAGCTTCAGTGATCTGCCTGCCTTGGCCTCCCAAAGTGTTGGGATTACAGGCGTGAGCGACCACACCCAGCCCATATTGGTCTTTCTTACTGTTCTTAAAAAGAGAATTCCTTTAAGGCAGGACCGATTACATATACACTCTAGAAAAGAAAATAGCAAGGAAGAAATAAATTGCCTTCAATTACCAAAGATTTGAGCTTCTGCTATGGCTGAGAGTGTTTTGGTCATTGCAAATTCAGGGGTTTCCCAAGTTCACCCTCAGTTCTGGCTAGAAAGAGAAAACTTACTAAAAGCTATTATACTCACAGTCATATTTATTACAGAGAAAGGAAATACAAATTAAAACCAGCCAAAGGAAGTGACACATAAAACAGAGTCTAGGAGTGGTCCAAACTTGAGGCTTTCGGTGTCCTTTTCTTGTAGCGTCATGGAAGGTGTTATCTACTCCTGACCACAATGTTTGACAGTACACACATAGTATTGCCATTCAGGGAAGCTCACCTAAGCTTTGGTGTCCAGATTTTTATTGAGAGAGGCTCTATTAGTTGGCATGGTTGGTTGATTTTTTTGCCCATGTAGTTGTCCTCTCTTTCCAACATCTGCCCCTCCCTGGAGATCTGGTTGACATCAAGACTTCAGGGCCTCACCATAGGTTATCTCGTTAGCATAAACTGTCAAGTGTTGTCTAAGGAACCCACAATGAATAATAAAGACATTCCTATCAGTGAGAACTCCCAAAGACTTACACCAGAACTTTCTTTGGATGGGCCAAATTTCTTACTACACAAAGACCATTCATCTCTATACACTTCCTTCTGAATTGATGAGGATGATACAAGCAACGACAATTCTTCTTTTCAGAGACTTTTAATTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTCTCGCTCTGTCACTCAGGCTGGAGTTCAGCGGCACGATCTCTGCTCACTGCAACCTCCGCCTCCTGGGTTTGAGCAATTCTCCTACCTCAGCCTCCCAAGTAGCTGGGATTACTGGTGCTCACCACCACGCCCAACTAATTTTTGTATTTTTGGTAGAGACGGGTTTTCACCATGTTGGCCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCTGCCGGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATCACACCCAGCCCAGAGACTTTTAATTTATACATTGTCATTTTTAATTTTAGAATATCTGTCAAATGATTCTGAACATAACATGAATCTAGTGTGGAAAAATGTTTATAATCAGATATTGTGTTAAGAACATATATATATATATAGAGAGAGAGAGAGCATAGTATTGTCATTTAGTTTTCACTTCTTATGTTTAATGACAACTTTATAAATGCTGGACTATTTTAAACTACAAATTTAAAACATGGTTTATAAACCTTTTACCTGTACTGCATATTAATAAACAATTTTGAACTAATTTTAAATTAGGCATTCCTATCACCATTATAAGGAAGAGCTAAAACCTGATACCAGTTGTTATTTTGGCTCACAGTAGGGCTTCACTAGTGTTAGGATCACATTTTCCTCCCTTGGACCTGAGAATCAGGTGTTTTTCGTTTAATGCATTTATTTTCATTCATTCATTTAGGATTTCTAAAATGTTAGAGGTAATTTGTTTAAA\n"
     ]
    }
   ],
   "source": [
    "#first convert onehot to string\n",
    "def onehot_to_seq(onehot):\n",
    "    seq = ''\n",
    "    for i in range(onehot.shape[0]):\n",
    "        if onehot[i,0] == 1:\n",
    "            seq += 'A'\n",
    "        elif onehot[i,1] == 1:\n",
    "            seq += 'C'\n",
    "        elif onehot[i,2] == 1:\n",
    "            seq += 'G'\n",
    "        elif onehot[i,3] == 1:\n",
    "            seq += 'T'\n",
    "    return seq\n",
    "print(a:=onehot_to_seq(peak_seqs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a == test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it would also be very easy to go from the bed if we had our list of seqs in bed format, as we can just add in the start and summit to get the new summit. But that's besides the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220311, 2024, 4) (220311, 4) (220311, 1800)\n"
     ]
    }
   ],
   "source": [
    "#now let's import the tokenizer\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "import pyBigWig\n",
    "import pyfaidx\n",
    "#it is indeed centered around this r summit + r start, where summit is how far into the peak the summit is\n",
    "cts_bw_file = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.bw'\n",
    "genome_fasta = '/data/leslie/sarthak/data/chrombpnet_test/hg38.fa'\n",
    "cts_bw = pyBigWig.open(cts_bw_file)\n",
    "genome = pyfaidx.Fasta(genome_fasta)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "arraypath = '/data/leslie/sarthak/data/chrombpnet_test/saved_data_1000/train/'\n",
    "peak_seqs = np.load(arraypath + 'peak_seqs.npy')\n",
    "peak_coords = np.load(arraypath + 'peak_coords.npy')\n",
    "peak_cts = np.load(arraypath + 'peak_cts.npy')\n",
    "print(peak_seqs.shape, peak_coords.shape, peak_cts.shape)\n",
    "\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=32770,  # add 2 since default adds eos/eos tokens, crop later\n",
    "    add_special_tokens=False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768 ACATTTAGAGGGCTGGTTAAATAAATTATCAGTATAATACATAGTATAGTGATTAAGATCATGTTCTCTCCTGAGCCCATGATTTACCAGTAGTGTGAGCTTGGGCAAGTTATGTAACCTCTCTGTGCTTCAGTTTCTTCATCTATAAAATAGACATCTACCTCATAGTGTTGTATGTGAGTTAACATACATGAAGCACTTAGAATAATTCTTGTTACATGCTGATACATACATATTTGCTATTATAATCCATACAAAGGAATATTATGCAACTGTTAGAATGCAGTAGTTCTGTTCATACTGACATGTATATATCCTTCTATAATGTATACTTTTTTAGTAATTACAAAGTCTTTTATAAATTAAAAAACTTTATTGGGTTTGGTGGCTTGTGCTTCTAATCCCAGATACTTGGGAGGCTGAGATGGGAGGATTGCTTGAGAACCAGGAGTTTTGAGACCTGAGCAACATAGTGAGACCCTGTTTCTTAAAAATATATATATATAAGCCTGGATACAGTGGCTCACGCCTGTAATCCCAGCACTTTGAGAGGCCGAGGCAGGTGGATTGTTTGAGCTTAGGAATTGGAGACCAGTCTGAGCAACACTACAAAACTCTGTACAAAAAAAAAAAAAAAAAATTAGCTGGGTGTGGTAGCGTGTGCCTGTGGTCCAAGCTACTTCGGAGGCTGAGGTGGGAGGATCGCTTGAGTCCAAGAAGTCGAGGCTGCAGTGAGTTGTGATTGTGCCTCTGCACTCCAGCCTGCTAAGTGACAGAGTGAAATCCTGTCTCAAAATATACATATACATATATATATGTTATATATAAAACTCTTCTGATGGTATTTTATATATGTGTGTATATATATACACACACATACCTATTTTATATATATGTAATACCATCATATAAAATACCATCACATGTGTTTTTGTAATTATATGCCAGCATTAATTTCATGTTTGAAATTAGACATTAAAAGGATAGAGCTGACTTAAATGGAATTTTAGAGCATTTCTCATCCTTTTCACTTCCTTTCTATTGTGGAATACTTTGATTAGTTATAACTTCATTTTTAAAAATTGCTTTACGCTTGCACTCCTTAACTGCACATATTTCAAAGGTGACCCATAAGAAGTGGTCCATGTAAGCATGAATTTCTTTATTAATAGCCTCGCAATGCTTCTGGGAATCTCTTATTCCAATAGTCATGTGATAGAATATGTTTTTGAGGTGAGATTCAACATGAACAAGACAGTTTCTGCTGTCACCAAGTTAACTTTCTCTAACAGTGAAACTTCAGTAACACAGAATCTAATGATTTCTATGACTATTTCTTAGCTGTGATTTTTTGGATATCAGTTATTATTAGTTATACTTTCAAAACAAAACTTTTATTTATTTATTTATTTTTTTAGAGACAGTCTCGCTGTTTTGCTCAGACTGGAATGCAGTGGTAGAATCATAGCCCACTGTAACCTCAAACTTCTGGGCTCAAGTGATTGTCCCTCCTCAGCCACCTGAGTAGCTAGGACTACAGGCATGCACCACCACACCTAACTAATTTAAAAATTTTTTTGTAGAGATGGAGTCTTGCTATGTTGCCTGGGTGGGTCTTAAACTCCTGGGGTCAAATGATTCTCCTGCTTTGGCCTTCCAAAGTGCTGGGATTTCAGGCGTGAGCCACTGTGCCCACTCTCAAAACTTCTTAATAGGGAAAAATATTGTACCCCCTGCCTCAGCTTTTATATTTGTATGTATCAATGTGAATATATGAAAAATCTAGATCTAGCAAGAGGAAGATGGAAGAACAATGACATATTCCAAATGTGAGCAATTTGCACATATTCGAATGGGCTAGCCATGTAATTATCTAATTTGTGCATTGTATTATTTTAGTGCTCCAAAGTTGGTAATGTATGTTATTTTTTGTATTCTATGTCTTTGTCTTTTTACATTTATATGTCTTTGTCTTGTAGTCATAATATAGTACTAAGTATACTTTTGTTAAGTATACAAAACCAAGTAAAAATTTTAGAAAGTAATCTAATTTTGGCTGGGCACGTAACTCACACCTGTAATCCCAGGACTTTGTGAGGCCAAGGCAGGTGGATCATGAGGTCAGGAGATCGAGACTAGCCTGGCTAACACAGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCTGGGCGTGGTGGCACGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGTGGAGGTTGCAGTAAGCCAAGATTGTGCCACTGCACTGCAGCCTGGGCAACAGAGCAAGACTCCGTCTCAAAAAAAAAAAAAAAAAAAAGAAAGTAATCTAATTTTATCAAAAAGAAAAAGTTAAGTGTAAAATTATTTGGAATGTCAGTGGAAAAAATATATCTCTTCAGACAAACTAAACGATATATCATGTAGACTTTACTTTTTTTTAGGTTTCTTTGGAAGTATATTTGGATTAATGGGTGTATACATTTATGATGGAGAACTGGTATCAAAGAATGGATTTTTTCAGGGATATAACCGACTGACCTGGATAGTAGTTGTTCTTCAGGTAAAGCATTTAAAGTCTTAGATTTAATGCTAATAAACTGTATTTTAATATAAAGAATCAAAGTAGCTCCTGATATACTGATGTGAGGGGGAAAAGGTCCCTCCTGTTAGTGCTCTCGTATCTAGTTTATTCAAGGAAGTGAAAATAGAAGTGATCTTAATGCTGTAATGAAATCTTATTTTGTACAGGGGTCTTTAATGAACTTTGTGGTTTTAAATGTAACATAATGAAAATACAAGTTTAAGAATAAAAGAGTTGAAGCTAAAGGCAAATATAACTTTTGATTTTTACAAGTCTTATCATTGTACAGGAAGTCTGGGTCAGCCATTAAGACCCAGGAAGCAGATCTAGATATTTAATTCTGTGACAGTTCATAGCCTCATATTGTAGTTACATATTGGAAAACATCTGTCAAATTCAGTTTATATATAGCTTGGAATTGTTCAAGGGTTGAGTAATTAATTACATCTTTAAAAAATACATATGGAGAATTCTAGAGGATATAAAAATGAAGATACAAGTCACTATCCTTAGAGCCTCCTATTCTTCTAGAGATATTTGAAGCCTAGGAAGCAAGGCTCCAGAAAGGAAGGTGAAGATGGCAAAGGATAGTTGTGTTTTAAAAGCAGGTAGGATACTTCTATTTTTTTTTTTTTTTTTTTTGAGACGGAGTCTAGCTCTGTCACCAGGCTGGAGTGCAGTGGTACGATCTCGTCTCACTGTAATCTCCACCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGGATTACAGGCATGTGCCACCACGCCCAGCTAATTTTTGTATTTTTAGTAGAGACGGGGGTTTCACCATGTTAGCCAGGATGATCTCGATCTCCTGACCTCGTTATCCACCCACCTTGACCTCCCAAAGTGCTGGGATTACAGGTGTGAGCCACCGCGCCCGGCCCGGATACTTCTATTTTTTTTTTTTTTTTGAGACGGAGTCTCGCTCTGTTGCCCAGGCTGGAGTGCAGTGGCGAGATCTTGGCTCACTGCAAGCTCCGCCTCCTGGGTTCACGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGACTACAGGCGCCCGCCACCAAGCCCAGCTAATTTTTTGTATTTTTAGTAGAGACGGGGTTTCACTGTGTTAGCCAGGATGGTTTTGATCTCCTGGCCTTGTGATCCGCCCACCTCGGCCTCCCAAAGTGCTGGGATTACAGGCGTGAGCCACCTCGCCTGGCCCGGATACTTCTGTTTTTAAGATAGCAGAAAAGGAAACAAAGATAAGTGAAAATTTTAAAAAAATAATTAAGAGAAAATTTGAAGCGGAGATAGGGTGAGTCAAGAGAGTTTATGTATGATGTATCTTATTCCAGCAAAGAGAAGAAGCCCCCTTCCCTTAAAAAGGGGCTGAAGATATGAGAAAAATTAAAAATGTTTGGTATAACTGGTATGAAGGATTCAATAGGGAGTAAATAGATGAATGACAACATTGCCAAGAAATACTGGGACTGAAACATTAATTTGTAGTAAGCTCAATTGGCATAATTTACGACTTTCTCTAAATAAAGCTTCCATCATAGAAGAGATGTTTTGAAAGTTGTTGCAGTAGGTAACCCAGATTTGGAAAATGATAAGTAGTCTTTGGCAGAAAGCAAAACTGTGTAGTATAGTAAAACCTCATTATTAAAGTAGCCACAATTGGGAATTTGAGAGGGATTAGGTAAGTTCTTTCTACAGTCTTAAAGAAAAAGCACAATTAAAACTGTGCTTTAGAAAACTTAATGTGACAGTAATGAGAGGTGGATTTGAAAGGGGAGAAACTATAGGAAAAAAGCCAAATTAAGGGGCTGTTGAAAGGCTGTAAAATGACTCTGATTACTAATTTTACTCAAAAGTAAAAATATTTCACTATTTAAAGTGACCAGTCCTCTTACATTTGTCCAAACAAGTGATGTTTAATTACTAAATTGGATAATGATAGACATTTTATTTGTAATATGGTCAAAGTCCATTATAACACACTGCAGATTGTCCTGGTAGTTACTTTAATTAAATGCCTTTTGAAATAAGAGGGCTATTGAACAAAACCTGTAAGTCTATTCGTTGTTAAGAGATGTTTAAAATAAATCCCTTACTATAATGGGATTGAATTTATGGGACAAAAGCTTAAAGCTTTATTAAATACTGAAAGTGTTTTGAAATGCAGTTTTACATGTGTGTTTTAAAAAATATTTTTTTAAAACTTCAGGCACTTGGAGGCCTTGTAATAGCTGCTGTTATTAAGTATGCAGATAATATTTTAAAAGGATTTGCAACCTCTTTATCGATAATATTATCAACATTGATCTCCTATTTTTGGCTTCAAGATTTTGTGCCAACCAGGTAAAATGTTCTTTTCTATTTTTTTAAATCCCCAGAAGTATATAGAAAAATTAGAATTCTTTGTAAGGTGATCTGATAAATTTGAAGAAGCACTGAAATATAATTTTAAAAATTTATTTAACTATGTTTTAAAAATATTTCAAATTAAATTCATGAAAGAAAGTCTTCTCCTGAAAAGTTTCTTATACAAATTTGGTTGGCCAGAATTTTCCCAATAAATAGATGTGAATATTTTCCCTGCAAAGTAAAAAAGTGATATAAAAAAGTTAATAAGTCATATAAAGTCATAAAAATAAGTCATGTAATGACATTTTTAAAATACGGAATTTACATGACTTCCTTGAAGGAGATAGTTTATATAGAGAATGTGGCTCATGAGGAAACTCAATTAGGTGGTAGCAGGAAGAGCAATCTGGAAAGGAATTGCAGGAGGAGTTATAGGTGAGGAAGAAGACCCAGGACTCTGTAGTTTTATGGATGCTAAGGGAGGAGATAGCATCAGGAAAAATTGAGTGTAATGAATATCCATTACAGACAGCATTGAAAATTCTAGTTTTTTGTTTAATTTATAATCTCAGACTAGTGGGATTATATTTTAATTGTTTCCAGTACACCACTCTTGGTTTTCCTCCAACTTTACTCCTCTCCTGATTTTCTCAACCTCTAAACTTTGGAATCTCCCAAGAATCTTTTATTTATTTATTACTTTATTATTTATATGTTCATATTTTTGAGACAGGATCTCAGTTTGTCACCCAGGTTGCAGTGCAGTGGCGTGATCGTAGCCCACTGCAGCTTCAAACTCCTGGGCTCAAGCAATCCTCCTGCCTCAGCCTCACAGGTAGCTGGGACTACACATGTGCTTTGTTGCCCAGGCTGATCTTGAACTCCTTACCTCGAGCAATCTTCCCACCTCTGCCTCTCAGAGTGCTGGGATTATAGGCATATGCCACTTCGCTGAACTCATTTGTGTTTTTGAGAGTGAAATAGGATTATGGATGGAATTTGGCACTATAAAAATATAATATCCTTTTTATATTTTTTATATTGAACATTGACGGTTTTAATTCTTTTCTAGCAATCTAGACAATAGGTCTGGAACATACATGTTGAGATCTGTAGTTCTTATGGGACAAAAATTTAAAAGCATTTCTGTTTTCAGCTTGGTATGGAAATGAATCACTTAGCTAGTGAGACTAGATGAACAATGAACACTTGTGTCTTAATTTTGTAATCTTAATCATATTCCCTGTTCATAAGATGAGGAAATCGTATGCTTTAATGTACTTTGAGAATGTTTGCACCTTGCAGATTTTTAGATTTTACTTTTAAAAGCATACCTTTTTTTTTTTTTTAAATTGAACCTTATGTAAGGCCGAGGTAATTAAATCTTTGAATTATGCAGAGAATAAACAAATAAATGAACAAGCAAACCATCAGAGGGACAAAAGAGTAGTAGTTTGGCAATGAGATGTATGCCTGCCTGTAAATTCACGTTTCTGAGGAAGTATATAAGGTTTGTTTAGTTGAAGATAGAAAGAGAAAAGACAATGATACTGCCATTATTGAATGAATGAATGAATGAATGAATGAATGAATGAATATTTTGAGAATATGCTCAGGACACTAAGCCAGATGGAGAAGATGGATTCTTTTTCCTAAAATAGGTAACTAAGCCCTCTATCTTTATGCATCTTACCTTTGTGGAGAATACATTTTCAGTAATAAATCTAGTGACATTTTCTCAATCTTCATTCAATTCTATCCTAAATGTAATAATATATTTGCCTTTAGCCTCATTCCATCTCCTGAAACTGGGTTTTTGTTTGTTTGTTTGTTTGGCTTCTCTGAAATGGTACGCCTATACTTGCATTTCTGATTGTCCCTTTGGTAGTTTGTCAATCCTTGCTCACTCTCTTGGCTATCTCATCCACTGCCATGTCATATCTGTCTTCTCTAGGAAGATAAGTTCAGATTTCACCTAAACTCTAGATCCCTTTATTCTATTGACTGTTAGACATCTCTACTGATACAATATCACTGTTACCCACTTCCCTAAATTATTTTTCTTGTGCTTGTTATTTTGCTTATTGGTGTTACCATTCTTCCATTTAGGGTTGCTTATTCTTTTATCAAAAGTTTGTGTCTACTATGTGCCTGCTGGCCACTGTGCTAGCTCCTGGAAAAGGAGTGCATGAATGAGACACAATTTCTTGTCTTCAAGGTCATCAAGGTTTGGTGATCATCAAGTGATTGGAAAGACAGATGTGAATAATTATAACAGTATGATCAATGCAACAATGGAAGTATACATACAATAATTCTGTTAGAGGGGGCTCAAATCCTTGAAGTAATCATTACATTCTGATAGTTTTATCCTAATTTTTTTTTTTCTGAGACAACTTTCTCTCTCAGAACCATCTTAAAATTGCTGGCTTGAGAAACAGGTGTTTCCTGACATGATTAGGTATTCAAGTAGATCCTGCTAAAGATATTATAGAAAGGATTCCTGCAGTGGATACAAAGGTAGACCACAGAATCTTTCAGTTGAGCATGAACTTTCAGTCCATGCCATTCCAAAGTTCCAGAAAGCTAAGAATAGTTCATAATGACTAAGGAAGACAGTGTCTAGTAATTAATAATTACCAATATTAATGTAGCAATTTCTTCAAGATACATTACAGGATGTTTTATGTTATCCTAATAACCTTATATGTATAACTGCATTCGAGTCTTTTGAAGAGTTAACCATTTGTATCAAGTCAAATCTAGTTATTAATCATAACAGATAATTACTGAGCTCCCTATATATGCTAGGCACTGTGGCTACCACAGTGAATGAAGAGCATATGGTTGAGCAGGGGAGTGGACTTGTAAATAATGCATCATTTAATGACAGTGTGGTAAGTGTTATGCTGGGGTAGTTCTGCCTAGGGGAGTGAGTCTTGGAGACATTAGACCTGAATTATTATGAATAAGCATTTTCTAAGAAGCAAGGAAACTGTGGACAGAGGGAATAGTATCTCTAAAGACACTGTCAAGAAAGTGTATGTTGTGTTCACTGATTGGAGTGGCTGAATTATAGATTGGGTGGCATGAAGTGGGAAGGATGAGTTTGGAGGGACCATATTATGAAGGGCCTTTTAATAATGATTAGTAATTATTTTTCGTTTATAGGCAATTAAAAACAAAAAAACAGGAGCATGGCTTTGGACTATTAGTAGAAACTTTAAATCCAAAACTAGCTTATGTCTCTATAATAGCATTTTAAAAAATGTACACATTAAAAGTTAAGATTGCACTTTGGGAGGCGGAGGTGGGCAGATCACTTGAGGTTAGGAGTTTGAGACCAGCCTGGCCAACATGGTGAAACCCCATCTCTACTAAAAATTCAAAAAATTAGCGGGGTGTGGTGGTGCACACTGGTAATCCCAGCTAGTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGCGGAGGTTGCAGTGAGCCGAGATCACACCACTGCACTCCAGCCTGGGCGACAGAGTGAGACTCCATCTCAAAAAAATAAATAAATAAAAGTTAAGATTCAGGCTGGTCACAGTGGCTCATGCCTGTAATCACAGCACATTGGGAGTCTGAGGCAGATCACTTGAGCCCAGGAGTTCAAGATCAGCTTGGGCAACAAAGTGAGACCTCAGCTCTACAAAAAATAGAAAAATTAGCCAGGTGTGGTGGTTTGCACCTGTGGTCTCAGCTACTTAGGAGACTGAGCTGGGATTGCTTGAACCCTGGAGGTCGAGGCTGCAGTGAGCCATGGTGACAGCACTGCACTCCAGAGCCTGAGGGACTGAGTGAGACCCTGTCTTTAAAAAGAAAAAAAAAAGTTAATATTTGTAGTCTAGTTTACATATCCTGTCTAAATTCATTTCTGATTTATTGTAAAGTAGACTATATCTATAAATATATAGGATAAAGAGGAAACCAAGAACTCCCTGCATTTAATGTAATTTCCCTTTGTAAAGCATCCTATAAAAATTGGTCCTGAGTTTTGCTTATTATTATCTTGGATTTTAAACTAAACTTTACCTTGGTTTAACATGACAGAATAAAATATTCCTTTCGTATGTTCAACTAAAGAATTAAGGCCTTTTGTTATTGAATGTATTGTATACAAAATCCCTATTAAGGTGAAATACAGAACTTAAAATAGCTTTGCTTTGGAAGACAGTATAGTGCAGTCGATGAGAGTGCAGGCTGTAGAGTTAGACATACCTGGGTTTGATCCTCTTGAAGTTGTGGGACTTTAGACAAGTTATTTAAATTCTTTAAAACTGTTTCCTCATGTGTAAAAAATGAATACTAATACATCTTCATAGATTTGCTGTGACAACTCAATGAAATAATGCATATAAAGCACTTAGCCAGATACCTGGAATGTAGTAAGTACTCAAGATTTGTTAGCCGTGTTACTGGAACATTTTTCCCCACATTATAATTGTTTTTATAGTAAAGAAATAGAAGGAACTTGTTCTGCTTTTAATCTGATTTTCTCTTTTTTATTTTGTCTTCATCTTATTCAGTGTCTTTTTCCTTGGAGCCATCCTTGTAATAACAGCTACTTTTTTGTATGGTTATGATCCCAAACCTGCAGGAAATCCCACTAAAGCATAGTTGTATACTATCTTTAACTGGTTTTTCACGATGGGGCACTAGGAATCTCGACATTAATCTTGCACAGAGGACTTCTACAGAGTCTGAGAAGATATCATCATGCTGAATCTGATCATACTGTTTTTTAAAAGTTTAAGGATAAGACATGTGTATATGTAACAAAACACATTGCATCTAGAAATCAAAACTTGAAAGTATTTCCAGGGATTAGGATTAGAAGGAATATTAGAGGAAACTTGAAATCTGAGTTTAAAAAGATTTTACCTTTTTGATTGCTGCAGAAATGTCCTATGCACTCTTTGCAAGAGCACACAACAAATGTCAGATACCAATTTTTGCAAATTAGATTTAATCTTATTAAATGTTTTTATCTTACTCTTTCTGTACAGATATATCAAATCACATGAAATATTTAAAGTTTGAAAATTATAATTACCTATAAAGCTGTGAAAAATAGAAGTATAATTTGAAAAAACATTTCACTTATCAGAGATTTTTATATTTATACAAAAGATTACTAAATGAAGGATTGCTAAATGTTTTTGGTTCAATTACATAAAAATTAATATTCTGGGTCTGATCTGTCAGAGAATAAATATCAAATCTAAATTTAATGTAGAGATACATACTATTTCTCCATATGAATTTTAAGATATTTTAGTGCTTCAAGACTGCTGAAAGCAATCCAGTTGCTCCTGTGCTAGATGGTAGCCAGAGAATTTTATAGTAATGGAGGTTAGCCCTTAATCTCTTCATTGCATTTCATTTCTGTAAATCAGATTAAGTCCTTAATATTATTTTAAATTAAAATTTGTGTGTAATTGCCATTAAATTTTCAAAATGTAATTTAAAAGGATTAAATACTCATTTAATAATTTAAAATAATTATTGTATAATATCTACATTTGGAGAATTTTGAACTATCAAGCATATACTGTATACAGTTAGAAAGTTATTAAATGAACATTTTACTCATTGATCTGTAAAAACTTCTTTAATCTACAACTGTTTACAAAAACAACATTCAAACAAATAGTACAGATGTCAGTGACAGAACAAAATGACTTTTCTTGGAGACATTCCAGATTGCCATATTACTTTATTTTAAACAGCGCTATGACTTTAAATCCAAGGCTGCTCGGAAGATTTTTTTAGGTCTCTCATAAGCCTATTCTTCCCTGATCACATGAGTGGGAGAGGTAGCCAAATTTTGAATTCCCTTTCTGTGTTCCCACAAGAGCTGCTGAGAAGGCCTGGCGCAGTGGCTCACGCCTGTAATCCTAGCAGTTTGGGAGGCTGAGGCAGGTGGATCACAAGGTCAAGAGATTGAGACTATCCTGACCAACATGGTGAAACCCCGTCTCTACCAAAAATACAAAAATTAGCTGGGTGTGGTGGCACGCACCTGTAGTCCCAGCTACTCGGGAGACTGAGGCAGGAGAATCACTTGAACCCGGGAGGCAGAGGTTGCAGTAAGTGGAGATCACGCCACTGCACTCCAACCTGGGCGACACAGTGAGAGTCTGTCTCAAAAAAAAGAAAAAAAAAAAAGCTACTGAGAAGGAGCCACCATTTTGCTTTAAAATAGTGGGTTTTTTTCCTTTTTAATGAACACTGGAGCTGAGGATGCAGATTACATGAGTTATTTAATAAGTTGATTCAAGAGAGAGAGAGTATATTAGAAATGGTAAGATGATTTGGTGGGCAAAAATGCTTTCTATAATTTGAATATGGGTTTAATTTAATTATTAACTATAGACAGAATTACTTACTGAGTATAAGAAGTATTTTACTGTCTGGGCACAGTGGCTCATGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGTGGATCACAAGGTCAGGAATTCGAGACCAGCCTGGCCAGCATGGTGAAATCCCATCTCTACTAATAATACAAAAATTAGCTGGGCATGGTGGTGCGCGCCTGTAGTCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATTTCTTGAACACACCAGGTGGAAGTTGCAGTGAGCCAAGATCGTGCCACTGCACCCCAGCCTGGGTGACAGAGCGAGACTCCGTCTCAAAAAAAAAAAAAAGAAAACACACACACACACACACACACCCACAGTATGAATGAAAAAAATAAAATACTTCTTTTTTTTTTTTTTTGAGACAGAGTCTCACTTTGTCGCCAGGCTGGAGTGCTGTGGCGCGATCTCGGCTTACTGCAACCTCCCACTCCCTGGTTCAAGGGATTCTCCTGCCTCAGCCTCTGAGTAGCTGGGATTGCAGGCGTGAGCCACTGCGCCCGGCCTATACTGTATATATTTTTAAAGACTGTTCTAATAGATATAAAAACTGTAAAAAATAAGTATTTTTATATAGCTCTCATGGATTTTATTAAACAGAATTGGCTCAAAAATACTATGTTACAGACTGTTGGGTACCCTTGCCTAACGTGAACTGGCAGTGTTACCTTGCTTTTGCAGTAATAGTCTACAGATTGCAGGTCTCATCAATTCCATCCAAAGTTTAAAAGCATTTAAAATTACCAAATCTTTAAAATCACTTTGGTGGTGATTCCAAATTGGTACCAAGCAAACTTTCTGGATGCCCAACATGATTTTCAGTAACCACCCTTTAGAGTATTTGTTTACTAAGTTCACCACATTTTGAACATGGTAGTTTTAGACTGCAATAATATTTAGACTTACATTATTACTTACTGCTAAGTAAAATCTAAATCCTGCAAATGCACAGAATTCAAGCTGAAATATAATGATTTATGTTTAGCTCACATTGAAGTATTGGTTGGTTACTTATGTATTAATGCAGTGTGCATTCACATTTAATCAGGTTTAGTCTGTTTCTATTTTAATAATTTTAAAAAATTATACAAGCAAATTAGATATTAGACATGTTAGTTACAATGGTAACACATTTTTAGGTGTCGAAACACAATTTTCAAAATTCCTAATGAAAGTTATAAAAATGTAAACAAGAATTGTAAAAATGGACAAAGTAGTCAAATATATTTTCAAAGCACAATTTTATTAGACAGGCATAATTTACATTTTGCTTTTCTAGTGGGTTTGAAAATGTTTATTGGAGATTGGGCTATGTAGTTTATAATTTTTAATTCATAAAAAAGTAATCATACATGAGAAGGTAGACCTGTGCCCTAGGATCATGTCACATATACAGATAATGCCATTTCCTTGTGTGTGTGATGTGTGTTTTGATGACCTCCACAGGCCTTACTGTATCAAGCTTTTATAATGATGACTCCTTCATTATTTAAATTCCTATACTTTTTATTTGTTATCACGCAACTACTTTGTTCAATGTGAAAATGTGCTAACTCATGGGAGAAGAGTGCCAATTGATAGTTCTTTTAGCAATTAAGAATATGGTATTTGGGAAGAAAAGTTTGAAATGCAACAAATGGATATTTCAACACAGTAGTATTATATTATCAGTTCTTTAGTAAGTGATTTTAGAGATGTTGTAGGCTACTTTTACGGTGGAATATATAGTATAGAGATGCAAAACTTAAATGTTTACATCAATTTATATTGAATGTCACATAATTTCATGGAAGGAAAGGTAGCTTGATATTTAGATTCTAAGATATAATCTGAAAGGAAACTAATTATGTTCTCTACACTTACTGTAATACTGATTATTCTTACATATCAAATTATTGAACTTTAAAAATTTCATTGTATAGTCATTAAACTGAGTTGGGTTTTTTCTTAAAGGGTTTAGCATCACTCATTTGATTTACACATTCACATTATAATATTTAATTATCATGGGTGTATGCTTTACATAAAAAAGGTTTATAAAAGTTATTTATGCTATATTGAAAGTCATCTTAAGAATCTCCAGGTTATTTAAAGTAGTTATAGGAGCAGAGAACAAGCACCTTTATCAAAATCTGGTCCTATGTGCCTTGCTTTACCAAATACCTGATTTTTCTGGAGGGTGTTCCTGTAATTCACAACTGTAGACACATGGGCAAAATTAGGATTTTTAAGAATAAATACATTTCTATTTTTTTGGTTGTTTCAACATTAGCTCTTCAAATTCATTAACAAAATTAAAATAGGTATATTACAAAAGCATAAACATTTGTGAACAGTACTTAAATAAATTGTGATACTATTGCTCCATCATTGAACTTTTTGAAACTTTAACAATTGTATAAAACTGTCAGTTTGTTGTTTCATTTGTAATTACAAAATAATTTAAAAACTTTTTAAAATAATTTGGATCCTGACTTTGTCTATATCTGTATTTCATTTGTTTAGAAAGATTCTTTTGGGTTTGATAATGTAATTTGTATATTTAAATTTTTTATGGACATAATTCAAAGGAATGTATAAATTGGTCTTTTGTTAAATGGCTTTTTAATTGATAAACTTCTCTTGTCATTTTTTGGTATCCAGCTATTACCTATTTAATAGATTTATTGAAATAGATTATTTTCATAAAGAACTCTATACAAATCTTTTTCTATATTTCCTTATTTTCCTATTTACCTGTGTCTATGACCTAACCTATGAATTAGTCTTCTCTCTTTATATATCAAAAATGAATTACTGATCTTTTTCTCTGGCTCTGTAGTATCTCTATCACTGTCACATGTGATCTTTCTTCCTTTTCTCTAGCCCATATTCTAGCATGAAATACTGGGTTGGCCAGGTGCAGTGGCTCATTCCTGTAATCCCAACACTTTAGGAAGCCAAGGCAGAAGAATCGCTTGAGCCCATGAATTTGAGGCCAGCCTGGGCAACATAGCCTTGTTTCTACAAAAAATCTTAAAAATAAAATTAGCCAATATGTGGGCATGCACCTGTGGGGCCAGCTACTCAGGAGGCTGAGGCAGAAGGATTGCTTGAGCCCAGCAATTTGATGCTGCAGTAAACCATGATGACACTACTGGACTCTAGCCTGAGTGACAGTGAGACTCTGTCTCAAAAAACAAACAAAACAAAAACTGAAACAACAAAAAAAGACTGGGTTTATTTAAGCTAGTTAGAATTTATCTTTCTATATGTTAATAACAGCCTAACAGATTTTTTGTTTTAAATATCTCTAGGCTAGCCTCAAGGTTAAGTAATTATAGAAGTTTGGTATGTATTTTCTTCATAATTTGAATATAATTGCTTCCATTGTGACTGTCAATTGAATGCATGGAGATCAATTGTGATAATATACAGGATTTTAGTCCTATCTCTACTGCTGAAGTAACCTTACACAAAATACTTTGTAAAAAAATCACTAAAGTGCCAGCATTTTTAAAGTGTATATTTTTCTTTGGCAACCTCTCATGAAAAGCACTAACTAAAAATATTTAATAATCTTTTTTGTATTACAGTGCTTCTTTTGTTGGAAATATATCACAATCCTCAAGTTCCACTGCTATGCAAAAGTATCTTAGAATCTGAATCTTATAGATAATACTACCTTTTTTTTTTTTTTTTTTTTTTTTTTGAGATGGTGTCTCGCTCTGTCACCTAGGCTGCAGTGGTGTGACCTCACTGCAACCTCTGCCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCCTAGTAACTGGGATTACAGGCCATGCCACCACGCCCAACTAATTTTTGTATTTTTAGTAGAGACAGGGTTTCACCATGTTGGCCAGGCTGGTCTCAAACTCCTGACCTCAGGTGATCCGCCTGCCTCGGCCTCCCAAAGTGCTGGGTTTACAGGCGTGAGCCGCTGCGCCCGGCCAATACTGCCTTTCAAAAGAGACTGATAGTGACCAGCATTAGTAATCATACTGGTGGGGTTTTTTGGGGTGTTTTTCTGTTTTGTTTTTTGTTTTTTGTTTTTGAGACAAGGTCTTCCTCCCGTTGCCTAGGCTGGAGTGCGGTGACACAATCTTGGCTCACTGCAGCCTTGATAGAGCAAGCTCAAGTGATCCTCCCAGGAGCCTCGGCCCCCAAGCAGCTGGGAATACAGGTGCGCGCCACCATGCCCAGCTGATTTTTGTATTTTTTTGTAGAGATGGGGTTTTGCCATGTTGCCCGGATTGGTCTCAAACTCCTGAGCTTCAGTGATCTGCCTGCCTTGGCCTCCCAAAGTGTTGGGATTACAGGCGTGAGCGACCACACCCAGCCCATATTGGTCTTTCTTACTGTTCTTAAAAAGAGAATTCCTTTAAGGCAGGACCGATTACATATACACTCTAGAAAAGAAAATAGCAAGGAAGAAATAAATTGCCTTCAATTACCAAAGATTTGAGCTTCTGCTATGGCTGAGAGTGTTTTGGTCATTGCAAATTCAGGGGTTTCCCAAGTTCACCCTCAGTTCTGGCTAGAAAGAGAAAACTTACTAAAAGCTATTATACTCACAGTCATATTTATTACAGAGAAAGGAAATACAAATTAAAACCAGCCAAAGGAAGTGACACATAAAACAGAGTCTAGGAGTGGTCCAAACTTGAGGCTTTCGGTGTCCTTTTCTTGTAGCGTCATGGAAGGTGTTATCTACTCCTGACCACAATGTTTGACAGTACACACATAGTATTGCCATTCAGGGAAGCTCACCTAAGCTTTGGTGTCCAGATTTTTATTGAGAGAGGCTCTATTAGTTGGCATGGTTGGTTGATTTTTTTGCCCATGTAGTTGTCCTCTCTTTCCAACATCTGCCCCTCCCTGGAGATCTGGTTGACATCAAGACTTCAGGGCCTCACCATAGGTTATCTCGTTAGCATAAACTGTCAAGTGTTGTCTAAGGAACCCACAATGAATAATAAAGACATTCCTATCAGTGAGAACTCCCAAAGACTTACACCAGAACTTTCTTTGGATGGGCCAAATTTCTTACTACACAAAGACCATTCATCTCTATACACTTCCTTCTGAATTGATGAGGATGATACAAGCAACGACAATTCTTCTTTTCAGAGACTTTTAATTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTCTCGCTCTGTCACTCAGGCTGGAGTTCAGCGGCACGATCTCTGCTCACTGCAACCTCCGCCTCCTGGGTTTGAGCAATTCTCCTACCTCAGCCTCCCAAGTAGCTGGGATTACTGGTGCTCACCACCACGCCCAACTAATTTTTGTATTTTTGGTAGAGACGGGTTTTCACCATGTTGGCCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCTGCCGGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATCACACCCAGCCCAGAGACTTTTAATTTATACATTGTCATTTTTAATTTTAGAATATCTGTCAAATGATTCTGAACATAACATGAATCTAGTGTGGAAAAATGTTTATAATCAGATATTGTGTTAAGAACATATATATATATATAGAGAGAGAGAGAGCATAGTATTGTCATTTAGTTTTCACTTCTTATGTTTAATGACAACTTTATAAATGCTGGACTATTTTAAACTACAAATTTAAAACATGGTTTATAAACCTTTTACCTGTACTGCATATTAATAAACAATTTTGAACTAATTTTAAATTAGGCATTCCTATCACCATTATAAGGAAGAGCTAAAACCTGATACCAGTTGTTATTTTGGCTCACAGTAGGGCTTCACTAGTGTTAGGATCACATTTTCCTCCCTTGGACCTGAGAATCAGGTGTTTTTCGTTTAATGCATTTATTTTCATTCATTCATTTAGGATTTCTAAAATGTTAGAGGTAATTTGTTTAAAACAAATAACATGGTGTTCTTTTGGAAATATACAGTTTATCTTAGATATACACTTTCCTCATTTTAAAGGTAGAACTCACTGCAGGTTTAAAAGAATAATGAATATTGTTACAGTGTTTAGTGATAATCACTGAAAATTCACGTTAAAATATACTTTAAGCTTGTCCAACGCGCGGCCTGCAGGCTGCATGCAGCCCAGGATGGCTTTGAATGTGGCTCAACACAAATTTGTAAACTTTCTTAAAACAGTATGAGATTTTTTTGCAATTTTTTAAAGCTCATCAGCTATTATTAGTTACTGTATTTTATATGTGGCCCAAGACAATTCTTCTTCTTTAAGGGTGACCCAGGGAAGCCAAAAGATTGGACACCCCAGCTTTAAATGTAGAGTATTTAAGAGGTTCAGTTAAATGTACTAAATCAGACTTGTGTGATTTAGACACAAAATCTGTGTGTGTGTGTGTTTGGTCTTTTAAAAGATACTGCATGTACAAACATGATTTCTAATAATTGAGGGTTTATTAAGTTTTTCCCTTTAGGCTAAAGTTTTATTTTATTCTGCTTGGATAGTAAATACAGCATTTTAAAATGATGATGCTAAGTGCTTTTTTAATATATGCATGCTGCTTCAATAGGTCTTTTAAATATAGCCAAGGCAGGGTAATATTTCCTGAGTGATGATAGAATTATAGAAAATCATGAAAGATGTGTGCCCATGTTTCATTTTATAACTGTTCCAGTCATATTGCTATGAATTCTAGAATTTTAAAACCTGCCACACTTATTTTGGATGTTATATACACATAATGCCTCAAATTTCCATTATTTCATATTATTTTTCTCCATTTCCCTCGTGTCAAGTTTCATTTTGTTTTCAATGTTTTGTTTTTATAATATTGCCATTTTAATGGCTTCAATCATGCTATGACAGATTGTGCCAATTTAGCTTTAAAAAACTCATGGGCTACCTCCATGAACTCACACATGCGGCATTGTTAAAGAACCTTTTTTAAAATTAAATTTTCTCCAGCTTTTCATTTTGCAGCATACAAAAAAATTGAAAAATTGGTATACCACCTAAATCCAACCATTATTAACATTTTGCCACACTAGCTTTATCTCTGTTGGGACACTTTACCCCTACATACTCCAGCATGCGTGACCTAAAGATATGGACATCTTCTTAATTAACCTCAGTGCCATTATGACACCTAAGAAAATTTAAATTAATTCAGGTCACTTCATCTAAGTTCAAATTCAAATACAATTGCCCCTCTGTAAACGTGGGGGATTGGTTCCAGGACCCCCACCTCGTGTATACCAAAATTTGGGCACACTCAAGTCCTTAAGGGGAAACCGCGTATAAGAAAAGTCAGCCCTCCCTATGGGTGGGTTTCACATCCTGCAAATAACTGTGTTTTCCATCTGCATTTGGCTGAAAAAAAATCTGCATATAAGTGGACCCATGCAGTTCAAATCTGTTGTTCAAGAGTCAACTGTATTCCCAGTTGTCTCAAAAATGATTTTTTATAGCTTTTTTGTTTTGGAACGAGGGTAAAATCAAGGTTAATGCTTTGTATTTGATTGTCTCCTTTGGTCTTGAAGTCCCCTTCCTGCACCACCACCTGCCCCTCAATGGCTTTTTTATACCAAGAGACTGAAAATGCTACCTCTAATGACTTTTCTCCCCATGATAGTACTCTTTGAAGGACCAAAGTTACTTTGCTCCTATTATGTGGTTTGATTTGTTCCCTTTATTCCTTTCATGTCCTTTGAACTGGAAGTCAAGTCTATAGGTTTGATTACCTTCAGGTTAAATGTATTGGGTAAAAATATTTCACAGATTTTATATTGTATCATATTAAAAGGCACAGATAATCTCATTATTAATGTTGGTAATTATTTGGTTAAGATGATGACTACCCCATTGTAAAGGTTATTGTAAAGTTCTCCCTGTGGTAAAGGTCATTTGTTCACCCTTAGTAATCTATGGAGTAATGTTTTAGCACCAGGAGAATATTCTGTTTCCCAATGCCTTTAGCATCAATTGATGATTCTTGACTGAATCATTTATTACACTGGGGTGCAAAATGGTGATAAATTTTTTTTAGTGCTCTAAATTTCTTCTACATTTATTAGTTTATATTCTTTTTTAAAATAAAAAAGTATATTTCCCTCCTGTTTCTCTAATATCCCTCTTTAAAAAAAAGACCATGTCTTCATGAATTTAAAAAAAATTACTCAATATATTATAGCCAATCGGTCACAGTTCTTTTTGATGCTCATATATATTGTGAGCCTCTAGCCCATGTAAGTCACAGCAAGCTAGCTCTTGGGTTTTTTTTGAGATGGAGTTTCACTCTGTCACCCAAGCTGGAGTGCAGTGGCACGATCTCGGCTCACTGCAACCTCCACCCTCCGGGTTCAAATGATTCTCCTTCCTCAGCCTCCTGAGTAGCTGGGACTATAGGTGCCTGCCACCGCGCCTGGCTAATTTTTTGTATTTTTAGTAGAGATGGGGTTTCACCATCTTGGCCAGGCTGGTCTTGAACTCCTGACCTCGTGATCCACCCGCCTCGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACCGCGCCCGGCCACTCTTGTGTTTTTTAAACATGACCACATTAGTCTTTTAGTCCTTATTTGCTTTAGGCACAAGAAATCTGAGGCTTACCTTATATTTGTCTTGTTCCAAATTTGGATTATCAGCTAATTCTCCAAGGAGCCCTATACCTTTTAAGGGGAGTGGGATTTATTTAATTTTTTCTTTGGAGTCCTTCTAGCCAGTGAATGGAATTTATTATAGAATCCATTGTTACTGAGCTGTCATTGTTTCTAGGCCATTTTAGTGGACATAGCTAGGAAATACAATTTTAAAAGATCATGAATTCAAATTAATATTTACAACTAAAATTTAGCAACATGGCCAGGTGCAGTGGCTCATCCCTGTAATTCCAGCACTTTGGGAGTCTGAGGTGGGAGGATCACTTAAGGCCAGGGGTTCAAGACCAGCCTGGGCAACAGAGTGAGATCCTGTCTCTACAAAAAAAAAAAAAAAGATAAATTTTTTCGAAAAGTTTTATATGAAAAGTGTACTCTGAAAAAATCTAGCTGTCATACCTATCCCTCCATTCCTAACCATCTCTTATAAGTAATTATTATCCATCTCTTATAAGTAATTATTATAAGTATTTTCTTAGTTTTCCAATTTATCTTTCATGTGTTTCTTTTTAAAAAGCAAACAAATATGCATATTGCTGTACTTACTGTGCCTCTTTCACAACATGGGGTCTTTATAAAATTTCTTTGGATGTTTAGAAAGGTTTGTATGCTCTAATTTATACTGATAACATTTTATATAATGCATTTAGAGTCCCCAATTTGCTTGTCTAGGCCTCTACTATTTAATTTGACACCTTTAAATGATGTTCTTTGACTCTTATCTAATTGTCTATGTGACCGTCAGTGAATATATTATCTGTTCATATTTTCTGTAGTCTTATTTCTTTTTGATTAGAACATATAATATTTACACACTATTTTCCCATCCTAATCCCCACCTTTGTAATAAAATTAATTAAAATTTTAAAAAAGTATATTTAATAACCTACTGATACTTACCTGAAGTCTCTCTGGTCAACTTTTGGCTGGATGAAGCTCTTCTGTAGAGTATTCAAGGAGGGCTTGTAAGTACTGTACAATATTCTCTGAGTTGTGACATGTTTAAAATTTCTTCTGGAGCCTAGATACTTAAAGTACACCTTGGCCATGTAGAAAATCCTTGACAAACATTTCCTTTTCTTGAGTTTGTCTGAATTTGTGCTTCACTGTTGGGTTGCTTGATGTTATCATCAAGAAGTGTGGCACCACTTGATTTTTTTCCCTTTTGGAAATGACATTTTGTTTGTTGTTTTTTACCTAGAGACCCAGATAATTTTTTCTATATTATTTTATCTATGATCCTTTTTATCTCATTTTTGTTCTCATAGCTGTTTTATTCCTCTTCTCTGTATTTCTTACTTTACTTTTATTTGTGTCTCTCAGGTACCCTGTGATTTAGTTTTCATTTTTAGATGATTTTATCTTTTCAGTATATTGCCTATGTTTGTTCAACTCTCATTTTACATCTTACTATTGAGTTTAAAAATTTTTATTTGCTGTGTTCTTTTATATGTGCAATTAATGATATTCATGTTGGAGTGTTGTATATTATTTGCCTCTGCTTCATATTGTCTAGAGGGTAGTATTTTCTATTGATTGAAAAGTTTTCATTTTCTGTTTTTTATAATAGCCTTGTATGGATTTTTGGTGGCTTTTCTATTCATGATTAAATGTGTAGGATCTTCTTCAATCAGCAATAACAGGTGGCTCTATAGAATGGAGGGTAGAAGGGATGTGGGTGACTTACTCAGTTTTTAGTTAAAGAGGACCCTCTTCTGTTAGCATGGTGAAGTGCAGTTTCTTTAATAAATTGTGCATGGTGGGGGTGGGATTTGGATTCTGTGATACAATCTTGTTTCTTTAGGAATCTTTTACTTTTGGCCACTTGCCTTTCTTTCCAAGGAATCCCACTCCCTTTCAAGGTGCCTCATGAACTGTTTTCATGAACTTTCCAAACATTGGTTTCTGCTTGTTTCTAAGCCTGATTCTTGGCCTTCTCATTAATTTTCAAAACTTCCAATATCCTTCCAAATAATTCCCTTTTGCTTACGTTAGCGAGTACTAGTTTGTTAGCCAGTGTTAGTTTCTGTTGATCCTAACCAAAAAACCCTAACTGAGATATCAGTCTCTTAGCGCAAAGTTTGTGAATACGGTCATCCCTCCATATGAGGAGGGGAGTGGGAATTGGTTGTGGGACTTCCGTGGATATCCAAATCAGTGGATGCTAAAGTCCCTTACATAAAATGGCATAGTATTTGGTTATCATCCTCCCCATACATCATCTCTAGATTATTTATAGTACTTAATACAATGTAATGCTGTGTAGTCATACTGTATTTTTTACTTGTATTTTTGTTGTTTTGTGGGATTTAAAAAATATTTTTATTCTGAGGATAGTTGAATCCACAGGATACTGAGGGCCAGCTGTATTCACAACCCAAATCACATACAAAGCGACAAGTTCATACACAATAGGCCTATTAGAACAGGACTGTTCTCTCTTGTTTATCATTGCAGCCTTTCTAGCACAAAGCCTGGGACATTCTGGACATTTAGTATGTGTTAAATTTCTCTTACTACATTATTTCCAACAGTATTTACTGCAATCTGCAATTACCTTCCTTTTGTTTTGTAACTGTGTCCCCCACTAGAATGTAAGCTCTGTGCAGATAGTGTCTCATTTATTGATGTATCCCTGGCATCTAATAAAACACTGACAACACAAGCACCCAGTAAATATTTTTTGAATGACTGAACAATAACCAGTTCATAAGGCTGATAAAATTGGTATAGCTAGATGAAGTATGATTTTGAGGGACTATGAAAATCAAAGTAACCACACAATAAATTATCAGCCCTCTACTTCCATTCAAAACAAGCTCCTGGGAATTGAATTATGAAATCTATCATATTACTTTCTCTAAAGAACTTCAAGTTGGGTGTCAACTAAAAAGTTGCAGGCGAGGCGCGGTGGCTCAAACCTGTAATCCCAGCACTTTGGTAGAACTGAGTATCTCTTGAGGCCAGGTTTGAAACCAGCCTGGTCAACATAACCAGACTCTGTCTTTACAAAAGAAAAATTAAAATTAGCCAGGCATGGTGGTGTGCATTTGTAGTCCCAGATACTTGAGACGCTGAGGCAGAAGGATCGTTTCGGAAGAGGCTGCAGGAGGCCATGATGGCACCACTGCACTCCAGCCTGGGTGACAGAGTGAGACCCTGCCTCAGAAAATAATAATAGGCCACGCATGGTGGCTCACACCTGTAATCCCAGCACTTTGGGAGGCTGGGGCGGGAACATCACCTCAGGTAAGGAGTTCAAGCCTGGCCAACACGGTGAAATTCCATCTCTACTAAAAATACAAAAAAAATTAGCCAGGCATGGTAGTGGGGACCTGTAATCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATCACTTGAACCTGGGAGCTGGAAGTTGCAGTGAGCCAAGTTGGCACTATTGCACTGCAGCCTGGGCAACAAGAGCAAAACTCTGTCTCAAAAAATAAATAATAAAAAAAGTTTCAAAATGAGAATATATGTTTCAAAACAAGTATAATGAATATACTTATTGATTGGAAAATATAATTAGAAGTATCTATCAGGCTATAAATTGCTTTTCTTCTCCCTTCCATGGAAATTAGTTTTTTTTTCCATTTTTAGTCAGTATGAAAATACAAGGAAAAGGAAATTCAATCAAATTTACTTTTTAACATTTTATTTGGAAATAATTTCAAACTTACAGAAAAGTTGCAAAAACAGTACAAAGAACTCATACATTCATTTACTGTTTTTCCTTTTACCCTATATATTAGTTATTTATAGCTGTGTAACAAATAACCCCAAAGCTTAGTGGCTTACACCAAGTACTTTTCATCTTACACTGTTTCTGAGTCAGGATTCCAGGAGTGGCTAAGCTAGGTGGTCCTACCTCTGGGTCTCTCATGAAGTTGTAGTCAGCCAAAGGCTTGACCAAGGTTGGAGGATCTACTTCCAAAGTGACTCACTCCGTGGCATTTGGTAGGAGGCTACAAACAGTTCCTGGACAACTGGATCTCTCCATAGGCTGCTTGAGTGTCCTGAAAACACGGAAGCAGGCTTCCCCAGGCTCCAAGCCCCAAAATGAATGAAAAAGAGACCCGCAAAGGAAGATGCAGTGCCTTTTATGACCTAGCCTCTGAAGTCAATACTGTCACTTCTGTTTTGATCTATCAAGAGTCACTAAGCCTAGTCTACACTCAAGGGGAGGGGAATTAGAGTCCACCTCTTCCAGGGAGGAATATCATTGAATCTGTGAACATATCTTAGAACTACCATACCTAGTTTCAGTACTTTTAAACATTCGCCATTTTGCTTTGTCCCTCTCTTTTCCCCACCTACATATACATACACATACATGTTACTCCCTAACCATCTGAGAGTAGGGAGCATGCGGTGTATCCCTATCCCTCGTGTTTTTCTCTTAAGGAAAAGGATATTCTATTATACAACACGGTAGTTATCAATATCTAATTTTAACATTGTGATACTTTAAAGTCCACTTCCACTTGTGTAAATTGCCCTTTCTAGCAATGTTCCCATCAAATTTATTTTTAAACAATACAGTAAAAACGTAGAGGGCCACAAAGGGTGACATCGGTCAGGTAAGGTATTTTTTTTGGCAGGGAATAAAAAAGGTCCTGGGTCTAGGGAGGTAAACAAGCGTGAGCCAGCTGAGTTCTAGCGGGGGTCCCTGAACACCAAAAGGACAAGACTGTTTCTGAAACACTACATTATCTCTTAAGTTACCCATTACTTACGGAAAATGATTTTTTACTGTTCCCTTCGGTTCCTGTCTTGGTTAGAACACAGCTGGAGATTGTGTTAATAGCTTAGGACGTCTGTTTCCGTGAGCAGGTAACAACTTTTTGAAACAAATTCCCTCATCTGCTGAAGAAGGGGGACAAAAACGGCCCCTATCGCCCAGAACCGTTGCGAGGATTTAGCTAGCTGGTGACGCCGGAGCACGAAGTTGTACAGGTAGCCAGCAGCACCCACGCGAGCCCGCGGTTACCCTGGCCGCGCGGCTACTGTAGAGTGGGCTGGCGGCGAGCGGGCGGGGCGGTATCACGCGGGAGGGGCGGGGCCCGCTCGTCGGCTGATCGCACGATTGTGACGCGCCGCCGGAGGCAGGCCGGGCCCTCAAGATGGCGGCGGGCGCCCAGAGCGGCTCGGCCCGGCAGTAGTGGTGGGACGGCACTAGCTGCTGGGGCCTGCCGCCCCGGGAGTGGCTGCAGCAGCGCCAGGAATCGAGGATGGTAAAATGACCCAGGGGAAGAAGAAGAAACGGGCCGCGAACCGCAGTATCATGCTGGCCAAGAAGATCATCATTAAGGACGGAGGCACGGTGAGCTGAGTTCCGCGCCGGCGAGCGTCCCTCGGGGCCCCCATCCGGTCTCTCCTTCAGACCCCCACACTGCCGTCTCTAGGCGTCCCGGTGCCTCCCTCCCTTCCCCCACCCTGTCCGAGCTGCCGGTGCCTCGGGGTCGCGGACCCGCATGCCGCCGCTCCGGGAATCGTCCTCCGCTGCTCGGGCTTGCGGCCTCCGGGGCCCGTCCTCTTTCTTTCCCGCACCTGCCGCCCTCTGCTCTGGCCGCCTCTGCAGGCCCTGCGGCCTCGAACCCCACGTGCGCCTCCGCCGCGGGGAGGAATGTGCGGGGCTCCCCCGGCGGCCCGCCCGCCGCGCCCCTCGTCGCCGCAGCCTCGCCTCGCCTTCGCCGCCAGGCCCCGCGGAGCCGTCGCCGCGCTTGTCAAGGGGCTGGGAACCATCCCTGCTCTCCCATATGTTGCTAACGGGGTGGCGTCTGGCGCGGGGATCCCGCTGCGGCCCCGTAGTACGTTCGCTTTCTGTTTCCACGTCTCTCTGCGTCGGTGCTCCGGCTCTGGGCTGCTTACAGTAAACCCTGACCGGAGATGGGCTTCCCTCACTTCCCGGAGTCGGAAGCATGACGGCAGACACCTGGGGCCTACATTCGAACCTGCTAGTTTTCAAAGAAAAGTCATCACTGTGTGTCTTAAGATCAAAAGTATTAGAATCAGTCATGGCCTAAGGATCGGAGGAGGACACTTTGAAGGGAAGAAAGGTTTGCTTTTTAGAAACAGTTGTCATCACAGTAAACTTTATGCAGTGTGTAGTTAACCAGCTGGGGACGTAGGATTTTTAATTGAAAAACAAAACAAAACAAAACTGTTTTATGCTAACATTTCTCCGTTGCTACACTGTGTGGTCTTTGTTGCATCCGCTGATACCGCGTTCTGAAATAGAATGGAAAGGTGATATATATGTTTCACTTACCTGAAGTGTGCAGAAATTGTACCATTAATTCCATTTCTGTTTATATCTTATTGGAGCCGCGATCAACTGCTAGCACAGTAGTAAATGTGTAAGTAGGCCACCATTGAGGATTTGCTGAATTCAGTTGAAAAACGTGACAAAATTTTATGACATTTCAGAACACGGCCCAGTCAATATGCCAAAGTTTAGAAAACTTGAGACATATGTAATGACTTTGGAATATATTTTTAGTTTAACGTTTATTATATGTTATAGCTTTGACATTTATTGAAAAAAAGAAACAAATTCCTCAAGTTCTTTTTATTGAACTTGATTAATTAAAACATTACTTTGATTAGATCGGTTATGAAGAGTCATAGCTCTTTTGACCAAGTAGGTAAGAACTATGTGGGGAGAAAAATACTGTTGCCTTTGTCTACCTTTAGAAAGAGACAATATTTTACATTCTTCATAAAATCTACAAAATAGTGGCAATGAAAGATTGTATTTTGTAAGACCAAGTGATATTTAAGATCAGTATTTTTTACAAAATGTAAGAATGAAACTGATTAAGAAACACAGCTTCCTTTTCCTTGGAAAGTTCAGTTTTATTACCTTTCTTTGGGGTTTTGTTTGATTTGCTTTACAGCAGATGCTTTCTTTCCAAATCCTGTGAGTTTTGGAAAAGATCGTTTTTAAACTTTCTTGTCCTATTATTAAGGTTGTAATTAATTCTTAGCCTGCTTTGGGACACAAAATAAAATGTTTGCACCAGCAATAGGTTTCACATAGAACAAATGAAGACTTTTCTTGAGGGCTGTGAACATGGGGGCTATTATCATTTCTCATCTTTATACACTTAATATTTCATTCTCTATTCTAAGAGCACTGGGCACTCCTTTAGAAAAGGGGCTTTGTTTTGTATGTTTGGATCCCACAGGGCCTAGTATGTGAATTTTAAAGTGATAAAAACACTTCTATTTTGTACTAGCACATTCCTAGATGAATTTTTATTGTAATTTTGTTTATTCTTATACGTAATCAGAGGATATATTTCAATAAATATCAGGGGAATATTTTGCATTATTTGTATTTTAATCCATCCCAGCTTTAAATTTAAAAAGTATAACTATTGCAGTCATAGAAATGATTGTAAAATGGTAGTTGCTTATCTACCTCTCTACTTACAATAGTTCAGACTACTATTATGAACTTTTTTTGTTTGTTTGTTTGAGATGGAGTCTCACTCTGTTGCCCAGGCTGGAGGAGTGCAGTGGCAGGATCTCGGCTCACTGTAACCACCGCCTCCTGGGTTCAAGTGATTCTCCTGCCTCAGCCTCCCGAGTAGCTGGGACTACAGGCACGTGCCACCATGCCTGGCTAATTTTTTATATTTTCAGTAGAGACAAAGTTTCACCATATTGGTCAGGCTGGTCTTGAACTCCTGACCTCATGATTCACCCACCTTGGCCTCCCAAAGTGCAGGGATTACAGGTGTGAGCCACCGTGCCCAGACTGAACATTTTTTAAGAAAGGGGAAAAAATTGCCATTTGATACTCTGTTGTTGTGTGTTTTTTAATTCATCGTATCATAGAATATTTCAGTGCTATTGCTGTTGACCTCAGAGTTTCAGAGTTTTTATAAAGTTCCGCCAATGGGTAGATTCATTCAGTGAGATGTCTGAGGCTCTATGGTCGGTACATGACAGTCGTGAACAGTATTTCACATACCTGGTCAATGGTACTGATTTGATCCCCCTTCTGATTTCTTCTTTTCAACAATGTTAATAAAATTCTTTCCCGTTGTCCTGCTAATGACATATATGTAAGCCTATTTGGCCAGTTTAAATATTTATAAACAAAACTAGTAAGAGTTGTTAATGATTTTTCTGAAAATTAGAGCAGATTAGAGCAGATTTGTAGTTTTCAACGGCTGAAGAAATAAATCCTTCTAAATGAGCCAGATTAATCGTAAGTTACTGATTTTTTTATTGAAATTGTATTTCATTGAATTGTATTTCATTCAGCTGAATGAAAAACAGGCCAGGATAAAGCTAACAAGTAGGCTACCTATGTGAGTAGACACAATTAAGATAAATTACATTAAGGTGTGTGATTTTATATTAGGTGTTTTTAACCTGGGTCTGTTCACCCTGAAGTTGTTTGCAAAATTTTCTTTTGGCTATACATGTTTCTTGGAAGAGTCCCAAAAGGTCCATACTTCCCAAAAGTTTAAGAGCAATTGTTCTGTTTGAAAACAGCATAAGTAACTAAAGAATAAGTTCCACATATTATATTCAGTAAATATTTAATCATATACTGTATACTACTTCACTGATGAAAGTAACCATATTAGTGAATTTGCTTTTAAAGCATCCATATATAGAAATAGTTTTTAGGCCAGGGGCAGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCAGTGTGGGCAGATCACTTGAGGCCAGGAGTCTGAGACTAGCCTGGCCAACATGGTGAAACCCCATCTTTACCAAAATTACAAAAATGAGCTAGGTGTGGTGGTATGTGCCTGTAATCCCAGCTACCCGGGAGGCCGAGGCACGAGAATCACTTGAACCTGGGAGGCCAAGATTGCAGTGAGCCTAGATCACGCCACTGCATTCCAGCCTGGGTGATGGAGTGAAACTGTCTCAAAAAAAAAAAAAAAGAAGTTTTTAGTTACAGGTTTTCATGTATGTAACATTCAGTGTAGGTATTTAAGACAGCTGAAATAAAAATACCTTCTGACATTTTCAAATACTAGAATTCTGTTTTGTTTTATTAAAGCATTACCACTTGTTTTTAAGCATTCCTGTTAGAGGCTAAGAGCTAAAGAGTTATTTACAGTATTCAAATTGAATTTTCCTTATCTTTTAAAATGCTCATCTTAAAATATGATCTTTATTGTTTTGGCCATACAATTGTGGAACTACATCTCTGACAGTGGAAAATGTATAGTTCTTTCAGAAGTTTGTGGTAAAATGACTTTAAAGATTTGATAGAAAGTAAGGCATATCTGAATTGCATGGTCGGAAGTACCTGAAAAAAGTAAAATTGATATATCATTTGAAAATGAAATGCATATCCCTGGATAAGCAGAGCACCAGATTTTTTTTTTCTTGGCATCCCTGATTTTAATTAAATAGGAGTCAGCAACCGTTTCAAGAGCAGGACCCAAGCTCTGACCCTTTGCACTCTTCACCTGCAAGGATGGCTGAAGTAGTGGCAGGAAAGCTCTCTGGGATGTAGGGCCTTTGTAGACCCAGAGAGCTGTTAAATAACCTTTGGTTGCTAGCATGCAAGCAATAAGAAGGGCCTGTGGTGCTTTTCTTTTTCTTTCTTTTTTTTTTTCTTTTGAGACAGAGTTTTGCTCTTGTTGCTCAGGCTGGGGTGCAATGGCGTGATCTTGGCTCACAGCAACCTCTGCCTCCCTGGTTCAAGGAATTCTCCTACCTTAGCCTCCTGAATAGCTGGGATTACAGGCATGTGCCATCATGCCCAGCTAATTTTTGTATTTTTTTAGTAGAGACCGGATTTTACCATGTTGGCCAGGCTGGTCTCGAACTCTTGACTTCGGGTGATCCACCTGCCTCAGTCTTCCAAAGTGGGATTACAGGTGTCAGCCACTGCGCCTGGCCCCGTGGTGCTTTTCAAAAAGCCTAGAAACATCAGGGTGTTTATATTGTCTTTGGCAGGTGTGTGGCTGGCAGCATCATTAATTACTTAGCTCCTTACCTCCATGGTTCAGTGTTTGGTTTAGATTGGTGTGTTTGGGGATAAATTAATATGCAGTTTTTTTTTCAGATGGCTATATGCATCCAGTTCATCCTCATGTAGTTAGAAGACTTGCATACCAACATAATCAGACCGTCTGCAGAAATTCTCCTACAGTTGAAATGTAACTCCTTTGCAGCTACTGAAAGTTTAAAGTTTAAGTAAAAAAATGAATAGCTTTCTTCAGGTAACATTCTGACAAGTCTGTATGATTTAAAAGTTTCAATTATAAGGAACTCTGATTGTCTTTTAGCATTATTTTAAATTGGAAGTGTGAAAGTAACAGTTGACAGTTTCAGCCAGGGTACATCAAGAAGAGATGAATATGGGTATAATATAGCTCTCAAAATTTCCAGTACTTTATAACAAAGAAATATCCCTCCCACTGCCCTGTTTTTTAAAAAATAAATAATACATGTTTTCCTTCCAGTCGTGGGAAACTTAATAGAATGGTTCAGGAGGGACAAGTATATGCAGCATACCTGTCATTTTCCATTCAAGTTTTACTTTATTTTTAAAATTTATTATTTTTTAAAATATTTCAATAGTTTTGGGGTACAGGTGGGTTTTTGGTTACATAGATGTTTTTTAGTGATGATTTCTGAGATTTTAGTGCACCTGTCACCTGACCAGTGTATACTGTACCCAATATATAGTCTTTTATCCCTCTCAAGCTTCCCCCCCATCCTCAAAGTCCATTCTATTAGTCTTACGCCTTTGCGTCCTCATAGCTGAACTCTCACTTGTAAGTGAGAACATACGACATTTGGTTTTCCATTCCTGAATTACTCACTTAGAATAATGGCCTCCAATTCCATCCAAGTTTCTGCAAAAGACATTATTTCATTCCTTTTTATGGCTAAGTATTCAATGGTATATATACACCACATTTTCTTTATCCACTTGTTGGTCATTGGGCACTTGGGTTGGTTCCATATCTTTGCAGTTGTGAATTGTGCTGCTATAAACATGCATGTACATGTGTCTTTTTCATATAATGACTTCTTTTACTTTGGGTGGGTACCCAGTAGTGGGATTGCTGGATCAAACAGTAGTTCTATTTTTAGTTCTTTAAGGAATCGCCATACTGTTTTCCATAGTGGTTGTACTAGTTTACATTCCCAACAGCAGTGTCAAAGTGTTCATTTGTCACCACATCCACACCATCTATTATTTTTTGATTTTTAAATTATGGCCATTCTTGCAGGAGTAAATGATATCTCATTGTGGTTTTAATTTGCATTTCCCTGATAATTGGTGATGTTGAGCATCTTTTCATATGTTTGTTGGCTTATTGTATGCCTTTTGAAAAATGTCTATTCATGTCTTTTGCCTACTTTTGATGGGATTGTTTGTTTTTTTTCTTGCTGATTTGAGTTCCTTGTAGATTCTGGGTACTAGTCCTTTGTCAGATGCACAGTTCATAAATATTTTCTCCAACTGTATGGGTTGTCTGTTTACTCTGCTGATTTTTTTTTTTTTTTTTTTTTGAGATGGAATTTTGCTCTTGTTTCCCAGGCTGGAGTGCAATGGCATGATCTTGGCTCCCTGCAACCTCTGCCTCTCAGGTTCAAGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGATTACAGGCACACACCACCATGCCTGGCTAACTTTTTTGTATTTTTAGTAGAGACGAGTTTTCTCTATGTTGGCCAGGCTGGACTCAAACTACTGACCTTAGGTGATCCACCCGCCTTGGCCTCCCAAGATGCTGGGATTACAGGCATGCCTAGGCGGCTATAAGTATTTTGCTTTATTTCTGGGTTATCTGTTGTGTTCCATTGGTCTTCATGCCTATTTTTATACCAGTACCATGCGGTTTTGGTAACTGTAGCCTTTTGTATAATTTAAAGTCGGGTAATGTGATGCCTCCAGATTTGTTTTTTGCTTAGTCTTGCTTTGGCTATGTGGGCTCTTTTTTGGTTCCATATGAATTTTAGGATTGTTTTTTCTTGTTCTGTGAAGTATGATGCTGGTATTTTGATGGGAATTGCATTGAATCTATAGATTGTTTTGGTCAGTATAGTCATTTTCACAATGTTGATTCTTCCCTTCCATGAACATGGGATGTGTTTCCCTTTGTGTCATTTATGATTTCTTTTAACAGTGTTTTGTACTTTTCCTTGTAAAGATCTTTCACTTCCTTGGTTAAGTGTATTCCTAGGTGTTTTGTTTTTTTTGCAGCTATTGTAAAAGGGATTGAGTTCTTGATTTGATTCTCAGCTTTGTCGTTGCTGGAATATAGCAGTGCTATTGATTTGTGTCATTGATTTTGTATCCTGAGACTTTACTGAATCGTTTATCAGATCTCGGAGCTTTTTGGATGCGTCATTAGGGTTTTCTAGGTATACAGTCATATCATTGGCAAACAGTGGCAGTTTGATTTCCTCTTTTCCAATTTGCATGCTCGTTATTCCTTTCTCTTGTCTGATTACTCTGGTTAGGACTTCTAAATTTTTTAATTACTATGGGTACAAAGTAGATACAGATATTTATCAGGTACATCTGATATTTTGATA\n"
     ]
    }
   ],
   "source": [
    "#if we do 1 million bp, we certainly cannot save it out, so instead have this widgth parameter!\n",
    "\n",
    "width = 32768\n",
    "seq1 = genome['chr1'][100029207-width//2:100029207+width//2].seq\n",
    "print(len(seq1), seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[CLS]',\n",
       " 1: '[SEP]',\n",
       " 2: '[BOS]',\n",
       " 3: '[MASK]',\n",
       " 4: '[PAD]',\n",
       " 5: '[RESERVED]',\n",
       " 6: '[UNK]',\n",
       " 7: 'A',\n",
       " 8: 'C',\n",
       " 9: 'G',\n",
       " 10: 'T',\n",
       " 11: 'N'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._vocab_int_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10, 10, 10, 10, 9, 7, 10, 7, 1]\n",
      "[7, 8, 7, 10, 10, 10, 7, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "len(tokenizer.encode(seq1)) #adds one, the eos token\n",
    "print(tokenizer.encode(seq1)[-10:]) #yeah uses sep as eos, but that's pretty good!\n",
    "print(tokenizer.encode(seq1)[:10]) #no bos token is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[CLS]', 1: '[SEP]', 2: '[BOS]', 3: '[MASK]', 4: '[PAD]', 5: '[RESERVED]', 6: '[UNK]', 7: 'A', 8: 'C', 9: 'G', 10: 'T', 11: 'N'}\n"
     ]
    }
   ],
   "source": [
    "#let's see what special tokens are\n",
    "\n",
    "tokenizer2 = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=32770,  # add 2 since default adds eos/eos tokens, crop later\n",
    "    add_special_tokens=False,\n",
    "    # padding='max_length',\n",
    "    # truncation=True\n",
    ")\n",
    "\n",
    "print(tokenizer2._vocab_int_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10, 10, 10, 10, 9, 7, 10, 7, 1]\n",
      "[7, 8, 7, 10, 10, 10, 7, 9, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.encode(seq1)[-10:]) #yeah uses sep as eos, but that's pretty good!\n",
    "print(tokenizer2.encode(seq1)[:10]) #no bos token is fine\n",
    "#so is exactly the same, still adds an eos pretty much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 10, 10, 10, 10, 9, 7, 10, 7, 1]\n",
      "[4, 7, 8, 7, 10, 10, 10, 7, 9, 7]\n",
      "32768\n"
     ]
    }
   ],
   "source": [
    "a = tokenizer2(seq1, add_special_tokens=True, truncation=True, max_length=32770, padding='max_length')['input_ids']\n",
    "print(a[-10:])\n",
    "print(a[:10])\n",
    "\n",
    "#ok so in this case a bos token is added\n",
    "\n",
    "a = a[1:-1]\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 10\n"
     ]
    }
   ],
   "source": [
    "print(min(a), max(a)) #no special tokens anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['chr1', '100029207', 'f', '1'], dtype='<U21')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so now we can tokenize it, let's make this in the dataloader\n",
    "peak_coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dna_to_one_hot(seqs):\n",
    "    \"\"\"\n",
    "    Converts a list of DNA (\"ACGT\") sequences to one-hot encodings, where the\n",
    "    position of 1s is ordered alphabetically by \"ACGT\". `seqs` must be a list\n",
    "    of N strings, where every string is the same length L. Returns an N x L x 4\n",
    "    NumPy array of one-hot encodings, in the same order as the input sequences.\n",
    "    All bases will be converted to upper-case prior to performing the encoding.\n",
    "    Any bases that are not \"ACGT\" will be given an encoding of all 0s.\n",
    "    \"\"\"\n",
    "    seq_len = len(seqs[0])\n",
    "    assert np.all(np.array([len(s) for s in seqs]) == seq_len)\n",
    "\n",
    "    # Join all sequences together into one long string, all uppercase\n",
    "    seq_concat = \"\".join(seqs).upper() + \"ACGT\"\n",
    "    # Add one example of each base, so np.unique doesn't miss indices later\n",
    "\n",
    "    one_hot_map = np.identity(5)[:, :-1].astype(np.int8)\n",
    "\n",
    "    # Convert string into array of ASCII character codes;\n",
    "    base_vals = np.frombuffer(bytearray(seq_concat, \"utf8\"), dtype=np.int8)\n",
    "\n",
    "    # Anything that's not an A, C, G, or T gets assigned a higher code\n",
    "    base_vals[~np.isin(base_vals, np.array([65, 67, 71, 84]))] = 85\n",
    "\n",
    "    # Convert the codes into indices in [0, 4], in ascending order by code\n",
    "    _, base_inds = np.unique(base_vals, return_inverse=True)\n",
    "\n",
    "    # Get the one-hot encoding for those indices, and reshape back to separate\n",
    "    return one_hot_map[base_inds[:-4]].reshape((len(seqs), seq_len, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATTTAGAGGGCTGGTTAAATAAATTATCAGTATAATACATAGTATAGTGATTAAGATCATGTTCTCTCCTGAGCCCATGATTTACCAGTAGTGTGAGCTTGGGCAAGTTATGTAACCTCTCTGTGCTTCAGTTTCTTCATCTATAAAATAGACATCTACCTCATAGTGTTGTATGTGAGTTAACATACATGAAGCACTTAGAATAATTCTTGTTACATGCTGATACATACATATTTGCTATTATAATCCATACAAAGGAATATTATGCAACTGTTAGAATGCAGTAGTTCTGTTCATACTGACATGTATATATCCTTCTATAATGTATACTTTTTTAGTAATTACAAAGTCTTTTATAAATTAAAAAACTTTATTGGGTTTGGTGGCTTGTGCTTCTAATCCCAGATACTTGGGAGGCTGAGATGGGAGGATTGCTTGAGAACCAGGAGTTTTGAGACCTGAGCAACATAGTGAGACCCTGTTTCTTAAAAATATATATATATAAGCCTGGATACAGTGGCTCACGCCTGTAATCCCAGCACTTTGAGAGGCCGAGGCAGGTGGATTGTTTGAGCTTAGGAATTGGAGACCAGTCTGAGCAACACTACAAAACTCTGTACAAAAAAAAAAAAAAAAAATTAGCTGGGTGTGGTAGCGTGTGCCTGTGGTCCAAGCTACTTCGGAGGCTGAGGTGGGAGGATCGCTTGAGTCCAAGAAGTCGAGGCTGCAGTGAGTTGTGATTGTGCCTCTGCACTCCAGCCTGCTAAGTGACAGAGTGAAATCCTGTCTCAAAATATACATATACATATATATATGTTATATATAAAACTCTTCTGATGGTATTTTATATATGTGTGTATATATATACACACACATACCTATTTTATATATATGTAATACCATCATATAAAATACCATCACATGTGTTTTTGTAATTATATGCCAGCATTAATTTCATGTTTGAAATTAGACATTAAAAGGATAGAGCTGACTTAAATGGAATTTTAGAGCATTTCTCATCCTTTTCACTTCCTTTCTATTGTGGAATACTTTGATTAGTTATAACTTCATTTTTAAAAATTGCTTTACGCTTGCACTCCTTAACTGCACATATTTCAAAGGTGACCCATAAGAAGTGGTCCATGTAAGCATGAATTTCTTTATTAATAGCCTCGCAATGCTTCTGGGAATCTCTTATTCCAATAGTCATGTGATAGAATATGTTTTTGAGGTGAGATTCAACATGAACAAGACAGTTTCTGCTGTCACCAAGTTAACTTTCTCTAACAGTGAAACTTCAGTAACACAGAATCTAATGATTTCTATGACTATTTCTTAGCTGTGATTTTTTGGATATCAGTTATTATTAGTTATACTTTCAAAACAAAACTTTTATTTATTTATTTATTTTTTTAGAGACAGTCTCGCTGTTTTGCTCAGACTGGAATGCAGTGGTAGAATCATAGCCCACTGTAACCTCAAACTTCTGGGCTCAAGTGATTGTCCCTCCTCAGCCACCTGAGTAGCTAGGACTACAGGCATGCACCACCACACCTAACTAATTTAAAAATTTTTTTGTAGAGATGGAGTCTTGCTATGTTGCCTGGGTGGGTCTTAAACTCCTGGGGTCAAATGATTCTCCTGCTTTGGCCTTCCAAAGTGCTGGGATTTCAGGCGTGAGCCACTGTGCCCACTCTCAAAACTTCTTAATAGGGAAAAATATTGTACCCCCTGCCTCAGCTTTTATATTTGTATGTATCAATGTGAATATATGAAAAATCTAGATCTAGCAAGAGGAAGATGGAAGAACAATGACATATTCCAAATGTGAGCAATTTGCACATATTCGAATGGGCTAGCCATGTAATTATCTAATTTGTGCATTGTATTATTTTAGTGCTCCAAAGTTGGTAATGTATGTTATTTTTTGTATTCTATGTCTTTGTCTTTTTACATTTATATGTCTTTGTCTTGTAGTCATAATATAGTACTAAGTATACTTTTGTTAAGTATACAAAACCAAGTAAAAATTTTAGAAAGTAATCTAATTTTGGCTGGGCACGTAACTCACACCTGTAATCCCAGGACTTTGTGAGGCCAAGGCAGGTGGATCATGAGGTCAGGAGATCGAGACTAGCCTGGCTAACACAGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCTGGGCGTGGTGGCACGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGTGGAGGTTGCAGTAAGCCAAGATTGTGCCACTGCACTGCAGCCTGGGCAACAGAGCAAGACTCCGTCTCAAAAAAAAAAAAAAAAAAAAGAAAGTAATCTAATTTTATCAAAAAGAAAAAGTTAAGTGTAAAATTATTTGGAATGTCAGTGGAAAAAATATATCTCTTCAGACAAACTAAACGATATATCATGTAGACTTTACTTTTTTTTAGGTTTCTTTGGAAGTATATTTGGATTAATGGGTGTATACATTTATGATGGAGAACTGGTATCAAAGAATGGATTTTTTCAGGGATATAACCGACTGACCTGGATAGTAGTTGTTCTTCAGGTAAAGCATTTAAAGTCTTAGATTTAATGCTAATAAACTGTATTTTAATATAAAGAATCAAAGTAGCTCCTGATATACTGATGTGAGGGGGAAAAGGTCCCTCCTGTTAGTGCTCTCGTATCTAGTTTATTCAAGGAAGTGAAAATAGAAGTGATCTTAATGCTGTAATGAAATCTTATTTTGTACAGGGGTCTTTAATGAACTTTGTGGTTTTAAATGTAACATAATGAAAATACAAGTTTAAGAATAAAAGAGTTGAAGCTAAAGGCAAATATAACTTTTGATTTTTACAAGTCTTATCATTGTACAGGAAGTCTGGGTCAGCCATTAAGACCCAGGAAGCAGATCTAGATATTTAATTCTGTGACAGTTCATAGCCTCATATTGTAGTTACATATTGGAAAACATCTGTCAAATTCAGTTTATATATAGCTTGGAATTGTTCAAGGGTTGAGTAATTAATTACATCTTTAAAAAATACATATGGAGAATTCTAGAGGATATAAAAATGAAGATACAAGTCACTATCCTTAGAGCCTCCTATTCTTCTAGAGATATTTGAAGCCTAGGAAGCAAGGCTCCAGAAAGGAAGGTGAAGATGGCAAAGGATAGTTGTGTTTTAAAAGCAGGTAGGATACTTCTATTTTTTTTTTTTTTTTTTTTGAGACGGAGTCTAGCTCTGTCACCAGGCTGGAGTGCAGTGGTACGATCTCGTCTCACTGTAATCTCCACCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGGATTACAGGCATGTGCCACCACGCCCAGCTAATTTTTGTATTTTTAGTAGAGACGGGGGTTTCACCATGTTAGCCAGGATGATCTCGATCTCCTGACCTCGTTATCCACCCACCTTGACCTCCCAAAGTGCTGGGATTACAGGTGTGAGCCACCGCGCCCGGCCCGGATACTTCTATTTTTTTTTTTTTTTTGAGACGGAGTCTCGCTCTGTTGCCCAGGCTGGAGTGCAGTGGCGAGATCTTGGCTCACTGCAAGCTCCGCCTCCTGGGTTCACGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGACTACAGGCGCCCGCCACCAAGCCCAGCTAATTTTTTGTATTTTTAGTAGAGACGGGGTTTCACTGTGTTAGCCAGGATGGTTTTGATCTCCTGGCCTTGTGATCCGCCCACCTCGGCCTCCCAAAGTGCTGGGATTACAGGCGTGAGCCACCTCGCCTGGCCCGGATACTTCTGTTTTTAAGATAGCAGAAAAGGAAACAAAGATAAGTGAAAATTTTAAAAAAATAATTAAGAGAAAATTTGAAGCGGAGATAGGGTGAGTCAAGAGAGTTTATGTATGATGTATCTTATTCCAGCAAAGAGAAGAAGCCCCCTTCCCTTAAAAAGGGGCTGAAGATATGAGAAAAATTAAAAATGTTTGGTATAACTGGTATGAAGGATTCAATAGGGAGTAAATAGATGAATGACAACATTGCCAAGAAATACTGGGACTGAAACATTAATTTGTAGTAAGCTCAATTGGCATAATTTACGACTTTCTCTAAATAAAGCTTCCATCATAGAAGAGATGTTTTGAAAGTTGTTGCAGTAGGTAACCCAGATTTGGAAAATGATAAGTAGTCTTTGGCAGAAAGCAAAACTGTGTAGTATAGTAAAACCTCATTATTAAAGTAGCCACAATTGGGAATTTGAGAGGGATTAGGTAAGTTCTTTCTACAGTCTTAAAGAAAAAGCACAATTAAAACTGTGCTTTAGAAAACTTAATGTGACAGTAATGAGAGGTGGATTTGAAAGGGGAGAAACTATAGGAAAAAAGCCAAATTAAGGGGCTGTTGAAAGGCTGTAAAATGACTCTGATTACTAATTTTACTCAAAAGTAAAAATATTTCACTATTTAAAGTGACCAGTCCTCTTACATTTGTCCAAACAAGTGATGTTTAATTACTAAATTGGATAATGATAGACATTTTATTTGTAATATGGTCAAAGTCCATTATAACACACTGCAGATTGTCCTGGTAGTTACTTTAATTAAATGCCTTTTGAAATAAGAGGGCTATTGAACAAAACCTGTAAGTCTATTCGTTGTTAAGAGATGTTTAAAATAAATCCCTTACTATAATGGGATTGAATTTATGGGACAAAAGCTTAAAGCTTTATTAAATACTGAAAGTGTTTTGAAATGCAGTTTTACATGTGTGTTTTAAAAAATATTTTTTTAAAACTTCAGGCACTTGGAGGCCTTGTAATAGCTGCTGTTATTAAGTATGCAGATAATATTTTAAAAGGATTTGCAACCTCTTTATCGATAATATTATCAACATTGATCTCCTATTTTTGGCTTCAAGATTTTGTGCCAACCAGGTAAAATGTTCTTTTCTATTTTTTTAAATCCCCAGAAGTATATAGAAAAATTAGAATTCTTTGTAAGGTGATCTGATAAATTTGAAGAAGCACTGAAATATAATTTTAAAAATTTATTTAACTATGTTTTAAAAATATTTCAAATTAAATTCATGAAAGAAAGTCTTCTCCTGAAAAGTTTCTTATACAAATTTGGTTGGCCAGAATTTTCCCAATAAATAGATGTGAATATTTTCCCTGCAAAGTAAAAAAGTGATATAAAAAAGTTAATAAGTCATATAAAGTCATAAAAATAAGTCATGTAATGACATTTTTAAAATACGGAATTTACATGACTTCCTTGAAGGAGATAGTTTATATAGAGAATGTGGCTCATGAGGAAACTCAATTAGGTGGTAGCAGGAAGAGCAATCTGGAAAGGAATTGCAGGAGGAGTTATAGGTGAGGAAGAAGACCCAGGACTCTGTAGTTTTATGGATGCTAAGGGAGGAGATAGCATCAGGAAAAATTGAGTGTAATGAATATCCATTACAGACAGCATTGAAAATTCTAGTTTTTTGTTTAATTTATAATCTCAGACTAGTGGGATTATATTTTAATTGTTTCCAGTACACCACTCTTGGTTTTCCTCCAACTTTACTCCTCTCCTGATTTTCTCAACCTCTAAACTTTGGAATCTCCCAAGAATCTTTTATTTATTTATTACTTTATTATTTATATGTTCATATTTTTGAGACAGGATCTCAGTTTGTCACCCAGGTTGCAGTGCAGTGGCGTGATCGTAGCCCACTGCAGCTTCAAACTCCTGGGCTCAAGCAATCCTCCTGCCTCAGCCTCACAGGTAGCTGGGACTACACATGTGCTTTGTTGCCCAGGCTGATCTTGAACTCCTTACCTCGAGCAATCTTCCCACCTCTGCCTCTCAGAGTGCTGGGATTATAGGCATATGCCACTTCGCTGAACTCATTTGTGTTTTTGAGAGTGAAATAGGATTATGGATGGAATTTGGCACTATAAAAATATAATATCCTTTTTATATTTTTTATATTGAACATTGACGGTTTTAATTCTTTTCTAGCAATCTAGACAATAGGTCTGGAACATACATGTTGAGATCTGTAGTTCTTATGGGACAAAAATTTAAAAGCATTTCTGTTTTCAGCTTGGTATGGAAATGAATCACTTAGCTAGTGAGACTAGATGAACAATGAACACTTGTGTCTTAATTTTGTAATCTTAATCATATTCCCTGTTCATAAGATGAGGAAATCGTATGCTTTAATGTACTTTGAGAATGTTTGCACCTTGCAGATTTTTAGATTTTACTTTTAAAAGCATACCTTTTTTTTTTTTTTAAATTGAACCTTATGTAAGGCCGAGGTAATTAAATCTTTGAATTATGCAGAGAATAAACAAATAAATGAACAAGCAAACCATCAGAGGGACAAAAGAGTAGTAGTTTGGCAATGAGATGTATGCCTGCCTGTAAATTCACGTTTCTGAGGAAGTATATAAGGTTTGTTTAGTTGAAGATAGAAAGAGAAAAGACAATGATACTGCCATTATTGAATGAATGAATGAATGAATGAATGAATGAATGAATATTTTGAGAATATGCTCAGGACACTAAGCCAGATGGAGAAGATGGATTCTTTTTCCTAAAATAGGTAACTAAGCCCTCTATCTTTATGCATCTTACCTTTGTGGAGAATACATTTTCAGTAATAAATCTAGTGACATTTTCTCAATCTTCATTCAATTCTATCCTAAATGTAATAATATATTTGCCTTTAGCCTCATTCCATCTCCTGAAACTGGGTTTTTGTTTGTTTGTTTGTTTGGCTTCTCTGAAATGGTACGCCTATACTTGCATTTCTGATTGTCCCTTTGGTAGTTTGTCAATCCTTGCTCACTCTCTTGGCTATCTCATCCACTGCCATGTCATATCTGTCTTCTCTAGGAAGATAAGTTCAGATTTCACCTAAACTCTAGATCCCTTTATTCTATTGACTGTTAGACATCTCTACTGATACAATATCACTGTTACCCACTTCCCTAAATTATTTTTCTTGTGCTTGTTATTTTGCTTATTGGTGTTACCATTCTTCCATTTAGGGTTGCTTATTCTTTTATCAAAAGTTTGTGTCTACTATGTGCCTGCTGGCCACTGTGCTAGCTCCTGGAAAAGGAGTGCATGAATGAGACACAATTTCTTGTCTTCAAGGTCATCAAGGTTTGGTGATCATCAAGTGATTGGAAAGACAGATGTGAATAATTATAACAGTATGATCAATGCAACAATGGAAGTATACATACAATAATTCTGTTAGAGGGGGCTCAAATCCTTGAAGTAATCATTACATTCTGATAGTTTTATCCTAATTTTTTTTTTTCTGAGACAACTTTCTCTCTCAGAACCATCTTAAAATTGCTGGCTTGAGAAACAGGTGTTTCCTGACATGATTAGGTATTCAAGTAGATCCTGCTAAAGATATTATAGAAAGGATTCCTGCAGTGGATACAAAGGTAGACCACAGAATCTTTCAGTTGAGCATGAACTTTCAGTCCATGCCATTCCAAAGTTCCAGAAAGCTAAGAATAGTTCATAATGACTAAGGAAGACAGTGTCTAGTAATTAATAATTACCAATATTAATGTAGCAATTTCTTCAAGATACATTACAGGATGTTTTATGTTATCCTAATAACCTTATATGTATAACTGCATTCGAGTCTTTTGAAGAGTTAACCATTTGTATCAAGTCAAATCTAGTTATTAATCATAACAGATAATTACTGAGCTCCCTATATATGCTAGGCACTGTGGCTACCACAGTGAATGAAGAGCATATGGTTGAGCAGGGGAGTGGACTTGTAAATAATGCATCATTTAATGACAGTGTGGTAAGTGTTATGCTGGGGTAGTTCTGCCTAGGGGAGTGAGTCTTGGAGACATTAGACCTGAATTATTATGAATAAGCATTTTCTAAGAAGCAAGGAAACTGTGGACAGAGGGAATAGTATCTCTAAAGACACTGTCAAGAAAGTGTATGTTGTGTTCACTGATTGGAGTGGCTGAATTATAGATTGGGTGGCATGAAGTGGGAAGGATGAGTTTGGAGGGACCATATTATGAAGGGCCTTTTAATAATGATTAGTAATTATTTTTCGTTTATAGGCAATTAAAAACAAAAAAACAGGAGCATGGCTTTGGACTATTAGTAGAAACTTTAAATCCAAAACTAGCTTATGTCTCTATAATAGCATTTTAAAAAATGTACACATTAAAAGTTAAGATTGCACTTTGGGAGGCGGAGGTGGGCAGATCACTTGAGGTTAGGAGTTTGAGACCAGCCTGGCCAACATGGTGAAACCCCATCTCTACTAAAAATTCAAAAAATTAGCGGGGTGTGGTGGTGCACACTGGTAATCCCAGCTAGTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGCGGAGGTTGCAGTGAGCCGAGATCACACCACTGCACTCCAGCCTGGGCGACAGAGTGAGACTCCATCTCAAAAAAATAAATAAATAAAAGTTAAGATTCAGGCTGGTCACAGTGGCTCATGCCTGTAATCACAGCACATTGGGAGTCTGAGGCAGATCACTTGAGCCCAGGAGTTCAAGATCAGCTTGGGCAACAAAGTGAGACCTCAGCTCTACAAAAAATAGAAAAATTAGCCAGGTGTGGTGGTTTGCACCTGTGGTCTCAGCTACTTAGGAGACTGAGCTGGGATTGCTTGAACCCTGGAGGTCGAGGCTGCAGTGAGCCATGGTGACAGCACTGCACTCCAGAGCCTGAGGGACTGAGTGAGACCCTGTCTTTAAAAAGAAAAAAAAAAGTTAATATTTGTAGTCTAGTTTACATATCCTGTCTAAATTCATTTCTGATTTATTGTAAAGTAGACTATATCTATAAATATATAGGATAAAGAGGAAACCAAGAACTCCCTGCATTTAATGTAATTTCCCTTTGTAAAGCATCCTATAAAAATTGGTCCTGAGTTTTGCTTATTATTATCTTGGATTTTAAACTAAACTTTACCTTGGTTTAACATGACAGAATAAAATATTCCTTTCGTATGTTCAACTAAAGAATTAAGGCCTTTTGTTATTGAATGTATTGTATACAAAATCCCTATTAAGGTGAAATACAGAACTTAAAATAGCTTTGCTTTGGAAGACAGTATAGTGCAGTCGATGAGAGTGCAGGCTGTAGAGTTAGACATACCTGGGTTTGATCCTCTTGAAGTTGTGGGACTTTAGACAAGTTATTTAAATTCTTTAAAACTGTTTCCTCATGTGTAAAAAATGAATACTAATACATCTTCATAGATTTGCTGTGACAACTCAATGAAATAATGCATATAAAGCACTTAGCCAGATACCTGGAATGTAGTAAGTACTCAAGATTTGTTAGCCGTGTTACTGGAACATTTTTCCCCACATTATAATTGTTTTTATAGTAAAGAAATAGAAGGAACTTGTTCTGCTTTTAATCTGATTTTCTCTTTTTTATTTTGTCTTCATCTTATTCAGTGTCTTTTTCCTTGGAGCCATCCTTGTAATAACAGCTACTTTTTTGTATGGTTATGATCCCAAACCTGCAGGAAATCCCACTAAAGCATAGTTGTATACTATCTTTAACTGGTTTTTCACGATGGGGCACTAGGAATCTCGACATTAATCTTGCACAGAGGACTTCTACAGAGTCTGAGAAGATATCATCATGCTGAATCTGATCATACTGTTTTTTAAAAGTTTAAGGATAAGACATGTGTATATGTAACAAAACACATTGCATCTAGAAATCAAAACTTGAAAGTATTTCCAGGGATTAGGATTAGAAGGAATATTAGAGGAAACTTGAAATCTGAGTTTAAAAAGATTTTACCTTTTTGATTGCTGCAGAAATGTCCTATGCACTCTTTGCAAGAGCACACAACAAATGTCAGATACCAATTTTTGCAAATTAGATTTAATCTTATTAAATGTTTTTATCTTACTCTTTCTGTACAGATATATCAAATCACATGAAATATTTAAAGTTTGAAAATTATAATTACCTATAAAGCTGTGAAAAATAGAAGTATAATTTGAAAAAACATTTCACTTATCAGAGATTTTTATATTTATACAAAAGATTACTAAATGAAGGATTGCTAAATGTTTTTGGTTCAATTACATAAAAATTAATATTCTGGGTCTGATCTGTCAGAGAATAAATATCAAATCTAAATTTAATGTAGAGATACATACTATTTCTCCATATGAATTTTAAGATATTTTAGTGCTTCAAGACTGCTGAAAGCAATCCAGTTGCTCCTGTGCTAGATGGTAGCCAGAGAATTTTATAGTAATGGAGGTTAGCCCTTAATCTCTTCATTGCATTTCATTTCTGTAAATCAGATTAAGTCCTTAATATTATTTTAAATTAAAATTTGTGTGTAATTGCCATTAAATTTTCAAAATGTAATTTAAAAGGATTAAATACTCATTTAATAATTTAAAATAATTATTGTATAATATCTACATTTGGAGAATTTTGAACTATCAAGCATATACTGTATACAGTTAGAAAGTTATTAAATGAACATTTTACTCATTGATCTGTAAAAACTTCTTTAATCTACAACTGTTTACAAAAACAACATTCAAACAAATAGTACAGATGTCAGTGACAGAACAAAATGACTTTTCTTGGAGACATTCCAGATTGCCATATTACTTTATTTTAAACAGCGCTATGACTTTAAATCCAAGGCTGCTCGGAAGATTTTTTTAGGTCTCTCATAAGCCTATTCTTCCCTGATCACATGAGTGGGAGAGGTAGCCAAATTTTGAATTCCCTTTCTGTGTTCCCACAAGAGCTGCTGAGAAGGCCTGGCGCAGTGGCTCACGCCTGTAATCCTAGCAGTTTGGGAGGCTGAGGCAGGTGGATCACAAGGTCAAGAGATTGAGACTATCCTGACCAACATGGTGAAACCCCGTCTCTACCAAAAATACAAAAATTAGCTGGGTGTGGTGGCACGCACCTGTAGTCCCAGCTACTCGGGAGACTGAGGCAGGAGAATCACTTGAACCCGGGAGGCAGAGGTTGCAGTAAGTGGAGATCACGCCACTGCACTCCAACCTGGGCGACACAGTGAGAGTCTGTCTCAAAAAAAAGAAAAAAAAAAAAGCTACTGAGAAGGAGCCACCATTTTGCTTTAAAATAGTGGGTTTTTTTCCTTTTTAATGAACACTGGAGCTGAGGATGCAGATTACATGAGTTATTTAATAAGTTGATTCAAGAGAGAGAGAGTATATTAGAAATGGTAAGATGATTTGGTGGGCAAAAATGCTTTCTATAATTTGAATATGGGTTTAATTTAATTATTAACTATAGACAGAATTACTTACTGAGTATAAGAAGTATTTTACTGTCTGGGCACAGTGGCTCATGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGTGGATCACAAGGTCAGGAATTCGAGACCAGCCTGGCCAGCATGGTGAAATCCCATCTCTACTAATAATACAAAAATTAGCTGGGCATGGTGGTGCGCGCCTGTAGTCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATTTCTTGAACACACCAGGTGGAAGTTGCAGTGAGCCAAGATCGTGCCACTGCACCCCAGCCTGGGTGACAGAGCGAGACTCCGTCTCAAAAAAAAAAAAAAGAAAACACACACACACACACACACACCCACAGTATGAATGAAAAAAATAAAATACTTCTTTTTTTTTTTTTTTGAGACAGAGTCTCACTTTGTCGCCAGGCTGGAGTGCTGTGGCGCGATCTCGGCTTACTGCAACCTCCCACTCCCTGGTTCAAGGGATTCTCCTGCCTCAGCCTCTGAGTAGCTGGGATTGCAGGCGTGAGCCACTGCGCCCGGCCTATACTGTATATATTTTTAAAGACTGTTCTAATAGATATAAAAACTGTAAAAAATAAGTATTTTTATATAGCTCTCATGGATTTTATTAAACAGAATTGGCTCAAAAATACTATGTTACAGACTGTTGGGTACCCTTGCCTAACGTGAACTGGCAGTGTTACCTTGCTTTTGCAGTAATAGTCTACAGATTGCAGGTCTCATCAATTCCATCCAAAGTTTAAAAGCATTTAAAATTACCAAATCTTTAAAATCACTTTGGTGGTGATTCCAAATTGGTACCAAGCAAACTTTCTGGATGCCCAACATGATTTTCAGTAACCACCCTTTAGAGTATTTGTTTACTAAGTTCACCACATTTTGAACATGGTAGTTTTAGACTGCAATAATATTTAGACTTACATTATTACTTACTGCTAAGTAAAATCTAAATCCTGCAAATGCACAGAATTCAAGCTGAAATATAATGATTTATGTTTAGCTCACATTGAAGTATTGGTTGGTTACTTATGTATTAATGCAGTGTGCATTCACATTTAATCAGGTTTAGTCTGTTTCTATTTTAATAATTTTAAAAAATTATACAAGCAAATTAGATATTAGACATGTTAGTTACAATGGTAACACATTTTTAGGTGTCGAAACACAATTTTCAAAATTCCTAATGAAAGTTATAAAAATGTAAACAAGAATTGTAAAAATGGACAAAGTAGTCAAATATATTTTCAAAGCACAATTTTATTAGACAGGCATAATTTACATTTTGCTTTTCTAGTGGGTTTGAAAATGTTTATTGGAGATTGGGCTATGTAGTTTATAATTTTTAATTCATAAAAAAGTAATCATACATGAGAAGGTAGACCTGTGCCCTAGGATCATGTCACATATACAGATAATGCCATTTCCTTGTGTGTGTGATGTGTGTTTTGATGACCTCCACAGGCCTTACTGTATCAAGCTTTTATAATGATGACTCCTTCATTATTTAAATTCCTATACTTTTTATTTGTTATCACGCAACTACTTTGTTCAATGTGAAAATGTGCTAACTCATGGGAGAAGAGTGCCAATTGATAGTTCTTTTAGCAATTAAGAATATGGTATTTGGGAAGAAAAGTTTGAAATGCAACAAATGGATATTTCAACACAGTAGTATTATATTATCAGTTCTTTAGTAAGTGATTTTAGAGATGTTGTAGGCTACTTTTACGGTGGAATATATAGTATAGAGATGCAAAACTTAAATGTTTACATCAATTTATATTGAATGTCACATAATTTCATGGAAGGAAAGGTAGCTTGATATTTAGATTCTAAGATATAATCTGAAAGGAAACTAATTATGTTCTCTACACTTACTGTAATACTGATTATTCTTACATATCAAATTATTGAACTTTAAAAATTTCATTGTATAGTCATTAAACTGAGTTGGGTTTTTTCTTAAAGGGTTTAGCATCACTCATTTGATTTACACATTCACATTATAATATTTAATTATCATGGGTGTATGCTTTACATAAAAAAGGTTTATAAAAGTTATTTATGCTATATTGAAAGTCATCTTAAGAATCTCCAGGTTATTTAAAGTAGTTATAGGAGCAGAGAACAAGCACCTTTATCAAAATCTGGTCCTATGTGCCTTGCTTTACCAAATACCTGATTTTTCTGGAGGGTGTTCCTGTAATTCACAACTGTAGACACATGGGCAAAATTAGGATTTTTAAGAATAAATACATTTCTATTTTTTTGGTTGTTTCAACATTAGCTCTTCAAATTCATTAACAAAATTAAAATAGGTATATTACAAAAGCATAAACATTTGTGAACAGTACTTAAATAAATTGTGATACTATTGCTCCATCATTGAACTTTTTGAAACTTTAACAATTGTATAAAACTGTCAGTTTGTTGTTTCATTTGTAATTACAAAATAATTTAAAAACTTTTTAAAATAATTTGGATCCTGACTTTGTCTATATCTGTATTTCATTTGTTTAGAAAGATTCTTTTGGGTTTGATAATGTAATTTGTATATTTAAATTTTTTATGGACATAATTCAAAGGAATGTATAAATTGGTCTTTTGTTAAATGGCTTTTTAATTGATAAACTTCTCTTGTCATTTTTTGGTATCCAGCTATTACCTATTTAATAGATTTATTGAAATAGATTATTTTCATAAAGAACTCTATACAAATCTTTTTCTATATTTCCTTATTTTCCTATTTACCTGTGTCTATGACCTAACCTATGAATTAGTCTTCTCTCTTTATATATCAAAAATGAATTACTGATCTTTTTCTCTGGCTCTGTAGTATCTCTATCACTGTCACATGTGATCTTTCTTCCTTTTCTCTAGCCCATATTCTAGCATGAAATACTGGGTTGGCCAGGTGCAGTGGCTCATTCCTGTAATCCCAACACTTTAGGAAGCCAAGGCAGAAGAATCGCTTGAGCCCATGAATTTGAGGCCAGCCTGGGCAACATAGCCTTGTTTCTACAAAAAATCTTAAAAATAAAATTAGCCAATATGTGGGCATGCACCTGTGGGGCCAGCTACTCAGGAGGCTGAGGCAGAAGGATTGCTTGAGCCCAGCAATTTGATGCTGCAGTAAACCATGATGACACTACTGGACTCTAGCCTGAGTGACAGTGAGACTCTGTCTCAAAAAACAAACAAAACAAAAACTGAAACAACAAAAAAAGACTGGGTTTATTTAAGCTAGTTAGAATTTATCTTTCTATATGTTAATAACAGCCTAACAGATTTTTTGTTTTAAATATCTCTAGGCTAGCCTCAAGGTTAAGTAATTATAGAAGTTTGGTATGTATTTTCTTCATAATTTGAATATAATTGCTTCCATTGTGACTGTCAATTGAATGCATGGAGATCAATTGTGATAATATACAGGATTTTAGTCCTATCTCTACTGCTGAAGTAACCTTACACAAAATACTTTGTAAAAAAATCACTAAAGTGCCAGCATTTTTAAAGTGTATATTTTTCTTTGGCAACCTCTCATGAAAAGCACTAACTAAAAATATTTAATAATCTTTTTTGTATTACAGTGCTTCTTTTGTTGGAAATATATCACAATCCTCAAGTTCCACTGCTATGCAAAAGTATCTTAGAATCTGAATCTTATAGATAATACTACCTTTTTTTTTTTTTTTTTTTTTTTTTTGAGATGGTGTCTCGCTCTGTCACCTAGGCTGCAGTGGTGTGACCTCACTGCAACCTCTGCCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCCTAGTAACTGGGATTACAGGCCATGCCACCACGCCCAACTAATTTTTGTATTTTTAGTAGAGACAGGGTTTCACCATGTTGGCCAGGCTGGTCTCAAACTCCTGACCTCAGGTGATCCGCCTGCCTCGGCCTCCCAAAGTGCTGGGTTTACAGGCGTGAGCCGCTGCGCCCGGCCAATACTGCCTTTCAAAAGAGACTGATAGTGACCAGCATTAGTAATCATACTGGTGGGGTTTTTTGGGGTGTTTTTCTGTTTTGTTTTTTGTTTTTTGTTTTTGAGACAAGGTCTTCCTCCCGTTGCCTAGGCTGGAGTGCGGTGACACAATCTTGGCTCACTGCAGCCTTGATAGAGCAAGCTCAAGTGATCCTCCCAGGAGCCTCGGCCCCCAAGCAGCTGGGAATACAGGTGCGCGCCACCATGCCCAGCTGATTTTTGTATTTTTTTGTAGAGATGGGGTTTTGCCATGTTGCCCGGATTGGTCTCAAACTCCTGAGCTTCAGTGATCTGCCTGCCTTGGCCTCCCAAAGTGTTGGGATTACAGGCGTGAGCGACCACACCCAGCCCATATTGGTCTTTCTTACTGTTCTTAAAAAGAGAATTCCTTTAAGGCAGGACCGATTACATATACACTCTAGAAAAGAAAATAGCAAGGAAGAAATAAATTGCCTTCAATTACCAAAGATTTGAGCTTCTGCTATGGCTGAGAGTGTTTTGGTCATTGCAAATTCAGGGGTTTCCCAAGTTCACCCTCAGTTCTGGCTAGAAAGAGAAAACTTACTAAAAGCTATTATACTCACAGTCATATTTATTACAGAGAAAGGAAATACAAATTAAAACCAGCCAAAGGAAGTGACACATAAAACAGAGTCTAGGAGTGGTCCAAACTTGAGGCTTTCGGTGTCCTTTTCTTGTAGCGTCATGGAAGGTGTTATCTACTCCTGACCACAATGTTTGACAGTACACACATAGTATTGCCATTCAGGGAAGCTCACCTAAGCTTTGGTGTCCAGATTTTTATTGAGAGAGGCTCTATTAGTTGGCATGGTTGGTTGATTTTTTTGCCCATGTAGTTGTCCTCTCTTTCCAACATCTGCCCCTCCCTGGAGATCTGGTTGACATCAAGACTTCAGGGCCTCACCATAGGTTATCTCGTTAGCATAAACTGTCAAGTGTTGTCTAAGGAACCCACAATGAATAATAAAGACATTCCTATCAGTGAGAACTCCCAAAGACTTACACCAGAACTTTCTTTGGATGGGCCAAATTTCTTACTACACAAAGACCATTCATCTCTATACACTTCCTTCTGAATTGATGAGGATGATACAAGCAACGACAATTCTTCTTTTCAGAGACTTTTAATTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTCTCGCTCTGTCACTCAGGCTGGAGTTCAGCGGCACGATCTCTGCTCACTGCAACCTCCGCCTCCTGGGTTTGAGCAATTCTCCTACCTCAGCCTCCCAAGTAGCTGGGATTACTGGTGCTCACCACCACGCCCAACTAATTTTTGTATTTTTGGTAGAGACGGGTTTTCACCATGTTGGCCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCTGCCGGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATCACACCCAGCCCAGAGACTTTTAATTTATACATTGTCATTTTTAATTTTAGAATATCTGTCAAATGATTCTGAACATAACATGAATCTAGTGTGGAAAAATGTTTATAATCAGATATTGTGTTAAGAACATATATATATATATAGAGAGAGAGAGAGCATAGTATTGTCATTTAGTTTTCACTTCTTATGTTTAATGACAACTTTATAAATGCTGGACTATTTTAAACTACAAATTTAAAACATGGTTTATAAACCTTTTACCTGTACTGCATATTAATAAACAATTTTGAACTAATTTTAAATTAGGCATTCCTATCACCATTATAAGGAAGAGCTAAAACCTGATACCAGTTGTTATTTTGGCTCACAGTAGGGCTTCACTAGTGTTAGGATCACATTTTCCTCCCTTGGACCTGAGAATCAGGTGTTTTTCGTTTAATGCATTTATTTTCATTCATTCATTTAGGATTTCTAAAATGTTAGAGGTAATTTGTTTAAAACAAATAACATGGTGTTCTTTTGGAAATATACAGTTTATCTTAGATATACACTTTCCTCATTTTAAAGGTAGAACTCACTGCAGGTTTAAAAGAATAATGAATATTGTTACAGTGTTTAGTGATAATCACTGAAAATTCACGTTAAAATATACTTTAAGCTTGTCCAACGCGCGGCCTGCAGGCTGCATGCAGCCCAGGATGGCTTTGAATGTGGCTCAACACAAATTTGTAAACTTTCTTAAAACAGTATGAGATTTTTTTGCAATTTTTTAAAGCTCATCAGCTATTATTAGTTACTGTATTTTATATGTGGCCCAAGACAATTCTTCTTCTTTAAGGGTGACCCAGGGAAGCCAAAAGATTGGACACCCCAGCTTTAAATGTAGAGTATTTAAGAGGTTCAGTTAAATGTACTAAATCAGACTTGTGTGATTTAGACACAAAATCTGTGTGTGTGTGTGTTTGGTCTTTTAAAAGATACTGCATGTACAAACATGATTTCTAATAATTGAGGGTTTATTAAGTTTTTCCCTTTAGGCTAAAGTTTTATTTTATTCTGCTTGGATAGTAAATACAGCATTTTAAAATGATGATGCTAAGTGCTTTTTTAATATATGCATGCTGCTTCAATAGGTCTTTTAAATATAGCCAAGGCAGGGTAATATTTCCTGAGTGATGATAGAATTATAGAAAATCATGAAAGATGTGTGCCCATGTTTCATTTTATAACTGTTCCAGTCATATTGCTATGAATTCTAGAATTTTAAAACCTGCCACACTTATTTTGGATGTTATATACACATAATGCCTCAAATTTCCATTATTTCATATTATTTTTCTCCATTTCCCTCGTGTCAAGTTTCATTTTGTTTTCAATGTTTTGTTTTTATAATATTGCCATTTTAATGGCTTCAATCATGCTATGACAGATTGTGCCAATTTAGCTTTAAAAAACTCATGGGCTACCTCCATGAACTCACACATGCGGCATTGTTAAAGAACCTTTTTTAAAATTAAATTTTCTCCAGCTTTTCATTTTGCAGCATACAAAAAAATTGAAAAATTGGTATACCACCTAAATCCAACCATTATTAACATTTTGCCACACTAGCTTTATCTCTGTTGGGACACTTTACCCCTACATACTCCAGCATGCGTGACCTAAAGATATGGACATCTTCTTAATTAACCTCAGTGCCATTATGACACCTAAGAAAATTTAAATTAATTCAGGTCACTTCATCTAAGTTCAAATTCAAATACAATTGCCCCTCTGTAAACGTGGGGGATTGGTTCCAGGACCCCCACCTCGTGTATACCAAAATTTGGGCACACTCAAGTCCTTAAGGGGAAACCGCGTATAAGAAAAGTCAGCCCTCCCTATGGGTGGGTTTCACATCCTGCAAATAACTGTGTTTTCCATCTGCATTTGGCTGAAAAAAAATCTGCATATAAGTGGACCCATGCAGTTCAAATCTGTTGTTCAAGAGTCAACTGTATTCCCAGTTGTCTCAAAAATGATTTTTTATAGCTTTTTTGTTTTGGAACGAGGGTAAAATCAAGGTTAATGCTTTGTATTTGATTGTCTCCTTTGGTCTTGAAGTCCCCTTCCTGCACCACCACCTGCCCCTCAATGGCTTTTTTATACCAAGAGACTGAAAATGCTACCTCTAATGACTTTTCTCCCCATGATAGTACTCTTTGAAGGACCAAAGTTACTTTGCTCCTATTATGTGGTTTGATTTGTTCCCTTTATTCCTTTCATGTCCTTTGAACTGGAAGTCAAGTCTATAGGTTTGATTACCTTCAGGTTAAATGTATTGGGTAAAAATATTTCACAGATTTTATATTGTATCATATTAAAAGGCACAGATAATCTCATTATTAATGTTGGTAATTATTTGGTTAAGATGATGACTACCCCATTGTAAAGGTTATTGTAAAGTTCTCCCTGTGGTAAAGGTCATTTGTTCACCCTTAGTAATCTATGGAGTAATGTTTTAGCACCAGGAGAATATTCTGTTTCCCAATGCCTTTAGCATCAATTGATGATTCTTGACTGAATCATTTATTACACTGGGGTGCAAAATGGTGATAAATTTTTTTTAGTGCTCTAAATTTCTTCTACATTTATTAGTTTATATTCTTTTTTAAAATAAAAAAGTATATTTCCCTCCTGTTTCTCTAATATCCCTCTTTAAAAAAAAGACCATGTCTTCATGAATTTAAAAAAAATTACTCAATATATTATAGCCAATCGGTCACAGTTCTTTTTGATGCTCATATATATTGTGAGCCTCTAGCCCATGTAAGTCACAGCAAGCTAGCTCTTGGGTTTTTTTTGAGATGGAGTTTCACTCTGTCACCCAAGCTGGAGTGCAGTGGCACGATCTCGGCTCACTGCAACCTCCACCCTCCGGGTTCAAATGATTCTCCTTCCTCAGCCTCCTGAGTAGCTGGGACTATAGGTGCCTGCCACCGCGCCTGGCTAATTTTTTGTATTTTTAGTAGAGATGGGGTTTCACCATCTTGGCCAGGCTGGTCTTGAACTCCTGACCTCGTGATCCACCCGCCTCGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACCGCGCCCGGCCACTCTTGTGTTTTTTAAACATGACCACATTAGTCTTTTAGTCCTTATTTGCTTTAGGCACAAGAAATCTGAGGCTTACCTTATATTTGTCTTGTTCCAAATTTGGATTATCAGCTAATTCTCCAAGGAGCCCTATACCTTTTAAGGGGAGTGGGATTTATTTAATTTTTTCTTTGGAGTCCTTCTAGCCAGTGAATGGAATTTATTATAGAATCCATTGTTACTGAGCTGTCATTGTTTCTAGGCCATTTTAGTGGACATAGCTAGGAAATACAATTTTAAAAGATCATGAATTCAAATTAATATTTACAACTAAAATTTAGCAACATGGCCAGGTGCAGTGGCTCATCCCTGTAATTCCAGCACTTTGGGAGTCTGAGGTGGGAGGATCACTTAAGGCCAGGGGTTCAAGACCAGCCTGGGCAACAGAGTGAGATCCTGTCTCTACAAAAAAAAAAAAAAAGATAAATTTTTTCGAAAAGTTTTATATGAAAAGTGTACTCTGAAAAAATCTAGCTGTCATACCTATCCCTCCATTCCTAACCATCTCTTATAAGTAATTATTATCCATCTCTTATAAGTAATTATTATAAGTATTTTCTTAGTTTTCCAATTTATCTTTCATGTGTTTCTTTTTAAAAAGCAAACAAATATGCATATTGCTGTACTTACTGTGCCTCTTTCACAACATGGGGTCTTTATAAAATTTCTTTGGATGTTTAGAAAGGTTTGTATGCTCTAATTTATACTGATAACATTTTATATAATGCATTTAGAGTCCCCAATTTGCTTGTCTAGGCCTCTACTATTTAATTTGACACCTTTAAATGATGTTCTTTGACTCTTATCTAATTGTCTATGTGACCGTCAGTGAATATATTATCTGTTCATATTTTCTGTAGTCTTATTTCTTTTTGATTAGAACATATAATATTTACACACTATTTTCCCATCCTAATCCCCACCTTTGTAATAAAATTAATTAAAATTTTAAAAAAGTATATTTAATAACCTACTGATACTTACCTGAAGTCTCTCTGGTCAACTTTTGGCTGGATGAAGCTCTTCTGTAGAGTATTCAAGGAGGGCTTGTAAGTACTGTACAATATTCTCTGAGTTGTGACATGTTTAAAATTTCTTCTGGAGCCTAGATACTTAAAGTACACCTTGGCCATGTAGAAAATCCTTGACAAACATTTCCTTTTCTTGAGTTTGTCTGAATTTGTGCTTCACTGTTGGGTTGCTTGATGTTATCATCAAGAAGTGTGGCACCACTTGATTTTTTTCCCTTTTGGAAATGACATTTTGTTTGTTGTTTTTTACCTAGAGACCCAGATAATTTTTTCTATATTATTTTATCTATGATCCTTTTTATCTCATTTTTGTTCTCATAGCTGTTTTATTCCTCTTCTCTGTATTTCTTACTTTACTTTTATTTGTGTCTCTCAGGTACCCTGTGATTTAGTTTTCATTTTTAGATGATTTTATCTTTTCAGTATATTGCCTATGTTTGTTCAACTCTCATTTTACATCTTACTATTGAGTTTAAAAATTTTTATTTGCTGTGTTCTTTTATATGTGCAATTAATGATATTCATGTTGGAGTGTTGTATATTATTTGCCTCTGCTTCATATTGTCTAGAGGGTAGTATTTTCTATTGATTGAAAAGTTTTCATTTTCTGTTTTTTATAATAGCCTTGTATGGATTTTTGGTGGCTTTTCTATTCATGATTAAATGTGTAGGATCTTCTTCAATCAGCAATAACAGGTGGCTCTATAGAATGGAGGGTAGAAGGGATGTGGGTGACTTACTCAGTTTTTAGTTAAAGAGGACCCTCTTCTGTTAGCATGGTGAAGTGCAGTTTCTTTAATAAATTGTGCATGGTGGGGGTGGGATTTGGATTCTGTGATACAATCTTGTTTCTTTAGGAATCTTTTACTTTTGGCCACTTGCCTTTCTTTCCAAGGAATCCCACTCCCTTTCAAGGTGCCTCATGAACTGTTTTCATGAACTTTCCAAACATTGGTTTCTGCTTGTTTCTAAGCCTGATTCTTGGCCTTCTCATTAATTTTCAAAACTTCCAATATCCTTCCAAATAATTCCCTTTTGCTTACGTTAGCGAGTACTAGTTTGTTAGCCAGTGTTAGTTTCTGTTGATCCTAACCAAAAAACCCTAACTGAGATATCAGTCTCTTAGCGCAAAGTTTGTGAATACGGTCATCCCTCCATATGAGGAGGGGAGTGGGAATTGGTTGTGGGACTTCCGTGGATATCCAAATCAGTGGATGCTAAAGTCCCTTACATAAAATGGCATAGTATTTGGTTATCATCCTCCCCATACATCATCTCTAGATTATTTATAGTACTTAATACAATGTAATGCTGTGTAGTCATACTGTATTTTTTACTTGTATTTTTGTTGTTTTGTGGGATTTAAAAAATATTTTTATTCTGAGGATAGTTGAATCCACAGGATACTGAGGGCCAGCTGTATTCACAACCCAAATCACATACAAAGCGACAAGTTCATACACAATAGGCCTATTAGAACAGGACTGTTCTCTCTTGTTTATCATTGCAGCCTTTCTAGCACAAAGCCTGGGACATTCTGGACATTTAGTATGTGTTAAATTTCTCTTACTACATTATTTCCAACAGTATTTACTGCAATCTGCAATTACCTTCCTTTTGTTTTGTAACTGTGTCCCCCACTAGAATGTAAGCTCTGTGCAGATAGTGTCTCATTTATTGATGTATCCCTGGCATCTAATAAAACACTGACAACACAAGCACCCAGTAAATATTTTTTGAATGACTGAACAATAACCAGTTCATAAGGCTGATAAAATTGGTATAGCTAGATGAAGTATGATTTTGAGGGACTATGAAAATCAAAGTAACCACACAATAAATTATCAGCCCTCTACTTCCATTCAAAACAAGCTCCTGGGAATTGAATTATGAAATCTATCATATTACTTTCTCTAAAGAACTTCAAGTTGGGTGTCAACTAAAAAGTTGCAGGCGAGGCGCGGTGGCTCAAACCTGTAATCCCAGCACTTTGGTAGAACTGAGTATCTCTTGAGGCCAGGTTTGAAACCAGCCTGGTCAACATAACCAGACTCTGTCTTTACAAAAGAAAAATTAAAATTAGCCAGGCATGGTGGTGTGCATTTGTAGTCCCAGATACTTGAGACGCTGAGGCAGAAGGATCGTTTCGGAAGAGGCTGCAGGAGGCCATGATGGCACCACTGCACTCCAGCCTGGGTGACAGAGTGAGACCCTGCCTCAGAAAATAATAATAGGCCACGCATGGTGGCTCACACCTGTAATCCCAGCACTTTGGGAGGCTGGGGCGGGAACATCACCTCAGGTAAGGAGTTCAAGCCTGGCCAACACGGTGAAATTCCATCTCTACTAAAAATACAAAAAAAATTAGCCAGGCATGGTAGTGGGGACCTGTAATCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATCACTTGAACCTGGGAGCTGGAAGTTGCAGTGAGCCAAGTTGGCACTATTGCACTGCAGCCTGGGCAACAAGAGCAAAACTCTGTCTCAAAAAATAAATAATAAAAAAAGTTTCAAAATGAGAATATATGTTTCAAAACAAGTATAATGAATATACTTATTGATTGGAAAATATAATTAGAAGTATCTATCAGGCTATAAATTGCTTTTCTTCTCCCTTCCATGGAAATTAGTTTTTTTTTCCATTTTTAGTCAGTATGAAAATACAAGGAAAAGGAAATTCAATCAAATTTACTTTTTAACATTTTATTTGGAAATAATTTCAAACTTACAGAAAAGTTGCAAAAACAGTACAAAGAACTCATACATTCATTTACTGTTTTTCCTTTTACCCTATATATTAGTTATTTATAGCTGTGTAACAAATAACCCCAAAGCTTAGTGGCTTACACCAAGTACTTTTCATCTTACACTGTTTCTGAGTCAGGATTCCAGGAGTGGCTAAGCTAGGTGGTCCTACCTCTGGGTCTCTCATGAAGTTGTAGTCAGCCAAAGGCTTGACCAAGGTTGGAGGATCTACTTCCAAAGTGACTCACTCCGTGGCATTTGGTAGGAGGCTACAAACAGTTCCTGGACAACTGGATCTCTCCATAGGCTGCTTGAGTGTCCTGAAAACACGGAAGCAGGCTTCCCCAGGCTCCAAGCCCCAAAATGAATGAAAAAGAGACCCGCAAAGGAAGATGCAGTGCCTTTTATGACCTAGCCTCTGAAGTCAATACTGTCACTTCTGTTTTGATCTATCAAGAGTCACTAAGCCTAGTCTACACTCAAGGGGAGGGGAATTAGAGTCCACCTCTTCCAGGGAGGAATATCATTGAATCTGTGAACATATCTTAGAACTACCATACCTAGTTTCAGTACTTTTAAACATTCGCCATTTTGCTTTGTCCCTCTCTTTTCCCCACCTACATATACATACACATACATGTTACTCCCTAACCATCTGAGAGTAGGGAGCATGCGGTGTATCCCTATCCCTCGTGTTTTTCTCTTAAGGAAAAGGATATTCTATTATACAACACGGTAGTTATCAATATCTAATTTTAACATTGTGATACTTTAAAGTCCACTTCCACTTGTGTAAATTGCCCTTTCTAGCAATGTTCCCATCAAATTTATTTTTAAACAATACAGTAAAAACGTAGAGGGCCACAAAGGGTGACATCGGTCAGGTAAGGTATTTTTTTTGGCAGGGAATAAAAAAGGTCCTGGGTCTAGGGAGGTAAACAAGCGTGAGCCAGCTGAGTTCTAGCGGGGGTCCCTGAACACCAAAAGGACAAGACTGTTTCTGAAACACTACATTATCTCTTAAGTTACCCATTACTTACGGAAAATGATTTTTTACTGTTCCCTTCGGTTCCTGTCTTGGTTAGAACACAGCTGGAGATTGTGTTAATAGCTTAGGACGTCTGTTTCCGTGAGCAGGTAACAACTTTTTGAAACAAATTCCCTCATCTGCTGAAGAAGGGGGACAAAAACGGCCCCTATCGCCCAGAACCGTTGCGAGGATTTAGCTAGCTGGTGACGCCGGAGCACGAAGTTGTACAGGTAGCCAGCAGCACCCACGCGAGCCCGCGGTTACCCTGGCCGCGCGGCTACTGTAGAGTGGGCTGGCGGCGAGCGGGCGGGGCGGTATCACGCGGGAGGGGCGGGGCCCGCTCGTCGGCTGATCGCACGATTGTGACGCGCCGCCGGAGGCAGGCCGGGCCCTCAAGATGGCGGCGGGCGCCCAGAGCGGCTCGGCCCGGCAGTAGTGGTGGGACGGCACTAGCTGCTGGGGCCTGCCGCCCCGGGAGTGGCTGCAGCAGCGCCAGGAATCGAGGATGGTAAAATGACCCAGGGGAAGAAGAAGAAACGGGCCGCGAACCGCAGTATCATGCTGGCCAAGAAGATCATCATTAAGGACGGAGGCACGGTGAGCTGAGTTCCGCGCCGGCGAGCGTCCCTCGGGGCCCCCATCCGGTCTCTCCTTCAGACCCCCACACTGCCGTCTCTAGGCGTCCCGGTGCCTCCCTCCCTTCCCCCACCCTGTCCGAGCTGCCGGTGCCTCGGGGTCGCGGACCCGCATGCCGCCGCTCCGGGAATCGTCCTCCGCTGCTCGGGCTTGCGGCCTCCGGGGCCCGTCCTCTTTCTTTCCCGCACCTGCCGCCCTCTGCTCTGGCCGCCTCTGCAGGCCCTGCGGCCTCGAACCCCACGTGCGCCTCCGCCGCGGGGAGGAATGTGCGGGGCTCCCCCGGCGGCCCGCCCGCCGCGCCCCTCGTCGCCGCAGCCTCGCCTCGCCTTCGCCGCCAGGCCCCGCGGAGCCGTCGCCGCGCTTGTCAAGGGGCTGGGAACCATCCCTGCTCTCCCATATGTTGCTAACGGGGTGGCGTCTGGCGCGGGGATCCCGCTGCGGCCCCGTAGTACGTTCGCTTTCTGTTTCCACGTCTCTCTGCGTCGGTGCTCCGGCTCTGGGCTGCTTACAGTAAACCCTGACCGGAGATGGGCTTCCCTCACTTCCCGGAGTCGGAAGCATGACGGCAGACACCTGGGGCCTACATTCGAACCTGCTAGTTTTCAAAGAAAAGTCATCACTGTGTGTCTTAAGATCAAAAGTATTAGAATCAGTCATGGCCTAAGGATCGGAGGAGGACACTTTGAAGGGAAGAAAGGTTTGCTTTTTAGAAACAGTTGTCATCACAGTAAACTTTATGCAGTGTGTAGTTAACCAGCTGGGGACGTAGGATTTTTAATTGAAAAACAAAACAAAACAAAACTGTTTTATGCTAACATTTCTCCGTTGCTACACTGTGTGGTCTTTGTTGCATCCGCTGATACCGCGTTCTGAAATAGAATGGAAAGGTGATATATATGTTTCACTTACCTGAAGTGTGCAGAAATTGTACCATTAATTCCATTTCTGTTTATATCTTATTGGAGCCGCGATCAACTGCTAGCACAGTAGTAAATGTGTAAGTAGGCCACCATTGAGGATTTGCTGAATTCAGTTGAAAAACGTGACAAAATTTTATGACATTTCAGAACACGGCCCAGTCAATATGCCAAAGTTTAGAAAACTTGAGACATATGTAATGACTTTGGAATATATTTTTAGTTTAACGTTTATTATATGTTATAGCTTTGACATTTATTGAAAAAAAGAAACAAATTCCTCAAGTTCTTTTTATTGAACTTGATTAATTAAAACATTACTTTGATTAGATCGGTTATGAAGAGTCATAGCTCTTTTGACCAAGTAGGTAAGAACTATGTGGGGAGAAAAATACTGTTGCCTTTGTCTACCTTTAGAAAGAGACAATATTTTACATTCTTCATAAAATCTACAAAATAGTGGCAATGAAAGATTGTATTTTGTAAGACCAAGTGATATTTAAGATCAGTATTTTTTACAAAATGTAAGAATGAAACTGATTAAGAAACACAGCTTCCTTTTCCTTGGAAAGTTCAGTTTTATTACCTTTCTTTGGGGTTTTGTTTGATTTGCTTTACAGCAGATGCTTTCTTTCCAAATCCTGTGAGTTTTGGAAAAGATCGTTTTTAAACTTTCTTGTCCTATTATTAAGGTTGTAATTAATTCTTAGCCTGCTTTGGGACACAAAATAAAATGTTTGCACCAGCAATAGGTTTCACATAGAACAAATGAAGACTTTTCTTGAGGGCTGTGAACATGGGGGCTATTATCATTTCTCATCTTTATACACTTAATATTTCATTCTCTATTCTAAGAGCACTGGGCACTCCTTTAGAAAAGGGGCTTTGTTTTGTATGTTTGGATCCCACAGGGCCTAGTATGTGAATTTTAAAGTGATAAAAACACTTCTATTTTGTACTAGCACATTCCTAGATGAATTTTTATTGTAATTTTGTTTATTCTTATACGTAATCAGAGGATATATTTCAATAAATATCAGGGGAATATTTTGCATTATTTGTATTTTAATCCATCCCAGCTTTAAATTTAAAAAGTATAACTATTGCAGTCATAGAAATGATTGTAAAATGGTAGTTGCTTATCTACCTCTCTACTTACAATAGTTCAGACTACTATTATGAACTTTTTTTGTTTGTTTGTTTGAGATGGAGTCTCACTCTGTTGCCCAGGCTGGAGGAGTGCAGTGGCAGGATCTCGGCTCACTGTAACCACCGCCTCCTGGGTTCAAGTGATTCTCCTGCCTCAGCCTCCCGAGTAGCTGGGACTACAGGCACGTGCCACCATGCCTGGCTAATTTTTTATATTTTCAGTAGAGACAAAGTTTCACCATATTGGTCAGGCTGGTCTTGAACTCCTGACCTCATGATTCACCCACCTTGGCCTCCCAAAGTGCAGGGATTACAGGTGTGAGCCACCGTGCCCAGACTGAACATTTTTTAAGAAAGGGGAAAAAATTGCCATTTGATACTCTGTTGTTGTGTGTTTTTTAATTCATCGTATCATAGAATATTTCAGTGCTATTGCTGTTGACCTCAGAGTTTCAGAGTTTTTATAAAGTTCCGCCAATGGGTAGATTCATTCAGTGAGATGTCTGAGGCTCTATGGTCGGTACATGACAGTCGTGAACAGTATTTCACATACCTGGTCAATGGTACTGATTTGATCCCCCTTCTGATTTCTTCTTTTCAACAATGTTAATAAAATTCTTTCCCGTTGTCCTGCTAATGACATATATGTAAGCCTATTTGGCCAGTTTAAATATTTATAAACAAAACTAGTAAGAGTTGTTAATGATTTTTCTGAAAATTAGAGCAGATTAGAGCAGATTTGTAGTTTTCAACGGCTGAAGAAATAAATCCTTCTAAATGAGCCAGATTAATCGTAAGTTACTGATTTTTTTATTGAAATTGTATTTCATTGAATTGTATTTCATTCAGCTGAATGAAAAACAGGCCAGGATAAAGCTAACAAGTAGGCTACCTATGTGAGTAGACACAATTAAGATAAATTACATTAAGGTGTGTGATTTTATATTAGGTGTTTTTAACCTGGGTCTGTTCACCCTGAAGTTGTTTGCAAAATTTTCTTTTGGCTATACATGTTTCTTGGAAGAGTCCCAAAAGGTCCATACTTCCCAAAAGTTTAAGAGCAATTGTTCTGTTTGAAAACAGCATAAGTAACTAAAGAATAAGTTCCACATATTATATTCAGTAAATATTTAATCATATACTGTATACTACTTCACTGATGAAAGTAACCATATTAGTGAATTTGCTTTTAAAGCATCCATATATAGAAATAGTTTTTAGGCCAGGGGCAGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCAGTGTGGGCAGATCACTTGAGGCCAGGAGTCTGAGACTAGCCTGGCCAACATGGTGAAACCCCATCTTTACCAAAATTACAAAAATGAGCTAGGTGTGGTGGTATGTGCCTGTAATCCCAGCTACCCGGGAGGCCGAGGCACGAGAATCACTTGAACCTGGGAGGCCAAGATTGCAGTGAGCCTAGATCACGCCACTGCATTCCAGCCTGGGTGATGGAGTGAAACTGTCTCAAAAAAAAAAAAAAAGAAGTTTTTAGTTACAGGTTTTCATGTATGTAACATTCAGTGTAGGTATTTAAGACAGCTGAAATAAAAATACCTTCTGACATTTTCAAATACTAGAATTCTGTTTTGTTTTATTAAAGCATTACCACTTGTTTTTAAGCATTCCTGTTAGAGGCTAAGAGCTAAAGAGTTATTTACAGTATTCAAATTGAATTTTCCTTATCTTTTAAAATGCTCATCTTAAAATATGATCTTTATTGTTTTGGCCATACAATTGTGGAACTACATCTCTGACAGTGGAAAATGTATAGTTCTTTCAGAAGTTTGTGGTAAAATGACTTTAAAGATTTGATAGAAAGTAAGGCATATCTGAATTGCATGGTCGGAAGTACCTGAAAAAAGTAAAATTGATATATCATTTGAAAATGAAATGCATATCCCTGGATAAGCAGAGCACCAGATTTTTTTTTTCTTGGCATCCCTGATTTTAATTAAATAGGAGTCAGCAACCGTTTCAAGAGCAGGACCCAAGCTCTGACCCTTTGCACTCTTCACCTGCAAGGATGGCTGAAGTAGTGGCAGGAAAGCTCTCTGGGATGTAGGGCCTTTGTAGACCCAGAGAGCTGTTAAATAACCTTTGGTTGCTAGCATGCAAGCAATAAGAAGGGCCTGTGGTGCTTTTCTTTTTCTTTCTTTTTTTTTTTCTTTTGAGACAGAGTTTTGCTCTTGTTGCTCAGGCTGGGGTGCAATGGCGTGATCTTGGCTCACAGCAACCTCTGCCTCCCTGGTTCAAGGAATTCTCCTACCTTAGCCTCCTGAATAGCTGGGATTACAGGCATGTGCCATCATGCCCAGCTAATTTTTGTATTTTTTTAGTAGAGACCGGATTTTACCATGTTGGCCAGGCTGGTCTCGAACTCTTGACTTCGGGTGATCCACCTGCCTCAGTCTTCCAAAGTGGGATTACAGGTGTCAGCCACTGCGCCTGGCCCCGTGGTGCTTTTCAAAAAGCCTAGAAACATCAGGGTGTTTATATTGTCTTTGGCAGGTGTGTGGCTGGCAGCATCATTAATTACTTAGCTCCTTACCTCCATGGTTCAGTGTTTGGTTTAGATTGGTGTGTTTGGGGATAAATTAATATGCAGTTTTTTTTTCAGATGGCTATATGCATCCAGTTCATCCTCATGTAGTTAGAAGACTTGCATACCAACATAATCAGACCGTCTGCAGAAATTCTCCTACAGTTGAAATGTAACTCCTTTGCAGCTACTGAAAGTTTAAAGTTTAAGTAAAAAAATGAATAGCTTTCTTCAGGTAACATTCTGACAAGTCTGTATGATTTAAAAGTTTCAATTATAAGGAACTCTGATTGTCTTTTAGCATTATTTTAAATTGGAAGTGTGAAAGTAACAGTTGACAGTTTCAGCCAGGGTACATCAAGAAGAGATGAATATGGGTATAATATAGCTCTCAAAATTTCCAGTACTTTATAACAAAGAAATATCCCTCCCACTGCCCTGTTTTTTAAAAAATAAATAATACATGTTTTCCTTCCAGTCGTGGGAAACTTAATAGAATGGTTCAGGAGGGACAAGTATATGCAGCATACCTGTCATTTTCCATTCAAGTTTTACTTTATTTTTAAAATTTATTATTTTTTAAAATATTTCAATAGTTTTGGGGTACAGGTGGGTTTTTGGTTACATAGATGTTTTTTAGTGATGATTTCTGAGATTTTAGTGCACCTGTCACCTGACCAGTGTATACTGTACCCAATATATAGTCTTTTATCCCTCTCAAGCTTCCCCCCCATCCTCAAAGTCCATTCTATTAGTCTTACGCCTTTGCGTCCTCATAGCTGAACTCTCACTTGTAAGTGAGAACATACGACATTTGGTTTTCCATTCCTGAATTACTCACTTAGAATAATGGCCTCCAATTCCATCCAAGTTTCTGCAAAAGACATTATTTCATTCCTTTTTATGGCTAAGTATTCAATGGTATATATACACCACATTTTCTTTATCCACTTGTTGGTCATTGGGCACTTGGGTTGGTTCCATATCTTTGCAGTTGTGAATTGTGCTGCTATAAACATGCATGTACATGTGTCTTTTTCATATAATGACTTCTTTTACTTTGGGTGGGTACCCAGTAGTGGGATTGCTGGATCAAACAGTAGTTCTATTTTTAGTTCTTTAAGGAATCGCCATACTGTTTTCCATAGTGGTTGTACTAGTTTACATTCCCAACAGCAGTGTCAAAGTGTTCATTTGTCACCACATCCACACCATCTATTATTTTTTGATTTTTAAATTATGGCCATTCTTGCAGGAGTAAATGATATCTCATTGTGGTTTTAATTTGCATTTCCCTGATAATTGGTGATGTTGAGCATCTTTTCATATGTTTGTTGGCTTATTGTATGCCTTTTGAAAAATGTCTATTCATGTCTTTTGCCTACTTTTGATGGGATTGTTTGTTTTTTTTCTTGCTGATTTGAGTTCCTTGTAGATTCTGGGTACTAGTCCTTTGTCAGATGCACAGTTCATAAATATTTTCTCCAACTGTATGGGTTGTCTGTTTACTCTGCTGATTTTTTTTTTTTTTTTTTTTTGAGATGGAATTTTGCTCTTGTTTCCCAGGCTGGAGTGCAATGGCATGATCTTGGCTCCCTGCAACCTCTGCCTCTCAGGTTCAAGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGATTACAGGCACACACCACCATGCCTGGCTAACTTTTTTGTATTTTTAGTAGAGACGAGTTTTCTCTATGTTGGCCAGGCTGGACTCAAACTACTGACCTTAGGTGATCCACCCGCCTTGGCCTCCCAAGATGCTGGGATTACAGGCATGCCTAGGCGGCTATAAGTATTTTGCTTTATTTCTGGGTTATCTGTTGTGTTCCATTGGTCTTCATGCCTATTTTTATACCAGTACCATGCGGTTTTGGTAACTGTAGCCTTTTGTATAATTTAAAGTCGGGTAATGTGATGCCTCCAGATTTGTTTTTTGCTTAGTCTTGCTTTGGCTATGTGGGCTCTTTTTTGGTTCCATATGAATTTTAGGATTGTTTTTTCTTGTTCTGTGAAGTATGATGCTGGTATTTTGATGGGAATTGCATTGAATCTATAGATTGTTTTGGTCAGTATAGTCATTTTCACAATGTTGATTCTTCCCTTCCATGAACATGGGATGTGTTTCCCTTTGTGTCATTTATGATTTCTTTTAACAGTGTTTTGTACTTTTCCTTGTAAAGATCTTTCACTTCCTTGGTTAAGTGTATTCCTAGGTGTTTTGTTTTTTTTGCAGCTATTGTAAAAGGGATTGAGTTCTTGATTTGATTCTCAGCTTTGTCGTTGCTGGAATATAGCAGTGCTATTGATTTGTGTCATTGATTTTGTATCCTGAGACTTTACTGAATCGTTTATCAGATCTCGGAGCTTTTTGGATGCGTCATTAGGGTTTTCTAGGTATACAGTCATATCATTGGCAAACAGTGGCAGTTTGATTTCCTCTTTTCCAATTTGCATGCTCGTTATTCCTTTCTCTTGTCTGATTACTCTGGTTAGGACTTCTAAATTTTTTAATTACTATGGGTACAAAGTAGATACAGATATTTATCAGGTACATCTGATATTTTGATA'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome['chr1'][100029207-width//2:100029207+width//2].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test it, set idx to 0\n",
    "idx = 0\n",
    "peak = peak_coords[idx]\n",
    "center = int(peak[1])\n",
    "max_length = 32768\n",
    "chrom = peak[0]\n",
    "seq = genome[chrom][center-max_length//2:center+max_length//2].seq\n",
    "one_hot_seq = dna_to_one_hot([seq])[0]\n",
    "cts = np.nan_to_num(cts_bw.values(chrom, center-max_length//2, center+max_length//2))\n",
    "counts = np.log(np.sum(cts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100029207'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center #have to specify int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32768, 4)\n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]]\n",
      "32768\n",
      "ACATTTAGAGGGCTGGTTAAATAAATTATCAGTATAATACATAGTATAGTGATTAAGATCATGTTCTCTCCTGAGCCCATGATTTACCAGTAGTGTGAGCTTGGGCAAGTTATGTAACCTCTCTGTGCTTCAGTTTCTTCATCTATAAAATAGACATCTACCTCATAGTGTTGTATGTGAGTTAACATACATGAAGCACTTAGAATAATTCTTGTTACATGCTGATACATACATATTTGCTATTATAATCCATACAAAGGAATATTATGCAACTGTTAGAATGCAGTAGTTCTGTTCATACTGACATGTATATATCCTTCTATAATGTATACTTTTTTAGTAATTACAAAGTCTTTTATAAATTAAAAAACTTTATTGGGTTTGGTGGCTTGTGCTTCTAATCCCAGATACTTGGGAGGCTGAGATGGGAGGATTGCTTGAGAACCAGGAGTTTTGAGACCTGAGCAACATAGTGAGACCCTGTTTCTTAAAAATATATATATATAAGCCTGGATACAGTGGCTCACGCCTGTAATCCCAGCACTTTGAGAGGCCGAGGCAGGTGGATTGTTTGAGCTTAGGAATTGGAGACCAGTCTGAGCAACACTACAAAACTCTGTACAAAAAAAAAAAAAAAAAATTAGCTGGGTGTGGTAGCGTGTGCCTGTGGTCCAAGCTACTTCGGAGGCTGAGGTGGGAGGATCGCTTGAGTCCAAGAAGTCGAGGCTGCAGTGAGTTGTGATTGTGCCTCTGCACTCCAGCCTGCTAAGTGACAGAGTGAAATCCTGTCTCAAAATATACATATACATATATATATGTTATATATAAAACTCTTCTGATGGTATTTTATATATGTGTGTATATATATACACACACATACCTATTTTATATATATGTAATACCATCATATAAAATACCATCACATGTGTTTTTGTAATTATATGCCAGCATTAATTTCATGTTTGAAATTAGACATTAAAAGGATAGAGCTGACTTAAATGGAATTTTAGAGCATTTCTCATCCTTTTCACTTCCTTTCTATTGTGGAATACTTTGATTAGTTATAACTTCATTTTTAAAAATTGCTTTACGCTTGCACTCCTTAACTGCACATATTTCAAAGGTGACCCATAAGAAGTGGTCCATGTAAGCATGAATTTCTTTATTAATAGCCTCGCAATGCTTCTGGGAATCTCTTATTCCAATAGTCATGTGATAGAATATGTTTTTGAGGTGAGATTCAACATGAACAAGACAGTTTCTGCTGTCACCAAGTTAACTTTCTCTAACAGTGAAACTTCAGTAACACAGAATCTAATGATTTCTATGACTATTTCTTAGCTGTGATTTTTTGGATATCAGTTATTATTAGTTATACTTTCAAAACAAAACTTTTATTTATTTATTTATTTTTTTAGAGACAGTCTCGCTGTTTTGCTCAGACTGGAATGCAGTGGTAGAATCATAGCCCACTGTAACCTCAAACTTCTGGGCTCAAGTGATTGTCCCTCCTCAGCCACCTGAGTAGCTAGGACTACAGGCATGCACCACCACACCTAACTAATTTAAAAATTTTTTTGTAGAGATGGAGTCTTGCTATGTTGCCTGGGTGGGTCTTAAACTCCTGGGGTCAAATGATTCTCCTGCTTTGGCCTTCCAAAGTGCTGGGATTTCAGGCGTGAGCCACTGTGCCCACTCTCAAAACTTCTTAATAGGGAAAAATATTGTACCCCCTGCCTCAGCTTTTATATTTGTATGTATCAATGTGAATATATGAAAAATCTAGATCTAGCAAGAGGAAGATGGAAGAACAATGACATATTCCAAATGTGAGCAATTTGCACATATTCGAATGGGCTAGCCATGTAATTATCTAATTTGTGCATTGTATTATTTTAGTGCTCCAAAGTTGGTAATGTATGTTATTTTTTGTATTCTATGTCTTTGTCTTTTTACATTTATATGTCTTTGTCTTGTAGTCATAATATAGTACTAAGTATACTTTTGTTAAGTATACAAAACCAAGTAAAAATTTTAGAAAGTAATCTAATTTTGGCTGGGCACGTAACTCACACCTGTAATCCCAGGACTTTGTGAGGCCAAGGCAGGTGGATCATGAGGTCAGGAGATCGAGACTAGCCTGGCTAACACAGTGAAACCCCGTCTCTACTAAAAATACAAAAAATTAGCTGGGCGTGGTGGCACGCGCCTGTAGTCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGTGGAGGTTGCAGTAAGCCAAGATTGTGCCACTGCACTGCAGCCTGGGCAACAGAGCAAGACTCCGTCTCAAAAAAAAAAAAAAAAAAAAGAAAGTAATCTAATTTTATCAAAAAGAAAAAGTTAAGTGTAAAATTATTTGGAATGTCAGTGGAAAAAATATATCTCTTCAGACAAACTAAACGATATATCATGTAGACTTTACTTTTTTTTAGGTTTCTTTGGAAGTATATTTGGATTAATGGGTGTATACATTTATGATGGAGAACTGGTATCAAAGAATGGATTTTTTCAGGGATATAACCGACTGACCTGGATAGTAGTTGTTCTTCAGGTAAAGCATTTAAAGTCTTAGATTTAATGCTAATAAACTGTATTTTAATATAAAGAATCAAAGTAGCTCCTGATATACTGATGTGAGGGGGAAAAGGTCCCTCCTGTTAGTGCTCTCGTATCTAGTTTATTCAAGGAAGTGAAAATAGAAGTGATCTTAATGCTGTAATGAAATCTTATTTTGTACAGGGGTCTTTAATGAACTTTGTGGTTTTAAATGTAACATAATGAAAATACAAGTTTAAGAATAAAAGAGTTGAAGCTAAAGGCAAATATAACTTTTGATTTTTACAAGTCTTATCATTGTACAGGAAGTCTGGGTCAGCCATTAAGACCCAGGAAGCAGATCTAGATATTTAATTCTGTGACAGTTCATAGCCTCATATTGTAGTTACATATTGGAAAACATCTGTCAAATTCAGTTTATATATAGCTTGGAATTGTTCAAGGGTTGAGTAATTAATTACATCTTTAAAAAATACATATGGAGAATTCTAGAGGATATAAAAATGAAGATACAAGTCACTATCCTTAGAGCCTCCTATTCTTCTAGAGATATTTGAAGCCTAGGAAGCAAGGCTCCAGAAAGGAAGGTGAAGATGGCAAAGGATAGTTGTGTTTTAAAAGCAGGTAGGATACTTCTATTTTTTTTTTTTTTTTTTTTGAGACGGAGTCTAGCTCTGTCACCAGGCTGGAGTGCAGTGGTACGATCTCGTCTCACTGTAATCTCCACCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCTGAGTAGCTGGGATTACAGGCATGTGCCACCACGCCCAGCTAATTTTTGTATTTTTAGTAGAGACGGGGGTTTCACCATGTTAGCCAGGATGATCTCGATCTCCTGACCTCGTTATCCACCCACCTTGACCTCCCAAAGTGCTGGGATTACAGGTGTGAGCCACCGCGCCCGGCCCGGATACTTCTATTTTTTTTTTTTTTTTGAGACGGAGTCTCGCTCTGTTGCCCAGGCTGGAGTGCAGTGGCGAGATCTTGGCTCACTGCAAGCTCCGCCTCCTGGGTTCACGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGACTACAGGCGCCCGCCACCAAGCCCAGCTAATTTTTTGTATTTTTAGTAGAGACGGGGTTTCACTGTGTTAGCCAGGATGGTTTTGATCTCCTGGCCTTGTGATCCGCCCACCTCGGCCTCCCAAAGTGCTGGGATTACAGGCGTGAGCCACCTCGCCTGGCCCGGATACTTCTGTTTTTAAGATAGCAGAAAAGGAAACAAAGATAAGTGAAAATTTTAAAAAAATAATTAAGAGAAAATTTGAAGCGGAGATAGGGTGAGTCAAGAGAGTTTATGTATGATGTATCTTATTCCAGCAAAGAGAAGAAGCCCCCTTCCCTTAAAAAGGGGCTGAAGATATGAGAAAAATTAAAAATGTTTGGTATAACTGGTATGAAGGATTCAATAGGGAGTAAATAGATGAATGACAACATTGCCAAGAAATACTGGGACTGAAACATTAATTTGTAGTAAGCTCAATTGGCATAATTTACGACTTTCTCTAAATAAAGCTTCCATCATAGAAGAGATGTTTTGAAAGTTGTTGCAGTAGGTAACCCAGATTTGGAAAATGATAAGTAGTCTTTGGCAGAAAGCAAAACTGTGTAGTATAGTAAAACCTCATTATTAAAGTAGCCACAATTGGGAATTTGAGAGGGATTAGGTAAGTTCTTTCTACAGTCTTAAAGAAAAAGCACAATTAAAACTGTGCTTTAGAAAACTTAATGTGACAGTAATGAGAGGTGGATTTGAAAGGGGAGAAACTATAGGAAAAAAGCCAAATTAAGGGGCTGTTGAAAGGCTGTAAAATGACTCTGATTACTAATTTTACTCAAAAGTAAAAATATTTCACTATTTAAAGTGACCAGTCCTCTTACATTTGTCCAAACAAGTGATGTTTAATTACTAAATTGGATAATGATAGACATTTTATTTGTAATATGGTCAAAGTCCATTATAACACACTGCAGATTGTCCTGGTAGTTACTTTAATTAAATGCCTTTTGAAATAAGAGGGCTATTGAACAAAACCTGTAAGTCTATTCGTTGTTAAGAGATGTTTAAAATAAATCCCTTACTATAATGGGATTGAATTTATGGGACAAAAGCTTAAAGCTTTATTAAATACTGAAAGTGTTTTGAAATGCAGTTTTACATGTGTGTTTTAAAAAATATTTTTTTAAAACTTCAGGCACTTGGAGGCCTTGTAATAGCTGCTGTTATTAAGTATGCAGATAATATTTTAAAAGGATTTGCAACCTCTTTATCGATAATATTATCAACATTGATCTCCTATTTTTGGCTTCAAGATTTTGTGCCAACCAGGTAAAATGTTCTTTTCTATTTTTTTAAATCCCCAGAAGTATATAGAAAAATTAGAATTCTTTGTAAGGTGATCTGATAAATTTGAAGAAGCACTGAAATATAATTTTAAAAATTTATTTAACTATGTTTTAAAAATATTTCAAATTAAATTCATGAAAGAAAGTCTTCTCCTGAAAAGTTTCTTATACAAATTTGGTTGGCCAGAATTTTCCCAATAAATAGATGTGAATATTTTCCCTGCAAAGTAAAAAAGTGATATAAAAAAGTTAATAAGTCATATAAAGTCATAAAAATAAGTCATGTAATGACATTTTTAAAATACGGAATTTACATGACTTCCTTGAAGGAGATAGTTTATATAGAGAATGTGGCTCATGAGGAAACTCAATTAGGTGGTAGCAGGAAGAGCAATCTGGAAAGGAATTGCAGGAGGAGTTATAGGTGAGGAAGAAGACCCAGGACTCTGTAGTTTTATGGATGCTAAGGGAGGAGATAGCATCAGGAAAAATTGAGTGTAATGAATATCCATTACAGACAGCATTGAAAATTCTAGTTTTTTGTTTAATTTATAATCTCAGACTAGTGGGATTATATTTTAATTGTTTCCAGTACACCACTCTTGGTTTTCCTCCAACTTTACTCCTCTCCTGATTTTCTCAACCTCTAAACTTTGGAATCTCCCAAGAATCTTTTATTTATTTATTACTTTATTATTTATATGTTCATATTTTTGAGACAGGATCTCAGTTTGTCACCCAGGTTGCAGTGCAGTGGCGTGATCGTAGCCCACTGCAGCTTCAAACTCCTGGGCTCAAGCAATCCTCCTGCCTCAGCCTCACAGGTAGCTGGGACTACACATGTGCTTTGTTGCCCAGGCTGATCTTGAACTCCTTACCTCGAGCAATCTTCCCACCTCTGCCTCTCAGAGTGCTGGGATTATAGGCATATGCCACTTCGCTGAACTCATTTGTGTTTTTGAGAGTGAAATAGGATTATGGATGGAATTTGGCACTATAAAAATATAATATCCTTTTTATATTTTTTATATTGAACATTGACGGTTTTAATTCTTTTCTAGCAATCTAGACAATAGGTCTGGAACATACATGTTGAGATCTGTAGTTCTTATGGGACAAAAATTTAAAAGCATTTCTGTTTTCAGCTTGGTATGGAAATGAATCACTTAGCTAGTGAGACTAGATGAACAATGAACACTTGTGTCTTAATTTTGTAATCTTAATCATATTCCCTGTTCATAAGATGAGGAAATCGTATGCTTTAATGTACTTTGAGAATGTTTGCACCTTGCAGATTTTTAGATTTTACTTTTAAAAGCATACCTTTTTTTTTTTTTTAAATTGAACCTTATGTAAGGCCGAGGTAATTAAATCTTTGAATTATGCAGAGAATAAACAAATAAATGAACAAGCAAACCATCAGAGGGACAAAAGAGTAGTAGTTTGGCAATGAGATGTATGCCTGCCTGTAAATTCACGTTTCTGAGGAAGTATATAAGGTTTGTTTAGTTGAAGATAGAAAGAGAAAAGACAATGATACTGCCATTATTGAATGAATGAATGAATGAATGAATGAATGAATGAATATTTTGAGAATATGCTCAGGACACTAAGCCAGATGGAGAAGATGGATTCTTTTTCCTAAAATAGGTAACTAAGCCCTCTATCTTTATGCATCTTACCTTTGTGGAGAATACATTTTCAGTAATAAATCTAGTGACATTTTCTCAATCTTCATTCAATTCTATCCTAAATGTAATAATATATTTGCCTTTAGCCTCATTCCATCTCCTGAAACTGGGTTTTTGTTTGTTTGTTTGTTTGGCTTCTCTGAAATGGTACGCCTATACTTGCATTTCTGATTGTCCCTTTGGTAGTTTGTCAATCCTTGCTCACTCTCTTGGCTATCTCATCCACTGCCATGTCATATCTGTCTTCTCTAGGAAGATAAGTTCAGATTTCACCTAAACTCTAGATCCCTTTATTCTATTGACTGTTAGACATCTCTACTGATACAATATCACTGTTACCCACTTCCCTAAATTATTTTTCTTGTGCTTGTTATTTTGCTTATTGGTGTTACCATTCTTCCATTTAGGGTTGCTTATTCTTTTATCAAAAGTTTGTGTCTACTATGTGCCTGCTGGCCACTGTGCTAGCTCCTGGAAAAGGAGTGCATGAATGAGACACAATTTCTTGTCTTCAAGGTCATCAAGGTTTGGTGATCATCAAGTGATTGGAAAGACAGATGTGAATAATTATAACAGTATGATCAATGCAACAATGGAAGTATACATACAATAATTCTGTTAGAGGGGGCTCAAATCCTTGAAGTAATCATTACATTCTGATAGTTTTATCCTAATTTTTTTTTTTCTGAGACAACTTTCTCTCTCAGAACCATCTTAAAATTGCTGGCTTGAGAAACAGGTGTTTCCTGACATGATTAGGTATTCAAGTAGATCCTGCTAAAGATATTATAGAAAGGATTCCTGCAGTGGATACAAAGGTAGACCACAGAATCTTTCAGTTGAGCATGAACTTTCAGTCCATGCCATTCCAAAGTTCCAGAAAGCTAAGAATAGTTCATAATGACTAAGGAAGACAGTGTCTAGTAATTAATAATTACCAATATTAATGTAGCAATTTCTTCAAGATACATTACAGGATGTTTTATGTTATCCTAATAACCTTATATGTATAACTGCATTCGAGTCTTTTGAAGAGTTAACCATTTGTATCAAGTCAAATCTAGTTATTAATCATAACAGATAATTACTGAGCTCCCTATATATGCTAGGCACTGTGGCTACCACAGTGAATGAAGAGCATATGGTTGAGCAGGGGAGTGGACTTGTAAATAATGCATCATTTAATGACAGTGTGGTAAGTGTTATGCTGGGGTAGTTCTGCCTAGGGGAGTGAGTCTTGGAGACATTAGACCTGAATTATTATGAATAAGCATTTTCTAAGAAGCAAGGAAACTGTGGACAGAGGGAATAGTATCTCTAAAGACACTGTCAAGAAAGTGTATGTTGTGTTCACTGATTGGAGTGGCTGAATTATAGATTGGGTGGCATGAAGTGGGAAGGATGAGTTTGGAGGGACCATATTATGAAGGGCCTTTTAATAATGATTAGTAATTATTTTTCGTTTATAGGCAATTAAAAACAAAAAAACAGGAGCATGGCTTTGGACTATTAGTAGAAACTTTAAATCCAAAACTAGCTTATGTCTCTATAATAGCATTTTAAAAAATGTACACATTAAAAGTTAAGATTGCACTTTGGGAGGCGGAGGTGGGCAGATCACTTGAGGTTAGGAGTTTGAGACCAGCCTGGCCAACATGGTGAAACCCCATCTCTACTAAAAATTCAAAAAATTAGCGGGGTGTGGTGGTGCACACTGGTAATCCCAGCTAGTCGGGAGGCTGAGGCAGGAGAATTGCTTGAACCTGGGAGGCGGAGGTTGCAGTGAGCCGAGATCACACCACTGCACTCCAGCCTGGGCGACAGAGTGAGACTCCATCTCAAAAAAATAAATAAATAAAAGTTAAGATTCAGGCTGGTCACAGTGGCTCATGCCTGTAATCACAGCACATTGGGAGTCTGAGGCAGATCACTTGAGCCCAGGAGTTCAAGATCAGCTTGGGCAACAAAGTGAGACCTCAGCTCTACAAAAAATAGAAAAATTAGCCAGGTGTGGTGGTTTGCACCTGTGGTCTCAGCTACTTAGGAGACTGAGCTGGGATTGCTTGAACCCTGGAGGTCGAGGCTGCAGTGAGCCATGGTGACAGCACTGCACTCCAGAGCCTGAGGGACTGAGTGAGACCCTGTCTTTAAAAAGAAAAAAAAAAGTTAATATTTGTAGTCTAGTTTACATATCCTGTCTAAATTCATTTCTGATTTATTGTAAAGTAGACTATATCTATAAATATATAGGATAAAGAGGAAACCAAGAACTCCCTGCATTTAATGTAATTTCCCTTTGTAAAGCATCCTATAAAAATTGGTCCTGAGTTTTGCTTATTATTATCTTGGATTTTAAACTAAACTTTACCTTGGTTTAACATGACAGAATAAAATATTCCTTTCGTATGTTCAACTAAAGAATTAAGGCCTTTTGTTATTGAATGTATTGTATACAAAATCCCTATTAAGGTGAAATACAGAACTTAAAATAGCTTTGCTTTGGAAGACAGTATAGTGCAGTCGATGAGAGTGCAGGCTGTAGAGTTAGACATACCTGGGTTTGATCCTCTTGAAGTTGTGGGACTTTAGACAAGTTATTTAAATTCTTTAAAACTGTTTCCTCATGTGTAAAAAATGAATACTAATACATCTTCATAGATTTGCTGTGACAACTCAATGAAATAATGCATATAAAGCACTTAGCCAGATACCTGGAATGTAGTAAGTACTCAAGATTTGTTAGCCGTGTTACTGGAACATTTTTCCCCACATTATAATTGTTTTTATAGTAAAGAAATAGAAGGAACTTGTTCTGCTTTTAATCTGATTTTCTCTTTTTTATTTTGTCTTCATCTTATTCAGTGTCTTTTTCCTTGGAGCCATCCTTGTAATAACAGCTACTTTTTTGTATGGTTATGATCCCAAACCTGCAGGAAATCCCACTAAAGCATAGTTGTATACTATCTTTAACTGGTTTTTCACGATGGGGCACTAGGAATCTCGACATTAATCTTGCACAGAGGACTTCTACAGAGTCTGAGAAGATATCATCATGCTGAATCTGATCATACTGTTTTTTAAAAGTTTAAGGATAAGACATGTGTATATGTAACAAAACACATTGCATCTAGAAATCAAAACTTGAAAGTATTTCCAGGGATTAGGATTAGAAGGAATATTAGAGGAAACTTGAAATCTGAGTTTAAAAAGATTTTACCTTTTTGATTGCTGCAGAAATGTCCTATGCACTCTTTGCAAGAGCACACAACAAATGTCAGATACCAATTTTTGCAAATTAGATTTAATCTTATTAAATGTTTTTATCTTACTCTTTCTGTACAGATATATCAAATCACATGAAATATTTAAAGTTTGAAAATTATAATTACCTATAAAGCTGTGAAAAATAGAAGTATAATTTGAAAAAACATTTCACTTATCAGAGATTTTTATATTTATACAAAAGATTACTAAATGAAGGATTGCTAAATGTTTTTGGTTCAATTACATAAAAATTAATATTCTGGGTCTGATCTGTCAGAGAATAAATATCAAATCTAAATTTAATGTAGAGATACATACTATTTCTCCATATGAATTTTAAGATATTTTAGTGCTTCAAGACTGCTGAAAGCAATCCAGTTGCTCCTGTGCTAGATGGTAGCCAGAGAATTTTATAGTAATGGAGGTTAGCCCTTAATCTCTTCATTGCATTTCATTTCTGTAAATCAGATTAAGTCCTTAATATTATTTTAAATTAAAATTTGTGTGTAATTGCCATTAAATTTTCAAAATGTAATTTAAAAGGATTAAATACTCATTTAATAATTTAAAATAATTATTGTATAATATCTACATTTGGAGAATTTTGAACTATCAAGCATATACTGTATACAGTTAGAAAGTTATTAAATGAACATTTTACTCATTGATCTGTAAAAACTTCTTTAATCTACAACTGTTTACAAAAACAACATTCAAACAAATAGTACAGATGTCAGTGACAGAACAAAATGACTTTTCTTGGAGACATTCCAGATTGCCATATTACTTTATTTTAAACAGCGCTATGACTTTAAATCCAAGGCTGCTCGGAAGATTTTTTTAGGTCTCTCATAAGCCTATTCTTCCCTGATCACATGAGTGGGAGAGGTAGCCAAATTTTGAATTCCCTTTCTGTGTTCCCACAAGAGCTGCTGAGAAGGCCTGGCGCAGTGGCTCACGCCTGTAATCCTAGCAGTTTGGGAGGCTGAGGCAGGTGGATCACAAGGTCAAGAGATTGAGACTATCCTGACCAACATGGTGAAACCCCGTCTCTACCAAAAATACAAAAATTAGCTGGGTGTGGTGGCACGCACCTGTAGTCCCAGCTACTCGGGAGACTGAGGCAGGAGAATCACTTGAACCCGGGAGGCAGAGGTTGCAGTAAGTGGAGATCACGCCACTGCACTCCAACCTGGGCGACACAGTGAGAGTCTGTCTCAAAAAAAAGAAAAAAAAAAAAGCTACTGAGAAGGAGCCACCATTTTGCTTTAAAATAGTGGGTTTTTTTCCTTTTTAATGAACACTGGAGCTGAGGATGCAGATTACATGAGTTATTTAATAAGTTGATTCAAGAGAGAGAGAGTATATTAGAAATGGTAAGATGATTTGGTGGGCAAAAATGCTTTCTATAATTTGAATATGGGTTTAATTTAATTATTAACTATAGACAGAATTACTTACTGAGTATAAGAAGTATTTTACTGTCTGGGCACAGTGGCTCATGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGTGGATCACAAGGTCAGGAATTCGAGACCAGCCTGGCCAGCATGGTGAAATCCCATCTCTACTAATAATACAAAAATTAGCTGGGCATGGTGGTGCGCGCCTGTAGTCCCAGCTACTCAGGAGGCTGAGGCAGGAGAATTTCTTGAACACACCAGGTGGAAGTTGCAGTGAGCCAAGATCGTGCCACTGCACCCCAGCCTGGGTGACAGAGCGAGACTCCGTCTCAAAAAAAAAAAAAAGAAAACACACACACACACACACACACCCACAGTATGAATGAAAAAAATAAAATACTTCTTTTTTTTTTTTTTTGAGACAGAGTCTCACTTTGTCGCCAGGCTGGAGTGCTGTGGCGCGATCTCGGCTTACTGCAACCTCCCACTCCCTGGTTCAAGGGATTCTCCTGCCTCAGCCTCTGAGTAGCTGGGATTGCAGGCGTGAGCCACTGCGCCCGGCCTATACTGTATATATTTTTAAAGACTGTTCTAATAGATATAAAAACTGTAAAAAATAAGTATTTTTATATAGCTCTCATGGATTTTATTAAACAGAATTGGCTCAAAAATACTATGTTACAGACTGTTGGGTACCCTTGCCTAACGTGAACTGGCAGTGTTACCTTGCTTTTGCAGTAATAGTCTACAGATTGCAGGTCTCATCAATTCCATCCAAAGTTTAAAAGCATTTAAAATTACCAAATCTTTAAAATCACTTTGGTGGTGATTCCAAATTGGTACCAAGCAAACTTTCTGGATGCCCAACATGATTTTCAGTAACCACCCTTTAGAGTATTTGTTTACTAAGTTCACCACATTTTGAACATGGTAGTTTTAGACTGCAATAATATTTAGACTTACATTATTACTTACTGCTAAGTAAAATCTAAATCCTGCAAATGCACAGAATTCAAGCTGAAATATAATGATTTATGTTTAGCTCACATTGAAGTATTGGTTGGTTACTTATGTATTAATGCAGTGTGCATTCACATTTAATCAGGTTTAGTCTGTTTCTATTTTAATAATTTTAAAAAATTATACAAGCAAATTAGATATTAGACATGTTAGTTACAATGGTAACACATTTTTAGGTGTCGAAACACAATTTTCAAAATTCCTAATGAAAGTTATAAAAATGTAAACAAGAATTGTAAAAATGGACAAAGTAGTCAAATATATTTTCAAAGCACAATTTTATTAGACAGGCATAATTTACATTTTGCTTTTCTAGTGGGTTTGAAAATGTTTATTGGAGATTGGGCTATGTAGTTTATAATTTTTAATTCATAAAAAAGTAATCATACATGAGAAGGTAGACCTGTGCCCTAGGATCATGTCACATATACAGATAATGCCATTTCCTTGTGTGTGTGATGTGTGTTTTGATGACCTCCACAGGCCTTACTGTATCAAGCTTTTATAATGATGACTCCTTCATTATTTAAATTCCTATACTTTTTATTTGTTATCACGCAACTACTTTGTTCAATGTGAAAATGTGCTAACTCATGGGAGAAGAGTGCCAATTGATAGTTCTTTTAGCAATTAAGAATATGGTATTTGGGAAGAAAAGTTTGAAATGCAACAAATGGATATTTCAACACAGTAGTATTATATTATCAGTTCTTTAGTAAGTGATTTTAGAGATGTTGTAGGCTACTTTTACGGTGGAATATATAGTATAGAGATGCAAAACTTAAATGTTTACATCAATTTATATTGAATGTCACATAATTTCATGGAAGGAAAGGTAGCTTGATATTTAGATTCTAAGATATAATCTGAAAGGAAACTAATTATGTTCTCTACACTTACTGTAATACTGATTATTCTTACATATCAAATTATTGAACTTTAAAAATTTCATTGTATAGTCATTAAACTGAGTTGGGTTTTTTCTTAAAGGGTTTAGCATCACTCATTTGATTTACACATTCACATTATAATATTTAATTATCATGGGTGTATGCTTTACATAAAAAAGGTTTATAAAAGTTATTTATGCTATATTGAAAGTCATCTTAAGAATCTCCAGGTTATTTAAAGTAGTTATAGGAGCAGAGAACAAGCACCTTTATCAAAATCTGGTCCTATGTGCCTTGCTTTACCAAATACCTGATTTTTCTGGAGGGTGTTCCTGTAATTCACAACTGTAGACACATGGGCAAAATTAGGATTTTTAAGAATAAATACATTTCTATTTTTTTGGTTGTTTCAACATTAGCTCTTCAAATTCATTAACAAAATTAAAATAGGTATATTACAAAAGCATAAACATTTGTGAACAGTACTTAAATAAATTGTGATACTATTGCTCCATCATTGAACTTTTTGAAACTTTAACAATTGTATAAAACTGTCAGTTTGTTGTTTCATTTGTAATTACAAAATAATTTAAAAACTTTTTAAAATAATTTGGATCCTGACTTTGTCTATATCTGTATTTCATTTGTTTAGAAAGATTCTTTTGGGTTTGATAATGTAATTTGTATATTTAAATTTTTTATGGACATAATTCAAAGGAATGTATAAATTGGTCTTTTGTTAAATGGCTTTTTAATTGATAAACTTCTCTTGTCATTTTTTGGTATCCAGCTATTACCTATTTAATAGATTTATTGAAATAGATTATTTTCATAAAGAACTCTATACAAATCTTTTTCTATATTTCCTTATTTTCCTATTTACCTGTGTCTATGACCTAACCTATGAATTAGTCTTCTCTCTTTATATATCAAAAATGAATTACTGATCTTTTTCTCTGGCTCTGTAGTATCTCTATCACTGTCACATGTGATCTTTCTTCCTTTTCTCTAGCCCATATTCTAGCATGAAATACTGGGTTGGCCAGGTGCAGTGGCTCATTCCTGTAATCCCAACACTTTAGGAAGCCAAGGCAGAAGAATCGCTTGAGCCCATGAATTTGAGGCCAGCCTGGGCAACATAGCCTTGTTTCTACAAAAAATCTTAAAAATAAAATTAGCCAATATGTGGGCATGCACCTGTGGGGCCAGCTACTCAGGAGGCTGAGGCAGAAGGATTGCTTGAGCCCAGCAATTTGATGCTGCAGTAAACCATGATGACACTACTGGACTCTAGCCTGAGTGACAGTGAGACTCTGTCTCAAAAAACAAACAAAACAAAAACTGAAACAACAAAAAAAGACTGGGTTTATTTAAGCTAGTTAGAATTTATCTTTCTATATGTTAATAACAGCCTAACAGATTTTTTGTTTTAAATATCTCTAGGCTAGCCTCAAGGTTAAGTAATTATAGAAGTTTGGTATGTATTTTCTTCATAATTTGAATATAATTGCTTCCATTGTGACTGTCAATTGAATGCATGGAGATCAATTGTGATAATATACAGGATTTTAGTCCTATCTCTACTGCTGAAGTAACCTTACACAAAATACTTTGTAAAAAAATCACTAAAGTGCCAGCATTTTTAAAGTGTATATTTTTCTTTGGCAACCTCTCATGAAAAGCACTAACTAAAAATATTTAATAATCTTTTTTGTATTACAGTGCTTCTTTTGTTGGAAATATATCACAATCCTCAAGTTCCACTGCTATGCAAAAGTATCTTAGAATCTGAATCTTATAGATAATACTACCTTTTTTTTTTTTTTTTTTTTTTTTTTGAGATGGTGTCTCGCTCTGTCACCTAGGCTGCAGTGGTGTGACCTCACTGCAACCTCTGCCTCCCAGGTTCAAGCAATTCTCCTGCCTCAGCCTCCCTAGTAACTGGGATTACAGGCCATGCCACCACGCCCAACTAATTTTTGTATTTTTAGTAGAGACAGGGTTTCACCATGTTGGCCAGGCTGGTCTCAAACTCCTGACCTCAGGTGATCCGCCTGCCTCGGCCTCCCAAAGTGCTGGGTTTACAGGCGTGAGCCGCTGCGCCCGGCCAATACTGCCTTTCAAAAGAGACTGATAGTGACCAGCATTAGTAATCATACTGGTGGGGTTTTTTGGGGTGTTTTTCTGTTTTGTTTTTTGTTTTTTGTTTTTGAGACAAGGTCTTCCTCCCGTTGCCTAGGCTGGAGTGCGGTGACACAATCTTGGCTCACTGCAGCCTTGATAGAGCAAGCTCAAGTGATCCTCCCAGGAGCCTCGGCCCCCAAGCAGCTGGGAATACAGGTGCGCGCCACCATGCCCAGCTGATTTTTGTATTTTTTTGTAGAGATGGGGTTTTGCCATGTTGCCCGGATTGGTCTCAAACTCCTGAGCTTCAGTGATCTGCCTGCCTTGGCCTCCCAAAGTGTTGGGATTACAGGCGTGAGCGACCACACCCAGCCCATATTGGTCTTTCTTACTGTTCTTAAAAAGAGAATTCCTTTAAGGCAGGACCGATTACATATACACTCTAGAAAAGAAAATAGCAAGGAAGAAATAAATTGCCTTCAATTACCAAAGATTTGAGCTTCTGCTATGGCTGAGAGTGTTTTGGTCATTGCAAATTCAGGGGTTTCCCAAGTTCACCCTCAGTTCTGGCTAGAAAGAGAAAACTTACTAAAAGCTATTATACTCACAGTCATATTTATTACAGAGAAAGGAAATACAAATTAAAACCAGCCAAAGGAAGTGACACATAAAACAGAGTCTAGGAGTGGTCCAAACTTGAGGCTTTCGGTGTCCTTTTCTTGTAGCGTCATGGAAGGTGTTATCTACTCCTGACCACAATGTTTGACAGTACACACATAGTATTGCCATTCAGGGAAGCTCACCTAAGCTTTGGTGTCCAGATTTTTATTGAGAGAGGCTCTATTAGTTGGCATGGTTGGTTGATTTTTTTGCCCATGTAGTTGTCCTCTCTTTCCAACATCTGCCCCTCCCTGGAGATCTGGTTGACATCAAGACTTCAGGGCCTCACCATAGGTTATCTCGTTAGCATAAACTGTCAAGTGTTGTCTAAGGAACCCACAATGAATAATAAAGACATTCCTATCAGTGAGAACTCCCAAAGACTTACACCAGAACTTTCTTTGGATGGGCCAAATTTCTTACTACACAAAGACCATTCATCTCTATACACTTCCTTCTGAATTGATGAGGATGATACAAGCAACGACAATTCTTCTTTTCAGAGACTTTTAATTTTTTTTTTTTTTTTTTTTTTGAGATGGAGTCTCGCTCTGTCACTCAGGCTGGAGTTCAGCGGCACGATCTCTGCTCACTGCAACCTCCGCCTCCTGGGTTTGAGCAATTCTCCTACCTCAGCCTCCCAAGTAGCTGGGATTACTGGTGCTCACCACCACGCCCAACTAATTTTTGTATTTTTGGTAGAGACGGGTTTTCACCATGTTGGCCAGGCTGGTCTCGAACTCCTGACCTCAGGTGATCTGCCGGCCTTGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCATCACACCCAGCCCAGAGACTTTTAATTTATACATTGTCATTTTTAATTTTAGAATATCTGTCAAATGATTCTGAACATAACATGAATCTAGTGTGGAAAAATGTTTATAATCAGATATTGTGTTAAGAACATATATATATATATAGAGAGAGAGAGAGCATAGTATTGTCATTTAGTTTTCACTTCTTATGTTTAATGACAACTTTATAAATGCTGGACTATTTTAAACTACAAATTTAAAACATGGTTTATAAACCTTTTACCTGTACTGCATATTAATAAACAATTTTGAACTAATTTTAAATTAGGCATTCCTATCACCATTATAAGGAAGAGCTAAAACCTGATACCAGTTGTTATTTTGGCTCACAGTAGGGCTTCACTAGTGTTAGGATCACATTTTCCTCCCTTGGACCTGAGAATCAGGTGTTTTTCGTTTAATGCATTTATTTTCATTCATTCATTTAGGATTTCTAAAATGTTAGAGGTAATTTGTTTAAAACAAATAACATGGTGTTCTTTTGGAAATATACAGTTTATCTTAGATATACACTTTCCTCATTTTAAAGGTAGAACTCACTGCAGGTTTAAAAGAATAATGAATATTGTTACAGTGTTTAGTGATAATCACTGAAAATTCACGTTAAAATATACTTTAAGCTTGTCCAACGCGCGGCCTGCAGGCTGCATGCAGCCCAGGATGGCTTTGAATGTGGCTCAACACAAATTTGTAAACTTTCTTAAAACAGTATGAGATTTTTTTGCAATTTTTTAAAGCTCATCAGCTATTATTAGTTACTGTATTTTATATGTGGCCCAAGACAATTCTTCTTCTTTAAGGGTGACCCAGGGAAGCCAAAAGATTGGACACCCCAGCTTTAAATGTAGAGTATTTAAGAGGTTCAGTTAAATGTACTAAATCAGACTTGTGTGATTTAGACACAAAATCTGTGTGTGTGTGTGTTTGGTCTTTTAAAAGATACTGCATGTACAAACATGATTTCTAATAATTGAGGGTTTATTAAGTTTTTCCCTTTAGGCTAAAGTTTTATTTTATTCTGCTTGGATAGTAAATACAGCATTTTAAAATGATGATGCTAAGTGCTTTTTTAATATATGCATGCTGCTTCAATAGGTCTTTTAAATATAGCCAAGGCAGGGTAATATTTCCTGAGTGATGATAGAATTATAGAAAATCATGAAAGATGTGTGCCCATGTTTCATTTTATAACTGTTCCAGTCATATTGCTATGAATTCTAGAATTTTAAAACCTGCCACACTTATTTTGGATGTTATATACACATAATGCCTCAAATTTCCATTATTTCATATTATTTTTCTCCATTTCCCTCGTGTCAAGTTTCATTTTGTTTTCAATGTTTTGTTTTTATAATATTGCCATTTTAATGGCTTCAATCATGCTATGACAGATTGTGCCAATTTAGCTTTAAAAAACTCATGGGCTACCTCCATGAACTCACACATGCGGCATTGTTAAAGAACCTTTTTTAAAATTAAATTTTCTCCAGCTTTTCATTTTGCAGCATACAAAAAAATTGAAAAATTGGTATACCACCTAAATCCAACCATTATTAACATTTTGCCACACTAGCTTTATCTCTGTTGGGACACTTTACCCCTACATACTCCAGCATGCGTGACCTAAAGATATGGACATCTTCTTAATTAACCTCAGTGCCATTATGACACCTAAGAAAATTTAAATTAATTCAGGTCACTTCATCTAAGTTCAAATTCAAATACAATTGCCCCTCTGTAAACGTGGGGGATTGGTTCCAGGACCCCCACCTCGTGTATACCAAAATTTGGGCACACTCAAGTCCTTAAGGGGAAACCGCGTATAAGAAAAGTCAGCCCTCCCTATGGGTGGGTTTCACATCCTGCAAATAACTGTGTTTTCCATCTGCATTTGGCTGAAAAAAAATCTGCATATAAGTGGACCCATGCAGTTCAAATCTGTTGTTCAAGAGTCAACTGTATTCCCAGTTGTCTCAAAAATGATTTTTTATAGCTTTTTTGTTTTGGAACGAGGGTAAAATCAAGGTTAATGCTTTGTATTTGATTGTCTCCTTTGGTCTTGAAGTCCCCTTCCTGCACCACCACCTGCCCCTCAATGGCTTTTTTATACCAAGAGACTGAAAATGCTACCTCTAATGACTTTTCTCCCCATGATAGTACTCTTTGAAGGACCAAAGTTACTTTGCTCCTATTATGTGGTTTGATTTGTTCCCTTTATTCCTTTCATGTCCTTTGAACTGGAAGTCAAGTCTATAGGTTTGATTACCTTCAGGTTAAATGTATTGGGTAAAAATATTTCACAGATTTTATATTGTATCATATTAAAAGGCACAGATAATCTCATTATTAATGTTGGTAATTATTTGGTTAAGATGATGACTACCCCATTGTAAAGGTTATTGTAAAGTTCTCCCTGTGGTAAAGGTCATTTGTTCACCCTTAGTAATCTATGGAGTAATGTTTTAGCACCAGGAGAATATTCTGTTTCCCAATGCCTTTAGCATCAATTGATGATTCTTGACTGAATCATTTATTACACTGGGGTGCAAAATGGTGATAAATTTTTTTTAGTGCTCTAAATTTCTTCTACATTTATTAGTTTATATTCTTTTTTAAAATAAAAAAGTATATTTCCCTCCTGTTTCTCTAATATCCCTCTTTAAAAAAAAGACCATGTCTTCATGAATTTAAAAAAAATTACTCAATATATTATAGCCAATCGGTCACAGTTCTTTTTGATGCTCATATATATTGTGAGCCTCTAGCCCATGTAAGTCACAGCAAGCTAGCTCTTGGGTTTTTTTTGAGATGGAGTTTCACTCTGTCACCCAAGCTGGAGTGCAGTGGCACGATCTCGGCTCACTGCAACCTCCACCCTCCGGGTTCAAATGATTCTCCTTCCTCAGCCTCCTGAGTAGCTGGGACTATAGGTGCCTGCCACCGCGCCTGGCTAATTTTTTGTATTTTTAGTAGAGATGGGGTTTCACCATCTTGGCCAGGCTGGTCTTGAACTCCTGACCTCGTGATCCACCCGCCTCGGCCTCCCAAAGTGCTGGGATTACAGGCATGAGCCACCGCGCCCGGCCACTCTTGTGTTTTTTAAACATGACCACATTAGTCTTTTAGTCCTTATTTGCTTTAGGCACAAGAAATCTGAGGCTTACCTTATATTTGTCTTGTTCCAAATTTGGATTATCAGCTAATTCTCCAAGGAGCCCTATACCTTTTAAGGGGAGTGGGATTTATTTAATTTTTTCTTTGGAGTCCTTCTAGCCAGTGAATGGAATTTATTATAGAATCCATTGTTACTGAGCTGTCATTGTTTCTAGGCCATTTTAGTGGACATAGCTAGGAAATACAATTTTAAAAGATCATGAATTCAAATTAATATTTACAACTAAAATTTAGCAACATGGCCAGGTGCAGTGGCTCATCCCTGTAATTCCAGCACTTTGGGAGTCTGAGGTGGGAGGATCACTTAAGGCCAGGGGTTCAAGACCAGCCTGGGCAACAGAGTGAGATCCTGTCTCTACAAAAAAAAAAAAAAAGATAAATTTTTTCGAAAAGTTTTATATGAAAAGTGTACTCTGAAAAAATCTAGCTGTCATACCTATCCCTCCATTCCTAACCATCTCTTATAAGTAATTATTATCCATCTCTTATAAGTAATTATTATAAGTATTTTCTTAGTTTTCCAATTTATCTTTCATGTGTTTCTTTTTAAAAAGCAAACAAATATGCATATTGCTGTACTTACTGTGCCTCTTTCACAACATGGGGTCTTTATAAAATTTCTTTGGATGTTTAGAAAGGTTTGTATGCTCTAATTTATACTGATAACATTTTATATAATGCATTTAGAGTCCCCAATTTGCTTGTCTAGGCCTCTACTATTTAATTTGACACCTTTAAATGATGTTCTTTGACTCTTATCTAATTGTCTATGTGACCGTCAGTGAATATATTATCTGTTCATATTTTCTGTAGTCTTATTTCTTTTTGATTAGAACATATAATATTTACACACTATTTTCCCATCCTAATCCCCACCTTTGTAATAAAATTAATTAAAATTTTAAAAAAGTATATTTAATAACCTACTGATACTTACCTGAAGTCTCTCTGGTCAACTTTTGGCTGGATGAAGCTCTTCTGTAGAGTATTCAAGGAGGGCTTGTAAGTACTGTACAATATTCTCTGAGTTGTGACATGTTTAAAATTTCTTCTGGAGCCTAGATACTTAAAGTACACCTTGGCCATGTAGAAAATCCTTGACAAACATTTCCTTTTCTTGAGTTTGTCTGAATTTGTGCTTCACTGTTGGGTTGCTTGATGTTATCATCAAGAAGTGTGGCACCACTTGATTTTTTTCCCTTTTGGAAATGACATTTTGTTTGTTGTTTTTTACCTAGAGACCCAGATAATTTTTTCTATATTATTTTATCTATGATCCTTTTTATCTCATTTTTGTTCTCATAGCTGTTTTATTCCTCTTCTCTGTATTTCTTACTTTACTTTTATTTGTGTCTCTCAGGTACCCTGTGATTTAGTTTTCATTTTTAGATGATTTTATCTTTTCAGTATATTGCCTATGTTTGTTCAACTCTCATTTTACATCTTACTATTGAGTTTAAAAATTTTTATTTGCTGTGTTCTTTTATATGTGCAATTAATGATATTCATGTTGGAGTGTTGTATATTATTTGCCTCTGCTTCATATTGTCTAGAGGGTAGTATTTTCTATTGATTGAAAAGTTTTCATTTTCTGTTTTTTATAATAGCCTTGTATGGATTTTTGGTGGCTTTTCTATTCATGATTAAATGTGTAGGATCTTCTTCAATCAGCAATAACAGGTGGCTCTATAGAATGGAGGGTAGAAGGGATGTGGGTGACTTACTCAGTTTTTAGTTAAAGAGGACCCTCTTCTGTTAGCATGGTGAAGTGCAGTTTCTTTAATAAATTGTGCATGGTGGGGGTGGGATTTGGATTCTGTGATACAATCTTGTTTCTTTAGGAATCTTTTACTTTTGGCCACTTGCCTTTCTTTCCAAGGAATCCCACTCCCTTTCAAGGTGCCTCATGAACTGTTTTCATGAACTTTCCAAACATTGGTTTCTGCTTGTTTCTAAGCCTGATTCTTGGCCTTCTCATTAATTTTCAAAACTTCCAATATCCTTCCAAATAATTCCCTTTTGCTTACGTTAGCGAGTACTAGTTTGTTAGCCAGTGTTAGTTTCTGTTGATCCTAACCAAAAAACCCTAACTGAGATATCAGTCTCTTAGCGCAAAGTTTGTGAATACGGTCATCCCTCCATATGAGGAGGGGAGTGGGAATTGGTTGTGGGACTTCCGTGGATATCCAAATCAGTGGATGCTAAAGTCCCTTACATAAAATGGCATAGTATTTGGTTATCATCCTCCCCATACATCATCTCTAGATTATTTATAGTACTTAATACAATGTAATGCTGTGTAGTCATACTGTATTTTTTACTTGTATTTTTGTTGTTTTGTGGGATTTAAAAAATATTTTTATTCTGAGGATAGTTGAATCCACAGGATACTGAGGGCCAGCTGTATTCACAACCCAAATCACATACAAAGCGACAAGTTCATACACAATAGGCCTATTAGAACAGGACTGTTCTCTCTTGTTTATCATTGCAGCCTTTCTAGCACAAAGCCTGGGACATTCTGGACATTTAGTATGTGTTAAATTTCTCTTACTACATTATTTCCAACAGTATTTACTGCAATCTGCAATTACCTTCCTTTTGTTTTGTAACTGTGTCCCCCACTAGAATGTAAGCTCTGTGCAGATAGTGTCTCATTTATTGATGTATCCCTGGCATCTAATAAAACACTGACAACACAAGCACCCAGTAAATATTTTTTGAATGACTGAACAATAACCAGTTCATAAGGCTGATAAAATTGGTATAGCTAGATGAAGTATGATTTTGAGGGACTATGAAAATCAAAGTAACCACACAATAAATTATCAGCCCTCTACTTCCATTCAAAACAAGCTCCTGGGAATTGAATTATGAAATCTATCATATTACTTTCTCTAAAGAACTTCAAGTTGGGTGTCAACTAAAAAGTTGCAGGCGAGGCGCGGTGGCTCAAACCTGTAATCCCAGCACTTTGGTAGAACTGAGTATCTCTTGAGGCCAGGTTTGAAACCAGCCTGGTCAACATAACCAGACTCTGTCTTTACAAAAGAAAAATTAAAATTAGCCAGGCATGGTGGTGTGCATTTGTAGTCCCAGATACTTGAGACGCTGAGGCAGAAGGATCGTTTCGGAAGAGGCTGCAGGAGGCCATGATGGCACCACTGCACTCCAGCCTGGGTGACAGAGTGAGACCCTGCCTCAGAAAATAATAATAGGCCACGCATGGTGGCTCACACCTGTAATCCCAGCACTTTGGGAGGCTGGGGCGGGAACATCACCTCAGGTAAGGAGTTCAAGCCTGGCCAACACGGTGAAATTCCATCTCTACTAAAAATACAAAAAAAATTAGCCAGGCATGGTAGTGGGGACCTGTAATCCCAGCTACTCGGGAGGCTGAGGCAGGAGAATCACTTGAACCTGGGAGCTGGAAGTTGCAGTGAGCCAAGTTGGCACTATTGCACTGCAGCCTGGGCAACAAGAGCAAAACTCTGTCTCAAAAAATAAATAATAAAAAAAGTTTCAAAATGAGAATATATGTTTCAAAACAAGTATAATGAATATACTTATTGATTGGAAAATATAATTAGAAGTATCTATCAGGCTATAAATTGCTTTTCTTCTCCCTTCCATGGAAATTAGTTTTTTTTTCCATTTTTAGTCAGTATGAAAATACAAGGAAAAGGAAATTCAATCAAATTTACTTTTTAACATTTTATTTGGAAATAATTTCAAACTTACAGAAAAGTTGCAAAAACAGTACAAAGAACTCATACATTCATTTACTGTTTTTCCTTTTACCCTATATATTAGTTATTTATAGCTGTGTAACAAATAACCCCAAAGCTTAGTGGCTTACACCAAGTACTTTTCATCTTACACTGTTTCTGAGTCAGGATTCCAGGAGTGGCTAAGCTAGGTGGTCCTACCTCTGGGTCTCTCATGAAGTTGTAGTCAGCCAAAGGCTTGACCAAGGTTGGAGGATCTACTTCCAAAGTGACTCACTCCGTGGCATTTGGTAGGAGGCTACAAACAGTTCCTGGACAACTGGATCTCTCCATAGGCTGCTTGAGTGTCCTGAAAACACGGAAGCAGGCTTCCCCAGGCTCCAAGCCCCAAAATGAATGAAAAAGAGACCCGCAAAGGAAGATGCAGTGCCTTTTATGACCTAGCCTCTGAAGTCAATACTGTCACTTCTGTTTTGATCTATCAAGAGTCACTAAGCCTAGTCTACACTCAAGGGGAGGGGAATTAGAGTCCACCTCTTCCAGGGAGGAATATCATTGAATCTGTGAACATATCTTAGAACTACCATACCTAGTTTCAGTACTTTTAAACATTCGCCATTTTGCTTTGTCCCTCTCTTTTCCCCACCTACATATACATACACATACATGTTACTCCCTAACCATCTGAGAGTAGGGAGCATGCGGTGTATCCCTATCCCTCGTGTTTTTCTCTTAAGGAAAAGGATATTCTATTATACAACACGGTAGTTATCAATATCTAATTTTAACATTGTGATACTTTAAAGTCCACTTCCACTTGTGTAAATTGCCCTTTCTAGCAATGTTCCCATCAAATTTATTTTTAAACAATACAGTAAAAACGTAGAGGGCCACAAAGGGTGACATCGGTCAGGTAAGGTATTTTTTTTGGCAGGGAATAAAAAAGGTCCTGGGTCTAGGGAGGTAAACAAGCGTGAGCCAGCTGAGTTCTAGCGGGGGTCCCTGAACACCAAAAGGACAAGACTGTTTCTGAAACACTACATTATCTCTTAAGTTACCCATTACTTACGGAAAATGATTTTTTACTGTTCCCTTCGGTTCCTGTCTTGGTTAGAACACAGCTGGAGATTGTGTTAATAGCTTAGGACGTCTGTTTCCGTGAGCAGGTAACAACTTTTTGAAACAAATTCCCTCATCTGCTGAAGAAGGGGGACAAAAACGGCCCCTATCGCCCAGAACCGTTGCGAGGATTTAGCTAGCTGGTGACGCCGGAGCACGAAGTTGTACAGGTAGCCAGCAGCACCCACGCGAGCCCGCGGTTACCCTGGCCGCGCGGCTACTGTAGAGTGGGCTGGCGGCGAGCGGGCGGGGCGGTATCACGCGGGAGGGGCGGGGCCCGCTCGTCGGCTGATCGCACGATTGTGACGCGCCGCCGGAGGCAGGCCGGGCCCTCAAGATGGCGGCGGGCGCCCAGAGCGGCTCGGCCCGGCAGTAGTGGTGGGACGGCACTAGCTGCTGGGGCCTGCCGCCCCGGGAGTGGCTGCAGCAGCGCCAGGAATCGAGGATGGTAAAATGACCCAGGGGAAGAAGAAGAAACGGGCCGCGAACCGCAGTATCATGCTGGCCAAGAAGATCATCATTAAGGACGGAGGCACGGTGAGCTGAGTTCCGCGCCGGCGAGCGTCCCTCGGGGCCCCCATCCGGTCTCTCCTTCAGACCCCCACACTGCCGTCTCTAGGCGTCCCGGTGCCTCCCTCCCTTCCCCCACCCTGTCCGAGCTGCCGGTGCCTCGGGGTCGCGGACCCGCATGCCGCCGCTCCGGGAATCGTCCTCCGCTGCTCGGGCTTGCGGCCTCCGGGGCCCGTCCTCTTTCTTTCCCGCACCTGCCGCCCTCTGCTCTGGCCGCCTCTGCAGGCCCTGCGGCCTCGAACCCCACGTGCGCCTCCGCCGCGGGGAGGAATGTGCGGGGCTCCCCCGGCGGCCCGCCCGCCGCGCCCCTCGTCGCCGCAGCCTCGCCTCGCCTTCGCCGCCAGGCCCCGCGGAGCCGTCGCCGCGCTTGTCAAGGGGCTGGGAACCATCCCTGCTCTCCCATATGTTGCTAACGGGGTGGCGTCTGGCGCGGGGATCCCGCTGCGGCCCCGTAGTACGTTCGCTTTCTGTTTCCACGTCTCTCTGCGTCGGTGCTCCGGCTCTGGGCTGCTTACAGTAAACCCTGACCGGAGATGGGCTTCCCTCACTTCCCGGAGTCGGAAGCATGACGGCAGACACCTGGGGCCTACATTCGAACCTGCTAGTTTTCAAAGAAAAGTCATCACTGTGTGTCTTAAGATCAAAAGTATTAGAATCAGTCATGGCCTAAGGATCGGAGGAGGACACTTTGAAGGGAAGAAAGGTTTGCTTTTTAGAAACAGTTGTCATCACAGTAAACTTTATGCAGTGTGTAGTTAACCAGCTGGGGACGTAGGATTTTTAATTGAAAAACAAAACAAAACAAAACTGTTTTATGCTAACATTTCTCCGTTGCTACACTGTGTGGTCTTTGTTGCATCCGCTGATACCGCGTTCTGAAATAGAATGGAAAGGTGATATATATGTTTCACTTACCTGAAGTGTGCAGAAATTGTACCATTAATTCCATTTCTGTTTATATCTTATTGGAGCCGCGATCAACTGCTAGCACAGTAGTAAATGTGTAAGTAGGCCACCATTGAGGATTTGCTGAATTCAGTTGAAAAACGTGACAAAATTTTATGACATTTCAGAACACGGCCCAGTCAATATGCCAAAGTTTAGAAAACTTGAGACATATGTAATGACTTTGGAATATATTTTTAGTTTAACGTTTATTATATGTTATAGCTTTGACATTTATTGAAAAAAAGAAACAAATTCCTCAAGTTCTTTTTATTGAACTTGATTAATTAAAACATTACTTTGATTAGATCGGTTATGAAGAGTCATAGCTCTTTTGACCAAGTAGGTAAGAACTATGTGGGGAGAAAAATACTGTTGCCTTTGTCTACCTTTAGAAAGAGACAATATTTTACATTCTTCATAAAATCTACAAAATAGTGGCAATGAAAGATTGTATTTTGTAAGACCAAGTGATATTTAAGATCAGTATTTTTTACAAAATGTAAGAATGAAACTGATTAAGAAACACAGCTTCCTTTTCCTTGGAAAGTTCAGTTTTATTACCTTTCTTTGGGGTTTTGTTTGATTTGCTTTACAGCAGATGCTTTCTTTCCAAATCCTGTGAGTTTTGGAAAAGATCGTTTTTAAACTTTCTTGTCCTATTATTAAGGTTGTAATTAATTCTTAGCCTGCTTTGGGACACAAAATAAAATGTTTGCACCAGCAATAGGTTTCACATAGAACAAATGAAGACTTTTCTTGAGGGCTGTGAACATGGGGGCTATTATCATTTCTCATCTTTATACACTTAATATTTCATTCTCTATTCTAAGAGCACTGGGCACTCCTTTAGAAAAGGGGCTTTGTTTTGTATGTTTGGATCCCACAGGGCCTAGTATGTGAATTTTAAAGTGATAAAAACACTTCTATTTTGTACTAGCACATTCCTAGATGAATTTTTATTGTAATTTTGTTTATTCTTATACGTAATCAGAGGATATATTTCAATAAATATCAGGGGAATATTTTGCATTATTTGTATTTTAATCCATCCCAGCTTTAAATTTAAAAAGTATAACTATTGCAGTCATAGAAATGATTGTAAAATGGTAGTTGCTTATCTACCTCTCTACTTACAATAGTTCAGACTACTATTATGAACTTTTTTTGTTTGTTTGTTTGAGATGGAGTCTCACTCTGTTGCCCAGGCTGGAGGAGTGCAGTGGCAGGATCTCGGCTCACTGTAACCACCGCCTCCTGGGTTCAAGTGATTCTCCTGCCTCAGCCTCCCGAGTAGCTGGGACTACAGGCACGTGCCACCATGCCTGGCTAATTTTTTATATTTTCAGTAGAGACAAAGTTTCACCATATTGGTCAGGCTGGTCTTGAACTCCTGACCTCATGATTCACCCACCTTGGCCTCCCAAAGTGCAGGGATTACAGGTGTGAGCCACCGTGCCCAGACTGAACATTTTTTAAGAAAGGGGAAAAAATTGCCATTTGATACTCTGTTGTTGTGTGTTTTTTAATTCATCGTATCATAGAATATTTCAGTGCTATTGCTGTTGACCTCAGAGTTTCAGAGTTTTTATAAAGTTCCGCCAATGGGTAGATTCATTCAGTGAGATGTCTGAGGCTCTATGGTCGGTACATGACAGTCGTGAACAGTATTTCACATACCTGGTCAATGGTACTGATTTGATCCCCCTTCTGATTTCTTCTTTTCAACAATGTTAATAAAATTCTTTCCCGTTGTCCTGCTAATGACATATATGTAAGCCTATTTGGCCAGTTTAAATATTTATAAACAAAACTAGTAAGAGTTGTTAATGATTTTTCTGAAAATTAGAGCAGATTAGAGCAGATTTGTAGTTTTCAACGGCTGAAGAAATAAATCCTTCTAAATGAGCCAGATTAATCGTAAGTTACTGATTTTTTTATTGAAATTGTATTTCATTGAATTGTATTTCATTCAGCTGAATGAAAAACAGGCCAGGATAAAGCTAACAAGTAGGCTACCTATGTGAGTAGACACAATTAAGATAAATTACATTAAGGTGTGTGATTTTATATTAGGTGTTTTTAACCTGGGTCTGTTCACCCTGAAGTTGTTTGCAAAATTTTCTTTTGGCTATACATGTTTCTTGGAAGAGTCCCAAAAGGTCCATACTTCCCAAAAGTTTAAGAGCAATTGTTCTGTTTGAAAACAGCATAAGTAACTAAAGAATAAGTTCCACATATTATATTCAGTAAATATTTAATCATATACTGTATACTACTTCACTGATGAAAGTAACCATATTAGTGAATTTGCTTTTAAAGCATCCATATATAGAAATAGTTTTTAGGCCAGGGGCAGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCAGTGTGGGCAGATCACTTGAGGCCAGGAGTCTGAGACTAGCCTGGCCAACATGGTGAAACCCCATCTTTACCAAAATTACAAAAATGAGCTAGGTGTGGTGGTATGTGCCTGTAATCCCAGCTACCCGGGAGGCCGAGGCACGAGAATCACTTGAACCTGGGAGGCCAAGATTGCAGTGAGCCTAGATCACGCCACTGCATTCCAGCCTGGGTGATGGAGTGAAACTGTCTCAAAAAAAAAAAAAAAGAAGTTTTTAGTTACAGGTTTTCATGTATGTAACATTCAGTGTAGGTATTTAAGACAGCTGAAATAAAAATACCTTCTGACATTTTCAAATACTAGAATTCTGTTTTGTTTTATTAAAGCATTACCACTTGTTTTTAAGCATTCCTGTTAGAGGCTAAGAGCTAAAGAGTTATTTACAGTATTCAAATTGAATTTTCCTTATCTTTTAAAATGCTCATCTTAAAATATGATCTTTATTGTTTTGGCCATACAATTGTGGAACTACATCTCTGACAGTGGAAAATGTATAGTTCTTTCAGAAGTTTGTGGTAAAATGACTTTAAAGATTTGATAGAAAGTAAGGCATATCTGAATTGCATGGTCGGAAGTACCTGAAAAAAGTAAAATTGATATATCATTTGAAAATGAAATGCATATCCCTGGATAAGCAGAGCACCAGATTTTTTTTTTCTTGGCATCCCTGATTTTAATTAAATAGGAGTCAGCAACCGTTTCAAGAGCAGGACCCAAGCTCTGACCCTTTGCACTCTTCACCTGCAAGGATGGCTGAAGTAGTGGCAGGAAAGCTCTCTGGGATGTAGGGCCTTTGTAGACCCAGAGAGCTGTTAAATAACCTTTGGTTGCTAGCATGCAAGCAATAAGAAGGGCCTGTGGTGCTTTTCTTTTTCTTTCTTTTTTTTTTTCTTTTGAGACAGAGTTTTGCTCTTGTTGCTCAGGCTGGGGTGCAATGGCGTGATCTTGGCTCACAGCAACCTCTGCCTCCCTGGTTCAAGGAATTCTCCTACCTTAGCCTCCTGAATAGCTGGGATTACAGGCATGTGCCATCATGCCCAGCTAATTTTTGTATTTTTTTAGTAGAGACCGGATTTTACCATGTTGGCCAGGCTGGTCTCGAACTCTTGACTTCGGGTGATCCACCTGCCTCAGTCTTCCAAAGTGGGATTACAGGTGTCAGCCACTGCGCCTGGCCCCGTGGTGCTTTTCAAAAAGCCTAGAAACATCAGGGTGTTTATATTGTCTTTGGCAGGTGTGTGGCTGGCAGCATCATTAATTACTTAGCTCCTTACCTCCATGGTTCAGTGTTTGGTTTAGATTGGTGTGTTTGGGGATAAATTAATATGCAGTTTTTTTTTCAGATGGCTATATGCATCCAGTTCATCCTCATGTAGTTAGAAGACTTGCATACCAACATAATCAGACCGTCTGCAGAAATTCTCCTACAGTTGAAATGTAACTCCTTTGCAGCTACTGAAAGTTTAAAGTTTAAGTAAAAAAATGAATAGCTTTCTTCAGGTAACATTCTGACAAGTCTGTATGATTTAAAAGTTTCAATTATAAGGAACTCTGATTGTCTTTTAGCATTATTTTAAATTGGAAGTGTGAAAGTAACAGTTGACAGTTTCAGCCAGGGTACATCAAGAAGAGATGAATATGGGTATAATATAGCTCTCAAAATTTCCAGTACTTTATAACAAAGAAATATCCCTCCCACTGCCCTGTTTTTTAAAAAATAAATAATACATGTTTTCCTTCCAGTCGTGGGAAACTTAATAGAATGGTTCAGGAGGGACAAGTATATGCAGCATACCTGTCATTTTCCATTCAAGTTTTACTTTATTTTTAAAATTTATTATTTTTTAAAATATTTCAATAGTTTTGGGGTACAGGTGGGTTTTTGGTTACATAGATGTTTTTTAGTGATGATTTCTGAGATTTTAGTGCACCTGTCACCTGACCAGTGTATACTGTACCCAATATATAGTCTTTTATCCCTCTCAAGCTTCCCCCCCATCCTCAAAGTCCATTCTATTAGTCTTACGCCTTTGCGTCCTCATAGCTGAACTCTCACTTGTAAGTGAGAACATACGACATTTGGTTTTCCATTCCTGAATTACTCACTTAGAATAATGGCCTCCAATTCCATCCAAGTTTCTGCAAAAGACATTATTTCATTCCTTTTTATGGCTAAGTATTCAATGGTATATATACACCACATTTTCTTTATCCACTTGTTGGTCATTGGGCACTTGGGTTGGTTCCATATCTTTGCAGTTGTGAATTGTGCTGCTATAAACATGCATGTACATGTGTCTTTTTCATATAATGACTTCTTTTACTTTGGGTGGGTACCCAGTAGTGGGATTGCTGGATCAAACAGTAGTTCTATTTTTAGTTCTTTAAGGAATCGCCATACTGTTTTCCATAGTGGTTGTACTAGTTTACATTCCCAACAGCAGTGTCAAAGTGTTCATTTGTCACCACATCCACACCATCTATTATTTTTTGATTTTTAAATTATGGCCATTCTTGCAGGAGTAAATGATATCTCATTGTGGTTTTAATTTGCATTTCCCTGATAATTGGTGATGTTGAGCATCTTTTCATATGTTTGTTGGCTTATTGTATGCCTTTTGAAAAATGTCTATTCATGTCTTTTGCCTACTTTTGATGGGATTGTTTGTTTTTTTTCTTGCTGATTTGAGTTCCTTGTAGATTCTGGGTACTAGTCCTTTGTCAGATGCACAGTTCATAAATATTTTCTCCAACTGTATGGGTTGTCTGTTTACTCTGCTGATTTTTTTTTTTTTTTTTTTTTGAGATGGAATTTTGCTCTTGTTTCCCAGGCTGGAGTGCAATGGCATGATCTTGGCTCCCTGCAACCTCTGCCTCTCAGGTTCAAGCCATTCTCCTGCCTCAGCCTCCCAAGTAGCTGGGATTACAGGCACACACCACCATGCCTGGCTAACTTTTTTGTATTTTTAGTAGAGACGAGTTTTCTCTATGTTGGCCAGGCTGGACTCAAACTACTGACCTTAGGTGATCCACCCGCCTTGGCCTCCCAAGATGCTGGGATTACAGGCATGCCTAGGCGGCTATAAGTATTTTGCTTTATTTCTGGGTTATCTGTTGTGTTCCATTGGTCTTCATGCCTATTTTTATACCAGTACCATGCGGTTTTGGTAACTGTAGCCTTTTGTATAATTTAAAGTCGGGTAATGTGATGCCTCCAGATTTGTTTTTTGCTTAGTCTTGCTTTGGCTATGTGGGCTCTTTTTTGGTTCCATATGAATTTTAGGATTGTTTTTTCTTGTTCTGTGAAGTATGATGCTGGTATTTTGATGGGAATTGCATTGAATCTATAGATTGTTTTGGTCAGTATAGTCATTTTCACAATGTTGATTCTTCCCTTCCATGAACATGGGATGTGTTTCCCTTTGTGTCATTTATGATTTCTTTTAACAGTGTTTTGTACTTTTCCTTGTAAAGATCTTTCACTTCCTTGGTTAAGTGTATTCCTAGGTGTTTTGTTTTTTTTGCAGCTATTGTAAAAGGGATTGAGTTCTTGATTTGATTCTCAGCTTTGTCGTTGCTGGAATATAGCAGTGCTATTGATTTGTGTCATTGATTTTGTATCCTGAGACTTTACTGAATCGTTTATCAGATCTCGGAGCTTTTTGGATGCGTCATTAGGGTTTTCTAGGTATACAGTCATATCATTGGCAAACAGTGGCAGTTTGATTTCCTCTTTTCCAATTTGCATGCTCGTTATTCCTTTCTCTTGTCTGATTACTCTGGTTAGGACTTCTAAATTTTTTAATTACTATGGGTACAAAGTAGATACAGATATTTATCAGGTACATCTGATATTTTGATA\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_seq.shape)\n",
    "print(one_hot_seq[:10]) #acatttagag\n",
    "print(len(seq))\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2adc12f35950>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(cts)) #have to wrap it with the nan to num\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cts) #yeah looks about right, see like 2 potential peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.196604827477849\n",
      "10.196642123515149\n"
     ]
    }
   ],
   "source": [
    "print(counts)\n",
    "print(np.log(1+np.sum(cts))) #basically identical since it's so large!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got numpy.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_atac_long\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprofile_atac_long\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m profile_atac_long\u001b[38;5;241m.\u001b[39mProfileATACLong(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m32768\u001b[39m, \u001b[38;5;241m20000\u001b[39m, tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m, rc_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, jitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m900\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/lila/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/profile_atac_long.py:307\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    305\u001b[0m     seq \u001b[38;5;241m=\u001b[39m seq[start:start\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n\u001b[1;32m    306\u001b[0m     cts \u001b[38;5;241m=\u001b[39m cts[start:start\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]\n\u001b[0;32m--> 307\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(counts)\n\u001b[1;32m    309\u001b[0m cts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(cts\u001b[38;5;241m.\u001b[39mcopy())\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got numpy.float64)"
     ]
    }
   ],
   "source": [
    "#test the dataset\n",
    "\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, 20000, tokenizer_name = 'char', rc_aug = True, jitter = 900)\n",
    "out = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's figure out the issue\n",
    "peak = dataset.peak_coords[idx]\n",
    "chrom = peak[0]\n",
    "center = int(peak[1])\n",
    "offset = dataset.max_length//2 + dataset.jitter\n",
    "seq = dataset.genome[chrom][center-offset:center+offset].seq.upper()\n",
    "one_hot_seq = dna_to_one_hot([seq])[0]\n",
    "offset = dataset.cts_max_length//2 + dataset.jitter\n",
    "cts = np.nan_to_num(dataset.cts_bw.values(chrom, center-offset, center+offset))\n",
    "counts = np.log(1+np.sum(cts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = dataset.tokenizer(seq,\n",
    "    add_special_tokens=True if dataset.add_eos else False,  # this is what controls adding eos\n",
    "    padding=\"max_length\",\n",
    "    max_length=dataset.max_length,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = seq['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783\n"
     ]
    }
   ],
   "source": [
    "start = np.random.choice(range(dataset.jitter*2+1))\n",
    "print(start)\n",
    "seq = seq[start:start+dataset.max_length]\n",
    "cts = cts[start:start+dataset.max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 9, 10, 7, 9, 9, 8, 10, 9, 10, 7, 7, 10, 9, 10, 7, 7, 9, 7, 7, 10, 10, 8, 8, 7, 7, 9, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 7, 9, 10, 8, 10, 7, 10, 8, 8, 10, 7, 10, 7, 7, 9, 7, 9, 8, 7, 8, 7, 7, 7, 9, 7, 10, 7, 9, 7, 10, 9, 10, 8, 8, 7, 9, 9, 7, 10, 10, 7, 10, 10, 9, 10, 10, 10, 8, 7, 9, 8, 8, 7, 10, 10, 9, 9, 10, 10, 7, 8, 8, 7, 10, 10, 8, 10, 7, 7, 7, 7, 7, 7, 8, 7, 9, 7, 7, 7, 7, 8, 7, 10, 10, 10, 7, 9, 7, 9, 9, 9, 8, 10, 9, 9, 10, 10, 7, 7, 7, 10, 7, 7, 7, 10, 10, 7, 10, 8, 7, 9, 10, 7, 10, 7, 7, 10, 7, 8, 7, 10, 7, 9, 10, 7, 10, 7, 9, 10, 9, 7, 10, 10, 7, 7, 9, 7, 10, 8, 7, 10, 9, 10, 10, 8, 10, 8, 10, 8, 8, 10, 9, 7, 9, 8, 8, 8, 7, 10, 9, 7, 10, 10, 10, 7, 8, 8, 7, 9, 10, 7, 9, 10, 9, 10, 9, 7, 9, 8, 10, 10, 9, 9, 9, 8, 7, 7, 9, 10, 10, 7, 10, 9, 10, 7, 7, 8, 8, 10, 8, 10, 8, 10, 9, 10, 9, 8, 10, 10, 8, 7, 9, 10, 10, 10, 8, 10, 10, 8, 7, 10, 8, 10, 7, 10, 7, 7, 7, 7, 10, 7, 9, 7, 8, 7, 10, 8, 10, 7, 8, 8, 10, 8, 7, 10, 7, 9, 10, 9, 10, 10, 9, 10, 7, 10, 9, 10, 9, 7, 9, 10, 10, 7, 7, 8, 7, 10, 7, 8, 7, 10, 9, 7, 7, 9, 8, 7, 8, 10, 10, 7, 9, 7, 7, 10, 7, 7, 10, 10, 8, 10, 10, 9, 10, 10, 7, 8, 7, 10, 9, 8, 10, 9, 7, 10, 7, 8, 7, 10, 7, 8, 7, 10, 7, 10, 10, 10, 9, 8, 10, 7, 10, 10, 7, 10, 7, 7, 10, 8, 8, 7, 10, 7, 8, 7, 7, 7, 9, 9, 7, 7, 10, 7, 10, 10, 7, 10, 9, 8, 7, 7, 8, 10, 9, 10, 10, 7, 9, 7, 7, 10, 9, 8, 7, 9, 10, 7, 9, 10, 10, 8, 10, 9, 10, 10, 8, 7, 10, 7, 8, 10, 9, 7, 8, 7, 10, 9, 10, 7, 10, 7, 10, 7, 10, 8, 8, 10, 10, 8, 10, 7, 10, 7, 7, 10, 9, 10, 7, 10, 7, 8, 10, 10, 10, 10, 10, 10, 7, 9, 10, 7, 7, 10, 10, 7, 8, 7, 7, 7, 9, 10, 8, 10, 10, 10, 10, 7, 10, 7, 7, 7, 10, 10, 7, 7, 7, 7, 7, 7, 8, 10, 10, 10, 7, 10, 10, 9, 9, 9, 10, 10, 10, 9, 9, 10, 9, 9, 8, 10, 10, 9, 10, 9, 8, 10, 10, 8, 10, 7, 7, 10, 8, 8, 8, 7, 9, 7, 10, 7, 8, 10, 10, 9, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 7, 10, 9, 9, 9, 7, 9, 9, 7, 10, 10, 9, 8, 10, 10, 9, 7, 9, 7, 7, 8, 8, 7, 9, 9, 7, 9, 10, 10, 10, 10, 9, 7, 9, 7, 8, 8, 10, 9, 7, 9, 8, 7, 7, 8, 7, 10, 7, 9, 10, 9, 7, 9, 7, 8, 8, 8, 10, 9, 10, 10, 10, 8, 10, 10, 7, 7, 7, 7, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 7, 9, 8, 8, 10, 9, 9, 7, 10, 7, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 8, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 7, 9, 7, 9, 9, 8, 8, 9, 7, 9, 9, 8, 7, 9, 9, 10, 9, 9, 7, 10, 10, 9, 10, 10, 10, 9, 7, 9, 8, 10, 10, 7, 9, 9, 7, 7, 10, 10, 9, 9, 7, 9, 7, 8, 8, 7, 9, 10, 8, 10, 9, 7, 9, 8, 7, 7, 8, 7, 8, 10, 7, 8, 7, 7, 7, 7, 8, 10, 8, 10, 9, 10, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 10, 9, 9, 9, 10, 9, 10, 9, 9, 10, 7, 9, 8, 9, 10, 9, 10, 9, 8, 8, 10, 9, 10, 9, 9, 10, 8, 8, 7, 7, 9, 8, 10, 7, 8, 10, 10, 8, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 10, 9, 9, 9, 7, 9, 9, 7, 10, 8, 9, 8, 10, 10, 9, 7, 9, 10, 8, 8, 7, 7, 9, 7, 7, 9, 10, 8, 9, 7, 9, 9, 8, 10, 9, 8, 7, 9, 10, 9, 7, 9, 10, 10, 9, 10, 9, 7, 10, 10, 9, 10, 9, 8, 8, 10, 8, 10, 9, 8, 7, 8, 10, 8, 8, 7, 9, 8, 8, 10, 9, 8, 10, 7, 7, 9, 10, 9, 7, 8, 7, 9, 7, 9, 10, 9, 7, 7, 7, 10, 8, 8, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 10, 7, 10, 7, 8, 7, 10, 7, 10, 7, 8, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 9, 10, 10, 7, 10, 7, 10, 7, 10, 7, 7, 7, 7, 8, 10, 8, 10, 10, 8, 10, 9, 7, 10, 9, 9, 10, 7, 10, 10, 10, 10, 7, 10, 7, 10, 7, 10, 9, 10, 9, 10, 9, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 8, 7, 8, 7, 8, 7, 8, 7, 10, 7, 8, 8, 10, 7, 10, 10, 10, 10, 7, 10, 7, 10, 7, 10, 7, 10, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 8, 7, 10, 7, 10, 7, 7, 7, 7, 10, 7, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 10, 9, 10, 10, 10, 10, 10, 9, 10, 7, 7, 10, 10, 7, 10, 7, 10, 9, 8, 8, 7, 9, 8, 7, 10, 10, 7, 7, 10, 10, 10, 8, 7, 10, 9, 10, 10, 10, 9, 7, 7, 7, 10, 10, 7, 9, 7, 8, 7, 10, 10, 7, 7, 7, 7, 9, 9, 7, 10, 7, 9, 7, 9, 8, 10, 9, 7, 8, 10, 10, 7, 7, 7, 10, 9, 9, 7, 7, 10, 10, 10, 10, 7, 9, 7, 9, 8, 7, 10, 10, 10, 8, 10, 8, 7, 10, 8, 8, 10, 10, 10, 10, 8, 7, 8, 10, 10, 8, 8, 10, 10, 10, 8, 10, 7, 10, 10, 9, 10, 9, 9, 7, 7, 10, 7, 8, 10, 10, 10, 9, 7, 10, 10, 7, 9, 10, 10, 7, 10, 7, 7, 8, 10, 10, 8, 7, 10, 10, 10, 10, 10, 7, 7, 7, 7, 7, 10, 10, 9, 8, 10, 10, 10, 7, 8, 9, 8, 10, 10, 9, 8, 7, 8, 10, 8, 8, 10, 10, 7, 7, 8, 10, 9, 8, 7, 8, 7, 10, 7, 10, 10, 10, 8, 7, 7, 7, 9, 9, 10, 9, 7, 8, 8, 8, 7, 10, 7, 7, 9, 7, 7, 9, 10, 9, 9, 10, 8, 8, 7, 10, 9, 10, 7, 7, 9, 8, 7, 10, 9, 7, 7, 10, 10, 10, 8, 10, 10, 10, 7, 10, 10, 7, 7, 10, 7, 9, 8, 8, 10, 8, 9, 8, 7, 7, 10, 9, 8, 10, 10, 8, 10, 9, 9, 9, 7, 7, 10, 8, 10, 8, 10, 10, 7, 10, 10, 8, 8, 7, 7, 10, 7, 9, 10, 8, 7, 10, 9, 10, 9, 7, 10, 7, 9, 7, 7, 10, 7, 10, 9, 10, 10, 10, 10, 10, 9, 7, 9, 9, 10, 9, 7, 9, 7, 10, 10, 8, 7, 7, 8, 7, 10, 9, 7, 7, 8, 7, 7, 9, 7, 8, 7, 9, 10, 10, 10, 8, 10, 9, 8, 10, 9, 10, 8, 7, 8, 8, 7, 7, 9, 10, 10, 7, 7, 8, 10, 10, 10, 8, 10, 8, 10, 7, 7, 8, 7, 9, 10, 9, 7, 7, 7, 8, 10, 10, 8, 7, 9, 10, 7, 7, 8, 7, 8, 7, 9, 7, 7, 10, 8, 10, 7, 7, 10, 9, 7, 10, 10, 10, 8, 10, 7, 10, 9, 7, 8, 10, 7, 10, 10, 10, 8, 10, 10, 7, 9, 8, 10, 9, 10, 9, 7, 10, 10, 10, 10, 10, 10, 9, 9, 7, 10, 7, 10, 8, 7, 9, 10, 10, 7, 10, 10, 7, 10, 10, 7, 9, 10, 10, 7, 10, 7, 8, 10, 10, 10, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 8, 10, 10, 10, 10, 7, 10, 10, 10, 7, 10, 10, 10, 7, 10, 10, 10, 7, 10, 10, 10, 10, 10, 10, 10, 7, 9, 7, 9, 7, 8, 7, 9, 10, 8, 10, 8, 9, 8, 10, 9, 10, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 8, 10, 9, 9, 7, 7, 10, 9, 8, 7, 9, 10, 9, 9, 10, 7, 9, 7, 7, 10, 8, 7, 10, 7, 9, 8, 8, 8, 7, 8, 10, 9, 10, 7, 7, 8, 8, 10, 8, 7, 7, 7, 8, 10, 10, 8, 10, 9, 9, 9, 8, 10, 8, 7, 7, 9, 10, 9, 7, 10, 10, 9, 10, 8, 8, 8, 10, 8, 8, 10, 8, 7, 9, 8, 8, 7, 8, 8, 10, 9, 7, 9, 10, 7, 9, 8, 10, 7, 9, 9, 7, 8, 10, 7, 8, 7, 9, 9, 8, 7, 10, 9, 8, 7, 8, 8, 7, 8, 8, 7, 8, 7, 8, 8, 10, 7, 7, 8, 10, 7, 7, 10, 10, 10, 7, 7, 7, 7, 7, 10, 10, 10, 10, 10, 10, 10, 9, 10, 7, 9, 7, 9, 7, 10, 9, 9, 7, 9, 10, 8, 10, 10, 9, 8, 10, 7, 10, 9, 10, 10, 9, 8, 8, 10, 9, 9, 9, 10, 9, 9, 9, 10, 8, 10, 10, 7, 7, 7, 8, 10, 8, 8, 10, 9, 9, 9, 9, 10, 8, 7, 7, 7, 10, 9, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 10, 10, 10, 9, 9, 8, 8, 10, 10, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 10, 8, 7, 9, 9, 8, 9, 10, 9, 7, 9, 8, 8, 7, 8, 10, 9, 10, 9, 8, 8, 8, 7, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 10, 10, 8, 10, 10, 7, 7, 10, 7, 9, 9, 9, 7, 7, 7, 7, 7, 10, 7, 10, 10, 9, 10, 7, 8, 8, 8, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 10, 10, 10, 10, 7, 10, 7, 10, 10, 10, 9, 10, 7, 10, 9, 10, 7, 10, 8, 7, 7, 10, 9, 10, 9, 7, 7, 10, 7, 10, 7, 10, 9, 7, 7, 7, 7, 7, 10, 8, 10, 7, 9, 7, 10, 8, 10, 7, 9, 8, 7, 7, 9, 7, 9, 9, 7, 7, 9, 7, 10, 9, 9, 7, 7, 9, 7, 7, 8, 7, 7, 10, 9, 7, 8, 7, 10, 7, 10, 10, 8, 8, 7, 7, 7, 10, 9, 10, 9, 7, 9, 8, 7, 7, 10, 10, 10, 9, 8, 7, 8, 7, 10, 7, 10, 10, 8, 9, 7, 7, 10, 9, 9, 9, 8, 10, 7, 9, 8, 8, 7, 10, 9, 10, 7, 7, 10, 10, 7, 10, 8, 10, 7, 7, 10, 10, 10, 9, 10, 9, 8, 7, 10, 10, 9, 10, 7, 10, 10, 7, 10, 10, 10, 10, 7, 9, 10, 9, 8, 10, 8, 8, 7, 7, 7, 9, 10, 10, 9, 9, 10, 7, 7, 10, 9, 10, 7, 10, 9, 10, 10, 7, 10, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 8, 10, 7, 10, 9, 10, 8, 10, 10, 10, 9, 10, 8, 10, 10, 10, 10, 10, 7, 8, 7, 10, 10, 10, 7, 10, 7, 10, 9, 10, 8, 10, 10, 10, 9, 10, 8, 10, 10, 9, 10, 7, 9, 10, 8, 7, 10, 7, 7, 10, 7, 10, 7, 9, 10, 7, 8, 10, 7, 7, 9, 10, 7, 10, 7, 8, 10, 10, 10, 10, 9, 10, 10, 7, 7, 9, 10, 7, 10, 7, 8, 7, 7, 7, 7, 8, 8, 7, 7, 9, 10, 7, 7, 7, 7, 7, 10, 10, 10, 10, 7, 9, 7, 7, 7, 9, 10, 7, 7, 10, 8, 10, 7, 7, 10, 10, 10, 10, 9, 9, 8, 10, 9, 9, 9, 8, 7, 8, 9, 10, 7, 7, 8, 10, 8, 7, 8, 7, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 9, 7, 8, 10, 10, 10, 9, 10, 9, 7, 9, 9, 8, 8, 7, 7, 9, 9, 8, 7, 9, 9, 10, 9, 9, 7, 10, 8, 7, 10, 9, 7, 9, 9, 10, 8, 7, 9, 9, 7, 9, 7, 10, 8, 9, 7, 9, 7, 8, 10, 7, 9, 8, 8, 10, 9, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 9, 7, 7, 7, 8, 8, 8, 8, 9, 10, 8, 10, 8, 10, 7, 8, 10, 7, 7, 7, 7, 7, 10, 7, 8, 7, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 10, 9, 9, 9, 8, 9, 10, 9, 9, 10, 9, 9, 8, 7, 8, 9, 8, 9, 8, 8, 10, 9, 10, 7, 9, 10, 8, 8, 8, 7, 9, 8, 10, 7, 8, 10, 8, 9, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 7, 9, 7, 7, 10, 10, 9, 8, 10, 10, 9, 7, 7, 8, 8, 10, 9, 9, 9, 7, 9, 9, 10, 9, 9, 7, 9, 9, 10, 10, 9, 8, 7, 9, 10, 7, 7, 9, 8, 8, 7, 7, 9, 7, 10, 10, 9, 10, 9, 8, 8, 7, 8, 10, 9, 8, 7, 8, 10, 9, 8, 7, 9, 8, 8, 10, 9, 9, 9, 8, 7, 7, 8, 7, 9, 7, 9, 8, 7, 7, 9, 7, 8, 10, 8, 8, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 9, 10, 7, 7, 10, 8, 10, 7, 7, 10, 10, 10, 10, 7, 10, 8, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 9, 10, 10, 7, 7, 9, 10, 9, 10, 7, 7, 7, 7, 10, 10, 7, 10, 10, 10, 9, 9, 7, 7, 10, 9, 10, 8, 7, 9, 10, 9, 9, 7, 7, 7, 7, 7, 7, 10, 7, 10, 7, 10, 8, 10, 8, 10, 10, 8, 7, 9, 7, 8, 7, 7, 7, 8, 10, 7, 7, 7, 8, 9, 7, 10, 7, 10, 7, 10, 8, 7, 10, 9, 10, 7, 9, 7, 8, 10, 10, 10, 7, 8, 10, 10, 10, 10, 10, 10, 10, 10, 7, 9, 9, 10, 10, 10, 8, 10, 10, 10, 9, 9, 7, 7, 9, 10, 7, 10, 7, 10, 10, 10, 9, 9, 7, 10, 10, 7, 7, 10, 9, 9, 9, 10, 9, 10, 7, 10, 7, 8, 7, 10, 10, 10, 7, 10, 9, 7, 10, 9, 9, 7, 9, 7, 7, 8, 10, 9, 9, 10, 7, 10, 8, 7, 7, 7, 9, 7, 7, 10, 9, 9, 7, 10, 10, 10, 10, 10, 10, 8, 7, 9, 9, 9, 7, 10, 7, 10, 7, 7, 8, 8, 9, 7, 8, 10, 9, 7, 8, 8, 10, 9, 9, 7, 10, 7, 9, 10, 7, 9, 10, 10, 9, 10, 10, 8, 10, 10, 8, 7, 9, 9, 10, 7, 7, 7, 9, 8, 7, 10, 10, 10, 7, 7, 7, 9, 10, 8, 10, 10, 7, 9, 7, 10, 10, 10, 7, 7, 10, 9, 8, 10, 7, 7, 10, 7, 7, 7, 8, 10, 9, 10, 7, 10, 10, 10, 10, 7, 7, 10, 7, 10, 7, 7, 7, 9, 7, 7, 10, 8, 7, 7, 7, 9, 10, 7, 9, 8, 10, 8, 8, 10, 9, 7, 10, 7, 10, 7, 8, 10, 9, 7, 10, 9, 10, 9, 7, 9, 9, 9, 9, 9, 7, 7, 7, 7, 9, 9, 10, 8, 8, 8, 10, 8, 8, 10, 9, 10, 10, 7, 9, 10, 9, 8, 10, 8, 10, 8, 9, 10, 7, 10, 8, 10, 7, 9, 10, 10, 10, 7, 10, 10, 8, 7, 7, 9, 9, 7, 7, 9, 10, 9, 7, 7, 7, 7, 10, 7, 9, 7, 7, 9, 10, 9, 7, 10, 8, 10, 10, 7, 7, 10, 9, 8, 10, 9, 10, 7, 7, 10, 9, 7, 7, 7, 10, 8, 10, 10, 7, 10, 10, 10, 10, 9, 10, 7, 8, 7, 9, 9, 9, 9, 10, 8, 10, 10, 10, 7, 7, 10, 9, 7, 7, 8, 10, 10, 10, 9, 10, 9, 9, 10, 10, 10, 10, 7, 7, 7, 10, 9, 10, 7, 7, 8, 7, 10, 7, 7, 10, 9, 7, 7, 7, 7, 10, 7, 8, 7, 7, 9, 10, 10, 10, 7, 7, 9, 7, 7, 10, 7, 7, 7, 7, 9, 7, 9, 10, 10, 9, 7, 7, 9, 8, 10, 7, 7, 7, 9, 9, 8, 7, 7, 7, 10, 7, 10, 7, 7, 8, 10, 10, 10, 10, 9, 7, 10, 10, 10, 10, 10, 7, 8, 7, 7, 9, 10, 8, 10, 10, 7, 10, 8, 7, 10, 10, 9, 10, 7, 8, 7, 9, 9, 7, 7, 9, 10, 8, 10, 9, 9, 9, 10, 8, 7, 9, 8, 8, 7, 10, 10, 7, 7, 9, 7, 8, 8, 8, 7, 9, 9, 7, 7, 9, 8, 7, 9, 7, 10, 8, 10, 7, 9, 7, 10, 7, 10, 10, 10, 7, 7, 10, 10, 8, 10, 9, 10, 9, 7, 8, 7, 9, 10, 10, 8, 7, 10, 7, 9, 8, 8, 10, 8, 7, 10, 7, 10, 10, 9, 10, 7, 9, 10, 10, 7, 8, 7, 10, 7, 10, 10, 9, 9, 7, 7, 7, 7, 8, 7, 10, 8, 10, 9, 10, 8, 7, 7, 7, 10, 10, 8, 7, 9, 10, 10, 10, 7, 10, 7, 10, 7, 10, 7, 9, 8, 10, 10, 9, 9, 7, 7, 10, 10, 9, 10, 10, 8, 7, 7, 9, 9, 9, 10, 10, 9, 7, 9, 10, 7, 7, 10, 10, 7, 7, 10, 10, 7, 8, 7, 10, 8, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 7, 8, 7, 10, 7, 10, 9, 9, 7, 9, 7, 7, 10, 10, 8, 10, 7, 9, 7, 9, 9, 7, 10, 7, 10, 7, 7, 7, 7, 7, 10, 9, 7, 7, 9, 7, 10, 7, 8, 7, 7, 9, 10, 8, 7, 8, 10, 7, 10, 8, 8, 10, 10, 7, 9, 7, 9, 8, 8, 10, 8, 8, 10, 7, 10, 10, 8, 10, 10, 8, 10, 7, 9, 7, 9, 7, 10, 7, 10, 10, 10, 9, 7, 7, 9, 8, 8, 10, 7, 9, 9, 7, 7, 9, 8, 7, 7, 9, 9, 8, 10, 8, 8, 7, 9, 7, 7, 7, 9, 9, 7, 7, 9, 9, 10, 9, 7, 7, 9, 7, 10, 9, 9, 8, 7, 7, 7, 9, 9, 7, 10, 7, 9, 10, 10, 9, 10, 9, 10, 10, 10, 10, 7, 7, 7, 7, 9, 8, 7, 9, 9, 10, 7, 9, 9, 7, 10, 7, 8, 10, 10, 8, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 8, 9, 9, 7, 9, 10, 8, 10, 7, 9, 8, 10, 8, 10, 9, 10, 8, 7, 8, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 7, 9, 10, 9, 9, 10, 7, 8, 9, 7, 10, 8, 10, 8, 9, 10, 8, 10, 8, 7, 8, 10, 9, 10, 7, 7, 10, 8, 10, 8, 8, 7, 8, 8, 10, 8, 8, 8, 7, 9, 9, 10, 10, 8, 7, 7, 9, 8, 7, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 10, 9, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 7, 10, 9, 10, 9, 8, 8, 7, 8, 8, 7, 8, 9, 8, 8, 8, 7, 9, 8, 10, 7, 7, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 7, 9, 7, 9, 7, 8, 9, 9, 9, 9, 9, 10, 10, 10, 8, 7, 8, 8, 7, 10, 9, 10, 10, 7, 9, 8, 8, 7, 9, 9, 7, 10, 9, 7, 10, 8, 10, 8, 9, 7, 10, 8, 10, 8, 8, 10, 9, 7, 8, 8, 10, 8, 9, 10, 10, 7, 10, 8, 8, 7, 8, 8, 8, 7, 8, 8, 10, 10, 9, 7, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 10, 9, 10, 9, 7, 9, 8, 8, 7, 8, 8, 9, 8, 9, 8, 8, 8, 9, 9, 8, 8, 8, 9, 9, 7, 10, 7, 8, 10, 10, 8, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 8, 9, 9, 7, 9, 10, 8, 10, 8, 9, 8, 10, 8, 10, 9, 10, 10, 9, 8, 8, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 9, 7, 9, 7, 10, 8, 10, 10, 9, 9, 8, 10, 8, 7, 8, 10, 9, 8, 7, 7, 9, 8, 10, 8, 8, 9, 8, 8, 10, 8, 8, 10, 9, 9, 9, 10, 10, 8, 7, 8, 9, 8, 8, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 8, 7, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 8, 10, 7, 8, 7, 9, 9, 8, 9, 8, 8, 8, 9, 8, 8, 7, 8, 8, 7, 7, 9, 8, 8, 8, 7, 9, 8, 10, 7, 7, 10, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 7, 9, 7, 9, 7, 8, 9, 9, 9, 9, 10, 10, 10, 8, 7, 8, 10, 9, 10, 9, 10, 10, 7, 9, 8, 8, 7, 9, 9, 7, 10, 9, 9, 10, 10, 10, 10, 9, 7, 10, 8, 10, 8, 8, 10, 9, 9, 8, 8, 10, 10, 9, 10, 9, 7, 10, 8, 8, 9, 8, 8, 8, 7, 8, 8, 10, 8, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 9, 10, 9, 7, 9, 8, 8, 7, 8, 8, 10, 8, 9, 8, 8, 10, 9, 9, 8, 8, 8, 9, 9, 7, 10, 7, 8, 10, 10, 8, 10, 9, 10, 10, 10, 10, 10, 7, 7, 9, 7, 10, 7, 9, 8, 7, 9, 7, 7, 7, 7, 9, 9, 7, 7, 7, 8, 7, 7, 7, 9, 7, 10, 7, 7, 9, 10, 9, 7, 7, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 7, 10, 7, 7, 10, 10, 7, 7, 9, 7, 9, 7, 7, 7, 7, 10, 10, 10, 9, 7, 7, 9, 8, 9, 9, 7, 9, 7, 10, 7, 9, 9, 9, 10, 9, 7, 9, 10, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 10, 7, 10, 9, 10, 7, 10, 9, 7, 10, 9, 10, 7, 10, 8, 10, 10, 7, 10, 10, 8, 8, 7, 9, 8, 7, 7, 7, 9, 7, 9, 7, 7, 9, 7, 7, 9, 8, 8, 8, 8, 8, 10, 10, 8, 8, 8, 10, 10, 7, 7, 7, 7, 7, 9, 9, 9, 9, 8, 10, 9, 7, 7, 9, 7, 10, 7, 10, 9, 7, 9, 7, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 7, 10, 9, 10, 10, 10, 9, 9, 10, 7, 10, 7, 7, 8, 10, 9, 9, 10, 7, 10, 9, 7, 7, 9, 9, 7, 10, 10, 8, 7, 7, 10, 7, 9, 9, 9, 7, 9, 10, 7, 7, 7, 10, 7, 9, 7, 10, 9, 7, 7, 10, 9, 7, 8, 7, 7, 8, 7, 10, 10, 9, 8, 8, 7, 7, 9, 7, 7, 7, 10, 7, 8, 10, 9, 9, 9, 7, 8, 10, 9, 7, 7, 7, 8, 7, 10, 10, 7, 7, 10, 10, 10, 9, 10, 7, 9, 10, 7, 7, 9, 8, 10, 8, 7, 7, 10, 10, 9, 9, 8, 7, 10, 7, 7, 10, 10, 10, 7, 8, 9, 7, 8, 10, 10, 10, 8, 10, 8, 10, 7, 7, 7, 10, 7, 7, 7, 9, 8, 10, 10, 8, 8, 7, 10, 8, 7, 10, 7, 9, 7, 7, 9, 7, 9, 7, 10, 9, 10, 10, 10, 10, 9, 7, 7, 7, 9, 10, 10, 9, 10, 10, 9, 8, 7, 9, 10, 7, 9, 9, 10, 7, 7, 8, 8, 8, 7, 9, 7, 10, 10, 10, 9, 9, 7, 7, 7, 7, 10, 9, 7, 10, 7, 7, 9, 10, 7, 9, 10, 8, 10, 10, 10, 9, 9, 8, 7, 9, 7, 7, 7, 9, 8, 7, 7, 7, 7, 8, 10, 9, 10, 9, 10, 7, 9, 10, 7, 10, 7, 9, 10, 7, 7, 7, 7, 8, 8, 10, 8, 7, 10, 10, 7, 10, 10, 7, 7, 7, 9, 10, 7, 9, 8, 8, 7, 8, 7, 7, 10, 10, 9, 9, 9, 7, 7, 10, 10, 10, 9, 7, 9, 7, 9, 9, 9, 7, 10, 10, 7, 9, 9, 10, 7, 7, 9, 10, 10, 8, 10, 10, 10, 8, 10, 7, 8, 7, 9, 10, 8, 10, 10, 7, 7, 7, 9, 7, 7, 7, 7, 7, 9, 8, 7, 8, 7, 7, 10, 10, 7, 7, 7, 7, 8, 10, 9, 10, 9, 8, 10, 10, 10, 7, 9, 7, 7, 7, 7, 8, 10, 10, 7, 7, 10, 9, 10, 9, 7, 8, 7, 9, 10, 7, 7, 10, 9, 7, 9, 7, 9, 9, 10, 9, 9, 7, 10, 10, 10, 9, 7, 7, 7, 9, 9, 9, 9, 7, 9, 7, 7, 7, 8, 10, 7, 10, 7, 9, 9, 7, 7, 7, 7, 7, 7, 9, 8, 8, 7, 7, 7, 10, 10, 7, 7, 9, 9, 9, 9, 8, 10, 9, 10, 10, 9, 7, 7, 7, 9, 9, 8, 10, 9, 10, 7, 7, 7, 7, 10, 9, 7, 8, 10, 8, 10, 9, 7, 10, 10, 7, 8, 10, 7, 7, 10, 10, 10, 10, 7, 8, 10, 8, 7, 7, 7, 7, 9, 10, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 8, 7, 8, 10, 7, 10, 10, 10, 7, 7, 7, 9, 10, 9, 7, 8, 8, 7, 9, 10, 8, 8, 10, 8, 10, 10, 7, 8, 7, 10, 10, 10, 9, 10, 8, 8, 7, 7, 7, 8, 7, 7, 9, 10, 9, 7, 10, 9, 10, 10, 10, 7, 7, 10, 10, 7, 8, 10, 7, 7, 7, 10, 10, 9, 9, 7, 10, 7, 7, 10, 9, 7, 10, 7, 9, 7, 8, 7, 10, 10, 10, 10, 7, 10, 10, 10, 9, 10, 7, 7, 10, 7, 10, 9, 9, 10, 8, 7, 7, 7, 9, 10, 8, 8, 7, 10, 10, 7, 10, 7, 7, 8, 7, 8, 7, 8, 10, 9, 8, 7, 9, 7, 10, 10, 9, 10, 8, 8, 10, 9, 9, 10, 7, 9, 10, 10, 7, 8, 10, 10, 10, 7, 7, 10, 10, 7, 7, 7, 10, 9, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 10, 7, 7, 9, 7, 9, 9, 9, 8, 10, 7, 10, 10, 9, 7, 7, 8, 7, 7, 7, 7, 8, 8, 10, 9, 10, 7, 7, 9, 10, 8, 10, 7, 10, 10, 8, 9, 10, 10, 9, 10, 10, 7, 7, 9, 7, 9, 7, 10, 9, 10, 10, 10, 7, 7, 7, 7, 10, 7, 7, 7, 10, 8, 8, 8, 10, 10, 7, 8, 10, 7, 10, 7, 7, 10, 9, 9, 9, 7, 10, 10, 9, 7, 7, 10, 10, 10, 7, 10, 9, 9, 9, 7, 8, 7, 7, 7, 7, 9, 8, 10, 10, 7, 7, 7, 9, 8, 10, 10, 10, 7, 10, 10, 7, 7, 7, 10, 7, 8, 10, 9, 7, 7, 7, 9, 10, 9, 10, 10, 10, 10, 9, 7, 7, 7, 10, 9, 8, 7, 9, 10, 10, 10, 10, 7, 8, 7, 10, 9, 10, 9, 10, 9, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 8, 10, 10, 8, 7, 9, 9, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 8, 8, 10, 10, 9, 10, 7, 7, 10, 7, 9, 8, 10, 9, 8, 10, 9, 10, 10, 7, 10, 10, 7, 7, 9, 10, 7, 10, 9, 8, 7, 9, 7, 10, 7, 7, 10, 7, 10, 10, 10, 10, 7, 7, 7, 7, 9, 9, 7, 10, 10, 10, 9, 8, 7, 7, 8, 8, 10, 8, 10, 10, 10, 7, 10, 8, 9, 7, 10, 7, 7, 10, 7, 10, 10, 7, 10, 8, 7, 7, 8, 7, 10, 10, 9, 7, 10, 8, 10, 8, 8, 10, 7, 10, 10, 10, 10, 10, 9, 9, 8, 10, 10, 8, 7, 7, 9, 7, 10, 10, 10, 10, 9, 10, 9, 8, 8, 7, 7, 8, 8, 7, 9, 9, 10, 7, 7, 7, 7, 10, 9, 10, 10, 8, 10, 10, 10, 10, 8, 10, 7, 10, 10, 10, 10, 10, 10, 10, 7, 7, 7, 10, 8, 8, 8, 8, 7, 9, 7, 7, 9, 10, 7, 10, 7, 10, 7, 9, 7, 7, 7, 7, 7, 10, 10, 7, 9, 7, 7, 10, 10, 8, 10, 10, 10, 9, 10, 7, 7, 9, 9, 10, 9, 7, 10, 8, 10, 9, 7, 10, 7, 7, 7, 10, 10, 10, 9, 7, 7, 9, 7, 7, 9, 8, 7, 8, 10, 9, 7, 7, 7, 10, 7, 10, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 7, 10, 10, 10, 7, 10, 10, 10, 7, 7, 8, 10, 7, 10, 9, 10, 10, 10, 10, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 8, 7, 7, 7, 10, 10, 7, 7, 7, 10, 10, 8, 7, 10, 9, 7, 7, 7, 9, 7, 7, 7, 9, 10, 8, 10, 10, 8, 10, 8, 8, 10, 9, 7, 7, 7, 7, 9, 10, 10, 10, 8, 10, 10, 7, 10, 7, 8, 7, 7, 7, 10, 10, 10, 9, 9, 10, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 10, 10, 10, 8, 8, 8, 7, 7, 10, 7, 7, 7, 10, 7, 9, 7, 10, 9, 10, 9, 7, 7, 10, 7, 10, 10, 10, 10, 8, 8, 8, 10, 9, 8, 7, 7, 7, 9, 10, 7, 7, 7, 7, 7, 7, 9, 10, 9, 7, 10, 7, 10, 7, 7, 7, 7, 7, 7, 9, 10, 10, 7, 7, 10, 7, 7, 9, 10, 8, 7, 10, 7, 10, 7, 7, 7, 9, 10, 8, 7, 10, 7, 7, 7, 7, 7, 10, 7, 7, 9, 10, 8, 7, 10, 9, 10, 7, 7, 10, 9, 7, 8, 7, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 7, 8, 9, 9, 7, 7, 10, 10, 10, 7, 8, 7, 10, 9, 7, 8, 10, 10, 8, 8, 10, 10, 9, 7, 7, 9, 9, 7, 9, 7, 10, 7, 9, 10, 10, 10, 7, 10, 7, 10, 7, 9, 7, 9, 7, 7, 10, 9, 10, 9, 9, 8, 10, 8, 7, 10, 9, 7, 9, 9, 7, 7, 7, 8, 10, 8, 7, 7, 10, 10, 7, 9, 9, 10, 9, 9, 10, 7, 9, 8, 7, 9, 9, 7, 7, 9, 7, 9, 8, 7, 7, 10, 8, 10, 9, 9, 7, 7, 7, 9, 9, 7, 7, 10, 10, 9, 8, 7, 9, 9, 7, 9, 9, 7, 9, 10, 10, 7, 10, 7, 9, 9, 10, 9, 7, 9, 9, 7, 7, 9, 7, 7, 9, 7, 8, 8, 8, 7, 9, 9, 7, 8, 10, 8, 10, 9, 10, 7, 9, 10, 10, 10, 10, 7, 10, 9, 9, 7, 10, 9, 8, 10, 7, 7, 9, 9, 9, 7, 9, 9, 7, 9, 7, 10, 7, 9, 8, 7, 10, 8, 7, 9, 9, 7, 7, 7, 7, 7, 10, 10, 9, 7, 9, 10, 9, 10, 7, 7, 10, 9, 7, 7, 10, 7, 10, 8, 8, 7, 10, 10, 7, 8, 7, 9, 7, 8, 7, 9, 8, 7, 10, 10, 9, 7, 7, 7, 7, 10, 10, 8, 10, 7, 9, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 7, 7, 10, 10, 10, 7, 10, 7, 7, 10, 8, 10, 8, 7, 9, 7, 8, 10, 7, 9, 10, 9, 9, 9, 7, 10, 10, 7, 10, 7, 10, 10, 10, 10, 7, 7, 10, 10, 9, 10, 10, 10, 8, 8, 7, 9, 10, 7, 8, 7, 8, 8, 7, 8, 10, 8, 10, 10, 9, 9, 10, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 10, 10, 10, 7, 8, 10, 8, 8, 10, 8, 10, 8, 8, 10, 9, 7, 10, 10, 10, 10, 8, 10, 8, 7, 7, 8, 8, 10, 8, 10, 7, 7, 7, 8, 10, 10, 10, 9, 9, 7, 7, 10, 8, 10, 8, 8, 8, 7, 7, 9, 7, 7, 10, 8, 10, 10, 10, 10, 7, 10, 10, 10, 7, 10, 10, 10, 7, 10, 10, 7, 8, 10, 10, 10, 7, 10, 10, 7, 10, 10, 10, 7, 10, 7, 10, 9, 10, 10, 8, 7, 10, 7, 10, 10, 10, 10, 10, 9, 7, 9, 7, 8, 7, 9, 9, 7, 10, 8, 10, 8, 7, 9, 10, 10, 10, 9, 10, 8, 7, 8, 8, 8, 7, 9, 9, 10, 10, 9, 8, 7, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 9, 10, 9, 7, 10, 8, 9, 10, 7, 9, 8, 8, 8, 7, 8, 10, 9, 8, 7, 9, 8, 10, 10, 8, 7, 7, 7, 8, 10, 8, 8, 10, 9, 9, 9, 8, 10, 8, 7, 7, 9, 8, 7, 7, 10, 8, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 7, 8, 7, 9, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 8, 10, 7, 8, 7, 8, 7, 10, 9, 10, 9, 8, 10, 10, 10, 9, 10, 10, 9, 8, 8, 8, 7, 9, 9, 8, 10, 9, 7, 10, 8, 10, 10, 9, 7, 7, 8, 10, 8, 8, 10, 10, 7, 8, 8, 10, 8, 9, 7, 9, 8, 7, 7, 10, 8, 10, 10, 8, 8, 8, 7, 8, 8, 10, 8, 10, 9, 8, 8, 10, 8, 10, 8, 7, 9, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 10, 7, 9, 9, 8, 7, 10, 7, 10, 9, 8, 8, 7, 8, 10, 10, 8, 9, 8, 10, 9, 7, 7, 8, 10, 8, 7, 10, 10, 10, 9, 10, 9, 10, 10, 10, 10, 10, 9, 7, 9, 7, 9, 10, 9, 7, 7, 7, 10, 7, 9, 9, 7, 10, 10, 7, 10, 9, 9, 7, 10, 9, 9, 7, 7, 10, 10, 10, 9, 9, 8, 7, 8, 10, 7, 10, 7, 7, 7, 7, 7, 10, 7, 10, 7, 7, 10, 7, 10, 8, 8, 10, 10, 10, 10, 10, 7, 10, 7, 10, 10, 10, 10, 10, 10, 7, 10, 7, 10, 10, 9, 7, 7, 8, 7, 10, 10, 9, 7, 8, 9, 9, 10, 10, 10, 10, 7, 7, 10, 10, 8, 10, 10, 10, 10, 8, 10, 7, 9, 8, 7, 7, 10, 8, 10, 7, 9, 7, 8, 7, 7, 10, 7, 9, 9, 10, 8, 10, 9, 9, 7, 7, 8, 7, 10, 7, 8, 7, 10, 9, 10, 10, 9, 7, 9, 7, 10, 8, 10, 9, 10, 7, 9, 10, 10, 8, 10, 10, 7, 10, 9, 9, 9, 7, 8, 7, 7, 7, 7, 7, 10, 10, 10, 7, 7, 7, 7, 9, 8, 7, 10, 10, 10, 8, 10, 9, 10, 10, 10, 10, 8, 7, 9, 8, 10, 10, 9, 9, 10, 7, 10, 9, 9, 7, 7, 7, 10, 9, 7, 7, 10, 8, 7, 8, 10, 10, 7, 9, 8, 10, 7, 9, 10, 9, 7, 9, 7, 8, 10, 7, 9, 7, 10, 9, 7, 7, 8, 7, 7, 10, 9, 7, 7, 8, 7, 8, 10, 10, 9, 10, 9, 10, 8, 10, 10, 7, 7, 10, 10, 10, 10, 9, 10, 7, 7, 10, 8, 10, 10, 7, 7, 10, 8, 7, 10, 7, 10, 10, 8, 8, 8, 10, 9, 10, 10, 8, 7, 10, 7, 7, 9, 7, 10, 9, 7, 9, 9, 7, 7, 7, 10, 8, 9, 10, 7, 10, 9, 8, 10, 10, 10, 7, 7, 10, 9, 10, 7, 8, 10, 10, 10, 9, 7, 9, 7, 7, 10, 9, 10, 10, 10, 9, 8, 7, 8, 8, 10, 10, 9, 8, 7, 9, 7, 10, 10, 10, 10, 10, 7, 9, 7, 10, 10, 10, 10, 7, 8, 10, 10, 10, 10, 7, 7, 7, 7, 9, 8, 7, 10, 7, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 7, 7, 7, 10, 10, 9, 7, 7, 8, 8, 10, 10, 7, 10, 9, 10, 7, 7, 9, 9, 8, 8, 9, 7, 9, 9, 10, 7, 7, 10, 10, 7, 7, 7, 10, 8, 10, 10, 10, 9, 7, 7, 10, 10, 7, 10, 9, 8, 7, 9, 7, 9, 7, 7, 10, 7, 7, 7, 8, 7, 7, 7, 10, 7, 7, 7, 10, 9, 7, 7, 8, 7, 7, 9, 8, 7, 7, 7, 8, 8, 7, 10, 8, 7, 9, 7, 9, 9, 9, 7, 8, 7, 7, 7, 7, 9, 7, 9, 10, 7, 9, 10, 7, 9, 10, 10, 10, 9, 9, 8, 7, 7, 10, 9, 7, 9, 7, 10, 9, 10, 7, 10, 9, 8, 8, 10, 9, 8, 8, 10, 9, 10, 7, 7, 7, 10, 10, 8, 7, 8, 9, 10, 10, 10, 8, 10, 9, 7, 9, 9, 7, 7, 9, 10, 7, 10, 7, 10, 7, 7, 9, 9, 10, 10, 10, 9, 10, 10, 10, 7, 9, 10, 10, 9, 7, 7, 9, 7, 10, 7, 9, 7, 7, 7, 9, 7, 9, 7, 7, 7, 7, 9, 7, 8, 7, 7, 10, 9, 7, 10, 7, 8, 10, 9, 8, 8, 7, 10, 10, 7, 10, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 10, 7, 10, 10, 10, 10, 9, 7, 9, 7, 7, 10, 7, 10, 9, 8, 10, 8, 7, 9, 9, 7, 8, 7, 8, 10, 7, 7, 9, 8, 8, 7, 9, 7, 10, 9, 9, 7, 9, 7, 7, 9, 7, 10, 9, 9, 7, 10, 10, 8, 10, 10, 10, 10, 10, 8, 8, 10, 7, 7, 7, 7, 10, 7, 9, 9, 10, 7, 7, 8, 10, 7, 7, 9, 8, 8, 8, 10, 8, 10, 7, 10, 8, 10, 10, 10, 7, 10, 9, 8, 7, 10, 8, 10, 10, 7, 8, 8, 10, 10, 10, 9, 10, 9, 9, 7, 9, 7, 7, 10, 7, 8, 7, 10, 10, 10, 10, 8, 7, 9, 10, 7, 7, 10, 7, 7, 7, 10, 8, 10, 7, 9, 10, 9, 7, 8, 7, 10, 10, 10, 10, 8, 10, 8, 7, 7, 10, 8, 10, 10, 8, 7, 10, 10, 8, 7, 7, 10, 10, 8, 10, 7, 10, 8, 8, 10, 7, 7, 7, 10, 9, 10, 7, 7, 10, 7, 7, 10, 7, 10, 7, 10, 10, 10, 9, 8, 8, 10, 10, 10, 7, 9, 8, 8, 10, 8, 7, 10, 10, 8, 8, 7, 10, 8, 10, 8, 8, 10, 9, 7, 7, 7, 8, 10, 9, 9, 9, 10, 10, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 9, 9, 8, 10, 10, 8, 10, 8, 10, 9, 7, 7, 7, 10, 9, 9, 10, 7, 8, 9, 8, 8, 10, 7, 10, 7, 8, 10, 10, 9, 8, 7, 10, 10, 10, 8, 10, 9, 7, 10, 10, 9, 10, 8, 8, 8, 10, 10, 10, 9, 9, 10, 7, 9, 10, 10, 10, 9, 10, 8, 7, 7, 10, 8, 8, 10, 10, 9, 8, 10, 8, 7, 8, 10, 8, 10, 8, 10, 10, 9, 9, 8, 10, 7, 10, 8, 10, 8, 7, 10, 8, 8, 7, 8, 10, 9, 8, 8, 7, 10, 9, 10, 8, 7, 10, 7, 10, 8, 10, 9, 10, 8, 10, 10, 8, 10, 8, 10, 7, 9, 9, 7, 7, 9, 7, 10, 7, 7, 9, 10, 10, 8, 7, 9, 7, 10, 10, 10, 8, 7, 8, 8, 10, 7, 7, 7, 8, 10, 8, 10, 7, 9, 7, 10, 8, 8, 8, 10, 10, 10, 7, 10, 10, 8, 10, 7, 10, 10, 9, 7, 8, 10, 9, 10, 10, 7, 9, 7, 8, 7, 10, 8, 10, 8, 10, 7, 8, 10, 9, 7, 10, 7, 8, 7, 7, 10, 7, 10, 8, 7, 8, 10, 9, 10, 10, 7, 8, 8, 8, 7, 8, 10, 10, 8, 8, 8, 10, 7, 7, 7, 10, 10, 7, 10, 10, 10, 10, 10, 8, 10, 10, 9, 10, 9, 8, 10, 10, 9, 10, 10, 7, 10, 10, 10, 10, 9, 8, 10, 10, 7, 10, 10, 9, 9, 10, 9, 10, 10, 7, 8, 8, 7, 10, 10, 8, 10, 10, 8, 8, 7, 10, 10, 10, 7, 9, 9, 9, 10, 10, 9, 8, 10, 10, 7, 10, 10, 8, 10, 10, 10, 10, 7, 10, 8, 7, 7, 7, 7, 9, 10, 10, 10, 9, 10, 9, 10, 8, 10, 7, 8, 10, 7, 10, 9, 10, 9, 8, 8, 10, 9, 8, 10, 9, 9, 8, 8, 7, 8, 10, 9, 10, 9, 8, 10, 7, 9, 8, 10, 8, 8, 10, 9, 9, 7, 7, 7, 7, 9, 9, 7, 9, 10, 9, 8, 7, 10, 9, 7, 7, 10, 9, 7, 9, 7, 8, 7, 8, 7, 7, 10, 10, 10, 8, 10, 10, 9, 10, 8, 10, 10, 8, 7, 7, 9, 9, 10, 8, 7, 10, 8, 7, 7, 9, 9, 10, 10, 10, 9, 9, 10, 9, 7, 10, 8, 7, 10, 8, 7, 7, 9, 10, 9, 7, 10, 10, 9, 9, 7, 7, 7, 9, 7, 8, 7, 9, 7, 10, 9, 10, 9, 7, 7, 10, 7, 7, 10, 10, 7, 10, 7, 7, 8, 7, 9, 10, 7, 10, 9, 7, 10, 8, 7, 7, 10, 9, 8, 7, 7, 8, 7, 7, 10, 9, 9, 7, 7, 9, 10, 7, 10, 7, 8, 7, 10, 7, 8, 7, 7, 10, 7, 7, 10, 10, 8, 10, 9, 10, 10, 7, 9, 7, 9, 9, 9, 9, 9, 8, 10, 8, 7, 7, 7, 10, 8, 8, 10, 10, 9, 7, 7, 9, 10, 7, 7, 10, 8, 7, 10, 10, 7, 8, 7, 10, 10, 8, 10, 9, 7, 10, 7, 9, 10, 10, 10, 10, 7, 10, 8, 8, 10, 7, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 10, 9, 7, 9, 7, 8, 7, 7, 8, 10, 10, 10, 8, 10, 8, 10, 8, 10, 8, 7, 9, 7, 7, 8, 8, 7, 10, 8, 10, 10, 7, 7, 7, 7, 10, 10, 9, 8, 10, 9, 9, 8, 10, 10, 9, 7, 9, 7, 7, 7, 8, 7, 9, 9, 10, 9, 10, 10, 10, 8, 8, 10, 9, 7, 8, 7, 10, 9, 7, 10, 10, 7, 9, 9, 10, 7, 10, 10, 8, 7, 7, 9, 10, 7, 9, 7, 10, 8, 8, 10, 9, 8, 10, 7, 7, 7, 9, 7, 10, 7, 10, 10, 7, 10, 7, 9, 7, 7, 7, 9, 9, 7, 10, 10, 8, 8, 10, 9, 8, 7, 9, 10, 9, 9, 7, 10, 7, 8, 7, 7, 7, 9, 9, 10, 7, 9, 7, 8, 8, 7, 8, 7, 9, 7, 7, 10, 8, 10, 10, 10, 8, 7, 9, 10, 10, 9, 7, 9, 8, 7, 10, 9, 7, 7, 8, 10, 10, 10, 8, 7, 9, 10, 8, 8, 7, 10, 9, 8, 8, 7, 10, 10, 8, 8, 7, 7, 7, 9, 10, 10, 8, 8, 7, 9, 7, 7, 7, 9, 8, 10, 7, 7, 9, 7, 7, 10, 7, 9, 10, 10, 8, 7, 10, 7, 7, 10, 9, 7, 8, 10, 7, 7, 9, 9, 7, 7, 9, 7, 8, 7, 9, 10, 9, 10, 8, 10, 7, 9, 10, 7, 7, 10, 10, 7, 7, 10, 7, 7, 10, 10, 7, 8, 8, 7, 7, 10, 7, 10, 10, 7, 7, 10, 9, 10, 7, 9, 8, 7, 7, 10, 10, 10, 8, 10, 10, 8, 7, 7, 9, 7, 10, 7, 8, 7, 10, 10, 7, 8, 7, 9, 9, 7, 10, 9, 10, 10, 10, 10, 7, 10, 9, 10, 10, 7, 10, 8, 8, 10, 7, 7, 10, 7, 7, 8, 8, 10, 10, 7, 10, 7, 10, 9, 10, 7, 10, 7, 7, 8, 10, 9, 8, 7, 10, 10, 8, 9, 7, 9, 10, 8, 10, 10, 10, 10, 9, 7, 7, 9, 7, 9, 10, 10, 7, 7, 8, 8, 7, 10, 10, 10, 9, 10, 7, 10, 8, 7, 7, 9, 10, 8, 7, 7, 7, 10, 8, 10, 7, 9, 10, 10, 7, 10, 10, 7, 7, 10, 8, 7, 10, 7, 7, 8, 7, 9, 7, 10, 7, 7, 10, 10, 7, 8, 10, 9, 7, 9, 8, 10, 8, 8, 8, 10, 7, 10, 7, 10, 7, 10, 9, 8, 10, 7, 9, 9, 8, 7, 8, 10, 9, 10, 9, 9, 8, 10, 7, 8, 8, 7, 8, 7, 9, 10, 9, 7, 7, 10, 9, 7, 7, 9, 7, 9, 8, 7, 10, 7, 10, 9, 9, 10, 10, 9, 7, 9, 8, 7, 9, 9, 9, 9, 7, 9, 10, 9, 9, 7, 8, 10, 10, 9, 10, 7, 7, 7, 10, 7, 7, 10, 9, 8, 7, 10, 8, 7, 10, 10, 10, 7, 7, 10, 9, 7, 8, 7, 9, 10, 9, 10, 9, 9, 10, 7, 7, 9, 10, 9, 10, 10, 7, 10, 9, 8, 10, 9, 9, 9, 9, 10, 7, 9, 10, 10, 8, 10, 9, 8, 8, 10, 7, 9, 9, 9, 9, 7, 9, 10, 9, 7, 9, 10, 8, 10, 10, 9, 9, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 8, 10, 9, 7, 7, 10, 10, 7, 10, 10, 7, 10, 9, 7, 7, 10, 7, 7, 9, 8, 7, 10, 10, 10, 10, 8, 10, 7, 7, 9, 7, 7, 9, 8, 7, 7, 9, 9, 7, 7, 7, 8, 10, 9, 10, 9, 9, 7, 8, 7, 9, 7, 9, 9, 9, 7, 7, 10, 7, 9, 10, 7, 10, 8, 10, 8, 10, 7, 7, 7, 9, 7, 8, 7, 8, 10, 9, 10, 8, 7, 7, 9, 7, 7, 7, 9, 10, 9, 10, 7, 10, 9, 10, 10, 9, 10, 9, 10, 10, 8, 7, 8, 10, 9, 7, 10, 10, 9, 9, 7, 9, 10, 9, 9, 8, 10, 9, 7, 7, 10, 10, 7, 10, 7, 9, 7, 10, 10, 9, 9, 9, 10, 9, 9, 8, 7, 10, 9, 7, 7, 9, 10, 9, 9, 9, 7, 7, 9, 9, 7, 10, 9, 7, 9, 10, 10, 10, 9, 9, 7, 9, 9, 9, 7, 8, 8, 7, 10, 7, 10, 10, 7, 10, 9, 7, 7, 9, 9, 9, 8, 8, 10, 10, 10, 10, 7, 7, 10, 7, 7, 10, 9, 7, 10, 10, 7, 9, 10, 7, 7, 10, 10, 7, 10, 10, 10, 10, 10, 8, 9, 10, 10, 10, 7, 10, 7, 9, 9, 8, 7, 7, 10, 10, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 8, 7, 9, 9, 7, 9, 8, 7, 10, 9, 9, 8, 10, 10, 10, 9, 9, 7, 8, 10, 7, 10, 10, 7, 9, 10, 7, 9, 7, 7, 7, 8, 10, 10, 10, 7, 7, 7, 10, 8, 8, 7, 7, 7, 7, 8, 10, 7, 9, 8, 10, 10, 7, 10, 9, 10, 8, 10, 8, 10, 7, 10, 7, 7, 10, 7, 9, 8, 7, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 9, 10, 7, 8, 7, 8, 7, 10, 10, 7, 7, 7, 7, 9, 10, 10, 7, 7, 9, 7, 10, 10, 9, 8, 7, 8, 10, 10, 10, 9, 9, 9, 7, 9, 9, 8, 9, 9, 7, 9, 9, 10, 9, 9, 9, 8, 7, 9, 7, 10, 8, 7, 8, 10, 10, 9, 7, 9, 9, 10, 10, 7, 9, 9, 7, 9, 10, 10, 10, 9, 7, 9, 7, 8, 8, 7, 9, 8, 8, 10, 9, 9, 8, 8, 7, 7, 8, 7, 10, 9, 9, 10, 9, 7, 7, 7, 8, 8, 8, 8, 7, 10, 8, 10, 8, 10, 7, 8, 10, 7, 7, 7, 7, 7, 10, 10, 8, 7, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 9, 9, 9, 9, 10, 9, 10, 9, 9, 10, 9, 9, 10, 9, 8, 7, 8, 7, 8, 10, 9, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 10, 7, 9, 10, 8, 9, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 7, 9, 7, 7, 10, 10, 9, 8, 10, 10, 9, 7, 7, 8, 8, 10, 9, 9, 9, 7, 9, 9, 8, 9, 9, 7, 9, 9, 10, 10, 9, 8, 7, 9, 10, 9, 7, 9, 8, 8, 9, 7, 9, 7, 10, 8, 7, 8, 7, 8, 8, 7, 8, 10, 9, 8, 7, 8, 10, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 8, 9, 7, 8, 7, 9, 7, 9, 10, 9, 7, 9, 7, 8, 10, 8, 8, 7, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 10, 7, 7, 7, 10, 7, 7, 7, 7, 9, 10, 10, 7, 7, 9, 7, 10, 10, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 7, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 10, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 7, 8, 7, 9, 8, 7, 8, 7, 10, 10, 9, 9, 9, 7, 9, 10, 8, 10, 9, 7, 9, 9, 8, 7, 9, 7, 10, 8, 7, 8, 10, 10, 9, 7, 9, 8, 8, 8, 7, 9, 9, 7, 9, 10, 10, 8, 7, 7, 9, 7, 10, 8, 7, 9, 8, 10, 10, 9, 9, 9, 8, 7, 7, 8, 7, 7, 7, 9, 10, 9, 7, 9, 7, 8, 8, 10, 8, 7, 9, 8, 10, 8, 10, 7, 8, 7, 7, 7, 7, 7, 7, 10, 7, 9, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 8, 7, 9, 9, 10, 9, 10, 9, 9, 10, 9, 9, 10, 10, 10, 9, 8, 7, 8, 8, 10, 9, 10, 9, 9, 10, 8, 10, 8, 7, 9, 8, 10, 7, 8, 10, 10, 7, 9, 9, 7, 9, 7, 8, 10, 9, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 9, 8, 10, 10, 9, 7, 7, 8, 8, 8, 10, 9, 9, 7, 9, 9, 10, 8, 9, 7, 9, 9, 8, 10, 9, 8, 7, 9, 10, 9, 7, 9, 8, 8, 7, 10, 9, 9, 10, 9, 7, 8, 7, 9, 8, 7, 8, 10, 9, 8, 7, 8, 10, 8, 8, 7, 9, 7, 9, 8, 8, 10, 9, 7, 9, 9, 9, 7, 8, 10, 9, 7, 9, 10, 9, 7, 9, 7, 8, 8, 8, 10, 9, 10, 8, 10, 10, 10, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 10, 10, 7, 7, 10, 7, 10, 10, 10, 9, 10, 7, 9, 10, 8, 10, 7, 9, 10, 10, 10, 7, 8, 7, 10, 7, 10, 8, 8, 10, 9, 10, 8, 10, 7, 7, 7, 10, 10, 8, 7, 10, 10, 10, 8, 10, 9, 7, 10, 10, 10, 7, 10, 10, 9, 10, 7, 7, 7, 9, 10, 7, 9, 7, 8, 10, 7, 10, 7, 10, 8, 10, 7, 10, 7, 7, 7, 10, 7, 10, 7, 10, 7, 9, 9, 7, 10, 7, 7, 7, 9, 7, 9, 9, 7, 7, 7, 8, 8, 7, 7, 9, 7, 7, 8, 10, 8, 8, 8, 10, 9, 8, 7, 10, 10, 10, 7, 7, 10, 9, 10, 7, 7, 10, 10, 10, 8, 8, 8, 10, 10, 10, 9, 10, 7, 7, 7, 9, 8, 7, 10, 8, 8, 10, 7, 10, 7, 7, 7, 7, 7, 10, 10, 9, 9, 10, 8, 8, 10, 9, 7, 9, 10, 10, 10, 10, 9, 8, 10, 10, 7, 10, 10, 7, 10, 10, 7, 10, 8, 10, 10, 9, 9, 7, 10, 10, 10, 10, 7, 7, 7, 8, 10, 7, 7, 7, 8, 10, 10, 10, 7, 8, 8, 10, 10, 9, 9, 10, 10, 10, 7, 7, 8, 7, 10, 9, 7, 8, 7, 9, 7, 7, 10, 7, 7, 7, 7, 10, 7, 10, 10, 8, 8, 10, 10, 10, 8, 9, 10, 7, 10, 9, 10, 10, 8, 7, 7, 8, 10, 7, 7, 7, 9, 7, 7, 10, 10, 7, 7, 9, 9, 8, 8, 10, 10, 10, 10, 9, 10, 10, 7, 10, 10, 9, 7, 7, 10, 9, 10, 7, 10, 10, 9, 10, 7, 10, 7, 8, 7, 7, 7, 7, 10, 8, 8, 8, 10, 7, 10, 10, 7, 7, 9, 9, 10, 9, 7, 7, 7, 10, 7, 8, 7, 9, 7, 7, 8, 10, 10, 7, 7, 7, 7, 10, 7, 9, 8, 10, 10, 10, 9, 8, 10, 10, 10, 9, 9, 7, 7, 9, 7, 8, 7, 9, 10, 7, 10, 7, 9, 10, 9, 8, 7, 9, 10, 8, 9, 7, 10, 9, 7, 9, 7, 9, 10, 9, 8, 7, 9, 9, 8, 10, 9, 10, 7, 9, 7, 9, 10, 10, 7, 9, 7, 8, 7, 10, 7, 8, 8, 10, 9, 9, 9, 10, 10, 10, 9, 7, 10, 8, 8, 10, 8, 10, 10, 9, 7, 7, 9, 10, 10, 9, 10, 9, 9, 9, 7, 8, 10, 10, 10, 7, 9, 7, 8, 7, 7, 9, 10, 10, 7, 10, 10, 10, 7, 7, 7, 10, 10, 8, 10, 10, 10, 7, 7, 7, 7, 8, 10, 9, 10, 10, 10, 8, 8, 10, 8, 7, 10, 9, 10, 9, 10, 7, 7, 7, 7, 7, 7, 10, 9, 7, 7, 10, 7, 8, 10, 7, 7, 10, 7, 8, 7, 10, 8, 10, 10, 8, 7, 10, 7, 9, 7, 10, 10, 10, 9, 8, 10, 9, 10, 9, 7, 8, 7, 7, 8, 10, 8, 7, 7, 10, 9, 7, 7, 7, 10, 7, 7, 10, 9, 8, 7, 10, 7, 10, 7, 7, 7, 9, 8, 7, 8, 10, 10, 7, 9, 8, 8, 7, 9, 7, 10, 7, 8, 8, 10, 9, 9, 7, 7, 10, 9, 10, 7, 9, 10, 7, 7, 9, 10, 7, 8, 10, 8, 7, 7, 9, 7, 10, 10, 10, 9, 10, 10, 7, 9, 8, 8, 9, 10, 9, 10, 10, 7, 8, 10, 9, 9, 7, 7, 8, 7, 10, 10, 10, 10, 10, 8, 8, 8, 8, 7, 8, 7, 10, 10, 7, 10, 7, 7, 10, 10, 9, 10, 10, 10, 10, 10, 7, 10, 7, 9, 10, 7, 7, 7, 9, 7, 7, 7, 10, 7, 9, 7, 7, 9, 9, 7, 7, 8, 10, 10, 9, 10, 10, 8, 10, 9, 8, 10, 10, 10, 10, 7, 7, 10, 8, 10, 9, 7, 10, 10, 10, 10, 8, 10, 8, 10, 10, 10, 10, 10, 10, 7, 10, 10, 10, 10, 9, 10, 8, 10, 10, 8, 7, 10, 8, 10, 10, 7, 10, 10, 8, 7, 9, 10, 9, 10, 8, 10, 10, 10, 10, 10, 8, 8, 10, 10, 9, 9, 7, 9, 8, 8, 7, 10, 8, 8, 10, 10, 9, 10, 7, 7, 10, 7, 7, 8, 7, 9, 8, 10, 7, 8, 10, 10, 10, 10, 10, 10, 9, 10, 7, 10, 9, 9, 10, 10, 7, 10, 9, 7, 10, 8, 8, 8, 7, 7, 7, 8, 8, 10, 9, 8, 7, 9, 9, 7, 7, 7, 10, 8, 8, 8, 7, 8, 10, 7, 7, 7, 9, 8, 7, 10, 7, 9, 10, 10, 9, 10, 7, 10, 7, 8, 10, 7, 10, 8, 10, 10, 10, 7, 7, 8, 10, 9, 9, 10, 10, 10, 10, 10, 8, 7, 8, 9, 7, 10, 9, 9, 9, 9, 8, 7, 8, 10, 7, 9, 9, 7, 7, 10, 8, 10, 8, 9, 7, 8, 7, 10, 10, 7, 7, 10, 8, 10, 10, 9, 8, 7, 8, 7, 9, 7, 9, 9, 7, 8, 10, 10, 8, 10, 7, 8, 7, 9, 7, 9, 10, 8, 10, 9, 7, 9, 7, 7, 9, 7, 10, 7, 10, 8, 7, 10, 8, 7, 10, 9, 8, 10, 9, 7, 7, 10, 8, 10, 9, 7, 10, 8, 7, 10, 7, 8, 10, 9, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 9, 10, 10, 10, 7, 7, 9, 9, 7, 10, 7, 7, 9, 7, 8, 7, 10, 9, 10, 9, 10, 7, 10, 7, 10, 9, 10, 7, 7, 8, 7, 7, 7, 7, 8, 7, 8, 7, 10, 10, 9, 8, 7, 10, 8, 10, 7, 9, 7, 7, 7, 10, 8, 7, 7, 7, 7, 8, 10, 10, 9, 7, 7, 7, 9, 10, 7, 10, 10, 10, 8, 8, 7, 9, 9, 9, 7, 10, 10, 7, 9, 9, 7, 10, 10, 7, 9, 7, 7, 9, 9, 7, 7, 10, 7, 10, 10, 7, 9, 7, 9, 9, 7, 7, 7, 8, 10, 10, 9, 7, 7, 7, 10, 8, 10, 9, 7, 9, 10, 10, 10, 7, 7, 7, 7, 7, 9, 7, 10, 10, 10, 10, 7, 8, 8, 10, 10, 10, 10, 10, 9, 7, 10, 10, 9, 8, 10, 9, 8, 7, 9, 7, 7, 7, 10, 9, 10, 8, 8, 10, 7, 10, 9, 8, 7, 8, 10, 8, 10, 10, 10, 9, 8, 7, 7, 9, 7, 9, 8, 7, 8, 7, 8, 7, 7, 8, 7, 7, 7, 10, 9, 10, 8, 7, 9, 7, 10, 7, 8, 8, 7, 7, 10, 10, 10, 10, 10, 9, 8, 7, 7, 7, 10, 10, 7, 9, 7, 10, 10, 10, 7, 7, 10, 8, 10, 10, 7, 10, 10, 7, 7, 7, 10, 9, 10, 10, 10, 10, 10, 7, 10, 8, 10, 10, 7, 8, 10, 8, 10, 10, 10, 8, 10, 9, 10, 7, 8, 7, 9, 7, 10, 7, 10, 7, 10, 8, 7, 7, 7, 10, 8, 7, 8, 7, 10, 9, 7, 7, 7, 10, 7, 10, 10, 10, 7, 7, 7, 9, 10, 10, 10, 9, 7, 7, 7, 7, 10, 10, 7, 10, 7, 7, 10, 10, 7, 8, 8, 10, 7, 10, 7, 7, 7, 9, 8, 10, 9, 10, 9, 7, 7, 7, 7, 7, 10, 7, 9, 7, 7, 9, 10, 7, 10, 7, 7, 10, 10, 10, 9, 7, 7, 7, 7, 7, 7, 8, 7, 10, 10, 10, 8, 7, 8, 10, 10, 7, 10, 8, 7, 9, 7, 9, 7, 10, 10, 10, 10, 10, 7, 10, 7, 10, 10, 10, 7, 10, 7, 8, 7, 7, 7, 7, 9, 7, 10, 10, 7, 8, 10, 7, 7, 7, 10, 9, 7, 7, 9, 9, 7, 10, 10, 9, 8, 10, 7, 7, 7, 10, 9, 10, 10, 10, 10, 10, 9, 9, 10, 10, 8, 7, 7, 10, 10, 7, 8, 7, 10, 7, 7, 7, 7, 7, 10, 10, 7, 7, 10, 7, 10, 10, 8, 10, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 10, 9, 10, 8, 7, 9, 7, 9, 7, 7, 10, 7, 7, 7, 10, 7, 10, 8, 7, 7, 7, 10, 8, 10, 7, 7, 7, 10, 10, 10, 7, 7, 10, 9, 10, 7, 9, 7, 9, 7, 10, 7, 8, 7, 10, 7, 8, 10, 7, 10, 10, 10, 8, 10, 8, 8, 7, 10, 7, 10, 9, 7, 7, 10, 10, 10, 10, 7, 7, 9, 7, 10, 7, 10, 10, 10, 10, 7, 9, 10, 9, 8, 10, 10, 8, 7, 7, 9, 7, 8, 10, 9, 8, 10, 9, 7, 7, 7, 9, 8, 7, 7, 10, 8, 8, 7, 9, 10, 10, 9, 8, 10, 8, 8, 10, 9, 10, 9, 8, 10, 7, 9, 7, 10, 9, 9, 10, 7, 9, 8, 8, 7, 9, 7, 9, 7, 7, 10, 10, 10, 10, 7, 10, 7, 9, 10, 7, 7, 10, 9, 9, 7, 9, 9, 10, 10, 7, 9, 8, 8, 8, 10, 10, 7, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 10, 9, 8, 7, 10, 10, 10, 8, 7, 10, 10, 10, 8, 10, 9, 10, 7, 7, 7, 10, 8, 7, 9, 7, 10, 10, 7, 7, 9, 10, 8, 8, 10, 10, 7, 7, 10, 7, 10, 10, 7, 10, 10, 10, 10, 7, 7, 7, 10, 10, 7, 7, 7, 7, 10, 10, 10, 9, 10, 9, 10, 9, 10, 7, 7, 10, 10, 9, 8, 8, 7, 10, 10, 7, 7, 7, 10, 10, 10, 10, 8, 7, 7, 7, 7, 10, 9, 10, 7, 7, 10, 10, 10, 7, 7, 7, 7, 9, 9, 7, 10, 10, 7, 7, 7, 10, 7, 8, 10, 8, 7, 10, 10, 10, 7, 7, 10, 7, 7, 10, 10, 10, 7, 7, 7, 7, 10, 7, 7, 10, 10, 7, 10, 10, 9, 10, 7, 10, 7, 7, 10, 7, 10, 8, 10, 7, 8, 7, 10, 10, 10, 9, 9, 7, 9, 7, 7, 10, 10, 10, 10, 9, 7, 7, 8, 10, 7, 10, 8, 7, 7, 9, 8, 7, 10, 7, 10, 7, 8, 10, 9, 10, 7, 10, 7, 8, 7, 9, 10, 10, 7, 9, 7, 7, 7, 9, 10, 10, 7, 10, 10, 7, 7, 7, 10, 9, 7, 7, 8, 7, 10, 10, 10, 10, 7, 8, 10, 8, 7, 10, 10, 9, 7, 10, 8, 10, 9, 10, 7, 7, 7, 7, 7, 8, 10, 10, 8, 10, 10, 10, 7, 7, 10, 8, 10, 7, 8, 7, 7, 8, 10, 9, 10, 10, 10, 7, 8, 7, 7, 7, 7, 7, 8, 7, 7, 8, 7, 10, 10, 8, 7, 7, 7, 8, 7, 7, 7, 10, 7, 9, 10, 7, 8, 7, 9, 7, 10, 9, 10, 8, 7, 9, 10, 9, 7, 8, 7, 9, 7, 7, 8, 7, 7, 7, 7, 10, 9, 7, 8, 10, 10, 10, 10, 8, 10, 10, 9, 9, 7, 9, 7, 8, 7, 10, 10, 8, 8, 7, 9, 7, 10, 10, 9, 8, 8, 7, 10, 7, 10, 10, 7, 8, 10, 10, 10, 7, 10, 10, 10, 10, 7, 7, 7, 8, 7, 9, 8, 9, 8, 10, 7, 10, 9, 7, 8, 10, 10, 10, 7, 7, 7, 10, 8, 8, 7, 7, 9, 9, 8, 10, 9, 8, 10, 8, 9, 9, 7, 7, 9, 7, 10, 10, 10, 10, 10, 10, 10, 7, 9, 9, 10, 8, 10, 8, 10, 8, 7, 10, 7, 7, 9, 8, 8, 10, 7, 10, 10, 8, 10, 10, 8, 8, 8, 10, 9, 7, 10, 8, 7, 8, 7, 10, 9, 7, 9, 10, 9, 9, 9, 7, 9, 7, 9, 9, 10, 7, 9, 8, 8, 7, 7, 7, 10, 10, 10, 10, 9, 7, 7, 10, 10, 8, 8, 8, 10, 10, 10, 8, 10, 9, 10, 9, 10, 10, 8, 8, 8, 7, 8, 7, 7, 9, 7, 9, 8, 10, 9, 8, 10, 9, 7, 9, 7, 7, 9, 9, 8, 8, 10, 9, 9, 8, 9, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 8, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 10, 7, 9, 8, 7, 9, 10, 10, 10, 9, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 10, 9, 9, 7, 10, 8, 7, 8, 7, 7, 9, 9, 10, 8, 7, 7, 9, 7, 9, 7, 10, 10, 9, 7, 9, 7, 8, 10, 7, 10, 8, 8, 10, 9, 7, 8, 8, 7, 7, 8, 7, 10, 9, 9, 10, 9, 7, 7, 7, 8, 8, 8, 8, 9, 10, 8, 10, 8, 10, 7, 8, 8, 7, 7, 7, 7, 7, 10, 7, 8, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 10, 9, 9, 9, 10, 9, 10, 9, 9, 10, 9, 9, 8, 7, 8, 9, 8, 7, 8, 8, 10, 9, 10, 7, 9, 10, 8, 8, 8, 7, 9, 8, 10, 7, 8, 10, 8, 9, 9, 9, 7, 9, 7, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 7, 9, 7, 7, 10, 8, 7, 8, 10, 10, 9, 7, 7, 8, 8, 8, 9, 9, 9, 7, 9, 9, 8, 7, 9, 7, 9, 9, 10, 10, 9, 8, 7, 9, 10, 7, 7, 9, 10, 9, 9, 7, 9, 7, 10, 8, 7, 8, 9, 8, 8, 7, 8, 10, 9, 8, 7, 8, 10, 8, 8, 7, 7, 8, 8, 10, 9, 9, 9, 8, 9, 7, 8, 7, 8, 7, 9, 10, 9, 7, 9, 7, 9, 10, 8, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 8, 10, 7, 8, 10, 9, 7, 9, 7, 7, 9, 9, 7, 9, 8, 8, 7, 8, 8, 7, 10, 10, 10, 10, 9, 8, 10, 10, 10, 7, 7, 7, 7, 10, 7, 9, 10, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 8, 8, 10, 10, 10, 10, 10, 7, 7, 10, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 9, 8, 10, 9, 7, 9, 9, 7, 10, 9, 8, 7, 9, 7, 10, 10, 7, 8, 7, 10, 9, 7, 9, 10, 10, 7, 10, 10, 10, 7, 7, 10, 7, 7, 9, 10, 10, 9, 7, 10, 10, 8, 7, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 10, 7, 10, 7, 10, 10, 7, 9, 7, 7, 7, 10, 9, 9, 10, 7, 7, 9, 7, 10, 9, 7, 10, 10, 10, 9, 9, 10, 9, 9, 9, 8, 7, 7, 7, 7, 7, 10, 9, 8, 10, 10, 10, 8, 10, 7, 10, 7, 7, 10, 10, 10, 9, 7, 7, 10, 7, 10, 9, 9, 9, 10, 10, 10, 7, 7, 10, 10, 10, 7, 7, 10, 10, 7, 10, 10, 7, 7, 8, 10, 7, 10, 7, 9, 7, 8, 7, 9, 7, 7, 10, 10, 7, 8, 10, 10, 7, 8, 10, 9, 7, 9, 10, 7, 10, 7, 7, 9, 7, 7, 9, 10, 7, 10, 10, 10, 10, 7, 8, 10, 9, 10, 8, 10, 9, 9, 9, 8, 7, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 10, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 9, 9, 7, 9, 9, 8, 8, 9, 7, 9, 9, 8, 9, 9, 9, 10, 9, 9, 7, 10, 8, 7, 8, 7, 7, 9, 9, 10, 8, 7, 9, 9, 7, 7, 10, 10, 8, 9, 7, 9, 7, 8, 8, 7, 9, 8, 8, 10, 9, 9, 8, 8, 7, 9, 8, 7, 10, 9, 9, 10, 9, 7, 7, 7, 10, 8, 8, 8, 7, 10, 8, 10, 8, 10, 7, 8, 10, 7, 7, 10, 7, 7, 10, 7, 8, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 10, 9, 9, 9, 8, 7, 10, 9, 9, 10, 9, 9, 10, 9, 8, 9, 8, 9, 8, 8, 10, 9, 10, 7, 9, 10, 8, 8, 8, 7, 9, 8, 10, 7, 8, 10, 8, 7, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 7, 9, 7, 7, 10, 10, 10, 8, 10, 10, 9, 7, 7, 8, 7, 8, 7, 8, 8, 7, 9, 9, 10, 9, 9, 7, 7, 9, 10, 10, 9, 8, 7, 9, 10, 9, 7, 9, 8, 8, 7, 7, 9, 7, 10, 8, 9, 10, 9, 8, 8, 7, 8, 10, 9, 8, 7, 8, 8, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 10, 9, 7, 8, 7, 9, 7, 9, 8, 9, 7, 9, 7, 8, 10, 8, 8, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 7, 8, 8, 8, 7, 8, 7, 9, 10, 7, 10, 9, 7, 7, 10, 9, 7, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 10, 7, 8, 10, 10, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 8, 7, 9, 7, 9, 10, 8, 10, 8, 7, 8, 10, 10, 10, 9, 10, 8, 9, 8, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 10, 9, 10, 9, 9, 8, 9, 8, 9, 7, 10, 8, 10, 8, 9, 9, 8, 10, 10, 7, 8, 10, 9, 8, 7, 7, 8, 8, 10, 8, 8, 8, 7, 8, 10, 8, 8, 8, 10, 9, 9, 10, 10, 8, 7, 7, 9, 9, 9, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 10, 9, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 9, 8, 7, 9, 9, 8, 9, 10, 9, 7, 9, 8, 8, 7, 8, 10, 9, 8, 9, 8, 8, 8, 9, 9, 8, 8, 10, 7, 10, 7, 8, 10, 9, 10, 7, 10, 7, 10, 7, 10, 10, 10, 10, 10, 7, 7, 7, 9, 7, 8, 10, 9, 10, 10, 8, 10, 7, 7, 10, 7, 9, 7, 10, 7, 10, 7, 7, 7, 7, 7, 8, 10, 9, 10, 7, 7, 7, 7, 7, 7, 10, 7, 7, 9, 10, 7, 10, 10, 10, 10, 10, 7, 10, 7, 10, 7, 9, 8, 10, 8, 10, 8, 7, 10, 9, 9, 7, 10, 10, 10, 10, 7, 10, 10, 7, 7, 7, 8, 7, 9, 7, 7, 10, 10, 9, 9, 8, 10, 8, 7, 7, 7, 7, 7, 10, 7, 8, 10, 7, 10, 9, 10, 10, 7, 8, 7, 9, 7, 8, 10, 9, 10, 10, 9, 9, 9, 10, 7, 8, 8, 8, 10, 10, 9, 8, 8, 10, 7, 7, 8, 9, 10, 9, 7, 7, 8, 10, 9, 9, 8, 7, 9, 10, 9, 10, 10, 7, 8, 8, 10, 10, 9, 8, 10, 10, 10, 10, 9, 8, 7, 9, 10, 7, 7, 10, 7, 9, 10, 8, 10, 7, 8, 7, 9, 7, 10, 10, 9, 8, 7, 9, 9, 10, 8, 10, 8, 7, 10, 8, 7, 7, 10, 10, 8, 8, 7, 10, 8, 8, 7, 7, 7, 9, 10, 10, 10, 7, 7, 7, 7, 9, 8, 7, 10, 10, 10, 7, 7, 7, 7, 10, 10, 7, 8, 8, 7, 7, 7, 10, 8, 10, 10, 10, 7, 7, 7, 7, 10, 8, 7, 8, 10, 10, 10, 9, 9, 10, 9, 9, 10, 9, 7, 10, 10, 8, 8, 7, 7, 7, 10, 10, 9, 9, 10, 7, 8, 8, 7, 7, 9, 8, 7, 7, 7, 8, 10, 10, 10, 8, 10, 9, 9, 7, 10, 9, 8, 8, 8, 7, 7, 8, 7, 10, 9, 7, 10, 10, 10, 10, 8, 7, 9, 10, 7, 7, 8, 8, 7, 8, 8, 8, 10, 10, 10, 7, 9, 7, 9, 10, 7, 10, 10, 10, 9, 10, 10, 10, 7, 8, 10, 7, 7, 9, 10, 10, 8, 7, 8, 8, 7, 8, 7, 10, 10, 10, 10, 9, 7, 7, 8, 7, 10, 9, 9, 10, 7, 9, 10, 10, 10, 10, 7, 9, 7, 8, 10, 9, 8, 7, 7, 10, 7, 7, 10, 7, 10, 10, 10, 7, 9, 7, 8, 10, 10, 7, 8, 7, 10, 10, 7, 10, 10, 7, 8, 10, 10, 7, 8, 10, 9, 8, 10, 7, 7, 9, 10, 7, 7, 7, 7, 10, 8, 10, 7, 7, 7, 10, 8, 8, 10, 9, 8, 7, 7, 7, 10, 9, 8, 7, 8, 7, 9, 7, 7, 10, 10, 8, 7, 7, 9, 8, 10, 9, 7, 7, 7, 10, 7, 10, 7, 7, 10, 9, 7, 10, 10, 10, 7, 10, 9, 10, 10, 10, 7, 9, 8, 10, 8, 7, 8, 7, 10, 10, 9, 7, 7, 9, 10, 7, 10, 10, 9, 9, 10, 10, 9, 9, 10, 10, 7, 8, 10, 10, 7, 10, 9, 10, 7, 10, 10, 7, 7, 10, 9, 8, 7, 9, 10, 9, 10, 9, 8, 7, 10, 10, 8, 7, 8, 7, 10, 10, 10, 7, 7, 10, 8, 7, 9, 9, 10, 10, 10, 7, 9, 10, 8, 10, 9, 10, 10, 10, 8, 10, 7, 10, 10, 10, 10, 7, 7, 10, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 10, 7, 10, 7, 8, 7, 7, 9, 8, 7, 7, 7, 10, 10, 7, 9, 7, 10, 7, 10, 10, 7, 9, 7, 8, 7, 10, 9, 10, 10, 7, 9, 10, 10, 7, 8, 7, 7, 10, 9, 9, 10, 7, 7, 8, 7, 8, 7, 10, 10, 10, 10, 10, 7, 9, 9, 10, 9, 10, 8, 9, 7, 7, 7, 8, 7, 8, 7, 7, 10, 10, 10, 10, 8, 7, 7, 7, 7, 10, 10, 8, 8, 10, 7, 7, 10, 9, 7, 7, 7, 9, 10, 10, 7, 10, 7, 7, 7, 7, 7, 10, 9, 10, 7, 7, 7, 8, 7, 7, 9, 7, 7, 10, 10, 9, 10, 7, 7, 7, 7, 7, 10, 9, 9, 7, 8, 7, 7, 7, 9, 10, 7, 9, 10, 8, 7, 7, 7, 10, 7, 10, 7, 10, 10, 10, 10, 8, 7, 7, 7, 9, 8, 7, 8, 7, 7, 10, 10, 10, 10, 7, 10, 10, 7, 9, 7, 8, 7, 9, 9, 8, 7, 10, 7, 7, 10, 10, 10, 7, 8, 7, 10, 10, 10, 10, 9, 8, 10, 10, 10, 10, 8, 10, 7, 9, 10, 9, 9, 9, 10, 10, 10, 9, 7, 7, 7, 7, 10, 9, 10, 10, 10, 7, 10, 10, 9, 9, 7, 9, 7, 10, 10, 9, 9, 9, 8, 10, 7, 10, 9, 10, 7, 9, 10, 10, 10, 7, 10, 7, 7, 10, 10, 10, 10, 10, 7, 7, 10, 10, 8, 7, 10, 7, 7, 7, 7, 7, 7, 9, 10, 7, 7, 10, 8, 7, 10, 7, 8, 7, 10, 9, 7, 9, 7, 7, 9, 9, 10, 7, 9, 7, 8, 8, 10, 9, 10, 9, 8, 8, 8, 10, 7, 9, 9, 7, 10, 8, 7, 10, 9, 10, 8, 7, 8, 7, 10, 7, 10, 7, 8, 7, 9, 7, 10, 7, 7, 10, 9, 8, 8, 7, 10, 10, 10, 8, 8, 10, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 7, 10, 9, 10, 9, 10, 9, 10, 10, 10, 10, 9, 7, 10, 9, 7, 8, 8, 10, 8, 8, 7, 8, 7, 9, 9, 8, 8, 10, 10, 7, 8, 10, 9, 10, 7, 10, 8, 7, 7, 9, 8, 10, 10, 10, 10, 7, 10, 7, 7, 10, 9, 7, 10, 9, 7, 8, 10, 8, 8, 10, 10, 8, 7, 10, 10, 7, 10, 10, 10, 7, 7, 7, 10, 10, 8, 8, 10, 7, 10, 7, 8, 10, 10, 10, 10, 10, 7, 10, 10, 10, 9, 10, 10, 7, 10, 8, 7, 8, 9, 8, 7, 7, 8, 10, 7, 8, 10, 10, 10, 9, 10, 10, 8, 7, 7, 10, 9, 10, 9, 7, 7, 7, 7, 10, 9, 10, 9, 8, 10, 7, 7, 8, 10, 8, 7, 10, 9, 9, 9, 7, 9, 7, 7, 9, 7, 9, 10, 9, 8, 8, 7, 7, 10, 10, 9, 7, 10, 7, 9, 10, 10, 8, 10, 10, 10, 10, 7, 9, 8, 7, 7, 10, 10, 7, 7, 9, 7, 7, 10, 7, 10, 9, 9, 10, 7, 10, 10, 10, 9, 9, 9, 7, 7, 9, 7, 7, 7, 7, 9, 10, 10, 10, 9, 7, 7, 7, 10, 9, 8, 7, 7, 8, 7, 7, 7, 10, 9, 9, 7, 10, 7, 10, 10, 10, 8, 7, 7, 8, 7, 8, 7, 9, 10, 7, 9, 10, 7, 10, 10, 7, 10, 7, 10, 10, 7, 10, 8, 7, 9, 10, 10, 8, 10, 10, 10, 7, 9, 10, 7, 7, 9, 10, 9, 7, 10, 10, 10, 10, 7, 9, 7, 9, 7, 10, 9, 10, 10, 9, 10, 7, 9, 9, 8, 10, 7, 8, 10, 10, 10, 10, 7, 8, 9, 9, 10, 9, 9, 7, 7, 10, 7, 10, 7, 10, 7, 9, 10, 7, 10, 7, 9, 7, 9, 7, 10, 9, 8, 7, 7, 7, 7, 8, 10, 10, 7, 7, 7, 10, 9, 10, 10, 10, 7, 8, 7, 10, 8, 7, 7, 10, 10, 10, 7, 10, 7, 10, 10, 9, 7, 7, 10, 9, 10, 8, 7, 8, 7, 10, 7, 7, 10, 10, 10, 8, 7, 10, 9, 9, 7, 7, 9, 9, 7, 7, 7, 9, 9, 10, 7, 9, 8, 10, 10, 9, 7, 10, 7, 10, 10, 10, 7, 9, 7, 10, 10, 8, 10, 7, 7, 9, 7, 10, 7, 10, 7, 7, 10, 8, 10, 9, 7, 7, 7, 9, 9, 7, 7, 7, 8, 10, 7, 7, 10, 10, 7, 10, 9, 10, 10, 8, 10, 8, 10, 7, 8, 7, 8, 10, 10, 7, 8, 10, 9, 10, 7, 7, 10, 7, 8, 10, 9, 7, 10, 10, 7, 10, 10, 8, 10, 10, 7, 8, 7, 10, 7, 10, 8, 7, 7, 7, 10, 10, 7, 10, 10, 9, 7, 7, 8, 10, 10, 10, 7, 7, 7, 7, 7, 10, 10, 10, 8, 7, 10, 10, 9, 10, 7, 10, 7, 9, 10, 8, 7, 10, 10, 7, 7, 7, 8, 10, 9, 7, 9, 10, 10, 9, 9, 9, 10, 10, 10, 10, 10, 10, 8, 10, 10, 7, 7, 7, 9, 9, 9, 10, 10, 10, 7, 9, 8, 7, 10, 8, 7, 8, 10, 8, 7, 10, 10, 10, 9, 7, 10, 10, 10, 7, 8, 7, 8, 7, 10, 10, 8, 7, 8, 7, 10, 10, 7, 10, 7, 7, 10, 7, 10, 10, 10, 7, 7, 10, 10, 7, 10, 8, 7, 10, 9, 9, 9, 10, 9, 10, 7, 10, 9, 8, 10, 10, 10, 7, 8, 7, 10, 7, 7, 7, 7, 7, 7, 9, 9, 10, 10, 10, 7, 10, 7, 7, 7, 7, 9, 10, 10, 7, 10, 10, 10, 7, 10, 9, 8, 10, 7, 10, 7, 10, 10, 9, 7, 7, 7, 9, 10, 8, 7, 10, 8, 10, 10, 7, 7, 9, 7, 7, 10, 8, 10, 8, 8, 7, 9, 9, 10, 10, 7, 10, 10, 10, 7, 7, 7, 9, 10, 7, 9, 10, 10, 7, 10, 7, 9, 9, 7, 9, 8, 7, 9, 7, 9, 7, 7, 8, 7, 7, 9, 8, 7, 8, 8, 10, 10, 10, 7, 10, 8, 7, 7, 7, 7, 10, 8, 10, 9, 9, 10, 8, 8, 10, 7, 10, 9, 10, 9, 8, 8, 10, 10, 9, 8, 10, 10, 10, 7, 8, 8, 7, 7, 7, 10, 7, 8, 8, 10, 9, 7, 10, 10, 10, 10, 10, 8, 10, 9, 9, 7, 9, 9, 9, 10, 9, 10, 10, 8, 8, 10, 9, 10, 7, 7, 10, 10, 8, 7, 8, 7, 7, 8, 10, 9, 10, 7, 9, 7, 8, 7, 8, 7, 10, 9, 9, 9, 8, 7, 7, 7, 7, 10, 10, 7, 9, 9, 7, 10, 10, 10, 10, 10, 7, 7, 9, 7, 7, 10, 7, 7, 7, 10, 7, 8, 7, 10, 10, 10, 8, 10, 7, 10, 10, 10, 10, 10, 10, 10, 9, 9, 10, 10, 9, 10, 10, 10, 8, 7, 7, 8, 7, 10, 10, 7, 9, 8, 10, 8, 10, 10, 8, 7, 7, 7, 10, 10, 8, 7, 10, 10, 7, 7, 8, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 10, 7, 9, 9, 10, 7, 10, 7, 10, 10, 7, 8, 7, 7, 7, 7, 9, 8, 7, 10, 7, 7, 7, 8, 7, 10, 10, 10, 9, 10, 9, 7, 7, 8, 7, 9, 10, 7, 8, 10, 10, 7, 7, 7, 10, 7, 7, 7, 10, 10, 9, 10, 9, 7, 10, 7, 8, 10, 7, 10, 10, 9, 8, 10, 8, 8, 7, 10, 8, 7, 10, 10, 9, 7, 7, 8, 10, 10, 10, 10, 10, 9, 7, 7, 7, 8, 10, 10, 10, 7, 7, 8, 7, 7, 10, 10, 9, 10, 7, 10, 7, 7, 7, 7, 8, 10, 9, 10, 8, 7, 9, 10, 10, 10, 9, 10, 10, 9, 10, 10, 10, 8, 7, 10, 10, 10, 9, 10, 7, 7, 10, 10, 7, 8, 7, 7, 7, 7, 10, 7, 7, 10, 10, 10, 7, 7, 7, 7, 7, 8, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 7, 7, 10, 10, 10, 9, 9, 7, 10, 8, 8, 10, 9, 7, 8, 10, 10, 10, 9, 10, 8, 10, 7, 10, 7, 10, 8, 10, 9, 10, 7, 10, 10, 10, 8, 7, 10, 10, 10, 9, 10, 10, 10, 7, 9, 7, 7, 7, 9, 7, 10, 10, 8, 10, 10, 10, 10, 9, 9, 9, 10, 10, 10, 9, 7, 10, 7, 7, 10, 9, 10, 7, 7, 10, 10, 10, 9, 10, 7, 10, 7, 10, 10, 10, 7, 7, 7, 10, 10, 10, 10, 10, 10, 7, 10, 9, 9, 7, 8, 7, 10, 7, 7, 10, 10, 8, 7, 7, 7, 9, 9, 7, 7, 10, 9, 10, 7, 10, 7, 7, 7, 10, 10, 9, 9, 10, 8, 10, 10, 10, 10, 9, 10, 10, 7, 7, 7, 10, 9, 9, 8, 10, 10, 10, 10, 10, 7, 7, 10, 10, 9, 7, 10, 7, 7, 7, 8, 10, 10, 8, 10, 8, 10, 10, 9, 10, 8, 7, 10, 10, 10, 10, 10, 10, 9, 9, 10, 7, 10, 8, 8, 7, 9, 8, 10, 7, 10, 10, 7, 8, 8, 10, 7, 10, 10, 10, 7, 7, 10, 7, 9, 7, 10, 10, 10, 7, 10, 10, 9, 7, 7, 7, 10, 7, 9, 7, 10, 10, 7, 10, 10, 10, 10, 8, 7, 10, 7, 7, 7, 9, 7, 7, 8, 10, 8, 10, 7, 10, 7, 8, 7, 7, 7, 10, 8, 10, 10, 10, 10, 10, 8, 10, 7, 10, 7, 10, 10, 10, 8, 8, 10, 10, 7, 10, 10, 10, 10, 8, 8, 10, 7, 10, 10, 10, 7, 8, 8, 10, 9, 10, 9, 10, 8, 10, 7, 10, 9, 7, 8, 8, 10, 7, 7, 8, 8, 10, 7, 10, 9, 7, 7, 10, 10, 7, 9, 10, 8, 10, 10, 8, 10, 8, 10, 8, 10, 10, 10, 7, 10, 7, 10, 7, 10, 8, 7, 7, 7, 7, 7, 10, 9, 7, 7, 10, 10, 7, 8, 10, 9, 7, 10, 8, 10, 10, 10, 10, 10, 8, 10, 8, 10, 9, 9, 8, 10, 8, 10, 9, 10, 7, 9, 10, 7, 10, 8, 10, 8, 10, 7, 10, 8, 7, 8, 10, 9, 10, 8, 7, 8, 7, 10, 9, 10, 9, 7, 10, 8, 10, 10, 10, 8, 10, 10, 8, 8, 10, 10, 10, 10, 8, 10, 8, 10, 7, 9, 8, 8, 8, 7, 10, 7, 10, 10, 8, 10, 7, 9, 8, 7, 10, 9, 7, 7, 7, 10, 7, 8, 10, 9, 9, 9, 10, 10, 9, 9, 8, 8, 7, 9, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 10, 10, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 7, 8, 7, 8, 10, 10, 10, 7, 9, 9, 7, 7, 9, 8, 8, 7, 7, 9, 9, 8, 7, 9, 7, 7, 9, 7, 7, 10, 8, 9, 8, 10, 10, 9, 7, 9, 8, 8, 8, 7, 10, 9, 7, 7, 10, 10, 10, 9, 7, 9, 9, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 8, 7, 7, 8, 7, 10, 7, 9, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 7, 8, 7, 7, 7, 7, 7, 7, 10, 8, 10, 10, 7, 7, 7, 7, 7, 10, 7, 7, 7, 7, 10, 10, 7, 9, 8, 8, 7, 7, 10, 7, 10, 9, 10, 9, 9, 9, 8, 7, 10, 9, 8, 7, 8, 8, 10, 9, 10, 9, 9, 9, 9, 8, 8, 7, 9, 8, 10, 7, 8, 10, 8, 7, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 7, 7, 9, 9, 7, 10, 10, 9, 8, 10, 10, 9, 7, 9, 8, 8, 8, 7, 9, 8, 7, 7, 10, 10, 10, 9, 7, 10, 9, 8, 10, 9, 8, 7, 9, 10, 7, 7, 7, 8, 8, 7, 10, 9, 7, 10, 9, 7, 8, 7, 8, 10, 7, 8, 10, 9, 9, 7, 8, 10, 8, 10, 7, 9, 8, 8, 10, 9, 7, 9, 10, 9, 7, 8, 7, 9, 10, 9, 7, 9, 7, 8, 10, 8, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 8, 7, 7, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 7, 8, 10, 9, 7, 7, 7, 8, 7, 7, 8, 7, 7, 7, 7, 7, 7, 7, 9, 7, 8, 10, 9, 9, 9, 10, 10, 10, 7, 10, 10, 10, 7, 7, 9, 8, 10, 7, 9, 10, 10, 7, 9, 7, 7, 10, 10, 10, 7, 10, 8, 10, 10, 10, 8, 10, 7, 10, 7, 10, 9, 10, 10, 7, 7, 10, 7, 7, 8, 7, 9, 8, 8, 10, 7, 7, 8, 7, 9, 7, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 7, 7, 7, 10, 7, 10, 8, 10, 8, 10, 7, 9, 9, 8, 10, 7, 9, 8, 8, 10, 8, 7, 7, 9, 9, 10, 10, 7, 7, 9, 10, 7, 7, 10, 10, 7, 10, 7, 9, 7, 7, 9, 10, 10, 10, 9, 9, 10, 7, 10, 9, 10, 7, 10, 10, 10, 10, 8, 10, 10, 8, 7, 10, 7, 7, 10, 10, 10, 9, 7, 7, 10, 7, 10, 7, 7, 10, 10, 9, 8, 10, 10, 8, 8, 7, 10, 10, 9, 10, 9, 7, 8, 10, 9, 10, 8, 7, 7, 10, 10, 9, 7, 7, 10, 9, 8, 7, 10, 9, 9, 7, 9, 7, 10, 8, 7, 7, 10, 10, 9, 10, 9, 7, 10, 7, 7, 10, 7, 10, 7, 8, 7, 9, 9, 7, 10, 10, 10, 10, 7, 9, 10, 8, 8, 10, 7, 10, 8, 10, 8, 10, 7, 8, 10, 9, 8, 10, 9, 7, 7, 9, 10, 7, 7, 8, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 7, 10, 7, 8, 10, 10, 10, 9, 10, 7, 7, 7, 7, 7, 7, 7, 10, 8, 7, 8, 10, 7, 7, 7, 9, 10, 9, 8, 8, 7, 9, 8, 7, 10, 10, 10, 10, 10, 7, 7, 7, 9, 10, 9, 10, 7, 10, 7, 10, 10, 10, 10, 10, 8, 10, 10, 10, 9, 9, 8, 7, 7, 8, 8, 10, 8, 10, 8, 7, 10, 9, 7, 7, 7, 7, 9, 8, 7, 8, 10, 7, 7, 8, 10, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 7, 7, 10, 7, 7, 10, 8, 10, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 7, 8, 7, 9, 10, 9, 8, 10, 10, 8, 10, 10, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 10, 7, 10, 7, 10, 8, 7, 8, 7, 7, 10, 8, 8, 10, 8, 7, 7, 9, 10, 10, 8, 8, 7, 8, 10, 9, 8, 10, 7, 10, 9, 8, 7, 7, 7, 7, 9, 10, 7, 10, 8, 10, 10, 7, 9, 7, 7, 10, 8, 10, 9, 7, 7, 10, 8, 10, 10, 7, 10, 7, 9, 7, 10, 7, 7, 10, 7, 8, 10, 7, 8, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 10, 9, 9, 10, 9, 10, 8, 10, 8, 9, 8, 10, 8, 10, 9, 10, 8, 7, 8, 8, 10, 7, 9, 9, 8, 10, 9, 8, 7, 9, 10, 9, 9, 10, 9, 10, 9, 7, 8, 8, 10, 8, 7, 8, 10, 9, 8, 7, 7, 8, 8, 10, 8, 10, 9, 8, 8, 10, 8, 8, 8, 7, 9, 9, 10, 10, 8, 7, 7, 9, 8, 7, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 8, 10, 7, 9, 10, 7, 7, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 8, 7, 10, 9, 8, 8, 7, 8, 8, 7, 8, 9, 8, 8, 8, 7, 7, 8, 10, 7, 7, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 7, 9, 7, 9, 7, 8, 7, 9, 9, 9, 10, 10, 10, 8, 7, 8, 8, 7, 10, 9, 10, 10, 9, 9, 8, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 10, 8, 7, 7, 7, 8, 10, 8, 8, 10, 9, 7, 8, 8, 10, 8, 7, 9, 9, 10, 9, 7, 10, 8, 8, 9, 8, 8, 10, 9, 8, 8, 10, 8, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 10, 10, 10, 7, 8, 7, 9, 9, 8, 9, 10, 9, 7, 9, 8, 8, 9, 8, 10, 9, 8, 9, 8, 8, 8, 9, 9, 8, 8, 7, 7, 10, 7, 8, 10, 9, 8, 8, 10, 10, 10, 8, 7, 7, 7, 7, 9, 7, 9, 7, 8, 10, 9, 7, 10, 7, 9, 10, 9, 7, 8, 8, 7, 9, 8, 7, 10, 10, 7, 9, 10, 7, 7, 10, 8, 7, 10, 7, 8, 10, 9, 9, 10, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 9, 9, 9, 9, 10, 9, 10, 10, 10, 10, 10, 8, 10, 9, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 9, 7, 9, 7, 8, 7, 7, 9, 9, 10, 8, 10, 10, 8, 8, 10, 8, 8, 8, 9, 10, 10, 9, 8, 8, 10, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 9, 9, 10, 9, 7, 8, 7, 8, 7, 7, 10, 8, 10, 10, 9, 9, 8, 10, 8, 7, 8, 10, 9, 8, 7, 9, 8, 8, 10, 10, 9, 7, 10, 7, 9, 7, 9, 8, 7, 7, 9, 8, 10, 8, 7, 7, 9, 10, 9, 7, 10, 8, 8, 10, 8, 8, 8, 7, 9, 9, 7, 9, 8, 8, 10, 8, 9, 9, 8, 8, 8, 8, 8, 7, 7, 9, 8, 7, 9, 8, 10, 9, 9, 9, 7, 7, 10, 7, 8, 7, 9, 9, 10, 9, 8, 9, 8, 9, 8, 8, 7, 8, 8, 7, 10, 9, 8, 8, 8, 7, 9, 8, 10, 9, 7, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 10, 10, 9, 10, 7, 9, 7, 9, 7, 10, 9, 9, 9, 9, 10, 10, 10, 10, 9, 8, 8, 7, 10, 9, 10, 10, 9, 8, 8, 8, 9, 9, 7, 10, 10, 9, 9, 10, 8, 10, 8, 7, 7, 7, 8, 10, 8, 8, 10, 9, 7, 9, 8, 10, 10, 8, 7, 9, 10, 9, 7, 10, 8, 10, 9, 8, 8, 10, 9, 8, 8, 10, 10, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 10, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 9, 10, 9, 7, 9, 8, 9, 7, 8, 8, 7, 8, 7, 8, 8, 8, 7, 9, 8, 8, 8, 7, 10, 7, 10, 10, 9, 9, 10, 8, 10, 10, 10, 8, 10, 10, 7, 8, 10, 9, 10, 10, 8, 10, 10, 7, 7, 7, 7, 7, 9, 7, 9, 7, 7, 10, 10, 8, 8, 10, 10, 10, 7, 7, 9, 9, 8, 7, 9, 9, 7, 8, 8, 9, 7, 10, 10, 7, 8, 7, 10, 7, 10, 7, 8, 7, 8, 10, 8, 10, 7, 9, 7, 7, 7, 7, 9, 7, 7, 7, 7, 10, 7, 9, 8, 7, 7, 9, 9, 7, 7, 9, 7, 7, 7, 10, 7, 7, 7, 10, 10, 9, 8, 8, 10, 10, 8, 7, 7, 10, 10, 7, 8, 8, 7, 7, 7, 9, 7, 10, 10, 10, 9, 7, 9, 8, 10, 10, 8, 10, 9, 8, 10, 7, 10, 9, 9, 8, 10, 9, 7, 9, 7, 9, 10, 9, 10, 10, 10, 10, 9, 9, 10, 8, 7, 10, 10, 9, 8, 7, 7, 7, 10, 10, 8, 7, 9, 9, 9, 9, 10, 10, 10, 8, 8, 8, 7, 7, 9, 10, 10, 8, 7, 8, 8, 8, 10, 8, 7, 9, 10, 10, 8, 10, 9, 9, 8, 10, 7, 9, 7, 7, 7, 9, 7, 9, 7, 7, 7, 7, 8, 10, 10, 7, 8, 10, 7, 7, 7, 7, 9, 8, 10, 7, 10, 10, 7, 10, 7, 8, 10, 8, 7, 8, 7, 9, 10, 8, 7, 10, 7, 10, 10, 10, 7, 10, 10, 7, 8, 7, 9, 7, 9, 7, 7, 7, 9, 9, 7, 7, 7, 10, 7, 8, 7, 7, 7, 10, 10, 7, 7, 7, 7, 8, 8, 7, 9, 8, 8, 7, 7, 7, 9, 9, 7, 7, 9, 10, 9, 7, 8, 7, 8, 7, 10, 7, 7, 7, 7, 8, 7, 9, 7, 9, 10, 8, 10, 7, 9, 9, 7, 9, 10, 9, 9, 10, 8, 8, 7, 7, 7, 8, 10, 10, 9, 7, 9, 9, 8, 10, 10, 10, 8, 9, 9, 10, 9, 10, 8, 8, 10, 10, 10, 10, 8, 10, 10, 9, 10, 7, 9, 8, 9, 10, 8, 7, 10, 9, 9, 7, 7, 9, 9, 10, 9, 10, 10, 7, 10, 8, 10, 7, 8, 10, 8, 8, 10, 9, 7, 8, 8, 7, 8, 7, 7, 10, 9, 10, 10, 10, 9, 7, 8, 7, 9, 10, 7, 8, 7, 8, 7, 8, 7, 10, 7, 9, 10, 7, 10, 10, 9, 8, 8, 7, 10, 10, 8, 7, 9, 9, 9, 7, 7, 9, 8, 10, 8, 7, 8, 8, 10, 7, 7, 9, 8, 10, 10, 10, 9, 9, 10, 9, 10, 8, 8, 7, 9, 7, 10, 10, 10, 10, 10, 7, 10, 10, 9, 7, 9, 7, 9, 7, 9, 9, 8, 10, 8, 10, 7, 10, 10, 7, 9, 10, 10, 9, 9, 8, 7, 10, 9, 9, 10, 10, 9, 9, 10, 10, 9, 7, 10, 10, 10, 10, 10, 10, 10, 9, 8, 8, 8, 7, 10, 9, 10, 7, 9, 10, 10, 9, 10, 8, 8, 10, 8, 10, 8, 10, 10, 10, 8, 8, 7, 7, 8, 7, 10, 8, 10, 9, 8, 8, 8, 8, 10, 8, 8, 8, 10, 9, 9, 7, 9, 7, 10, 8, 10, 9, 9, 10, 10, 9, 7, 8, 7, 10, 8, 7, 7, 9, 7, 8, 10, 10, 8, 7, 9, 9, 9, 8, 8, 10, 8, 7, 8, 8, 7, 10, 7, 9, 9, 10, 10, 7, 10, 8, 10, 8, 9, 10, 10, 7, 9, 8, 7, 10, 7, 7, 7, 8, 10, 9, 10, 8, 7, 7, 9, 10, 9, 10, 10, 9, 10, 8, 10, 7, 7, 9, 9, 7, 7, 8, 8, 8, 7, 8, 7, 7, 10, 9, 7, 7, 10, 7, 7, 10, 7, 7, 7, 9, 7, 8, 7, 10, 10, 8, 8, 10, 7, 10, 8, 7, 9, 10, 9, 7, 9, 7, 7, 8, 10, 8, 8, 8, 7, 7, 7, 9, 7, 8, 10, 10, 7, 8, 7, 8, 8, 7, 9, 7, 7, 8, 10, 10, 10, 8, 10, 10, 10, 9, 9, 7, 10, 9, 9, 9, 8, 8, 7, 7, 7, 10, 10, 10, 8, 10, 10, 7, 8, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 7, 10, 7, 8, 7, 8, 10, 10, 8, 8, 10, 10, 8, 10, 9, 7, 7, 10, 10, 9, 7, 10, 9, 7, 9, 9, 7, 10, 9, 7, 10, 7, 8, 7, 7, 9, 8, 7, 7, 8, 9, 7, 8, 7, 7, 10, 10, 8, 10, 10, 8, 10, 10, 10, 10, 8, 7, 9, 7, 9, 7, 8, 10, 10, 10, 10, 7, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 10, 9, 9, 7, 9, 10, 8, 10, 8, 9, 8, 10, 8, 10, 9, 10, 8, 7, 8, 10, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 10, 8, 7, 9, 8, 9, 9, 8, 7, 8, 9, 7, 10, 8, 10, 8, 10, 9, 8, 10, 8, 7, 8, 10, 9, 8, 7, 7, 8, 8, 10, 8, 8, 9, 8, 8, 10, 8, 8, 10, 9, 9, 9, 10, 10, 10, 9, 7, 9, 8, 7, 7, 10, 10, 8, 10, 8, 8, 10, 7, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 8, 7, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 10, 9, 9, 10, 9, 8, 10, 8, 7, 8, 8, 7, 8, 8, 7, 8, 9, 8, 8, 8, 7, 7, 8, 10, 7, 7, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 9, 9, 10, 7, 9, 7, 9, 7, 8, 9, 9, 9, 10, 10, 10, 10, 8, 7, 8, 8, 7, 10, 9, 10, 10, 9, 9, 8, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 10, 8, 9, 7, 7, 8, 10, 8, 8, 10, 9, 7, 8, 8, 10, 8, 7, 9, 9, 10, 9, 7, 10, 8, 10, 9, 8, 8, 9, 9, 8, 8, 10, 10, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 7, 10, 9, 7, 9, 8, 8, 7, 10, 8, 7, 8, 7, 8, 8, 8, 7, 9, 8, 8, 8, 7, 9, 7, 9, 7, 8, 10, 10, 10, 10, 7, 7, 10, 10, 10, 7, 10, 7, 8, 7, 10, 10, 9, 10, 8, 7, 10, 10, 10, 10, 10, 7, 7, 10, 10, 10, 10, 7, 9, 7, 7, 10, 7, 10, 8, 10, 9, 10, 8, 7, 7, 7, 10, 9, 7, 10, 10, 8, 10, 9, 7, 7, 8, 7, 10, 7, 7, 8, 7, 10, 9, 7, 7, 10, 8, 10, 7, 9, 10, 9, 10, 9, 9, 7, 7, 7, 7, 7, 10, 9, 10, 10, 10, 7, 10, 7, 7, 10, 8, 7, 9, 7, 10, 7, 10, 10, 9, 10, 9, 10, 10, 7, 7, 9, 7, 7, 8, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 10, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 7, 9, 8, 7, 10, 7, 9, 10, 7, 10, 10, 9, 10, 8, 7, 10, 10, 10, 7, 9, 10, 10, 10, 10, 8, 7, 8, 10, 10, 8, 10, 10, 7, 10, 9, 10, 10, 10, 7, 7, 10, 9, 7, 8, 7, 7, 8, 10, 10, 10, 7, 10, 7, 7, 7, 10, 9, 8, 10, 9, 9, 7, 8, 10, 7, 10, 10, 10, 10, 7, 7, 7, 8, 10, 7, 8, 7, 7, 7, 10, 10, 10, 7, 7, 7, 7, 8, 7, 10, 9, 9, 10, 10, 10, 7, 10, 7, 7, 7, 8, 8, 10, 10, 10, 10, 7, 8, 8, 10, 9, 10, 7, 8, 10, 9, 8, 7, 10, 7, 10, 10, 7, 7, 10, 7, 7, 7, 8, 7, 7, 10, 10, 10, 10, 9, 7, 7, 8, 10, 7, 7, 10, 10, 10, 10, 7, 7, 7, 10, 10, 7, 9, 9, 8, 7, 10, 10, 8, 8, 10, 7, 10, 8, 7, 8, 8, 7, 10, 10, 7, 10, 7, 7, 9, 9, 7, 7, 9, 7, 9, 8, 10, 7, 7, 7, 7, 8, 8, 10, 9, 7, 10, 7, 8, 8, 7, 9, 10, 10, 9, 10, 10, 7, 10, 10, 10, 10, 9, 9, 8, 10, 8, 7, 8, 7, 9, 10, 7, 9, 9, 9, 8, 10, 10, 8, 7, 8, 10, 7, 9, 10, 9, 10, 10, 7, 9, 9, 7, 10, 8, 7, 8, 7, 10, 10, 10, 10, 8, 8, 10, 8, 8, 8, 10, 10, 9, 9, 7, 8, 8, 10, 9, 7, 9, 7, 7, 10, 8, 7, 9, 9, 10, 9, 10, 10, 10, 10, 10, 8, 9, 10, 10, 10, 7, 7, 10, 9, 8, 7, 10, 10, 10, 7, 10, 10, 10, 10, 8, 7, 10, 10, 8, 7, 10, 10, 8, 7, 10, 10, 10, 7, 9, 9, 7, 10, 10, 10, 8, 10, 7, 7, 7, 7, 10, 9, 10, 10, 7, 9, 7, 9, 9, 10, 7, 7, 10, 10, 10, 9, 10, 10, 10, 7, 7, 7, 7, 8, 7, 7, 7, 10, 7, 7, 8, 7, 10, 9, 9, 10, 9, 10, 10, 8, 10, 10, 10, 10, 9, 9, 7, 7, 7, 10, 7, 10, 7, 8, 7, 9, 10, 10, 10, 7, 10, 8, 10, 10, 7, 9, 7, 10, 7, 10, 7, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 7, 10, 10, 10, 10, 7, 7, 7, 9, 9, 10, 7, 9, 7, 7, 8, 10, 8, 7, 8, 10, 9, 8, 7, 9, 9, 10, 10, 10, 7, 7, 7, 7, 9, 7, 7, 10, 7, 7, 10, 9, 7, 7, 10, 7, 10, 10, 9, 10, 10, 7, 8, 7, 9, 10, 9, 10, 10, 10, 7, 9, 10, 9, 7, 10, 7, 7, 10, 8, 7, 8, 10, 9, 7, 7, 7, 7, 10, 10, 8, 7, 8, 9, 10, 10, 7, 7, 7, 7, 10, 7, 10, 7, 8, 10, 10, 10, 7, 7, 9, 8, 10, 10, 9, 10, 8, 8, 7, 7, 8, 9, 8, 9, 8, 9, 9, 8, 8, 10, 9, 8, 7, 9, 9, 8, 10, 9, 8, 7, 10, 9, 8, 7, 9, 8, 8, 8, 7, 9, 9, 7, 10, 9, 9, 8, 10, 10, 10, 9, 7, 7, 10, 9, 10, 9, 9, 8, 10, 8, 7, 7, 8, 7, 8, 7, 7, 7, 10, 10, 10, 9, 10, 7, 7, 7, 8, 10, 10, 10, 8, 10, 10, 7, 7, 7, 7, 8, 7, 9, 10, 7, 10, 9, 7, 9, 7, 10, 10, 10, 10, 10, 10, 10, 9, 8, 7, 7, 10, 10, 10, 10, 10, 10, 7, 7, 7, 9, 8, 10, 8, 7, 10, 8, 7, 9, 8, 10, 7, 10, 10, 7, 10, 10, 7, 9, 10, 10, 7, 8, 10, 9, 10, 7, 10, 10, 10, 10, 7, 10, 7, 10, 9, 10, 9, 9, 8, 8, 8, 7, 7, 9, 7, 8, 7, 7, 10, 10, 8, 10, 10, 8, 10, 10, 8, 10, 10, 10, 7, 7, 9, 9, 9, 10, 9, 7, 8, 8, 8, 7, 9, 9, 9, 7, 7, 9, 8, 8, 7, 7, 7, 7, 9, 7, 10, 10, 9, 9, 7, 8, 7, 8, 8, 8, 8, 7, 9, 8, 10, 10, 10, 7, 7, 7, 10, 9, 10, 7, 9, 7, 9, 10, 7, 10, 10, 10, 7, 7, 9, 7, 9, 9, 10, 10, 8, 7, 9, 10, 10, 7, 7, 7, 10, 9, 10, 7, 8, 10, 7, 7, 7, 10, 8, 7, 9, 7, 8, 10, 10, 9, 10, 9, 10, 9, 7, 10, 10, 10, 7, 9, 7, 8, 7, 8, 7, 7, 7, 7, 10, 8, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 9, 10, 10, 10, 9, 9, 10, 8, 10, 10, 10, 10, 7, 7, 7, 7, 9, 7, 10, 7, 8, 10, 9, 8, 7, 10, 9, 10, 7, 8, 7, 7, 7, 8, 7, 10, 9, 7, 10, 10, 10, 8, 10, 7, 7, 10, 7, 7, 10, 10, 9, 7, 9, 9, 9, 10, 10, 10, 7, 10, 10, 7, 7, 9, 10, 10, 10, 10, 10, 8, 8, 8, 10, 10, 10, 7, 9, 9, 8, 10, 7, 7, 7, 9, 10, 10, 10, 10, 7, 10, 10, 10, 10, 7, 10, 10, 8, 10, 9, 8, 10, 10, 9, 9, 7, 10, 7, 9, 10, 7, 7, 7, 10, 7, 8, 7, 9, 8, 7, 10, 10, 10, 10, 7, 7, 7, 7, 10, 9, 7, 10, 9, 7, 10, 9, 8, 10, 7, 7, 9, 10, 9, 8, 10, 10, 10, 10, 10, 10, 7, 7, 10, 7, 10, 7, 10, 9, 8, 7, 10, 9, 8, 10, 9, 8, 10, 10, 8, 7, 7, 10, 7, 9, 9, 10, 8, 10, 10, 10, 10, 7, 7, 7, 10, 7, 10, 7, 9, 8, 8, 7, 7, 9, 9, 8, 7, 9, 9, 9, 10, 7, 7, 10, 7, 10, 10, 10, 8, 8, 10, 9, 7, 9, 10, 9, 7, 10, 9, 7, 10, 7, 9, 7, 7, 10, 10, 7, 10, 7, 9, 7, 7, 7, 7, 10, 8, 7, 10, 9, 7, 7, 7, 9, 7, 10, 9, 10, 9, 10, 9, 8, 8, 8, 7, 10, 9, 10, 10, 10, 8, 7, 10, 10, 10, 10, 7, 10, 7, 7, 8, 10, 9, 10, 10, 8, 8, 7, 9, 10, 8, 7, 10, 7, 10, 10, 9, 8, 10, 7, 10, 9, 7, 7, 10, 10, 8, 10, 7, 9, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 8, 8, 10, 9, 8, 8, 7, 8, 7, 8, 10, 10, 7, 10, 10, 10, 10, 9, 9, 7, 10, 9, 10, 10, 7, 10, 7, 10, 7, 8, 7, 8, 7, 10, 7, 7, 10, 9, 8, 8, 10, 8, 7, 7, 7, 10, 10, 10, 8, 8, 7, 10, 10, 7, 10, 10, 10, 8, 7, 10, 7, 10, 10, 7, 10, 10, 10, 10, 10, 8, 10, 8, 8, 7, 10, 10, 10, 8, 8, 8, 10, 8, 9, 10, 9, 10, 8, 7, 7, 9, 10, 10, 10, 8, 7, 10, 10, 10, 10, 9, 10, 10, 10, 10, 8, 7, 7, 10, 9, 10, 10, 10, 10, 9, 10, 10, 10, 10, 10, 7, 10, 7, 7, 10, 7, 10, 10, 9, 8, 8, 7, 10, 10, 10, 10, 7, 7, 10, 9, 9, 8, 10, 10, 8, 7, 7, 10, 8, 7, 10, 9, 8, 10, 7, 10, 9, 7, 8, 7, 9, 7, 10, 10, 9, 10, 9, 8, 8, 7, 7, 10, 10, 10, 7, 9, 8, 10, 10, 10, 7, 7, 7, 7, 7, 7, 8, 10, 8, 7, 10, 9, 9, 9, 8, 10, 7, 8, 8, 10, 8, 8, 7, 10, 9, 7, 7, 8, 10, 8, 7, 8, 7, 8, 7, 10, 9, 8, 9, 9, 8, 7, 10, 10, 9, 10, 10, 7, 7, 7, 9, 7, 7, 8, 8, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 10, 7, 7, 7, 10, 10, 10, 10, 8, 10, 8, 8, 7, 9, 8, 10, 10, 10, 10, 8, 7, 10, 10, 10, 10, 9, 8, 7, 9, 8, 7, 10, 7, 8, 7, 7, 7, 7, 7, 7, 7, 10, 10, 9, 7, 7, 7, 7, 7, 10, 10, 9, 9, 10, 7, 10, 7, 8, 8, 7, 8, 8, 10, 7, 7, 7, 10, 8, 8, 7, 7, 8, 8, 7, 10, 10, 7, 10, 10, 7, 7, 8, 7, 10, 10, 10, 10, 9, 8, 8, 7, 8, 7, 8, 10, 7, 9, 8, 10, 10, 10, 7, 10, 8, 10, 8, 10, 9, 10, 10, 9, 9, 9, 7, 8, 7, 8, 10, 10, 10, 7, 8, 8, 8, 8, 10, 7, 8, 7, 10, 7, 8, 10, 8, 8, 7, 9, 8, 7, 10, 9, 8, 9, 10, 9, 7, 8, 8, 10, 7, 7, 7, 9, 7, 10, 7, 10, 9, 9, 7, 8, 7, 10, 8, 10, 10, 8, 10, 10, 7, 7, 10, 10, 7, 7, 8, 8, 10, 8, 7, 9, 10, 9, 8, 8, 7, 10, 10, 7, 10, 9, 7, 8, 7, 8, 8, 10, 7, 7, 9, 7, 7, 7, 7, 10, 10, 10, 7, 7, 7, 10, 10, 7, 7, 10, 10, 8, 7, 9, 9, 10, 8, 7, 8, 10, 10, 8, 7, 10, 8, 10, 7, 7, 9, 10, 10, 8, 7, 7, 7, 10, 10, 8, 7, 7, 7, 10, 7, 8, 7, 7, 10, 10, 9, 8, 8, 8, 8, 10, 8, 10, 9, 10, 7, 7, 7, 8, 9, 10, 9, 9, 9, 9, 9, 7, 10, 10, 9, 9, 10, 10, 8, 8, 7, 9, 9, 7, 8, 8, 8, 8, 8, 7, 8, 8, 10, 8, 9, 10, 9, 10, 7, 10, 7, 8, 8, 7, 7, 7, 7, 10, 10, 10, 9, 9, 9, 8, 7, 8, 7, 8, 10, 8, 7, 7, 9, 10, 8, 8, 10, 10, 7, 7, 9, 9, 9, 9, 7, 7, 7, 8, 8, 9, 8, 9, 10, 7, 10, 7, 7, 9, 7, 7, 7, 7, 9, 10, 8, 7, 9, 8, 8, 8, 10, 8, 8, 8, 10, 7, 10, 9, 9, 9, 10, 9, 9, 9, 10, 10, 10, 8, 7, 8, 7, 10, 8, 8, 10, 9, 8, 7, 7, 7, 10, 7, 7, 8, 10, 9, 10, 9, 10, 10, 10, 10, 8, 8, 7, 10, 8, 10, 9, 8, 7, 10, 10, 10, 9, 9, 8, 10, 9, 7, 7, 7, 7, 7, 7, 7, 7, 10, 8, 10, 9, 8, 7, 10, 7, 10, 7, 7, 9, 10, 9, 9, 7, 8, 8, 8, 7, 10, 9, 8, 7, 9, 10, 10, 8, 7, 7, 7, 10, 8, 10, 9, 10, 10, 9, 10, 10, 8, 7, 7, 9, 7, 9, 10, 8, 7, 7, 8, 10, 9, 10, 7, 10, 10, 8, 8, 8, 7, 9, 10, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 10, 9, 7, 10, 10, 10, 10, 10, 10, 7, 10, 7, 9, 8, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 10, 9, 9, 7, 7, 8, 9, 7, 9, 9, 9, 10, 7, 7, 7, 7, 10, 8, 7, 7, 9, 9, 10, 10, 7, 7, 10, 9, 8, 10, 10, 10, 9, 10, 7, 10, 10, 10, 9, 7, 10, 10, 9, 10, 8, 10, 8, 8, 10, 10, 10, 9, 9, 10, 8, 10, 10, 9, 7, 7, 9, 10, 8, 8, 8, 8, 10, 10, 8, 8, 10, 9, 8, 7, 8, 8, 7, 8, 8, 7, 8, 8, 10, 9, 8, 8, 8, 8, 10, 8, 7, 7, 10, 9, 9, 8, 10, 10, 10, 10, 10, 10, 7, 10, 7, 8, 8, 7, 7, 9, 7, 9, 7, 8, 10, 9, 7, 7, 7, 7, 10, 9, 8, 10, 7, 8, 8, 10, 8, 10, 7, 7, 10, 9, 7, 8, 10, 10, 10, 10, 8, 10, 8, 8, 8, 8, 7, 10, 9, 7, 10, 7, 9, 10, 7, 8, 10, 8, 10, 10, 10, 9, 7, 7, 9, 9, 7, 8, 8, 7, 7, 7, 9, 10, 10, 7, 8, 10, 10, 10, 9, 8, 10, 8, 8, 10, 7, 10, 10, 7, 10, 9, 10, 9, 9, 10, 10, 10, 9, 7, 10, 10, 10, 9, 10, 10, 8, 8, 8, 10, 10, 10, 7, 10, 10, 8, 8, 10, 10, 10, 8, 7, 10, 9, 10, 8, 8, 10, 10, 10, 9, 7, 7, 8, 10, 9, 9, 7, 7, 9, 10, 8, 7, 7, 9, 10, 8, 10, 7, 10, 7, 9, 9, 10, 10, 10, 9, 7, 10, 10, 7, 8, 8, 10, 10, 8, 7, 9, 9, 10, 10, 7, 7, 7, 10, 9, 10, 7, 10, 10, 9, 9, 9, 10, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 8, 7, 8, 7, 9, 7, 10, 10, 10, 10, 7, 10, 7, 10, 10, 9, 10, 7, 10, 8, 7, 10, 7, 10, 10, 7, 7, 7, 7, 9, 9, 8, 7, 8, 7, 9, 7, 10, 7, 7, 10, 8, 10, 8, 7, 10, 10, 7, 10, 10, 7, 7, 10, 9, 10, 10, 9, 9, 10, 7, 7, 10, 10, 7, 10, 10, 10, 9, 9, 10, 10, 7, 7, 9, 7, 10, 9, 7, 10, 9, 7, 8, 10, 7, 8, 8, 8, 8, 7, 10, 10, 9, 10, 7, 7, 7, 9, 9, 10, 10, 7, 10, 10, 9, 10, 7, 7, 7, 9, 10, 10, 8, 10, 8, 8, 8, 10, 9, 10, 9, 9, 10, 7, 7, 7, 9, 9, 10, 8, 7, 10, 10, 10, 9, 10, 10, 8, 7, 8, 8, 8, 10, 10, 7, 9, 10, 7, 7, 10, 8, 10, 7, 10, 9, 9, 7, 9, 10, 7, 7, 10, 9, 10, 10, 10, 10, 7, 9, 8, 7, 8, 8, 7, 9, 9, 7, 9, 7, 7, 10, 7, 10, 10, 8, 10, 9, 10, 10, 10, 8, 8, 8, 7, 7, 10, 9, 8, 8, 10, 10, 10, 7, 9, 8, 7, 10, 8, 7, 7, 10, 10, 9, 7, 10, 9, 7, 10, 10, 8, 10, 10, 9, 7, 8, 10, 9, 7, 7, 10, 8, 7, 10, 10, 10, 7, 10, 10, 7, 8, 7, 8, 10, 9, 9, 9, 9, 10, 9, 8, 7, 7, 7, 7, 10, 9, 9, 10, 9, 7, 10, 7, 7, 7, 10, 10, 10, 10, 10, 10, 10, 10, 7, 9, 10, 9, 8, 10, 8, 10, 7, 7, 7, 10, 10, 10, 8, 10, 10, 8, 10, 7, 8, 7, 10, 10, 10, 7, 10, 10, 7, 9, 10, 10, 10, 7, 10, 7, 10, 10, 8, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 7, 7, 7, 7, 7, 7, 9, 10, 7, 10, 7, 10, 10, 10, 8, 8, 8, 10, 8, 8, 10, 9, 10, 10, 10, 8, 10, 8, 10, 7, 7, 10, 7, 10, 8, 8, 8, 10, 8, 10, 10, 10, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 8, 8, 7, 10, 9, 10, 8, 10, 10, 8, 7, 10, 9, 7, 7, 10, 10, 10, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 8, 10, 8, 7, 7, 10, 7, 10, 7, 10, 10, 7, 10, 7, 9, 8, 8, 7, 7, 10, 8, 9, 9, 10, 8, 7, 8, 7, 9, 10, 10, 8, 10, 10, 10, 10, 10, 9, 7, 10, 9, 8, 10, 8, 7, 10, 7, 10, 7, 10, 7, 10, 10, 9, 10, 9, 7, 9, 8, 8, 10, 8, 10, 7, 9, 8, 8, 8, 7, 10, 9, 10, 7, 7, 9, 10, 8, 7, 8, 7, 9, 8, 7, 7, 9, 8, 10, 7, 9, 8, 10, 8, 10, 10, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 10, 9, 9, 7, 9, 10, 10, 10, 8, 7, 8, 10, 8, 10, 9, 10, 8, 7, 8, 8, 8, 7, 7, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 7, 8, 9, 7, 10, 8, 10, 8, 9, 9, 8, 10, 8, 7, 8, 10, 9, 8, 7, 7, 8, 8, 10, 8, 8, 7, 8, 8, 8, 10, 8, 8, 9, 9, 9, 10, 10, 8, 7, 7, 7, 10, 9, 7, 10, 10, 8, 10, 8, 8, 10, 10, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 10, 9, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 8, 10, 7, 10, 7, 9, 9, 10, 9, 8, 8, 10, 9, 8, 8, 7, 8, 8, 9, 8, 9, 8, 8, 10, 9, 9, 8, 10, 7, 7, 10, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 7, 9, 7, 9, 7, 10, 9, 9, 9, 9, 10, 10, 10, 8, 7, 8, 8, 7, 10, 8, 10, 10, 9, 9, 8, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 10, 10, 9, 7, 7, 8, 10, 8, 8, 10, 9, 7, 8, 8, 10, 8, 9, 10, 9, 7, 10, 8, 8, 7, 8, 8, 8, 9, 8, 8, 10, 8, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 7, 10, 9, 7, 9, 8, 8, 7, 8, 8, 9, 8, 9, 8, 8, 8, 9, 9, 8, 8, 7, 8, 10, 8, 10, 10, 9, 10, 9, 10, 10, 10, 10, 10, 10, 7, 7, 7, 8, 7, 10, 9, 7, 8, 8, 7, 8, 7, 10, 10, 7, 9, 10, 8, 10, 10, 10, 10, 7, 9, 10, 8, 8, 10, 10, 7, 10, 10, 10, 9, 8, 10, 10, 10, 7, 9, 9, 8, 7, 8, 7, 7, 9, 7, 7, 7, 10, 8, 10, 9, 7, 9, 9, 8, 10, 10, 7, 8, 8, 10, 10, 7, 10, 7, 10, 10, 10, 9, 10, 8, 10, 10, 9, 10, 10, 8, 8, 7, 7, 7, 10, 10, 10, 9, 9, 7, 10, 10, 7, 10, 8, 7, 9, 8, 10, 7, 7, 10, 10, 8, 10, 8, 8, 7, 7, 9, 9, 7, 9, 8, 8, 8, 10, 7, 10, 7, 8, 8, 10, 10, 10, 10, 7, 7, 9, 9, 9, 9, 7, 9, 10, 9, 9, 9, 7, 10, 10, 10, 7, 10, 10, 10, 7, 7, 10, 10, 10, 10, 10, 10, 8, 10, 10, 10, 9, 9, 7, 9, 10, 8, 8, 10, 10, 8, 10, 7, 9, 8, 8, 7, 9, 10, 9, 7, 7, 10, 9, 9, 7, 7, 10, 10, 10, 7, 10, 10, 7, 10, 7, 9, 7, 7, 10, 8, 8, 7, 10, 10, 9, 10, 10, 7, 8, 10, 9, 7, 9, 8, 10, 9, 10, 8, 7, 10, 10, 9, 10, 10, 10, 8, 10, 7, 9, 9, 8, 8, 7, 10, 10, 10, 10, 7, 9, 10, 9, 9, 7, 8, 7, 10, 7, 9, 8, 10, 7, 9, 9, 7, 7, 7, 10, 7, 8, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 9, 7, 10, 8, 7, 10, 9, 7, 7, 10, 10, 8, 7, 7, 7, 10, 10, 7, 7, 10, 7, 10, 10, 10, 7, 8, 7, 7, 8, 10, 7, 7, 7, 7, 10, 10, 10, 7, 9, 8, 7, 7, 8, 7, 10, 9, 9, 8, 8, 7, 9, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 10, 8, 8, 8, 10, 9, 10, 7, 7, 10, 10, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 9, 9, 7, 9, 10, 8, 10, 9, 7, 9, 9, 10, 9, 9, 9, 7, 9, 9, 7, 10, 8, 7, 8, 10, 10, 7, 7, 9, 9, 8, 8, 7, 9, 9, 9, 9, 10, 10, 8, 7, 7, 9, 7, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 8, 7, 7, 8, 7, 9, 7, 9, 10, 9, 7, 9, 7, 10, 8, 8, 10, 9, 10, 8, 10, 8, 10, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 10, 7, 7, 7, 10, 10, 10, 10, 10, 10, 8, 9, 7, 7, 7, 7, 9, 10, 10, 10, 10, 7, 10, 7, 10, 9, 7, 7, 7, 7, 9, 10, 9, 10, 7, 8, 10, 8, 10, 9, 7, 7, 7, 7, 7, 7, 10, 8, 10, 7, 9, 8, 10, 9, 10, 8, 7, 10, 7, 8, 8, 10, 7, 10, 8, 8, 8, 10, 8, 8, 7, 10, 10, 8, 8, 10, 7, 7, 8, 8, 7, 10, 8, 10, 8, 10, 10, 7, 10, 7, 7, 9, 10, 7, 7, 10, 10, 7, 10, 10, 7, 10, 8, 8, 7, 10, 8, 10, 8, 10, 10, 7, 10, 7, 7, 9, 10, 7, 7, 10, 10, 7, 10, 10, 7, 10, 7, 7, 9, 10, 7, 10, 10, 10, 10, 8, 10, 10, 7, 9, 10, 10, 10, 10, 8, 8, 7, 7, 10, 10, 10, 7, 10, 8, 10, 10, 10, 8, 7, 10, 9, 10, 9, 10, 10, 10, 8, 10, 10, 10, 10, 10, 7, 7, 7, 7, 7, 9, 8, 7, 7, 7, 8, 7, 7, 7, 10, 7, 10, 9, 8, 7, 10, 7, 10, 10, 9, 8, 10, 9, 10, 7, 8, 10, 10, 7, 8, 10, 9, 10, 9, 8, 8, 10, 8, 10, 10, 10, 8, 7, 8, 7, 7, 8, 7, 10, 9, 9, 9, 9, 10, 8, 10, 10, 10, 7, 10, 7, 7, 7, 7, 10, 10, 10, 8, 10, 10, 10, 9, 9, 7, 10, 9, 10, 10, 10, 7, 9, 7, 7, 7, 9, 9, 10, 10, 10, 9, 10, 7, 10, 9, 8, 10, 8, 10, 7, 7, 10, 10, 10, 7, 10, 7, 8, 10, 9, 7, 10, 7, 7, 8, 7, 10, 10, 10, 10, 7, 10, 7, 10, 7, 7, 10, 9, 8, 7, 10, 10, 10, 7, 9, 7, 9, 10, 8, 8, 8, 8, 7, 7, 10, 10, 10, 9, 8, 10, 10, 9, 10, 8, 10, 7, 9, 9, 8, 8, 10, 8, 10, 7, 8, 10, 7, 10, 10, 10, 7, 7, 10, 10, 10, 9, 7, 8, 7, 8, 8, 10, 10, 10, 7, 7, 7, 10, 9, 7, 10, 9, 10, 10, 8, 10, 10, 10, 9, 7, 8, 10, 8, 10, 10, 7, 10, 8, 10, 7, 7, 10, 10, 9, 10, 8, 10, 7, 10, 9, 10, 9, 7, 8, 8, 9, 10, 8, 7, 9, 10, 9, 7, 7, 10, 7, 10, 7, 10, 10, 7, 10, 8, 10, 9, 10, 10, 8, 7, 10, 7, 10, 10, 10, 10, 8, 10, 9, 10, 7, 9, 10, 8, 10, 10, 7, 10, 10, 10, 8, 10, 10, 10, 10, 10, 9, 7, 10, 10, 7, 9, 7, 7, 8, 7, 10, 7, 10, 7, 7, 10, 7, 10, 10, 10, 7, 8, 7, 8, 7, 8, 10, 7, 10, 10, 10, 10, 8, 8, 8, 7, 10, 8, 8, 10, 7, 7, 10, 8, 8, 8, 8, 7, 8, 8, 10, 10, 10, 9, 10, 7, 7, 10, 7, 7, 7, 7, 10, 10, 7, 7, 10, 10, 7, 7, 7, 7, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 9, 10, 7, 10, 7, 10, 10, 10, 7, 7, 10, 7, 7, 8, 8, 10, 7, 8, 10, 9, 7, 10, 7, 8, 10, 10, 7, 8, 8, 10, 9, 7, 7, 9, 10, 8, 10, 8, 10, 8, 10, 9, 9, 10, 8, 7, 7, 8, 10, 10, 10, 10, 9, 9, 8, 10, 9, 9, 7, 10, 9, 7, 7, 9, 8, 10, 8, 10, 10, 8, 10, 9, 10, 7, 9, 7, 9, 10, 7, 10, 10, 8, 7, 7, 9, 9, 7, 9, 9, 9, 8, 10, 10, 9, 10, 7, 7, 9, 10, 7, 8, 10, 9, 10, 7, 8, 7, 7, 10, 7, 10, 10, 8, 10, 8, 10, 9, 7, 9, 10, 10, 9, 10, 9, 7, 8, 7, 10, 9, 10, 10, 10, 7, 7, 7, 7, 10, 10, 10, 8, 10, 10, 8, 10, 9, 9, 7, 9, 8, 8, 10, 7, 9, 7, 10, 7, 8, 10, 10, 7, 7, 7, 9, 10, 7, 8, 7, 8, 8, 10, 10, 9, 9, 8, 8, 7, 10, 9, 10, 7, 9, 7, 7, 7, 7, 10, 8, 8, 10, 10, 9, 7, 8, 7, 7, 7, 8, 7, 10, 10, 10, 8, 8, 10, 10, 10, 10, 8, 10, 10, 9, 7, 9, 10, 10, 10, 9, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 10, 9, 8, 10, 10, 8, 7, 8, 10, 9, 10, 10, 9, 9, 9, 10, 10, 9, 8, 10, 10, 9, 7, 10, 9, 10, 10, 7, 10, 8, 7, 10, 8, 7, 7, 9, 7, 7, 9, 10, 9, 10, 9, 9, 8, 7, 8, 8, 7, 8, 10, 10, 9, 7, 10, 10, 10, 10, 10, 10, 10, 8, 8, 8, 10, 10, 10, 10, 9, 9, 7, 7, 7, 10, 9, 7, 8, 7, 10, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 9, 10, 10, 10, 10, 10, 10, 7, 8, 8, 10, 7, 9, 7, 9, 7, 8, 8, 8, 7, 9, 7, 10, 7, 7, 10, 10, 10, 10, 10, 10, 8, 10, 7, 10, 7, 10, 10, 7, 10, 10, 10, 10, 7, 10, 8, 10, 7, 10, 9, 7, 10, 8, 8, 10, 10, 10, 10, 10, 7, 10, 8, 10, 8, 7, 10, 10, 10, 10, 10, 9, 10, 10, 8, 10, 8, 7, 10, 7, 9, 8, 10, 9, 10, 10, 10, 10, 7, 10, 10, 8, 8, 10, 8, 10, 10, 8, 10, 8, 10, 9, 10, 7, 10, 10, 10, 8, 10, 10, 7, 8, 10, 10, 10, 7, 8, 10, 10, 10, 10, 7, 10, 10, 10, 9, 10, 9, 10, 8, 10, 8, 10, 8, 7, 9, 9, 10, 7, 8, 8, 8, 10, 9, 10, 9, 7, 10, 10, 10, 7, 9, 10, 10, 10, 10, 8, 7, 10, 10, 10, 10, 10, 7, 9, 7, 10, 9, 7, 10, 10, 10, 10, 7, 10, 8, 10, 10, 10, 10, 8, 7, 9, 10, 7, 10, 7, 10, 10, 9, 8, 8, 10, 7, 10, 9, 10, 10, 10, 9, 10, 10, 8, 7, 7, 8, 10, 8, 10, 8, 7, 10, 10, 10, 10, 7, 8, 7, 10, 8, 10, 10, 7, 8, 10, 7, 10, 10, 9, 7, 9, 10, 10, 10, 7, 7, 7, 7, 7, 10, 10, 10, 10, 10, 7, 10, 10, 10, 9, 8, 10, 9, 10, 9, 10, 10, 8, 10, 10, 10, 10, 7, 10, 7, 10, 9, 10, 9, 8, 7, 7, 10, 10, 7, 7, 10, 9, 7, 10, 7, 10, 10, 8, 7, 10, 9, 10, 10, 9, 9, 7, 9, 10, 9, 10, 10, 9, 10, 7, 10, 7, 10, 10, 7, 10, 10, 10, 9, 8, 8, 10, 8, 10, 9, 8, 10, 10, 8, 7, 10, 7, 10, 10, 9, 10, 8, 10, 7, 9, 7, 9, 9, 9, 10, 7, 9, 10, 7, 10, 10, 10, 10, 8, 10, 7, 10, 10, 9, 7, 10, 10, 9, 7, 7, 7, 7, 9, 10, 10, 10, 10, 8, 7, 10, 10, 10, 10, 8, 10, 9, 10, 10, 10, 10, 10, 10, 7, 10, 7, 7, 10, 7, 9, 8, 8, 10, 10, 9, 10, 7, 10, 9, 9, 7, 10, 10, 10, 10, 10, 9, 9, 10, 9, 9, 8, 10, 10, 10, 10, 8, 10, 7, 10, 10, 8, 7, 10, 9, 7, 10, 10, 7, 7, 7, 10, 9, 10, 9, 10, 7, 9, 9, 7, 10, 8, 10, 10, 8, 10, 10, 8, 7, 7, 10, 8, 7, 9, 8, 7, 7, 10, 7, 7, 8, 7, 9, 9, 10, 9, 9, 8, 10, 8, 10, 7, 10, 7, 9, 7, 7, 10, 9, 9, 7, 9, 9, 9, 10, 7, 9, 7, 7, 9, 9, 9, 7, 10, 9, 10, 9, 9, 9, 10, 9, 7, 8, 10, 10, 7, 8, 10, 8, 7, 9, 10, 10, 10, 10, 10, 7, 9, 10, 10, 7, 7, 7, 9, 7, 9, 9, 7, 8, 8, 8, 10, 8, 10, 10, 8, 10, 9, 10, 10, 7, 9, 8, 7, 10, 9, 9, 10, 9, 7, 7, 9, 10, 9, 8, 7, 9, 10, 10, 10, 8, 10, 10, 10, 7, 7, 10, 7, 7, 7, 10, 10, 9, 10, 9, 8, 7, 10, 9, 9, 10, 9, 9, 9, 9, 9, 10, 9, 9, 9, 7, 10, 10, 10, 9, 9, 7, 10, 10, 8, 10, 9, 10, 9, 7, 10, 7, 8, 7, 7, 10, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 10, 7, 9, 9, 7, 7, 10, 8, 10, 10, 10, 10, 7, 8, 10, 10, 10, 10, 9, 9, 8, 8, 7, 8, 10, 10, 9, 8, 8, 10, 10, 10, 8, 10, 10, 10, 8, 8, 7, 7, 9, 9, 7, 7, 10, 8, 8, 8, 7, 8, 10, 8, 8, 8, 10, 10, 10, 8, 7, 7, 9, 9, 10, 9, 8, 8, 10, 8, 7, 10, 9, 7, 7, 8, 10, 9, 10, 10, 10, 10, 8, 7, 10, 9, 7, 7, 8, 10, 10, 10, 8, 8, 7, 7, 7, 8, 7, 10, 10, 9, 9, 10, 10, 10, 8, 10, 9, 8, 10, 10, 9, 10, 10, 10, 8, 10, 7, 7, 9, 8, 8, 10, 9, 7, 10, 10, 8, 10, 10, 9, 9, 8, 8, 10, 10, 8, 10, 8, 7, 10, 10, 7, 7, 10, 10, 10, 10, 8, 7, 7, 7, 7, 8, 10, 10, 8, 8, 7, 7, 10, 7, 10, 8, 8, 10, 10, 8, 8, 7, 7, 7, 10, 7, 7, 10, 10, 8, 8, 8, 10, 10, 10, 10, 9, 8, 10, 10, 7, 8, 9, 10, 10, 7, 9, 8, 9, 7, 9, 10, 7, 8, 10, 7, 9, 10, 10, 10, 9, 10, 10, 7, 9, 8, 8, 7, 9, 10, 9, 10, 10, 7, 9, 10, 10, 10, 8, 10, 9, 10, 10, 9, 7, 10, 8, 8, 10, 7, 7, 8, 8, 7, 7, 7, 7, 7, 7, 8, 8, 8, 10, 7, 7, 8, 10, 9, 7, 9, 7, 10, 7, 10, 8, 7, 9, 10, 8, 10, 8, 10, 10, 7, 9, 8, 9, 8, 7, 7, 7, 9, 10, 10, 10, 9, 10, 9, 7, 7, 10, 7, 8, 9, 9, 10, 8, 7, 10, 8, 8, 8, 10, 8, 8, 7, 10, 7, 10, 9, 7, 9, 9, 7, 9, 9, 9, 9, 7, 9, 10, 9, 9, 9, 7, 7, 10, 10, 9, 9, 10, 10, 9, 10, 9, 9, 9, 7, 8, 10, 10, 8, 8, 9, 10, 9, 9, 7, 10, 7, 10, 8, 8, 7, 7, 7, 10, 8, 7, 9, 10, 9, 9, 7, 10, 9, 8, 10, 7, 7, 7, 9, 10, 8, 8, 8, 10, 10, 7, 8, 7, 10, 7, 7, 7, 7, 10, 9, 9, 8, 7, 10, 7, 9, 10, 7, 10, 10, 10, 9, 9, 10, 10, 7, 10, 8, 7, 10, 8, 8, 10, 8, 8, 8, 8, 7, 10, 7, 8, 7, 10, 8, 7, 10, 8, 10, 8, 10, 7, 9, 7, 10, 10, 7, 10, 10, 10, 7, 10, 7, 9, 10, 7, 8, 10, 10, 7, 7, 10, 7, 8, 7, 7, 10, 9, 10, 7, 7, 10, 9, 8, 10, 9, 10, 9, 10, 7, 9, 10, 8, 7, 10, 7, 8, 10, 9, 10, 7, 10, 10, 10, 10, 10, 10, 7, 8, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 9, 10, 10, 9, 10, 10, 10, 10, 9, 10, 9, 9, 9, 7, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 7, 10, 10, 10, 10, 10, 7, 10, 10, 8, 10, 9, 7, 9, 9, 7, 10, 7, 9, 10, 10, 9, 7, 7, 10, 8, 8, 7, 8, 7, 9, 9, 7, 10, 7, 8, 10, 9, 7, 9, 9, 9, 8, 8, 7, 9, 8, 10, 9, 10, 7, 10, 10, 8, 7, 8, 7, 7, 8, 8, 8, 7, 7, 7, 10, 8, 7, 8, 7, 10, 7, 8, 7, 7, 7, 9, 8, 9, 7, 8, 7, 7, 9, 10, 10, 8, 7, 10, 7, 8, 7, 8, 7, 7, 10, 7, 9, 9, 8, 8, 10, 7, 10, 10, 7, 9, 7, 7, 8, 7, 9, 9, 7, 8, 10, 9, 10, 10, 8, 10, 8, 10, 8, 10, 10, 9, 10, 10, 10, 7, 10, 8, 7, 10, 10, 9, 8, 7, 9, 8, 8, 10, 10, 10, 8, 10, 7, 9, 8, 7, 8, 7, 7, 7, 9, 8, 8, 10, 9, 9, 9, 7, 8, 7, 10, 10, 8, 10, 9, 9, 7, 8, 7, 10, 10, 10, 7, 9, 10, 7, 10, 9, 10, 9, 10, 10, 7, 7, 7, 10, 10, 10, 8, 10, 8, 10, 10, 7, 8, 10, 7, 8, 7, 10, 10, 7, 10, 10, 10, 8, 8, 7, 7, 8, 7, 9, 10, 7, 10, 10, 10, 7, 8, 10, 9, 8, 7, 7, 10, 8, 10, 9, 8, 7, 7, 10, 10, 7, 8, 8, 10, 10, 8, 8, 10, 10, 10, 10, 9, 10, 10, 10, 10, 9, 10, 7, 7, 8, 10, 9, 10, 9, 10, 8, 8, 8, 8, 8, 7, 8, 10, 7, 9, 7, 7, 10, 9, 10, 7, 7, 9, 8, 10, 8, 10, 9, 10, 9, 8, 7, 9, 7, 10, 7, 9, 10, 9, 10, 8, 10, 8, 7, 10, 10, 10, 7, 10, 10, 9, 7, 10, 9, 10, 7, 10, 8, 8, 8, 10, 9, 9, 8, 7, 10, 8, 10, 7, 7, 10, 7, 7, 7, 7, 8, 7, 8, 10, 9, 7, 8, 7, 7, 8, 7, 8, 7, 7, 9, 8, 7, 8, 8, 8, 7, 9, 10, 7, 7, 7, 10, 7, 10, 10, 10, 10, 10, 10, 9, 7, 7, 10, 9, 7, 8, 10, 9, 7, 7, 8, 7, 7, 10, 7, 7, 8, 8, 7, 9, 10, 10, 8, 7, 10, 7, 7, 9, 9, 8, 10, 9, 7, 10, 7, 7, 7, 7, 10, 10, 9, 9, 10, 7, 10, 7, 9, 8, 10, 7, 9, 7, 10, 9, 7, 7, 9, 10, 7, 10, 9, 7, 10, 10, 10, 10, 9, 7, 9, 9, 9, 7, 8, 10, 7, 10, 9, 7, 7, 7, 7, 10, 8, 7, 7, 7, 9, 10, 7, 7, 8, 8, 7, 8, 7, 8, 7, 7, 10, 7, 7, 7, 10, 10, 7, 10, 8, 7, 9, 8, 8, 8, 10, 8, 10, 7, 8, 10, 10, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 8, 7, 7, 9, 8, 10, 8, 8, 10, 9, 9, 9, 7, 7, 10, 10, 9, 7, 7, 10, 10, 7, 10, 9, 7, 7, 7, 10, 8, 10, 7, 10, 8, 7, 10, 7, 10, 10, 7, 8, 10, 10, 10, 8, 10, 8, 10, 7, 7, 7, 9, 7, 7, 8, 10, 10, 8, 7, 7, 9, 10, 10, 9, 9, 9, 10, 9, 10, 8, 7, 7, 8, 10, 7, 7, 7, 7, 7, 9, 10, 10, 9, 8, 7, 9, 9, 8, 9, 7, 9, 9, 8, 9, 8, 9, 9, 10, 9, 9, 8, 10, 8, 7, 7, 7, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 9, 10, 7, 9, 7, 7, 8, 10, 9, 7, 9, 10, 7, 10, 8, 10, 8, 10, 10, 9, 7, 9, 9, 8, 8, 7, 9, 9, 10, 10, 10, 9, 7, 7, 7, 8, 8, 7, 9, 8, 8, 10, 9, 9, 10, 8, 7, 7, 8, 7, 10, 7, 7, 8, 8, 7, 9, 7, 8, 10, 8, 10, 9, 10, 8, 10, 10, 10, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7, 10, 10, 7, 7, 7, 7, 10, 10, 7, 9, 8, 8, 7, 9, 9, 8, 7, 10, 9, 9, 10, 9, 9, 10, 9, 10, 9, 8, 7, 10, 10, 10, 9, 10, 7, 9, 10, 8, 8, 8, 7, 9, 7, 10, 7, 8, 10, 10, 9, 7, 9, 7, 8, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 7, 7, 9, 9, 7, 10, 8, 9, 10, 10, 10, 8, 9, 9, 7, 7, 9, 7, 9, 9, 8, 10, 9, 8, 7, 9, 9, 7, 9, 9, 8, 8, 7, 10, 9, 7, 10, 9, 9, 8, 7, 8, 8, 7, 8, 10, 9, 8, 7, 8, 10, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 10, 9, 7, 8, 7, 9, 7, 9, 10, 9, 7, 9, 7, 8, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 7, 7, 7, 7, 10, 7, 7, 10, 7, 7, 10, 7, 9, 9, 8, 8, 7, 8, 9, 8, 7, 10, 9, 9, 10, 9, 9, 8, 10, 8, 7, 8, 7, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 9, 9, 7, 9, 9, 8, 10, 9, 9, 9, 9, 8, 9, 9, 9, 7, 7, 8, 7, 10, 8, 7, 8, 8, 10, 8, 7, 9, 9, 10, 7, 7, 9, 9, 7, 9, 10, 10, 8, 7, 7, 9, 8, 8, 10, 9, 9, 8, 8, 7, 7, 8, 7, 8, 9, 9, 10, 9, 7, 7, 7, 10, 10, 8, 8, 7, 10, 8, 10, 8, 10, 7, 8, 10, 7, 7, 7, 7, 7, 10, 7, 8, 7, 7, 7, 7, 7, 7, 7, 7, 10, 10, 7, 9, 8, 8, 7, 9, 9, 8, 7, 10, 9, 9, 10, 7, 9, 10, 9, 9, 9, 9, 7, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 10, 7, 8, 10, 8, 9, 9, 9, 7, 9, 9, 8, 10, 9, 7, 9, 9, 8, 7, 9, 9, 7, 9, 7, 7, 10, 8, 7, 8, 10, 10, 9, 7, 7, 8, 8, 10, 9, 9, 9, 7, 9, 8, 10, 9, 9, 7, 7, 9, 10, 10, 9, 8, 7, 9, 10, 9, 7, 9, 8, 8, 7, 7, 9, 10, 10, 9, 9, 8, 7, 8, 10, 7, 10, 10, 9, 8, 7, 8, 10, 9, 8, 7, 9, 8, 8, 10, 9, 9, 9, 8, 7, 7, 8, 7, 7, 9, 7, 9, 8, 7, 7, 7, 7, 8, 10, 8, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 10, 7, 7, 10, 7, 7, 7, 7, 7, 7, 7, 9, 10, 10, 10, 8, 7, 7, 7, 7, 10, 9, 7, 9, 7, 7, 10, 7, 10, 7, 10, 9, 10, 10, 10, 8, 7, 7, 7, 7, 8, 7, 7, 9, 10, 7, 10, 7, 7, 10, 9, 7, 7, 10, 7, 10, 7, 8, 10, 10, 7, 10, 10, 9, 7, 10, 10, 9, 9, 7, 7, 7, 7, 10, 7, 10, 7, 7, 10, 10, 7, 9, 7, 7, 9, 10, 7, 10, 8, 10, 7, 10, 8, 7, 9, 9, 8, 10, 7, 10, 7, 7, 7, 10, 10, 9, 8, 10, 10, 10, 10, 8, 10, 10, 8, 10, 8, 8, 8, 10, 10, 8, 8, 7, 10, 9, 9, 7, 7, 7, 10, 10, 7, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 8, 7, 10, 10, 10, 10, 10, 7, 9, 10, 8, 7, 9, 10, 7, 10, 9, 7, 7, 7, 7, 10, 7, 8, 7, 7, 9, 9, 7, 7, 7, 7, 9, 9, 7, 7, 7, 10, 10, 8, 7, 7, 10, 8, 7, 7, 7, 10, 10, 10, 7, 8, 10, 10, 10, 10, 10, 7, 7, 8, 7, 10, 10, 10, 10, 7, 10, 10, 10, 9, 9, 7, 7, 7, 10, 7, 7, 10, 10, 10, 8, 7, 7, 7, 8, 10, 10, 7, 8, 7, 9, 7, 7, 7, 7, 9, 10, 10, 9, 8, 7, 7, 7, 7, 7, 8, 7, 9, 10, 7, 8, 7, 7, 7, 9, 7, 7, 8, 10, 8, 7, 10, 7, 8, 7, 10, 10, 8, 7, 10, 10, 10, 7, 8, 10, 9, 10, 10, 10, 10, 10, 8, 8, 10, 10, 10, 10, 7, 8, 8, 8, 10, 7, 10, 7, 10, 7, 10, 10, 7, 9, 10, 10, 7, 10, 10, 10, 7, 10, 7, 9, 8, 10, 9, 10, 9, 10, 7, 7, 8, 7, 7, 7, 10, 7, 7, 8, 8, 8, 8, 7, 7, 7, 9, 8, 10, 10, 7, 9, 10, 9, 9, 8, 10, 10, 7, 8, 7, 8, 8, 7, 7, 9, 10, 7, 8, 10, 10, 10, 10, 8, 7, 10, 8, 10, 10, 7, 8, 7, 8, 10, 9, 10, 10, 10, 8, 10, 9, 7, 9, 10, 8, 7, 9, 9, 7, 10, 10, 8, 8, 7, 9, 9, 7, 9, 10, 9, 9, 8, 10, 7, 7, 9, 8, 10, 7, 9, 9, 10, 9, 9, 10, 8, 8, 10, 7, 8, 8, 10, 8, 10, 9, 9, 9, 10, 8, 10, 8, 10, 8, 7, 10, 9, 7, 7, 9, 10, 10, 9, 10, 7, 9, 10, 8, 7, 9, 8, 8, 7, 7, 7, 9, 9, 8, 10, 10, 9, 7, 8, 8, 7, 7, 9, 9, 10, 10, 9, 9, 7, 9, 9, 7, 10, 8, 10, 7, 8, 10, 10, 8, 8, 7, 7, 7, 9, 10, 9, 7, 8, 10, 8, 7, 8, 10, 8, 8, 9, 10, 9, 9, 8, 7, 10, 10, 10, 9, 9, 10, 7, 9, 9, 7, 9, 9, 8, 10, 7, 8, 7, 7, 7, 8, 7, 9, 10, 10, 8, 8, 10, 9, 9, 7, 8, 7, 7, 8, 10, 9, 9, 7, 10, 8, 10, 8, 10, 8, 8, 7, 10, 7, 9, 9, 8, 10, 9, 8, 10, 10, 9, 7, 9, 10, 9, 10, 8, 8, 10, 9, 7, 7, 7, 7, 8, 7, 8, 9, 9, 7, 7, 9, 8, 7, 9, 9, 8, 10, 10, 8, 8, 8, 8, 7, 9, 9, 8, 10, 8, 8, 7, 7, 9, 8, 8, 8, 8, 7, 7, 7, 7, 10, 9, 7, 7, 10, 9, 7, 7, 7, 7, 7, 9, 7, 9, 7, 8, 8, 8, 9, 8, 7, 7, 7, 9, 9, 7, 7, 9, 7, 10, 9, 8, 7, 9, 10, 9, 8, 8, 10, 10, 10, 10, 7, 10, 9, 7, 8, 8, 10, 7, 9, 8, 8, 10, 8, 10, 9, 7, 7, 9, 10, 8, 7, 7, 10, 7, 8, 10, 9, 10, 8, 7, 8, 10, 10, 8, 10, 9, 10, 10, 10, 10, 9, 7, 10, 8, 10, 7, 10, 8, 7, 7, 9, 7, 9, 10, 8, 7, 8, 10, 7, 7, 9, 8, 8, 10, 7, 9, 10, 8, 10, 7, 8, 7, 8, 10, 8, 7, 7, 9, 9, 9, 9, 7, 9, 9, 9, 9, 7, 7, 10, 10, 7, 9, 7, 9, 10, 8, 8, 7, 8, 8, 10, 8, 10, 10, 8, 8, 7, 9, 9, 9, 7, 9, 9, 7, 7, 10, 7, 10, 8, 7, 10, 10, 9, 7, 7, 10, 8, 10, 9, 10, 9, 7, 7, 8, 7, 10, 7, 10, 8, 10, 10, 7, 9, 7, 7, 8, 10, 7, 8, 8, 7, 10, 7, 8, 8, 10, 7, 9, 10, 10, 10, 8, 7, 9, 10, 7, 8, 10, 10, 10, 10, 7, 7, 7, 8, 7, 10, 10, 8, 9, 8, 8, 7, 10, 10, 10, 10, 9, 8, 10, 10, 10, 9, 10, 8, 8, 8, 10, 8, 10, 8, 10, 10, 10, 10, 8, 8, 8, 8, 7, 8, 8, 10, 7, 8, 7, 10, 7, 10, 7, 8, 7, 10, 7, 8, 7, 8, 7, 10, 7, 8, 7, 10, 9, 10, 10, 7, 8, 10, 8, 8, 8, 10, 7, 7, 8, 8, 7, 10, 8, 10, 9, 7, 9, 7, 9, 10, 7, 9, 9, 9, 7, 9, 8, 7, 10, 9, 8, 9, 9, 10, 9, 10, 7, 10, 8, 8, 8, 10, 7, 10, 8, 8, 8, 10, 8, 9, 10, 9, 10, 10, 10, 10, 10, 8, 10, 8, 10, 10, 7, 7, 9, 9, 7, 7, 7, 7, 9, 9, 7, 10, 7, 10, 10, 8, 10, 7, 10, 10, 7, 10, 7, 8, 7, 7, 8, 7, 8, 9, 9, 10, 7, 9, 10, 10, 7, 10, 8, 7, 7, 10, 7, 10, 8, 10, 7, 7, 10, 10, 10, 10, 7, 7, 8, 7, 10, 10, 9, 10, 9, 7, 10, 7, 8, 10, 10, 10, 7, 7, 7, 9, 10, 8, 8, 7, 8, 10, 10, 8, 8, 7, 8, 10, 10, 9, 10, 9, 10, 7, 7, 7, 10, 10, 9, 8, 8, 8, 10, 10, 10, 8, 10, 7, 9, 8, 7, 7, 10, 9, 10, 10, 8, 8, 8, 7, 10, 8, 7, 7, 7, 10, 10, 10, 7, 10, 10, 10, 10, 10, 7, 7, 7, 8, 7, 7, 10, 7, 8, 7, 9, 10, 7, 7, 7, 7, 7, 8, 9, 10, 7, 9, 7, 9, 9, 9, 8, 8, 7, 8, 7, 7, 7, 9, 9, 9, 10, 9, 7, 8, 7, 10, 8, 9, 9, 10, 8, 7, 9, 9, 10, 7, 7, 9, 9, 10, 7, 10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 8, 7, 9, 9, 9, 7, 7, 10, 7, 7, 7, 7, 7, 7, 9, 9, 10, 8, 8, 10, 9, 9, 9, 10, 8, 10, 7, 9, 9, 9, 7, 9, 9, 10, 7, 7, 7, 8, 7, 7, 9, 8, 9, 10, 9, 7, 9, 8, 8, 7, 9, 8, 10, 9, 7, 9, 10, 10, 8, 10, 7, 9, 8, 9, 9, 9, 9, 9, 10, 8, 8, 8, 10, 9, 7, 7, 8, 7, 8, 8, 7, 7, 7, 7, 9, 9, 7, 8, 7, 7, 9, 7, 8, 10, 9, 10, 10, 10, 8, 10, 9, 7, 7, 7, 8, 7, 8, 10, 7, 8, 7, 10, 10, 7, 10, 8, 10, 8, 10, 10, 7, 7, 9, 10, 10, 7, 8, 8, 8, 7, 10, 10, 7, 8, 10, 10, 7, 8, 9, 9, 7, 7, 7, 7, 10, 9, 7, 10, 10, 10, 10, 10, 10, 7, 8, 10, 9, 10, 10, 8, 8, 8, 10, 10, 8, 9, 9, 10, 10, 8, 8, 10, 9, 10, 8, 10, 10, 9, 9, 10, 10, 7, 9, 7, 7, 8, 7, 8, 7, 9, 8, 10, 9, 9, 7, 9, 7, 10, 10, 9, 10, 9, 10, 10, 7, 7, 10, 7, 9, 8, 10, 10, 7, 9, 9, 7, 8, 9, 10, 8, 10, 9, 10, 10, 10, 8, 8, 9, 10, 9, 7, 9, 8, 7, 9, 9, 10, 7, 7, 8, 7, 7, 8, 10, 10, 10, 10, 10, 9, 7, 7, 7, 8, 7, 7, 7, 10, 10, 8, 8, 8, 10, 8, 7, 10, 8, 10, 9, 8, 10, 9, 7, 7, 9, 7, 7, 9, 9, 9, 9, 9, 7, 8, 7, 7, 7, 7, 7, 8, 9, 9, 8, 8, 8, 8, 10, 7, 10, 8, 9, 8, 8, 8, 7, 9, 7, 7, 8, 8, 9, 10, 10, 9, 8, 9, 7, 9, 9, 7, 10, 10, 10, 7, 9, 8, 10, 7, 9, 8, 10, 9, 9, 10, 9, 7, 8, 9, 8, 8, 9, 9, 7, 9, 8, 7, 8, 9, 7, 7, 9, 10, 10, 9, 10, 7, 8, 7, 9, 9, 10, 7, 9, 8, 8, 7, 9, 8, 7, 9, 8, 7, 8, 8, 8, 7, 8, 9, 8, 9, 7, 9, 8, 8, 8, 9, 8, 9, 9, 10, 10, 7, 8, 8, 8, 10, 9, 9, 8, 8, 9, 8, 9, 8, 9, 9, 8, 10, 7, 8, 10, 9, 10, 7, 9, 7, 9, 10, 9, 9, 9, 8, 10, 9, 9, 8, 9, 9, 8, 9, 7, 9, 8, 9, 9, 9, 8, 9, 9, 9, 9, 8, 9, 9, 10, 7, 10, 8, 7, 8, 9, 8, 9, 9, 9, 7, 9, 9, 9, 9, 8, 9, 9, 9, 9, 8, 8, 8, 9, 8, 10, 8, 9, 10, 8, 9, 9, 8, 10, 9, 7, 10, 8, 9, 8, 7, 8, 9, 7, 10, 10, 9, 10, 9, 7, 8, 9, 8, 9, 8, 8, 9, 8, 8, 9, 9, 7, 9, 9, 8, 7, 9, 9, 8, 8, 9, 9, 9, 8, 8, 8, 10, 8, 7, 7, 9, 7, 10, 9, 9, 8, 9, 9, 8, 9, 9, 9, 8, 9, 8, 8, 8, 7, 9, 7, 9, 8, 9, 9, 8, 10, 8, 9, 9, 8, 8, 8, 9, 9, 8, 7, 9, 10, 7, 9, 10, 9, 9, 10, 9, 9, 9, 7, 8, 9, 9, 8, 7, 8, 10, 7, 9, 8, 10, 9, 8, 10, 9, 9, 9, 9, 8, 8, 10, 9, 8, 8, 9, 8, 8, 8, 8, 9, 9, 9, 7, 9, 10, 9, 9, 8, 10, 9, 8, 7, 9, 8, 7, 9, 8, 9, 8, 8, 7, 9, 9, 7, 7, 10, 8, 9, 7, 9, 9, 7, 10, 9, 9, 10, 7, 7, 7, 7, 10, 9, 7, 8, 8, 8, 7, 9, 9, 9, 9, 7, 7, 9, 7, 7, 9, 7, 7, 9, 7, 7, 7, 8, 9, 9, 9, 8, 8, 9, 8, 9, 7, 7, 8, 8, 9, 8, 7, 9, 10, 7, 10, 8, 7, 10, 9, 8, 10, 9, 9, 8, 8, 7, 7, 9, 7, 7, 9, 7, 10, 8, 7, 10, 8, 7, 10, 10, 7, 7, 9, 9, 7, 8, 9, 9, 7, 9, 9, 8, 7, 8, 9, 9, 10, 9, 7, 9, 8, 10, 9, 7, 9, 10, 10, 8, 8, 9, 8, 9, 8, 8, 9, 9, 8, 9, 7, 9, 8, 9, 10, 8, 8, 8, 10, 8, 9, 9, 9, 9, 8, 8, 8, 8, 8, 7, 10, 8, 8, 9, 9, 10, 8, 10, 8, 10, 8, 8, 10, 10, 8, 7, 9, 7, 8, 8, 8, 8, 8, 7, 8, 7, 8, 10, 9, 8, 8, 9, 10, 8, 10, 8, 10, 7, 9, 9, 8, 9, 10, 8, 8, 8, 9, 9, 10, 9, 8, 8, 10, 8, 8, 8, 10, 8, 8, 8, 10, 10, 8, 8, 8, 8, 8, 7, 8, 8, 8, 10, 9, 10, 8, 8, 9, 7, 9, 8, 10, 9, 8, 8, 9, 9, 10, 9, 8, 8, 10, 8, 9, 9, 9, 9, 10, 8, 9, 8, 9, 9, 7, 8, 8, 8, 9, 8, 7, 10, 9, 8, 8, 9, 8, 8, 9, 8, 10, 8, 8, 9, 9, 9, 7, 7, 10, 8, 9, 10, 8, 8, 10, 8, 8, 9, 8, 10, 9, 8, 10, 8, 9, 9, 9, 8, 10, 10, 9, 8, 9, 9, 8, 8, 10, 8, 8, 9, 9, 9, 9, 8, 8, 8, 9, 10, 8, 8, 10, 8, 10, 10, 10, 8, 10, 10, 10, 8, 8, 8, 9, 8, 7, 8, 8, 10, 9, 8, 8, 9, 8, 8, 8, 10, 8, 10, 9, 8, 10, 8, 10, 9, 9, 8, 8, 9, 8, 8, 10, 8, 10, 9, 8, 7, 9, 9, 8, 8, 8, 10, 9, 8, 9, 9, 8, 8, 10, 8, 9, 7, 7, 8, 8, 8, 8, 7, 8, 9, 10, 9, 8, 9, 8, 8, 10, 8, 8, 9, 8, 8, 9, 8, 9, 9, 9, 9, 7, 9, 9, 7, 7, 10, 9, 10, 9, 8, 9, 9, 9, 9, 8, 10, 8, 8, 8, 8, 8, 9, 9, 8, 9, 9, 8, 8, 8, 9, 8, 8, 8, 9, 8, 8, 9, 8, 9, 8, 8, 8, 8, 10, 8, 9, 10, 8, 9, 8, 8, 9, 8, 7, 9, 8, 8, 10, 8, 9, 8, 8, 10, 8, 9, 8, 8, 10, 10, 8, 9, 8, 8, 9, 8, 8, 7, 9, 9, 8, 8, 8, 8, 9, 8, 9, 9, 7, 9, 8, 8, 9, 10, 8, 9, 8, 8, 9, 8, 9, 8, 10, 10, 9, 10, 8, 7, 7, 9, 9, 9, 9, 8, 10, 9, 9, 9, 7, 7, 8, 8, 7, 10, 8, 8, 8, 10, 9, 8, 10, 8, 10, 8, 8, 8, 7, 10, 7, 10, 9, 10, 10, 9, 8, 10, 7, 7, 8, 9, 9, 9, 9, 10, 9, 9, 8, 9, 10, 8, 10, 9, 9, 8, 9, 8, 9, 9, 9, 9, 7, 10, 8, 8, 8, 9, 8, 10, 9, 8, 9, 9, 8, 8, 8, 8, 9, 10, 7, 9, 10, 7, 8, 9, 10, 10, 8, 9, 8, 10, 10, 10, 8, 10, 9, 10, 10, 10, 8, 8, 7, 8, 9, 10, 8, 10, 8, 10, 8, 10, 9, 8, 9, 10, 8, 9, 9, 10, 9, 8, 10, 8, 8, 9, 9, 8, 10, 8, 10, 9, 9, 9, 8, 10, 9, 8, 10, 10, 7, 8, 7, 9, 10, 7, 7, 7, 8, 8, 8, 10, 9, 7, 8, 8, 9, 9, 7, 9, 7, 10, 9, 9, 9, 8, 10, 10, 8, 8, 8, 10, 8, 7, 8, 10, 10, 8, 8, 8, 9, 9, 7, 9, 10, 8, 9, 9, 7, 7, 9, 8, 7, 10, 9, 7, 8, 9, 9, 8, 7, 9, 7, 8, 7, 8, 8, 10, 9, 9, 9, 9, 8, 8, 10, 7, 8, 7, 10, 10, 8, 9, 7, 7, 8, 8, 10, 9, 8, 10, 7, 9, 10, 10, 10, 10, 8, 7, 7, 7, 9, 7, 7, 7, 7, 9, 10, 8, 7, 10, 8, 7, 8, 10, 9, 10, 9, 10, 9, 10, 8, 10, 10, 7, 7, 9, 7, 10, 8, 7, 7, 7, 7, 9, 10, 7, 10, 10, 7, 9, 7, 7, 10, 8, 7, 9, 10, 8, 7, 10, 9, 9, 8, 8, 10, 7, 7, 9, 9, 7, 10, 8, 9, 9, 7, 9, 9, 7, 9, 9, 7, 8, 7, 8, 10, 10, 10, 9, 7, 7, 9, 9, 9, 7, 7, 9, 7, 7, 7, 9, 9, 10, 10, 10, 9, 8, 10, 10, 10, 10, 10, 7, 9, 7, 7, 7, 8, 7, 9, 10, 10, 9, 10, 8, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 7, 8, 10, 10, 10, 7, 10, 9, 8, 7, 9, 10, 9, 10, 9, 10, 7, 9, 10, 10, 7, 7, 8, 8, 7, 9, 8, 10, 9, 9, 9, 9, 7, 8, 9, 10, 7, 9, 9, 7, 10, 10, 10, 10, 10, 7, 7, 10, 10, 9, 7, 7, 7, 7, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 8, 7, 7, 7, 7, 8, 10, 9, 10, 10, 10, 10, 7, 10, 9, 8, 10, 7, 7, 8, 7, 10, 10, 10, 8, 10, 8, 8, 9, 10, 10, 9, 8, 10, 7, 8, 7, 8, 10, 9, 10, 9, 10, 9, 9, 10, 8, 10, 10, 10, 9, 10, 10, 9, 8, 7, 10, 8, 8, 9, 8, 10, 9, 7, 10, 7, 8, 8, 9, 8, 9, 10, 10, 8, 10, 9, 7, 7, 7, 10, 7, 9, 7, 7, 10, 9, 9, 7, 7, 7, 9, 9, 10, 9, 7, 10, 7, 10, 7, 10, 7, 10, 9, 10, 10, 10, 8, 7, 8, 10, 10, 7, 8, 8, 10, 9, 7, 7, 9, 10, 9, 10, 9, 8, 7, 9, 7, 7, 7, 10, 10, 9, 10, 7, 8, 8, 7, 10, 10, 7, 7, 10, 10, 8, 8, 7, 10, 10, 10, 8, 10, 9, 10, 10, 10, 7, 10, 7, 10, 8, 10, 10, 7, 10, 10, 9, 9, 7, 9, 8, 8, 9, 8, 9, 7, 10, 8, 7, 7, 8, 10, 9, 8, 10, 7, 9, 8, 7, 8, 7, 9, 10, 7, 9, 10, 7, 7, 7, 10, 9, 10, 9, 10, 7, 7, 9, 10, 7, 9, 9, 8, 8, 7, 8, 8, 7, 10, 10, 9, 7, 9, 9, 7, 10, 10, 10, 9, 8, 10, 9, 7, 7, 10, 10, 8, 7, 9, 10, 10, 9, 7, 7, 7, 7, 7, 8, 9, 10, 9, 7, 8, 7, 7, 7, 7, 10, 10, 10, 10, 7, 10, 9, 7, 8, 7, 10, 10, 10, 8, 7, 9, 7, 7, 8, 7, 8, 9, 9, 8, 8, 8, 7, 9, 10, 8, 7, 7, 10, 7, 10, 9, 8, 8, 7, 7, 7, 9, 10, 10, 10, 7, 9, 7, 7, 7, 7, 8, 10, 10, 9, 7, 9, 7, 8, 7, 10, 7, 10, 9, 10, 7, 7, 10, 9, 7, 8, 10, 10, 10, 9, 9, 7, 7, 10, 7, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 10, 10, 7, 7, 8, 9, 10, 10, 10, 7, 10, 10, 7, 10, 7, 10, 9, 10, 10, 7, 10, 7, 9, 8, 10, 10, 10, 9, 7, 8, 7, 10, 10, 10, 7, 10, 10, 9, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 8, 7, 7, 7, 10, 10, 8, 8, 10, 8, 7, 7, 9, 10, 10, 8, 10, 10, 10, 10, 10, 7, 10, 10, 9, 7, 7, 8, 10, 10, 9, 7, 10, 10, 7, 7, 10, 10, 7, 7, 7, 7, 8, 7, 10, 10, 7, 8, 10, 10, 10, 9, 7, 10, 10, 7, 9, 7, 10, 8, 9, 9, 10, 10, 7, 10, 9, 7, 7, 9, 7, 9, 10, 8, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 10, 9, 7, 8, 8, 7, 7, 9, 10, 7, 9, 9, 10, 7, 7, 9, 7, 7, 8, 10, 7, 10, 9, 10, 9, 9, 9, 9, 7, 9, 7, 7, 7, 7, 7, 10, 7, 8, 10, 9, 10, 10, 9, 8, 8, 10, 10, 10, 9, 10, 8, 10, 7, 8, 8, 10, 10, 10, 7, 9, 7, 7, 7, 9, 7, 9, 7, 8, 7, 7, 10, 7, 10, 10, 10, 10, 7, 8, 7, 10, 10, 8, 10, 10, 8, 7, 10, 7, 7, 7, 7, 10, 8, 10, 7, 8, 7, 7, 7, 7, 10, 7, 9, 10, 9, 9, 8, 7, 7, 10, 9, 7, 7, 7, 9, 7, 10, 10, 9, 10, 7, 10, 10, 10, 10, 9, 10, 7, 7, 9, 7, 8, 8, 7, 7, 9, 10, 9, 7, 10, 7, 10, 10, 10, 7, 7, 9, 7, 10, 8, 7, 9, 10, 7, 10, 10, 10, 10, 10, 10, 7, 8, 7, 7, 7, 7, 10, 9, 10, 7, 7, 9, 7, 7, 10, 9, 7, 7, 7, 8, 10, 9, 7, 10, 10, 7, 7, 9, 7, 7, 7, 8, 7, 8, 7, 9, 8, 10, 10, 8, 8, 10, 10, 10, 10, 8, 8, 10, 10, 9, 9, 7, 7, 7, 9, 10, 10, 8, 7, 9, 10, 10, 10, 10, 7, 10, 10, 7, 8, 8, 10, 10, 10, 8, 10, 10, 10, 9, 9, 9, 9, 10, 10, 10, 10, 9, 10, 10, 10, 9, 7, 10, 10, 10, 9, 8, 10, 10, 10, 7, 8, 7, 9, 8, 7, 9, 7, 10, 9, 8, 10, 10, 10, 8, 10, 10, 10, 8, 8, 7, 7, 7, 10, 8, 8, 10, 9, 10, 9, 7, 9, 10, 10, 10, 10, 9, 9, 7, 7, 7, 7, 9, 7, 10, 8, 9, 10, 10, 10, 10, 10, 7, 7, 7, 8, 10, 10, 10, 8, 10, 10, 9, 10, 8, 8, 10, 7, 10, 10, 7, 10, 10, 7, 7, 9, 9, 10, 10, 9, 10, 7, 7, 10, 10, 7, 7, 10, 10, 8, 10, 10, 7, 9, 8, 8, 10, 9, 8, 10, 10, 10, 9, 9, 9, 7, 8, 7, 8, 7, 7, 7, 7, 10, 7, 7, 7, 7, 10, 9, 10, 10, 10, 9, 8, 7, 8, 8, 7, 9, 8, 7, 7, 10, 7, 9, 9, 10, 10, 10, 8, 7, 8, 7, 10, 7, 9, 7, 7, 8, 7, 7, 7, 10, 9, 7, 7, 9, 7, 8, 10, 10, 10, 10, 8, 10, 10, 9, 7, 9, 9, 9, 8, 10, 9, 10, 9, 7, 7, 8, 7, 10, 9, 9, 9, 9, 9, 8, 10, 7, 10, 10, 7, 10, 8, 7, 10, 10, 10, 8, 10, 8, 7, 10, 8, 10, 10, 10, 7, 10, 7, 8, 7, 8, 10, 10, 7, 7, 10, 7, 10, 10, 10, 8, 7, 10, 10, 8, 10, 8, 10, 7, 10, 10, 8, 10, 7, 7, 9, 7, 9, 8, 7, 8, 10, 9, 9, 9, 8, 7, 8, 10, 8, 8, 10, 10, 10, 7, 9, 7, 7, 7, 7, 9, 9, 9, 9, 8, 10, 10, 10, 9, 10, 10, 10, 10, 9, 10, 7, 10, 9, 10, 10, 10, 9, 9, 7, 10, 8, 8, 8, 7, 8, 7, 9, 9, 9, 8, 8, 10, 7, 9, 10, 7, 10, 9, 10, 9, 7, 7, 10, 10, 10, 10, 7, 7, 7, 9, 10, 9, 7, 10, 7, 7, 7, 7, 7, 8, 7, 8, 10, 10, 8, 10, 7, 10, 10, 10, 10, 9, 10, 7, 8, 10, 7, 9, 8, 7, 8, 7, 10, 10, 8, 8, 10, 7, 9, 7, 10, 9, 7, 7, 10, 10, 10, 10, 10, 7, 10, 10, 9, 10, 7, 7, 10, 10, 10, 10, 9, 10, 10, 10, 7, 10, 10, 8, 10, 10, 7, 10, 7, 8, 9, 10, 7, 7, 10, 8, 7, 9, 7, 9, 9, 7, 10, 7, 10, 7, 10, 10, 10, 8, 7, 7, 10, 7, 7, 7, 10, 7, 10, 8, 7, 9, 9, 9, 9, 7, 7, 10, 7, 10, 10, 10, 10, 9, 8, 7, 10, 10, 7, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 7, 7, 10, 8, 8, 7, 10, 8, 8, 8, 7, 9, 8, 10, 10, 10, 7, 7, 7, 10, 10, 10, 7, 7, 7, 7, 7, 9, 10, 7, 10, 7, 7, 8, 10, 7, 10, 10, 9, 8, 7, 9, 10, 8, 7, 10, 7, 9, 7, 7, 7, 10, 9, 7, 10, 10, 9, 10, 7, 7, 7, 7, 10, 9, 9, 10, 7, 9, 10, 10, 9, 8, 10, 10, 7, 10, 8, 10, 7, 8, 8, 10, 8, 10, 8, 10, 7, 8, 10, 10, 7, 8, 7, 7, 10, 7, 9, 10, 10, 8, 7, 9, 7, 8, 10, 7, 8, 10, 7, 10, 10, 7, 10, 9, 7, 7, 8, 10, 10, 10, 10, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 9, 7, 9, 7, 10, 9, 9, 7, 9, 10, 8, 10, 8, 7, 8, 10, 8, 10, 9, 10, 10, 9, 8, 8, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 9, 7, 9, 10, 9, 8, 7, 9, 10, 9, 9, 8, 7, 9, 9, 7, 10, 8, 10, 8, 9, 9, 8, 10, 8, 7, 8, 10, 9, 10, 7, 7, 8, 8, 7, 8, 8, 9, 8, 8, 10, 8, 8, 10, 9, 9, 9, 10, 10, 8, 7, 7, 9, 10, 9, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 8, 9, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 8, 10, 7, 8, 7, 9, 9, 8, 7, 8, 9, 10, 9, 8, 8, 7, 8, 8, 7, 10, 9, 8, 8, 10, 9, 9, 8, 10, 7, 7, 10, 10, 10, 10, 10, 10, 7, 10, 7, 10, 10, 10, 10, 8, 7, 9, 10, 7, 9, 7, 9, 7, 8, 7, 7, 7, 9, 10, 10, 10, 8, 7, 8, 8, 7, 10, 7, 10, 10, 9, 9, 10, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 10, 10, 9, 7, 7, 8, 10, 8, 8, 10, 9, 7, 8, 8, 10, 8, 7, 10, 9, 7, 10, 10, 8, 7, 8, 8, 8, 7, 8, 8, 10, 10, 9, 9, 8, 8, 10, 8, 8, 8, 7, 7, 7, 9, 10, 9, 8, 7, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 10, 9, 10, 9, 7, 9, 8, 8, 7, 8, 8, 9, 10, 9, 8, 8, 8, 7, 9, 7, 8, 10, 9, 7, 7, 8, 7, 10, 10, 10, 10, 10, 10, 7, 7, 9, 7, 7, 7, 9, 9, 9, 9, 7, 7, 7, 7, 7, 7, 10, 10, 9, 8, 8, 7, 10, 10, 10, 9, 7, 10, 7, 8, 10, 8, 10, 9, 10, 10, 9, 10, 10, 9, 10, 9, 10, 9, 10, 10, 10, 10, 10, 10, 7, 7, 10, 10, 8, 7, 10, 8, 9, 10, 7, 10, 8, 7, 10, 7, 9, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 10, 9, 8, 10, 7, 10, 10, 9, 8, 10, 9, 10, 10, 9, 7, 8, 8, 10, 8, 7, 9, 7, 9, 10, 10, 10, 8, 7, 9, 7, 9, 10, 10, 10, 10, 10, 7, 10, 7, 7, 7, 9, 10, 10, 8, 8, 9, 8, 8, 7, 7, 10, 9, 9, 9, 10, 7, 9, 7, 10, 10, 8, 7, 10, 10, 8, 7, 9, 10, 9, 7, 9, 7, 10, 9, 10, 8, 10, 9, 7, 9, 9, 8, 10, 8, 10, 7, 10, 9, 9, 10, 8, 9, 9, 10, 7, 8, 7, 10, 9, 7, 8, 7, 9, 10, 8, 9, 10, 9, 7, 7, 8, 7, 9, 10, 7, 10, 10, 10, 8, 7, 8, 7, 10, 7, 8, 8, 10, 9, 9, 10, 8, 7, 7, 10, 9, 9, 10, 7, 8, 10, 9, 7, 10, 10, 10, 9, 7, 10, 8, 8, 8, 8, 8, 10, 10, 8, 10, 9, 7, 10, 10, 10, 8, 10, 10, 8, 10, 10, 10, 10, 8, 7, 7, 8, 7, 7, 10, 9, 10, 10, 7, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 10, 10, 8, 8, 8, 9, 10, 10, 9, 10, 8, 8, 10, 9, 8, 10, 7, 7, 10, 9, 7, 8, 7, 10, 7, 10, 7, 10, 9, 10, 7, 7, 9, 8, 8, 10, 7, 10, 10, 10, 9, 9, 8, 8, 7, 9, 10, 10, 10, 7, 7, 7, 10, 7, 10, 10, 10, 7, 10, 7, 7, 7, 8, 7, 7, 7, 7, 8, 10, 7, 9, 10, 7, 7, 9, 7, 9, 10, 10, 9, 10, 10, 7, 7, 10, 9, 7, 10, 10, 10, 10, 10, 8, 10, 9, 7, 7, 7, 7, 10, 10, 7, 9, 7, 9, 8, 7, 9, 7, 10, 10, 7, 9, 7, 9, 8, 7, 9, 7, 10, 10, 10, 9, 10, 7, 9, 10, 10, 10, 10, 8, 7, 7, 8, 9, 9, 8, 10, 9, 7, 7, 9, 7, 7, 7, 10, 7, 7, 7, 10, 8, 8, 10, 10, 8, 10, 7, 7, 7, 10, 9, 7, 9, 8, 8, 7, 9, 7, 10, 10, 7, 7, 10, 8, 9, 10, 7, 7, 9, 10, 10, 7, 8, 10, 9, 7, 10, 10, 10, 10, 10, 10, 10, 7, 10, 10, 9, 7, 7, 7, 10, 10, 9, 10, 7, 10, 10, 10, 8, 7, 10, 10, 9, 7, 7, 10, 10, 9, 10, 7, 10, 10, 10, 8, 7, 10, 10, 8, 7, 9, 8, 10, 9, 7, 7, 10, 9, 7, 7, 7, 7, 7, 8, 7, 9, 9, 8, 8, 7, 9, 9, 7, 10, 7, 7, 7, 9, 8, 10, 7, 7, 8, 7, 7, 9, 10, 7, 9, 9, 8, 10, 7, 8, 8, 10, 7, 10, 9, 10, 9, 7, 9, 10, 7, 9, 7, 8, 7, 8, 7, 7, 10, 10, 7, 7, 9, 7, 10, 7, 7, 7, 10, 10, 7, 8, 7, 10, 10, 7, 7, 9, 9, 10, 9, 10, 9, 10, 9, 7, 10, 10, 10, 10, 7, 10, 7, 10, 10, 7, 9, 9, 10, 9, 10, 10, 10, 10, 10, 7, 7, 8, 8, 10, 9, 9, 9, 10, 8, 10, 9, 10, 10, 8, 7, 8, 8, 8, 10, 9, 7, 7, 9, 10, 10, 9, 10, 10, 10, 9, 8, 7, 7, 7, 7, 10, 10, 10, 10, 8, 10, 10, 10, 10, 9, 9, 8, 10, 7, 10, 7, 8, 7, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 7, 7, 9, 7, 9, 10, 8, 8, 8, 7, 7, 7, 7, 9, 9, 10, 8, 8, 7, 10, 7, 8, 10, 10, 8, 8, 8, 7, 7, 7, 7, 9, 10, 10, 10, 7, 7, 9, 7, 9, 8, 7, 7, 10, 10, 9, 10, 10, 8, 10, 9, 10, 10, 10, 9, 7, 7, 7, 7, 8, 7, 9, 8, 7, 10, 7, 7, 9, 10, 7, 7, 8, 10, 7, 7, 7, 9, 7, 7, 10, 7, 7, 9, 10, 10, 8, 8, 7, 8, 7, 10, 7, 10, 10, 7, 10, 7, 10, 10, 8, 7, 9, 10, 7, 7, 7, 10, 7, 10, 10, 10, 7, 7, 10, 8, 7, 10, 7, 10, 7, 8, 10, 9, 10, 7, 10, 7, 8, 10, 7, 8, 10, 10, 8, 7, 8, 10, 9, 7, 10, 9, 7, 7, 7, 9, 10, 7, 7, 8, 8, 7, 10, 7, 10, 10, 7, 9, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 10, 10, 10, 7, 7, 7, 9, 8, 7, 10, 8, 8, 7, 10, 7, 10, 7, 10, 7, 9, 7, 7, 7, 10, 7, 9, 10, 10, 10, 10, 10, 7, 9, 9, 8, 8, 7, 9, 9, 9, 9, 8, 7, 9, 10, 9, 9, 8, 10, 8, 7, 8, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 7, 8, 10, 10, 10, 9, 9, 9, 7, 9, 9, 8, 8, 7, 9, 10, 9, 10, 9, 9, 9, 8, 7, 9, 7, 10, 8, 7, 8, 10, 10, 9, 7, 9, 9, 8, 8, 7, 9, 9, 7, 9, 10, 8, 10, 9, 7, 9, 7, 8, 10, 7, 9, 8, 8, 10, 9, 9, 8, 8, 7, 7, 8, 7, 10, 9, 9, 10, 9, 7, 7, 7, 8, 8, 8, 8, 7, 10, 8, 10, 10, 10, 7, 8, 8, 7, 7, 7, 7, 10, 10, 7, 8, 7, 7, 7, 7, 7, 10, 9, 7, 9, 8, 10, 7, 9, 9, 10, 9, 10, 9, 9, 10, 9, 9, 10, 7, 10, 9, 10, 9, 8, 8, 10, 9, 10, 7, 7, 10, 8, 8, 8, 7, 9, 8, 10, 7, 8, 8, 8, 9, 9, 9, 7, 9, 9, 8, 8, 9, 7, 9, 9, 8, 7, 8, 9, 7, 9, 7, 7, 10, 8, 7, 8, 10, 10, 9, 7, 7, 8, 8, 10, 9, 9, 9, 7, 9, 9, 8, 8, 7, 7, 9, 7, 10, 10, 9, 8, 7, 9, 10, 9, 7, 9, 8, 8, 10, 7, 9, 7, 10, 8, 7, 8, 9, 8, 8, 7, 8, 10, 9, 8, 7, 10, 10, 8, 8, 7, 9, 8, 8, 10, 9, 9, 9, 10, 9, 7, 10, 9, 9, 7, 9, 10, 9, 7, 7, 7, 8, 10, 9, 10, 8, 10, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 9, 10, 10, 10, 10, 10, 7, 9, 10, 10, 7, 8, 7, 9, 9, 10, 10, 10, 10, 8, 7, 10, 9, 10, 7, 10, 9, 10, 7, 7, 8, 7, 10, 10, 8, 7, 9, 10, 9, 10, 7, 9, 9, 10, 7, 10, 10, 10, 7, 7, 9, 7, 8, 7, 9, 8, 10, 9, 7, 7, 7, 10, 7, 7, 7, 7, 7, 10, 7, 8, 8, 10, 10, 8, 10, 9, 7, 8, 7, 10, 10, 10, 10, 8, 7, 7, 7, 10, 7, 8, 10, 7, 9, 7, 7, 10, 10, 8, 10, 9, 10, 10, 10, 10, 9, 10, 10, 10, 10, 7, 10, 10, 7, 7, 7, 9, 8, 7, 10, 10, 7, 8, 8, 7, 8, 10, 10, 9, 10, 10, 10, 10, 10, 7, 7, 9, 8, 7, 10, 10, 8, 8, 10, 9, 10, 10, 7, 9, 7, 9, 9, 8, 10, 7, 7, 9, 7, 9, 8, 10, 7, 7, 7, 9, 7, 9, 10, 10, 7, 10, 10, 10, 7, 8, 7, 9, 10, 7, 10, 10, 8, 7, 7, 7, 10, 10, 9, 7, 7, 10, 10, 10, 10, 8, 8, 10, 10, 7, 10, 8, 10, 10, 10, 10, 7, 7, 7, 7, 10, 9, 8, 10, 8, 7, 10, 8, 10, 10, 7, 7, 7, 7, 10, 7, 10, 9, 7, 10, 8, 10, 10, 10, 7, 10, 10, 9, 10, 10, 10, 10, 9, 9, 8, 8, 7, 10, 7, 8, 7, 7, 10, 10, 9, 10, 9, 9, 7, 7, 8, 10, 7, 8, 7, 10, 8, 10, 8, 10, 9, 7, 8, 7, 9, 10, 9, 9, 7, 7, 7, 7, 10, 9, 10, 7, 10, 7, 9, 10, 10, 8, 10, 10, 10, 8, 7, 9, 7, 7, 9, 10, 10, 10, 9, 10, 9, 9, 10, 7, 7, 7, 7, 10, 9, 7, 8, 10, 10, 10, 7, 7, 7, 9, 7, 10, 10, 10, 9, 7, 10, 7, 9, 7, 7, 7, 9, 10, 7, 7, 9, 9, 8, 7, 10, 7, 10, 8, 10, 9, 7, 7, 10, 10, 9, 8, 7, 10, 9, 9, 10, 8, 9, 9, 7, 7, 9, 10, 7, 8, 8, 10, 9, 7, 7, 7, 7, 7, 7, 9, 10, 7, 7, 7, 7, 10, 10, 9, 7, 10, 7, 10, 7, 10, 8, 7, 10, 10, 10, 9, 7, 7, 7, 7, 10, 9, 7, 7, 7, 10, 9, 8, 7, 10, 7, 10, 8, 8, 8, 10, 9, 9, 7, 10, 7, 7, 9, 8, 7, 9, 7, 9, 8, 7, 8, 8, 7, 9, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 10, 8, 8, 8, 10, 9, 7, 10, 10, 10, 10, 7, 7, 10, 10, 7, 7, 7, 10, 7, 9, 9, 7, 9, 10, 8, 7, 9, 8, 7, 7, 8, 8, 9, 10, 10, 10, 8, 7, 7, 9, 7, 9, 8, 7, 9, 9, 7, 8, 8, 8, 7, 7, 9, 8, 10, 8, 10, 9, 7, 8, 8, 8, 10, 10, 10, 9, 8, 7, 8, 10, 8, 10, 10, 8, 7, 8, 8, 10, 9, 8, 7, 7, 9, 9, 7, 10, 9, 9, 8, 10, 9, 7, 7, 9, 10, 7, 9, 10, 9, 9, 8, 7, 9, 9, 7, 7, 7, 9, 8, 10, 8, 10, 8, 10, 9, 9, 9, 7, 10, 9, 10, 7, 9, 9, 9, 8, 8, 10, 10, 10, 9, 10, 7, 9, 7, 8, 8, 8, 7, 9, 7, 9, 7, 9, 8, 10, 9, 10, 10, 7, 7, 7, 10, 7, 7, 8, 8, 10, 10, 10, 9, 9, 10, 10, 9, 8, 10, 7, 9, 8, 7, 10, 9, 8, 7, 7, 9, 8, 7, 7, 10, 7, 7, 9, 7, 7, 9, 9, 9, 8, 8, 10, 9, 10, 9, 9, 10, 9, 8, 10, 10, 10, 10, 8, 10, 10, 10, 10, 10, 8, 10, 10, 10, 8, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 10, 10, 10, 10, 9, 7, 9, 7, 8, 7, 9, 7, 9, 10, 10, 10, 10, 9, 8, 10, 8, 10, 10, 9, 10, 10, 9, 8, 10, 8, 7, 9, 9, 8, 10, 9, 9, 9, 9, 10, 9, 8, 7, 7, 10, 9, 9, 8, 9, 10, 9, 7, 10, 8, 10, 10, 9, 9, 8, 10, 8, 7, 8, 7, 9, 8, 7, 7, 8, 8, 10, 8, 10, 9, 8, 8, 10, 8, 8, 8, 10, 9, 9, 10, 10, 8, 7, 7, 9, 9, 7, 7, 10, 10, 8, 10, 8, 8, 10, 7, 8, 8, 10, 10, 7, 9, 8, 8, 10, 8, 8, 10, 9, 7, 7, 10, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 7, 10, 9, 10, 9, 8, 8, 7, 10, 8, 7, 10, 9, 8, 8, 8, 7, 9, 8, 10, 7, 7, 10, 10, 10, 10, 10, 9, 10, 7, 10, 10, 10, 10, 10, 10, 10, 7, 9, 10, 7, 9, 7, 9, 7, 8, 8, 9, 9, 7, 10, 10, 10, 10, 7, 8, 8, 7, 10, 9, 10, 10, 9, 9, 8, 8, 7, 9, 9, 8, 10, 9, 9, 10, 8, 10, 8, 9, 7, 7, 8, 10, 8, 10, 10, 9, 7, 8, 10, 10, 8, 9, 9, 9, 10, 9, 7, 10, 8, 8, 7, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 10, 8, 10, 10, 8, 8, 7, 7, 7, 9, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 10, 9, 10, 8, 7, 9, 8, 8, 7, 8, 10, 9, 8, 9, 8, 8, 10, 9, 9, 8, 8, 8, 8, 9, 10, 9, 9, 10, 9, 8, 10, 10, 10, 10, 8, 7, 7, 7, 7, 7, 9, 8, 8, 10, 7, 9, 7, 7, 7, 8, 7, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 10, 7, 10, 7, 10, 10, 9, 10, 8, 10, 10, 10, 9, 9, 8, 7, 9, 9, 10, 9, 10, 9, 10, 9, 9, 8, 10, 9, 9, 8, 7, 9, 8, 7, 10, 8, 7, 10, 10, 7, 7, 10, 10, 7, 8, 10, 10, 7, 9, 8, 10, 8, 8, 10, 10, 7, 8, 8, 10, 8, 8, 7, 10, 9, 9, 10, 10, 8, 7, 9, 10, 9, 10, 10, 10, 9, 9, 10, 10, 10, 7, 9, 7, 10, 10, 9, 9, 10, 9, 10, 9, 10, 10, 10, 9, 9, 9, 9, 7, 10, 7, 7, 7, 10, 10, 7, 7, 10, 7, 10, 9, 8, 7, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 8, 7, 9, 7, 10, 9, 9, 8, 10, 7, 10, 7, 10, 9, 8, 7, 10, 8, 8, 7, 9, 10, 10, 8, 7, 10, 8, 8, 10, 8, 7, 10, 9, 10, 7, 9, 10, 10, 7, 9, 7, 7, 9, 7, 8, 10, 10, 9, 8, 7, 10, 7, 8, 8, 7, 7, 8, 7, 10, 7, 7, 10, 8, 7, 9, 7, 8, 8, 9, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 10, 10, 8, 10, 8, 8, 10, 7, 8, 7, 9, 10, 10, 9, 7, 7, 7, 10, 9, 10, 7, 7, 8, 10, 8, 8, 10, 10, 10, 9, 8, 7, 9, 8, 10, 7, 8, 10, 9, 7, 7, 7, 9, 10, 10, 10, 7, 7, 7, 9, 10, 10, 10, 7, 7, 9, 10, 7, 7, 7, 7, 7, 7, 7, 10, 9, 7, 7, 10, 7, 9, 8, 10, 10, 10, 8, 10, 10, 8, 7, 9, 9, 10, 7, 7, 8, 7, 10, 10, 8, 10, 9, 7, 8, 7, 7, 9, 10, 8, 10, 9, 10, 7, 10, 9, 7, 10, 10, 10, 7, 7, 7, 7, 9, 10, 10, 10, 8, 7, 7, 10, 10, 7, 10, 7, 7, 9, 9, 7, 7, 8, 10, 8, 10, 9, 7, 10, 10, 9, 10, 8, 10, 10, 10, 10, 7, 9, 8, 7, 10, 10, 7, 10, 10, 10, 10, 7, 7, 7, 10, 10, 9, 9, 7, 7, 9, 10, 9, 10, 9, 7, 7, 7, 9, 10, 7, 7, 8, 7, 9, 10, 10, 9, 7, 8, 7, 9, 10, 10, 10, 8, 7, 9, 8, 8, 7, 9, 9, 9, 10, 7, 8, 7, 10, 8, 7, 7, 9, 7, 7, 9, 7, 9, 7, 10, 9, 7, 7, 10, 7, 10, 9, 9, 9, 10, 7, 10, 7, 7, 10, 7, 10, 7, 9, 8, 10, 8, 10, 8, 7, 7, 7, 7, 10, 10, 10, 8, 8, 7, 9, 10, 7, 8, 10, 10, 10, 7, 10, 7, 7, 8, 7, 7, 7, 9, 7, 7, 7, 10, 7, 10, 8, 8, 8, 10, 8, 8, 8, 7, 8, 10, 9, 8, 8, 8, 10, 9, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 7, 7, 10, 7, 7, 7, 10, 7, 7, 10, 7, 8, 7, 10, 9, 10, 10, 10, 10, 8, 8, 10, 10, 8, 8, 7, 9, 10, 8, 9, 10, 9, 9, 9, 7, 7, 7, 8, 10, 10, 7, 7, 10, 7, 9, 7, 7, 10, 9, 9, 10, 10, 8, 7, 9, 9, 7, 9, 9, 9, 7, 8, 7, 7, 9, 10, 7, 10, 7, 10, 9, 8, 7, 9, 8, 7, 10, 7, 8, 8, 10, 9, 10, 8, 7, 10, 10, 10, 10, 8, 8, 7, 10, 10, 8, 7, 7, 9, 10, 10, 10, 10, 7, 8, 10, 10, 10, 7, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 10, 10, 7, 10, 10, 7, 10, 10, 10, 10, 10, 10, 7, 7, 7, 7, 10, 7, 10, 10, 10, 8, 7, 7, 10, 7, 9, 10, 10, 10, 10, 9, 9, 9, 9, 10, 7, 8, 7, 9, 9, 10, 9, 9, 9, 10, 10, 10, 10, 10, 9, 9, 10, 10, 7, 8, 7, 10, 7, 9, 7, 10, 9, 10, 10, 10, 10, 10, 10, 7, 9, 10, 9, 7, 10, 9, 7, 10, 10, 10, 8, 10, 9, 7, 9, 7, 10, 10, 10, 10, 7, 9, 10, 9, 8, 7, 8, 8, 10, 9, 10, 8, 7, 8, 8, 10, 9, 7, 8, 8, 7, 9, 10, 9, 10, 7, 10, 7, 8, 10, 9, 10, 7, 8, 8, 8, 7, 7, 10, 7, 10, 7, 10, 7, 9, 10, 8, 10, 10, 10, 10, 7, 10, 8, 8, 8, 10, 8, 10, 8, 7, 7, 9, 8, 10, 10, 8, 8, 8, 8, 8, 8, 8, 7, 10, 8, 8, 10, 8, 7, 7, 7, 9, 10, 8, 8, 7, 10, 10, 8, 10, 7, 10, 10, 7, 9, 10, 8, 10, 10, 7, 8, 9, 8, 8, 10, 10, 10, 9, 8, 9, 10, 8, 8, 10, 8, 7, 10, 7, 9, 8, 10, 9, 7, 7, 8, 10, 8, 10, 8, 7, 8, 10, 10, 9, 10, 7, 7, 9, 10, 9, 7, 9, 7, 7, 8, 7, 10, 7, 8, 9, 7, 8, 7, 10, 10, 10, 9, 9, 10, 10, 10, 10, 8, 8, 7, 10, 10, 8, 8, 10, 9, 7, 7, 10, 10, 7, 8, 10, 8, 7, 8, 10, 10, 7, 9, 7, 7, 10, 7, 7, 10, 9, 9, 8, 8, 10, 8, 8, 7, 7, 10, 10, 8, 8, 7, 10, 8, 8, 7, 7, 9, 10, 10, 10, 8, 10, 9, 8, 7, 7, 7, 7, 9, 7, 8, 7, 10, 10, 7, 10, 10, 10, 8, 7, 10, 10, 8, 8, 10, 10, 10, 10, 10, 7, 10, 9, 9, 8, 10, 7, 7, 9, 10, 7, 10, 10, 8, 7, 7, 10, 9, 9, 10, 7, 10, 7, 10, 7, 10, 7, 8, 7, 8, 8, 7, 8, 7, 10, 10, 10, 10, 8, 10, 10, 10, 7, 10, 8, 8, 7, 8, 10, 10, 9, 10, 10, 9, 9, 10, 8, 7, 10, 10, 9, 9, 9, 8, 7, 8, 10, 10, 9, 9, 9, 10, 10, 9, 9, 10, 10, 8, 8, 7, 10, 7, 10, 8, 10, 10, 10, 9, 8, 7, 9, 10, 10, 9, 10, 9, 7, 7, 10, 10, 9, 10, 9, 8, 10, 9, 8, 10, 7, 10, 7, 7, 7, 8, 7, 10, 9, 8, 7, 10, 9, 10, 7, 8, 7, 10, 9, 10, 9, 10, 8, 10, 10, 10, 10, 10, 8, 7, 10, 7, 10, 7, 7, 10, 9, 7, 8, 10, 10, 8, 10, 10, 10, 10, 7, 8, 10, 10, 10, 9, 9, 9, 10, 9, 9, 9, 10, 7, 8, 8, 8, 7, 9, 10, 7, 9, 10, 9, 9, 9, 7, 10, 10, 9, 8, 10, 9, 9, 7, 10, 8, 7, 7, 7, 8, 7, 9, 10, 7, 9, 10, 10, 8, 10, 7, 10, 10, 10, 10, 10, 7, 9, 10, 10, 8, 10, 10, 10, 7, 7, 9, 9, 7, 7, 10, 8, 9, 8, 8, 7, 10, 7, 8, 10, 9, 10, 10, 10, 10, 8, 8, 7, 10, 7, 9, 10, 9, 9, 10, 10, 9, 10, 7, 8, 10, 7, 9, 10, 10, 10, 7, 8, 7, 10, 10, 8, 8, 8, 7, 7, 8, 7, 9, 8, 7, 9, 10, 9, 10, 8, 7, 7, 7, 9, 10, 9, 10, 10, 8, 7, 10, 10, 10, 9, 10, 8, 7, 8, 8, 7, 8, 7, 10, 8, 8, 7, 8, 7, 8, 8, 7, 10, 8, 10, 7, 10, 10, 7, 10, 10, 10, 10, 10, 10, 9, 7, 10, 10, 10, 10, 10, 7, 7, 7, 10, 10, 7, 10, 9, 9, 8, 8, 7, 10, 10, 8, 10, 10, 9, 8, 7, 9, 9, 7, 9, 10, 7, 7, 7, 10, 9, 7, 10, 7, 10, 8, 10, 8, 7, 10, 10, 9, 10, 9, 9, 10, 10, 10, 10, 7, 7, 10, 10, 10, 9, 8, 7, 10, 10, 10, 8, 8, 8, 10, 9, 7, 10, 7, 7, 10, 10, 9, 9, 10, 9, 7, 10, 9, 10, 10, 9, 7, 9, 8, 7, 10, 8, 10, 10, 10, 10, 8, 7, 10, 7, 10, 9, 10, 10, 10, 9, 10, 10, 9, 9, 8, 10, 10, 7, 10, 10, 9, 10, 7, 10, 9, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 7, 7, 10, 9, 10, 8, 10, 7, 10, 10, 8, 7, 10, 9, 10, 8, 10, 10, 10, 10, 9, 8, 8, 10, 7, 8, 10, 10, 10, 10, 9, 7, 10, 9, 9, 9, 7, 10, 10, 9, 10, 10, 10, 9, 10, 10, 10, 10, 10, 10, 10, 10, 8, 10, 10, 9, 8, 10, 9, 7, 10, 10, 10, 9, 7, 9, 10, 10, 8, 8, 10, 10, 9, 10, 7, 9, 7, 10, 10, 8, 10, 9, 9, 9, 10, 7, 8, 10, 7, 9, 10, 8, 8, 10, 10, 10, 9, 10, 8, 7, 9, 7, 10, 9, 8, 7, 8, 7, 9, 10, 10, 8, 7, 10, 7, 7, 7, 10, 7, 10, 10, 10, 10, 8, 10, 8, 8, 7, 7, 8, 10, 9, 10, 7, 10, 9, 9, 9, 10, 10, 9, 10, 8, 10, 9, 10, 10, 10, 7, 8, 10, 8, 10, 9, 8, 10, 9, 7, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 7, 9, 7, 10, 9, 9, 7, 7, 10, 10, 10, 10, 9, 8, 10, 8, 10, 10, 9, 10, 10, 10, 8, 8, 8, 7, 9, 9, 8, 10, 9, 9, 7, 9, 10, 9, 8, 7, 7, 10, 9, 9, 8, 7, 10, 9, 7, 10, 8, 10, 10, 9, 9, 8, 10, 8, 8, 8, 10, 9, 8, 7, 7, 8, 8, 10, 8, 10, 9, 8, 8, 10, 8, 10, 8, 7, 9, 9, 10, 10, 8, 7, 7, 9, 8, 8, 7, 10, 10, 8, 10, 8, 8, 10, 9, 8, 8, 10, 8, 7, 9, 8, 8, 10, 8, 8, 8, 7, 7, 9, 10, 7, 9, 8, 10, 9, 9, 9, 7, 10, 10, 7, 8, 7, 9, 9, 8, 7, 8, 7, 8, 7, 8, 8, 7, 8, 8, 7, 10, 9, 8, 8, 10, 9, 9, 8, 10, 7, 7, 8, 10, 10, 10, 10, 10, 10, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 2. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10,  9,  ..., 10,  9, 10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0]], dtype=int8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try it again after some changes so it works?\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 900)\n",
    "out = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32768]) tensor([ 7, 10,  7,  ...,  7,  7,  8])\n",
      "torch.Size([32768]) tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "torch.Size([1]) tensor([10.1981])\n"
     ]
    }
   ],
   "source": [
    "print(out[0][0].shape,out[0][0]) #this is a single thing\n",
    "print(out[1][0].shape,out[1][0]) #wait why is this a different size... oh tokenizer, did the cropping before tokenizer now\n",
    "print(out[1][1].shape,out[1][1]) #this is a single thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7, 10,  7,  ...,  7,  7,  8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0][0] #no eos at the beginning or the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0986])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got numpy.float64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mFloatTensor([np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m3\u001b[39m)]))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got numpy.float64)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print(torch.FloatTensor([np.log(3)])) #see this works\n",
    "torch.FloatTensor(np.log(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220311"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#now we can test how long it takes to load in the whole dataset\n",
    "for i in range(1000):\n",
    "    out = dataset[i]\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "#it's quite slow, 1000 every 20 or so seconds? i am not sure we can really speed it up tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took like 20 seconds\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 9000)\n",
    "for i in range(1000):\n",
    "    out = dataset[i] #jitter doesn't slow it down as we obviouslyl expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m profile_atac_long\u001b[38;5;241m.\u001b[39mProfileATACLong(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000000\u001b[39m, tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m, rc_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, jitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m#jitter doesn't slow it down as we obviouslyl expect\u001b[39;00m\n",
      "File \u001b[0;32m/lila/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/profile_atac_long.py:281\u001b[0m, in \u001b[0;36mProfileATACLong.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m#now we need to add our cell type tokens\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m#we will add them to the left\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# seq = seq\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;66;03m#will stick with this for sure\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_eos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is what controls adding eos\u001b[39;49;00m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     seq \u001b[38;5;241m=\u001b[39m seq[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# get input_ids\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbpe\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2523\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2611\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2626\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2627\u001b[0m     )\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2694\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2695\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2700\u001b[0m )\n\u001b[0;32m-> 2702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    616\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:579\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m--> 579\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:582\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    579\u001b[0m         ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_token_to_id_with_added_voc(token))\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ids\n\u001b[0;32m--> 582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_token_to_id_with_added_voc\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = profile_atac_long.ProfileATACLong('train', 1000000, tokenizer_name = 'char', rc_aug = True, jitter = 90000)\n",
    "for i in range(1000):\n",
    "    out = dataset[i] #but longer context does! 1000 takes 4 min for 386!!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try the steps to see how fast it is\n",
    "offset = 500000\n",
    "chrom = 'chr1'\n",
    "center = 1000000 #this is quite quick\n",
    "for i in range(1000):\n",
    "    seq = dataset.genome[chrom][center-offset:center+offset].seq.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:42<00:00, 23.47it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #this is a bit slower but it's still quite quick!\n",
    "    dna_to_one_hot([seq])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [01:16<00:00, 13.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    cts = np.nan_to_num(dataset.cts_bw.values(chrom, center-offset, center+offset))\n",
    "#this is indeed a major slowdown...\n",
    "#we need to find a way to speed this up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 530/1000 [03:17<02:55,  2.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     temp \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# this is what controls adding eos\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#ahh indeed this is the slow part!\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2523\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2611\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2626\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2627\u001b[0m     )\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2632\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2644\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2694\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2695\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2699\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2700\u001b[0m )\n\u001b[0;32m-> 2702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2705\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2720\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    516\u001b[0m no_split_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique_no_split_tokens)\n\u001b[0;32m--> 517\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_trie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# [\"This is something\", \"<special_token_1>\", \"  else\"]\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/transformers/tokenization_utils.py:151\u001b[0m, in \u001b[0;36mTrie.split\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    148\u001b[0m reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# In this case, we already have partial matches (But unfinished)\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, trie_pointer \u001b[38;5;129;01min\u001b[39;00m states\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m trie_pointer:\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;66;03m# This is a final match, we need to reset and\u001b[39;00m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# store the results in `offsets`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# matches\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;66;03m# \"[CLS]\", \"L\", we need to match CLS even if L is special\u001b[39;00m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m lookstart, looktrie_pointer \u001b[38;5;129;01min\u001b[39;00m states\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    temp = dataset.tokenizer(seq,\n",
    "                add_special_tokens=False,  # this is what controls adding eos\n",
    "                padding=\"max_length\",\n",
    "                max_length=dataset.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "#ahh indeed this is the slow part!\n",
    "#this is priority number 1 in terms of needing to speed up!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[CLS]',\n",
       " 1: '[SEP]',\n",
       " 2: '[BOS]',\n",
       " 3: '[MASK]',\n",
       " 4: '[PAD]',\n",
       " 5: '[RESERVED]',\n",
       " 6: '[UNK]',\n",
       " 7: 'A',\n",
       " 8: 'C',\n",
       " 9: 'G',\n",
       " 10: 'T',\n",
       " 11: 'N'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tokenizer._vocab_int_to_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7, 7, 9, 9, 9, 8, 8, 8, 10, 10, 10, 11, 11, 11, 11, 11, 10]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's just create a dictionary to tokenize\n",
    "mapping = {\n",
    "                    'A': 7,\n",
    "                    'C': 8,\n",
    "                    'G': 9,\n",
    "                    'T': 10,\n",
    "                    'N': 11\n",
    "                    }\n",
    "\n",
    "def tokenize_sequence(sequence, mapping):\n",
    "    return [mapping.get(nuc, 11) for nuc in sequence]  # Default to '11' for unknown nucleotides\n",
    "tokenize_sequence('AAAGGGCCCTTTNNNNNT', mapping) #yeah clearly seems correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenize_sequence(1_000_000*'A',mapping) #instant even for 1 million!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 7,  ..., 7, 7, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.LongTensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:07<00:00, 130.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#now let's try it again\n",
    "from tqdm import tqdm\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 900)\n",
    "for i in tqdm(range(1000)):\n",
    "    out = dataset[i] #this is quite quick now! 7 seconds for 30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 121/1000 [00:30<03:38,  4.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m profile_atac_long\u001b[38;5;241m.\u001b[39mProfileATACLong(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000000\u001b[39m, tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m, rc_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, jitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m#this is \u001b[39;00m\n",
      "File \u001b[0;32m/lila/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/profile_atac_long.py:275\u001b[0m, in \u001b[0;36mProfileATACLong.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    273\u001b[0m     one_hot_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# offset = self.max_length//2 + self.jitter\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m cts \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan_to_num\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcts_bw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc_aug \u001b[38;5;129;01mand\u001b[39;00m coin_flip():\n\u001b[1;32m    278\u001b[0m     seq \u001b[38;5;241m=\u001b[39m string_reverse_complement(seq)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/numpy/lib/type_check.py:497\u001b[0m, in \u001b[0;36mnan_to_num\u001b[0;34m(x, copy, nan, posinf, neginf)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_nan_to_num_dispatcher)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnan_to_num\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, posinf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, neginf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    405\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    Replace NaN with zero and infinity with large finite numbers (default\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    behaviour) or with the numbers defined by the user using the `nan`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m    array([222222.+111111.j, 111111.     +0.j, 111111.+222222.j])\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     x \u001b[38;5;241m=\u001b[39m _nx\u001b[38;5;241m.\u001b[39marray(x, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    498\u001b[0m     xtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m    500\u001b[0m     isscalar \u001b[38;5;241m=\u001b[39m (x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = profile_atac_long.ProfileATACLong('train', 1000000, tokenizer_name = 'char', rc_aug = True, jitter = 90000)\n",
    "for i in tqdm(range(1000)):\n",
    "    out = dataset[i] #this is still quite slow, like several minutes still, but much faster than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 94/1000 [00:07<01:08, 13.22it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rev_comp\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mstring_reverse_complement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mstring_reverse_complement\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m seq[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m string_complement_map:\n\u001b[0;32m----> 7\u001b[0m         rev_comp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m string_complement_map[base]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# if bp not complement map, use the same bp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         rev_comp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq = 1_000_000*'A'\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "for i in tqdm(range(1000)):\n",
    "    string_reverse_complement(seq)\n",
    "#not a major slowdown, but could be quite a bit quicker if we do numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500000\n",
      "3500000\n",
      "[7, 8, 8, 8, 7, 8, 9, 10, 10, 8]\n",
      "[9, 7, 7, 8, 9, 10, 9, 9, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "mapping = {\n",
    "                    'A': 7,\n",
    "                    'C': 8,\n",
    "                    'G': 9,\n",
    "                    'T': 10,\n",
    "                    'N': 11\n",
    "                    }\n",
    "seq = 'ACCCACGTTCGTTTTTTGGGCCCAAAAAAAAAAAA'*100000\n",
    "print(len(seq))\n",
    "tokenized = [mapping.get(nuc, 11) for nuc in seq]\n",
    "print(len(tokenized))\n",
    "#now let's try to RC it\n",
    "RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}\n",
    "def tokenized_rc(tokenized):\n",
    "    return [RC_mapping.get(nuc, 11) for nuc in tokenized[::-1]]\n",
    "tokenized_rc(tokenized)\n",
    "print(tokenized[:10])\n",
    "print(tokenized_rc(tokenized)[-10:]) #clearly if you flip it it's complement, so yees seems to be working niceley!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "seq = 'ACCCACGTTCGTTTTTTGGGCCCAAAAAAAAAAAA'\n",
    "print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 104/1000 [00:08<01:09, 12.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m [mapping\u001b[38;5;241m.\u001b[39mget(nuc, \u001b[38;5;241m11\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m nuc \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#now let's try to RC it\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_rc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#this is quite quick\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mtokenized_rc\u001b[0;34m(tokenized)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenized_rc\u001b[39m(tokenized):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mRC_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnuc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnuc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenized_rc\u001b[39m(tokenized):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [RC_mapping\u001b[38;5;241m.\u001b[39mget(nuc, \u001b[38;5;241m11\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m nuc \u001b[38;5;129;01min\u001b[39;00m tokenized[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#let's test the speed of this mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2000007 function calls in 0.391 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.386    0.386 1580664104.py:3(main)\n",
      "        1    0.138    0.138    0.190    0.190 1580664104.py:5(<listcomp>)\n",
      "        1    0.004    0.004    0.196    0.196 3760089249.py:14(tokenized_rc)\n",
      "        1    0.139    0.139    0.193    0.193 3760089249.py:15(<listcomp>)\n",
      "        1    0.005    0.005    0.391    0.391 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.391    0.391 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "  2000000    0.105    0.000    0.105    0.000 {method 'get' of 'dict' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "def main():\n",
    "    seq = 'A' * 1_000_000\n",
    "    tokenized = [mapping.get(nuc, 11) for nuc in seq]\n",
    "    a = tokenized_rc(tokenized)\n",
    "\n",
    "cProfile.run('main()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCCGGGT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's try calling this with C\n",
    "\n",
    "import subprocess\n",
    "c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/reverse_complement'\n",
    "input_sequence = 'ACCCGGGTN' #huh ignores this last index for some reason...\n",
    "process = subprocess.Popen(\n",
    "    c_program,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "stdout, stderr = process.communicate(input=input_sequence.encode())\n",
    "\n",
    "print(stdout.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PNNGT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's try calling this with C\n",
    "\n",
    "import subprocess\n",
    "c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/reverse_complement'\n",
    "input_sequence = 'ACNNPP' #huh ignores this last index for some reason and doesn't map it\n",
    "process = subprocess.Popen(\n",
    "    c_program,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "stdout, stderr = process.communicate(input=input_sequence.encode())\n",
    "\n",
    "print(stdout.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:09<00:00, 105.53it/s]\n"
     ]
    }
   ],
   "source": [
    "#alright let's try this with a huge thing\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    input_sequence = 'A'*1_000_000\n",
    "    process = subprocess.Popen(\n",
    "        c_program,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=input_sequence.encode())\n",
    "    a = stdout.decode()\n",
    "    # print(stdout.decode())\n",
    "#very fast reverse complement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT\n"
     ]
    }
   ],
   "source": [
    "print(a[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so this is how we could do it with multiple workders\n",
    "\n",
    "import subprocess\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define the path to the compiled C program\n",
    "C_PROGRAM = \"./reverse_complement\"\n",
    "\n",
    "class GenomeDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx]\n",
    "\n",
    "def call_c_program(sequence):\n",
    "    process = subprocess.Popen(\n",
    "        C_PROGRAM,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=sequence.encode())\n",
    "    \n",
    "    if stderr:\n",
    "        raise RuntimeError(stderr.decode())\n",
    "    \n",
    "    return stdout.decode().strip()\n",
    "\n",
    "def worker_function(sequence):\n",
    "    return call_c_program(sequence)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    with Pool(len(batch)) as pool:\n",
    "        results = pool.map(worker_function, batch)\n",
    "    return results\n",
    "\n",
    "# Example input sequences\n",
    "sequences = ['A' * 1_000_000 for _ in range(100)]  # 100 sequences, each 1 million 'A's\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = GenomeDataset(sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=4, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# Iterate over dataloader\n",
    "for batch in dataloader:\n",
    "    for result in batch:\n",
    "        print(result[:50])  # Print the first 50 characters of each result to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123012301230123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we also did a tokenize for C\n",
    "\n",
    "import subprocess\n",
    "c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/tokenize'\n",
    "input_sequence = 'ACGTACGTACGTACGTN' #huh ignores this last index for some reason and doesn't map it\n",
    "process = subprocess.Popen(\n",
    "    c_program,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "stdout, stderr = process.communicate(input=input_sequence.encode())\n",
    "\n",
    "print(stdout.decode())\n",
    "#oh this doesn't work because T outputs 2, that's actualy quite annoying!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9, 10, 7, 8, 9, 10, 7, 8, 9, 10, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "#turn every element of the string to a list\n",
    "out = [int(char)+7 for char in stdout.decode()[:-1]]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'A'*1_000_000\n",
    "process = subprocess.Popen(\n",
    "    c_program,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "stdout, stderr = process.communicate(input=seq.encode())\n",
    "tokenized = [int(char)+7 for char in stdout.decode()[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999999"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:45<00:00, 21.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#now try with the further speedups\n",
    "#let's compare the RC for example\n",
    "RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}\n",
    "def tokenized_rc(tokenized):\n",
    "    return [RC_mapping.get(nuc, 11) for nuc in tokenized[::-1]]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    tokenized_rc(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTTTTTTTTT\n",
      " 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq = 'A'*10+'N' #need this N so that it's correct\n",
    "    #first RC\n",
    "c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/reverse_complement'\n",
    "process = subprocess.Popen(\n",
    "    c_program,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "stdout, stderr = process.communicate(input=seq.encode())\n",
    "print(stdout.decode(), len(stdout.decode().strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 141/1000 [00:17<01:48,  7.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m     c_program \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m     process2 \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m     16\u001b[0m         c_program,\n\u001b[1;32m     17\u001b[0m         stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     18\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     19\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# tokenized = [int(char)+7 for char in stdout.decode()[:-1]]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#lmao we jsut wasted this time, it's very slow!\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#let's combine both together and see how fast it is\n",
    "for i in tqdm(range(1000)):\n",
    "    seq = 'A'*1_000_000\n",
    "    #first RC\n",
    "    c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/reverse_complement'\n",
    "    process = subprocess.Popen(\n",
    "        c_program,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=seq.encode())\n",
    "    a = stdout.decode().strip()\n",
    "    c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/tokenize'\n",
    "    process2 = subprocess.Popen(\n",
    "        c_program,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process2.communicate(input=seq.encode())\n",
    "    tokenized = [int(char)+7 for char in stdout.decode()[:-1]] #this part is a bit slow but it's not the slow part, about 50% of the slowdown\n",
    "\n",
    "#lmao we jsut wasted this time, it's very slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:43<00:00, 22.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#so this seems to be the slow part rn\n",
    "#first let's just solve the tokenizer and see if we are indeed faster than the hugging face tokenizer\n",
    "mapping = {\n",
    "                    'A': 7,\n",
    "                    'C': 8,\n",
    "                    'G': 9,\n",
    "                    'T': 10,\n",
    "                    'N': 11\n",
    "                    }\n",
    "seq = 'A'*1_000_000\n",
    "for i in tqdm(range(1000)):\n",
    "    tokenized = [mapping.get(nuc, 11) for nuc in seq]\n",
    "#it is indeed decently fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 74/1000 [00:20<04:20,  3.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m     c_program \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     process \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m      6\u001b[0m         c_program,\n\u001b[1;32m      7\u001b[0m         stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m      8\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m      9\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[1;32m     10\u001b[0m     )\n\u001b[0;32m---> 11\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(char)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m stdout\u001b[38;5;241m.\u001b[39mdecode()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#ok so this tokenizer is much much faster\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/subprocess.py:2108\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2102\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2103\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2106\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2108\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2111\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#and the C implementation\n",
    "seq = 'A'*1_000_000+'N'\n",
    "for i in tqdm(range(1000)):\n",
    "    c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/tokenize'\n",
    "    process = subprocess.Popen(\n",
    "        c_program,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=seq.encode())\n",
    "    tokenized = [int(char)+7 for char in stdout.decode()[:-1]]\n",
    "#ok so this tokenizer is much much slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:18<00:00, 52.71it/s]\n"
     ]
    }
   ],
   "source": [
    "#let's look at the C implementation of the RC\n",
    "seq = 'A'*1_000_000+'N'\n",
    "for i in tqdm(range(1000)):\n",
    "    c_program = '/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/reverse_complement'\n",
    "    process = subprocess.Popen(\n",
    "        c_program,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n",
    "    stdout, stderr = process.communicate(input=seq.encode())\n",
    "    a = stdout.decode().strip()\n",
    "#this is decently fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 95/1000 [00:10<01:43,  8.75it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rev_comp\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m---> 14\u001b[0m     \u001b[43mstring_reverse_complement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 8\u001b[0m, in \u001b[0;36mstring_reverse_complement\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m seq[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m string_complement_map:\n\u001b[0;32m----> 8\u001b[0m         rev_comp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m string_complement_map[base]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# if bp not complement map, use the same bp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m         rev_comp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#and let's make a python implementation\n",
    "seq = 'A'*1_000_000\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "for i in tqdm(range(1000)):\n",
    "    string_reverse_complement(seq) #this is much slower than the C version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:47<00:00, 21.14it/s]\n"
     ]
    }
   ],
   "source": [
    "#what about using my method\n",
    "seq = 'A'*1_000_000\n",
    "RCmap = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}\n",
    "for i in tqdm(range(1000)):\n",
    "    a = [RCmap.get(nuc, 'N') for nuc in seq] #faster than their implementation, but it also does make it a list. 44 seconds without joining\n",
    "    #and turn it back into a string\n",
    "    a = ''.join(a) #ok basically the same time with this, so it's not the joining that's slow\n",
    "    \n",
    "#let's use the c implementation for RC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cwd \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 4\u001b[0m dir_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18;43m__file__\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(dir_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(subprocess\u001b[38;5;241m.\u001b[39mcheck_output([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msetup.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#now let's try this fast seqpy approach\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "os.chdir(dir_path)\n",
    "print(subprocess.check_output(['python','setup.py', 'build_ext', '-i']))\n",
    "os.chdir(cwd)\n",
    "import seqpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 7, 9, 9, 9, 7, 8, 9, 10, 10, 10, 10, 10, 10, 10]\n",
      "[7, 7, 7, 7, 7, 7, 7, 8, 9, 10, 8, 8, 8, 10, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "#not worth the hassle, for now we will just let the training be slower, especially with a large number of workers, it's all good!\n",
    "#lots of overlap likely lmfao!\n",
    "#can use the C RC if we really want to\n",
    "\n",
    "#the other crazy option is to have the entire RC of the human chromosome... could just access that and reverse it..\n",
    "#let's test what I have with some random sequences\n",
    "\n",
    "seq = 'AAAGGGACGTTTTTTT'\n",
    "tokenizer_mapping = {'A': 7,'C': 8,'G': 9,'T': 10,'N': 11}\n",
    "RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}\n",
    "seq2 = [tokenizer_mapping.get(nuc, 11) for nuc in seq]\n",
    "print(seq2)\n",
    "seq3 = [RC_mapping.get(nuc, 11) for nuc in seq2[::-1]] #i'm pretty sure this works\n",
    "print(seq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [01:31<00:00, 10.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#combined let's see the speed\n",
    "for i in tqdm(range(1000)):\n",
    "    seq = 'A'*1_000_000\n",
    "    seq2 = [tokenizer_mapping.get(nuc, 11) for nuc in seq]\n",
    "    seq3 = [RC_mapping.get(nuc, 11) for nuc in seq2[::-1]]\n",
    "    #so 1000 sequences does indeed take a while... but it's not the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m output_fasta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Run the reverse complement function\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mreverse_complement_fasta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fasta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_fasta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#there's no way this is this fast\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[74], line 13\u001b[0m, in \u001b[0;36mreverse_complement_fasta\u001b[0;34m(input_fasta, output_fasta)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a list to hold the reverse complement records\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rev_comp_records \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m195\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Compute the reverse complement\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrev_comp_seq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrecord\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreverse_complement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create a new SeqRecord with the reverse complement sequence\u001b[39;49;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/Bio/SeqIO/Interfaces.py:85\u001b[0m, in \u001b[0;36mSequenceIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the next entry.\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecords)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_close_stream:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/Bio/SeqIO/FastaIO.py:207\u001b[0m, in \u001b[0;36mFastaIterator.iterate\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;66;03m# Should we use SeqRecord default for no ID?\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     first_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m SeqRecord(\n\u001b[0;32m--> 207\u001b[0m     \u001b[43mSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mfirst_word, name\u001b[38;5;241m=\u001b[39mfirst_word, description\u001b[38;5;241m=\u001b[39mtitle\n\u001b[1;32m    208\u001b[0m )\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/Bio/Seq.py:2123\u001b[0m, in \u001b[0;36mSeq.__init__\u001b[0;34m(self, data, length)\u001b[0m\n\u001b[1;32m   2121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(data)\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mASCII\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#what about storing whole reference genome\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def reverse_complement_fasta(input_fasta, output_fasta):\n",
    "    # Read the input FASTA file\n",
    "    records = SeqIO.parse(input_fasta, \"fasta\")\n",
    "    \n",
    "    # Create a list to hold the reverse complement records\n",
    "    rev_comp_records = []\n",
    "    \n",
    "    for record in tqdm(records, total=195):\n",
    "        # Compute the reverse complement\n",
    "        rev_comp_seq = record.seq.reverse_complement()\n",
    "        \n",
    "        # Create a new SeqRecord with the reverse complement sequence\n",
    "        rev_comp_record = SeqRecord(\n",
    "            rev_comp_seq,\n",
    "            id=record.id,\n",
    "            name=record.name,\n",
    "            description=f\"reverse complement of {record.description}\"\n",
    "        )\n",
    "        \n",
    "        # Append the reverse complement record to the list\n",
    "        rev_comp_records.append(rev_comp_record)\n",
    "    \n",
    "    # Write the reverse complement records to the output FASTA file\n",
    "    SeqIO.write(rev_comp_records, output_fasta, \"fasta\")\n",
    "\n",
    "# Define the input and output FASTA file paths\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\"\n",
    "output_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa\"\n",
    "\n",
    "# Run the reverse complement function\n",
    "reverse_complement_fasta(input_fasta, output_fasta)\n",
    "#there's no way this is this fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_records(fasta_file):\n",
    "    # Count the number of records in the FASTA file\n",
    "    with open(fasta_file, \"r\") as handle:\n",
    "        return sum(1 for _ in SeqIO.parse(handle, \"fasta\"))\n",
    "count_records(input_fasta) #this is the number of records or reads, this is what we put into the total for tqdm\n",
    "#195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTAAGCACA'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's test if my reverse complement is indeed the case\n",
    "import pyfaidx\n",
    "genome = pyfaidx.Fasta(input_fasta)\n",
    "genome_rc = pyfaidx.Fasta(output_fasta)\n",
    "#let's test a random sequence\n",
    "genome['chr1'][100000:100010].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TGTGCTTAGT'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's find the corresponding elements for the RC\n",
    "genome_rc['chr1'][248956422-100010:248956422-100000].seq\n",
    "\n",
    "#this is indeed the reverse complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTAAGCACACAGAGAATAATGTCTAGAATCTGAGTGCCATGTTATCAAA'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try a larger sequence\n",
    "genome['chr1'][100000:100050].seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTTGATAACATGGCACTCAGATTCTAGACATTATTCTCTGTGTGCTTAGT'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = genome_rc['chr1'][248956422-100050:248956422-100000]\n",
    "a.seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACTAAGCACACAGAGAATAATGTCTAGAATCTGAGTGCCATGTTATCAAA'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do the rc of this sequence a\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "string_reverse_complement(a.seq)\n",
    "#copy paste from above\n",
    "#ACTAAGCACACAGAGAATAATGTCTAGAATCTGAGTGCCATGTTATCAAA\n",
    "#ACTAAGCACACAGAGAATAATGTCTAGAATCTGAGTGCCATGTTATCAAA\n",
    "#perfect match!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's test this reverse complement stuff\n",
    "import numpy as np\n",
    "import pyfaidx\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\"\n",
    "output_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa\"\n",
    "genome = pyfaidx.Fasta(input_fasta)\n",
    "genome_rc = pyfaidx.Fasta(output_fasta)\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "for i in range(1000):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1000)\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    chrom_len = len(genome[chrom])\n",
    "    seq_rc = genome_rc[chrom][chrom_len-end:chrom_len-start].seq\n",
    "    if seq != string_reverse_complement(seq_rc):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(seq), len(seq_rc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTCAAACAAC\n",
      "GTTGTTTGAG\n"
     ]
    }
   ],
   "source": [
    "#huh what's the error\n",
    "print(seq[:10])\n",
    "print(seq_rc[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": [
    "#so it seems fine for lengths of 1000, but at 1 million it causes issues\n",
    "\n",
    "for i in range(1000):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    chrom_len = len(genome[chrom])\n",
    "    seq_rc = genome_rc[chrom][chrom_len-end:chrom_len-start].seq\n",
    "    if seq != string_reverse_complement(seq_rc):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGCTCTTTC GGAGCTTGCA\n",
      "TGCAAGCTCC GAAAGAGCGA\n"
     ]
    }
   ],
   "source": [
    "print(seq[:10], seq[-10:]) #so not at the edge, unclear where the issue is\n",
    "print(seq_rc[:10], seq_rc[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Y 100916\n"
     ]
    }
   ],
   "source": [
    "temp = string_reverse_complement(seq_rc)\n",
    "for i in range(len(seq)):\n",
    "    if seq[i] != temp[i]:\n",
    "        print(seq[i], temp[i], i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388457 chr17\n"
     ]
    }
   ],
   "source": [
    "#wtf is R and Y??\n",
    "print(start, chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GRCCCTCGCTC'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome['chr17'][start+100915:start+100926].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Y 100916\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(seq)):\n",
    "    if seq[i] != temp[i]:\n",
    "        print(seq[i], temp[i], i)\n",
    "#just this one issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:03<00:00, 286.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#let's test the speed\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    #the random stuff actually slows it down the most, but if we have our peaks already will be quite fast!\n",
    "    #this is good!\n",
    "    chrom = 'chr1'\n",
    "    start = 1000000\n",
    "    end = int(start + 1e6)\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    chrom_len = len(genome[chrom])\n",
    "    seq_rc = genome_rc[chrom][chrom_len-end:chrom_len-start].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's try it with the actual dataset\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([10,  7, 10,  ..., 10,  9, 10]), None), (tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([10.1966])))\n",
      "((tensor([ 7,  8,  7,  ...,  7, 10,  7]), None), (tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor([10.1966])))\n"
     ]
    }
   ],
   "source": [
    "out1 = dataset[0]\n",
    "out2 = dataset[0] #keep trying until one is the rc of the other\n",
    "print(out1)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32768\n"
     ]
    }
   ],
   "source": [
    "mapping = {7:10, 8:9, 9:8, 10:7, 11:11}\n",
    "rcd = []\n",
    "for i in out1[0][0]:\n",
    "    rcd.append(mapping[i.item()])\n",
    "print(len(rcd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "temp = torch.tensor(rcd[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(temp, out2[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:12<00:00, 83.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#I guess we can just assume it works, but let's see how fast the dataset is\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #quite quick for 32k, but let's try 1 million\n",
    "    out = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = profile_atac_long.ProfileATACLong('train', 1_000_000, tokenizer_name = 'char', rc_aug = True, jitter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 178/1000 [00:59<04:32,  3.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)): \u001b[38;5;66;03m#quite quick for 32k, but let's try 1 million\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/profile_atac_long.py:269\u001b[0m, in \u001b[0;36mProfileATACLong.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    267\u001b[0m center \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(peak[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    268\u001b[0m offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjitter\n\u001b[0;32m--> 269\u001b[0m cts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcts_bw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m#first we see if we do RC, if it is RC get it from the other file\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc_aug \u001b[38;5;129;01mand\u001b[39;00m coin_flip():\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# seq = [self.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #and it's back to very slow...\n",
    "    out = dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think the next slowdown is the bigwig file\n",
    "test = dataset.cts_bw.values('chr1', 1000000, 1000000*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 337/1000 [00:24<00:49, 13.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)): \u001b[38;5;66;03m#and it's back to very slow...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcts_bw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchr1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)): #and it's back to very slow...\n",
    "    dataset.cts_bw.values('chr1', 1000000, 1000000*2)\n",
    "#a little bit slow, but 4 times faster, so 25% of the total speed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1000072 function calls (1000071 primitive calls) in 0.507 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 2592476886.py:3(coin_flip)\n",
      "        1    0.225    0.225    0.310    0.310 2592476886.py:38(<listcomp>)\n",
      "        1    0.079    0.079    0.507    0.507 2592476886.py:8(main)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1207(_handle_fromlist)\n",
      "        1    0.000    0.000    0.507    0.507 <string>:1(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 __init__.py:1117(__getitem__)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:1136(get_seq)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:203(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:523(__contains__)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:714(fetch)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:725(from_file)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:786(format_seq)\n",
      "        2    0.000    0.000    0.000    0.000 __init__.py:85(__init__)\n",
      "        1    0.000    0.000    0.002    0.002 __init__.py:899(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:94(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:960(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2172(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:2177(sum)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:71(_wrapreduction)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:72(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 getlimits.py:484(__new__)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:1080(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 type_check.py:393(_getmaxmin)\n",
      "        1    0.000    0.000    0.000    0.000 type_check.py:399(_nan_to_num_dispatcher)\n",
      "        1    0.006    0.006    0.060    0.060 type_check.py:403(nan_to_num)\n",
      "        2    0.000    0.000    0.000    0.000 ufunclike.py:14(_dispatcher)\n",
      "        1    0.001    0.001    0.001    0.001 ufunclike.py:142(isneginf)\n",
      "        1    0.002    0.002    0.002    0.002 ufunclike.py:71(isposinf)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        1    0.000    0.000    0.507    0.507 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "      7/6    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.051    0.051    0.051    0.051 {built-in method numpy.array}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        1    0.002    0.002    0.002    0.002 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "  1000001    0.085    0.000    0.085    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'indices' of 'slice' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        2    0.001    0.001    0.001    0.001 {method 'replace' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'seek' of '_io.BufferedReader' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'upper' of 'str' objects}\n",
      "        1    0.054    0.054    0.054    0.054 {method 'values' of 'pyBigWig.bigWigFile' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "idx = 0\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "#now use the profiler to determine slowdown\n",
    "import cProfile\n",
    "def main():\n",
    "    peak = dataset.peak_coords[idx]\n",
    "    chrom = peak[0]\n",
    "    center = int(peak[1])\n",
    "    offset = dataset.max_length//2 + dataset.jitter\n",
    "    cts = np.nan_to_num(dataset.cts_bw.values(chrom, center-offset, center+offset))\n",
    "    #first we see if we do RC, if it is RC get it from the other file\n",
    "    if dataset.rc_aug and coin_flip():\n",
    "        # seq = [dataset.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]\n",
    "        lenchrom = len(dataset.genome[chrom])\n",
    "        seq = dataset.genome_rc[chrom][lenchrom-(center+offset):lenchrom-(center-offset)].seq.upper()\n",
    "        cts = cts[::-1]\n",
    "    else:\n",
    "        seq = dataset.genome[chrom][center-offset:center+offset].seq.upper()\n",
    "    if dataset.one_hot:\n",
    "        one_hot_seq = dna_to_one_hot([seq])[0]\n",
    "    else:\n",
    "        one_hot_seq = None\n",
    "    # offset = dataset.max_length//2 + dataset.jitter\n",
    "\n",
    "\n",
    "\n",
    "    if dataset.jitter:\n",
    "        start = np.random.choice(range(dataset.jitter*2+1)) #because range excludes the last one\n",
    "        seq = seq[start:start+dataset.max_length]\n",
    "        cts = cts[start:start+dataset.max_length]\n",
    "        if dataset.one_hot:\n",
    "            one_hot_seq = one_hot_seq[start:start+dataset.max_length]\n",
    "\n",
    "    #just do a manual mapping\n",
    "    seq = [dataset.tokenizer_mapping.get(nuc, 11) for nuc in seq]\n",
    "    #RC after tokenization\n",
    "    counts = np.log(1+np.sum(cts))\n",
    "    seq = torch.LongTensor(seq)\n",
    "    # print(counts)\n",
    "    counts = torch.FloatTensor([counts])\n",
    "    cts = torch.FloatTensor(cts.copy())\n",
    "    if dataset.one_hot:\n",
    "        one_hot_seq = torch.FloatTensor(one_hot_seq.copy()) #is a numpy array so can do .copy()\n",
    "\n",
    "cProfile.run('main()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sequences:   2%|         | 4/195 [03:53<3:05:54, 58.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m output_fasta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.fa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Run the tokenization function\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mtokenize_fasta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fasta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_fasta\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 32\u001b[0m, in \u001b[0;36mtokenize_fasta\u001b[0;34m(input_fasta, output_fasta)\u001b[0m\n\u001b[1;32m     28\u001b[0m tokenized_seq \u001b[38;5;241m=\u001b[39m tokenize_sequence(record\u001b[38;5;241m.\u001b[39mseq)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Create a new SeqRecord with the tokenized sequence\u001b[39;00m\n\u001b[1;32m     31\u001b[0m tokenized_record \u001b[38;5;241m=\u001b[39m SeqRecord(\n\u001b[0;32m---> 32\u001b[0m     Seq(\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m),  \u001b[38;5;66;03m# Join integers as space-separated string\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mrecord\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m     34\u001b[0m     name\u001b[38;5;241m=\u001b[39mrecord\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     35\u001b[0m     description\u001b[38;5;241m=\u001b[39mrecord\u001b[38;5;241m.\u001b[39mdescription\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Append the tokenized record to the list\u001b[39;00m\n\u001b[1;32m     39\u001b[0m tokenized_records\u001b[38;5;241m.\u001b[39mappend(tokenized_record)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#the slowdown is the bigwig and the list comprehension and dictionary parts, let's figure out howw to speed up the gathering of the data\n",
    "#the most obvious method is to just make a new reference genome which is the tokenized version of it already\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the mapping\n",
    "token_mapping = {\n",
    "    'A': 7,\n",
    "    'C': 8,\n",
    "    'G': 9,\n",
    "    'T': 10\n",
    "}\n",
    "\n",
    "def tokenize_sequence(sequence):\n",
    "    # Tokenize the sequence using the mapping, defaulting to 11 for any non-ACGT characters\n",
    "    return [token_mapping.get(base, 11) for base in sequence]\n",
    "\n",
    "def tokenize_fasta(input_fasta, output_fasta):\n",
    "    # Read the input FASTA file\n",
    "    records = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    \n",
    "    # Process each record with progress tracking\n",
    "    tokenized_records = []\n",
    "    for record in tqdm(records, desc=\"Tokenizing sequences\"):\n",
    "        tokenized_seq = tokenize_sequence(record.seq)\n",
    "        \n",
    "        # Create a new SeqRecord with the tokenized sequence\n",
    "        tokenized_record = SeqRecord(\n",
    "            Seq(' '.join(map(str, tokenized_seq))),  # Join integers as space-separated string\n",
    "            id=record.id,\n",
    "            name=record.name,\n",
    "            description=record.description\n",
    "        )\n",
    "        \n",
    "        # Append the tokenized record to the list\n",
    "        tokenized_records.append(tokenized_record)\n",
    "    \n",
    "    # Write the tokenized records to the output FASTA file\n",
    "    SeqIO.write(tokenized_records, output_fasta, \"fasta\")\n",
    "\n",
    "# Define the input and output FASTA file paths\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\"\n",
    "output_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.fa\"\n",
    "\n",
    "# Run the tokenization function\n",
    "tokenize_fasta(input_fasta, output_fasta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 76/1000 [00:14<02:54,  5.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m test_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0123\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m250000\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)):\n\u001b[0;32m----> 5\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#that stores it in an inefficient way, let's try 2 things to see the speed\n",
    "#first is to see hwo fast you can convert a string of 0,1,2,3 into a torch tensor\n",
    "test_seq = '0123'*250000\n",
    "for i in tqdm(range(1000)): #it's also quite slow...\n",
    "    a = torch.LongTensor([int(char) for char in test_seq])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best alternative then seems to be to store the data in hdf5 format with h5py, but that will be a bit complicated, could use something like this\n",
    "\n",
    "import h5py\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Define the mapping\n",
    "token_mapping = {\n",
    "    'A': 7,\n",
    "    'C': 8,\n",
    "    'G': 9,\n",
    "    'T': 10\n",
    "}\n",
    "\n",
    "def tokenize_sequence(sequence):\n",
    "    # Tokenize the sequence using the mapping, defaulting to 11 for any non-ACGT characters\n",
    "    return np.array([token_mapping.get(base, 11) for base in sequence], dtype=np.int32)\n",
    "\n",
    "def tokenize_fasta(input_fasta, output_file):\n",
    "    # Read the input FASTA file\n",
    "    records = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    \n",
    "    # Create an HDF5 file\n",
    "    with h5py.File(output_file, 'w') as h5f:\n",
    "        # Process each record with progress tracking\n",
    "        for record in tqdm(records, desc=\"Tokenizing sequences\"):\n",
    "            tokenized_seq = tokenize_sequence(record.seq)\n",
    "            \n",
    "            # Create a dataset for each sequence\n",
    "            h5f.create_dataset(record.id, data=tokenized_seq)\n",
    "\n",
    "def load_tokenized_data(file_path):\n",
    "    # Load the tokenized sequences from the HDF5 file\n",
    "    with h5py.File(file_path, 'r') as h5f:\n",
    "        tokenized_records = {name: np.array(dataset) for name, dataset in h5f.items()}\n",
    "    return tokenized_records\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_fasta = \"input_genome.fasta\"\n",
    "output_file = \"tokenized_genome.h5\"\n",
    "\n",
    "# Run the tokenization function\n",
    "tokenize_fasta(input_fasta, output_file)\n",
    "\n",
    "# Load the tokenized data (for demonstration purposes)\n",
    "tokenized_data = load_tokenized_data(output_file)\n",
    "\n",
    "# Access a tokenized sequence (for demonstration purposes)\n",
    "example_key = list(tokenized_data.keys())[0]\n",
    "print(f\"Tokenized sequence for {example_key}: {tokenized_data[example_key][:50]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sequences: 100%|| 195/195 [05:16<00:00,  1.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#for now we'll just stick with this slower approach and call it goood, no need to stress that much about it we know this works, let's just test some stuff with it!\n",
    "\n",
    "#wait, let's try storing it as an npz file where we have each chromosome as one of the keys\n",
    "\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the mapping\n",
    "token_mapping = {\n",
    "    'A': 7,\n",
    "    'C': 8,\n",
    "    'G': 9,\n",
    "    'T': 10\n",
    "}\n",
    "\n",
    "def tokenize_sequence(sequence):\n",
    "    # Tokenize the sequence using the mapping, defaulting to 11 for any non-ACGT characters\n",
    "    return np.array([token_mapping.get(base, 11) for base in sequence], dtype=np.int32)\n",
    "\n",
    "def tokenize_fasta(input_fasta, output_file):\n",
    "    # Read the input FASTA file\n",
    "    records = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    \n",
    "    # Create a dictionary to hold tokenized sequences\n",
    "    tokenized_records = {}\n",
    "    \n",
    "    # Process each record with progress tracking\n",
    "    for record in tqdm(records, desc=\"Tokenizing sequences\"):\n",
    "        tokenized_seq = tokenize_sequence(record.seq)\n",
    "        \n",
    "        # Store the tokenized sequence in the dictionary\n",
    "        tokenized_records[record.id] = tokenized_seq\n",
    "    \n",
    "    # Save the tokenized sequences to a .npz file\n",
    "    np.savez_compressed(output_file, **tokenized_records)\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\"\n",
    "output_file = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz\"\n",
    "\n",
    "# Run the tokenization function\n",
    "tokenize_fasta(input_fasta, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz' with keys: chr1, chr2, chr3, chr4, chr5...\n"
     ]
    }
   ],
   "source": [
    "#load it in\n",
    "tokenized_data = np.load(output_file)\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chr1',\n",
       " 'chr2',\n",
       " 'chr3',\n",
       " 'chr4',\n",
       " 'chr5',\n",
       " 'chr6',\n",
       " 'chr7',\n",
       " 'chr8',\n",
       " 'chr9',\n",
       " 'chr10',\n",
       " 'chr11',\n",
       " 'chr12',\n",
       " 'chr13',\n",
       " 'chr14',\n",
       " 'chr15',\n",
       " 'chr16',\n",
       " 'chr17',\n",
       " 'chr18',\n",
       " 'chr19',\n",
       " 'chr20',\n",
       " 'chr21',\n",
       " 'chr22',\n",
       " 'chrX',\n",
       " 'chrY',\n",
       " 'chrM',\n",
       " 'chr1_KI270706v1_random',\n",
       " 'chr1_KI270707v1_random',\n",
       " 'chr1_KI270708v1_random',\n",
       " 'chr1_KI270709v1_random',\n",
       " 'chr1_KI270710v1_random',\n",
       " 'chr1_KI270711v1_random',\n",
       " 'chr1_KI270712v1_random',\n",
       " 'chr1_KI270713v1_random',\n",
       " 'chr1_KI270714v1_random',\n",
       " 'chr2_KI270715v1_random',\n",
       " 'chr2_KI270716v1_random',\n",
       " 'chr3_GL000221v1_random',\n",
       " 'chr4_GL000008v2_random',\n",
       " 'chr5_GL000208v1_random',\n",
       " 'chr9_KI270717v1_random',\n",
       " 'chr9_KI270718v1_random',\n",
       " 'chr9_KI270719v1_random',\n",
       " 'chr9_KI270720v1_random',\n",
       " 'chr11_KI270721v1_random',\n",
       " 'chr14_GL000009v2_random',\n",
       " 'chr14_GL000225v1_random',\n",
       " 'chr14_KI270722v1_random',\n",
       " 'chr14_GL000194v1_random',\n",
       " 'chr14_KI270723v1_random',\n",
       " 'chr14_KI270724v1_random',\n",
       " 'chr14_KI270725v1_random',\n",
       " 'chr14_KI270726v1_random',\n",
       " 'chr15_KI270727v1_random',\n",
       " 'chr16_KI270728v1_random',\n",
       " 'chr17_GL000205v2_random',\n",
       " 'chr17_KI270729v1_random',\n",
       " 'chr17_KI270730v1_random',\n",
       " 'chr22_KI270731v1_random',\n",
       " 'chr22_KI270732v1_random',\n",
       " 'chr22_KI270733v1_random',\n",
       " 'chr22_KI270734v1_random',\n",
       " 'chr22_KI270735v1_random',\n",
       " 'chr22_KI270736v1_random',\n",
       " 'chr22_KI270737v1_random',\n",
       " 'chr22_KI270738v1_random',\n",
       " 'chr22_KI270739v1_random',\n",
       " 'chrY_KI270740v1_random',\n",
       " 'chrUn_KI270302v1',\n",
       " 'chrUn_KI270304v1',\n",
       " 'chrUn_KI270303v1',\n",
       " 'chrUn_KI270305v1',\n",
       " 'chrUn_KI270322v1',\n",
       " 'chrUn_KI270320v1',\n",
       " 'chrUn_KI270310v1',\n",
       " 'chrUn_KI270316v1',\n",
       " 'chrUn_KI270315v1',\n",
       " 'chrUn_KI270312v1',\n",
       " 'chrUn_KI270311v1',\n",
       " 'chrUn_KI270317v1',\n",
       " 'chrUn_KI270412v1',\n",
       " 'chrUn_KI270411v1',\n",
       " 'chrUn_KI270414v1',\n",
       " 'chrUn_KI270419v1',\n",
       " 'chrUn_KI270418v1',\n",
       " 'chrUn_KI270420v1',\n",
       " 'chrUn_KI270424v1',\n",
       " 'chrUn_KI270417v1',\n",
       " 'chrUn_KI270422v1',\n",
       " 'chrUn_KI270423v1',\n",
       " 'chrUn_KI270425v1',\n",
       " 'chrUn_KI270429v1',\n",
       " 'chrUn_KI270442v1',\n",
       " 'chrUn_KI270466v1',\n",
       " 'chrUn_KI270465v1',\n",
       " 'chrUn_KI270467v1',\n",
       " 'chrUn_KI270435v1',\n",
       " 'chrUn_KI270438v1',\n",
       " 'chrUn_KI270468v1',\n",
       " 'chrUn_KI270510v1',\n",
       " 'chrUn_KI270509v1',\n",
       " 'chrUn_KI270518v1',\n",
       " 'chrUn_KI270508v1',\n",
       " 'chrUn_KI270516v1',\n",
       " 'chrUn_KI270512v1',\n",
       " 'chrUn_KI270519v1',\n",
       " 'chrUn_KI270522v1',\n",
       " 'chrUn_KI270511v1',\n",
       " 'chrUn_KI270515v1',\n",
       " 'chrUn_KI270507v1',\n",
       " 'chrUn_KI270517v1',\n",
       " 'chrUn_KI270529v1',\n",
       " 'chrUn_KI270528v1',\n",
       " 'chrUn_KI270530v1',\n",
       " 'chrUn_KI270539v1',\n",
       " 'chrUn_KI270538v1',\n",
       " 'chrUn_KI270544v1',\n",
       " 'chrUn_KI270548v1',\n",
       " 'chrUn_KI270583v1',\n",
       " 'chrUn_KI270587v1',\n",
       " 'chrUn_KI270580v1',\n",
       " 'chrUn_KI270581v1',\n",
       " 'chrUn_KI270579v1',\n",
       " 'chrUn_KI270589v1',\n",
       " 'chrUn_KI270590v1',\n",
       " 'chrUn_KI270584v1',\n",
       " 'chrUn_KI270582v1',\n",
       " 'chrUn_KI270588v1',\n",
       " 'chrUn_KI270593v1',\n",
       " 'chrUn_KI270591v1',\n",
       " 'chrUn_KI270330v1',\n",
       " 'chrUn_KI270329v1',\n",
       " 'chrUn_KI270334v1',\n",
       " 'chrUn_KI270333v1',\n",
       " 'chrUn_KI270335v1',\n",
       " 'chrUn_KI270338v1',\n",
       " 'chrUn_KI270340v1',\n",
       " 'chrUn_KI270336v1',\n",
       " 'chrUn_KI270337v1',\n",
       " 'chrUn_KI270363v1',\n",
       " 'chrUn_KI270364v1',\n",
       " 'chrUn_KI270362v1',\n",
       " 'chrUn_KI270366v1',\n",
       " 'chrUn_KI270378v1',\n",
       " 'chrUn_KI270379v1',\n",
       " 'chrUn_KI270389v1',\n",
       " 'chrUn_KI270390v1',\n",
       " 'chrUn_KI270387v1',\n",
       " 'chrUn_KI270395v1',\n",
       " 'chrUn_KI270396v1',\n",
       " 'chrUn_KI270388v1',\n",
       " 'chrUn_KI270394v1',\n",
       " 'chrUn_KI270386v1',\n",
       " 'chrUn_KI270391v1',\n",
       " 'chrUn_KI270383v1',\n",
       " 'chrUn_KI270393v1',\n",
       " 'chrUn_KI270384v1',\n",
       " 'chrUn_KI270392v1',\n",
       " 'chrUn_KI270381v1',\n",
       " 'chrUn_KI270385v1',\n",
       " 'chrUn_KI270382v1',\n",
       " 'chrUn_KI270376v1',\n",
       " 'chrUn_KI270374v1',\n",
       " 'chrUn_KI270372v1',\n",
       " 'chrUn_KI270373v1',\n",
       " 'chrUn_KI270375v1',\n",
       " 'chrUn_KI270371v1',\n",
       " 'chrUn_KI270448v1',\n",
       " 'chrUn_KI270521v1',\n",
       " 'chrUn_GL000195v1',\n",
       " 'chrUn_GL000219v1',\n",
       " 'chrUn_GL000220v1',\n",
       " 'chrUn_GL000224v1',\n",
       " 'chrUn_KI270741v1',\n",
       " 'chrUn_GL000226v1',\n",
       " 'chrUn_GL000213v1',\n",
       " 'chrUn_KI270743v1',\n",
       " 'chrUn_KI270744v1',\n",
       " 'chrUn_KI270745v1',\n",
       " 'chrUn_KI270746v1',\n",
       " 'chrUn_KI270747v1',\n",
       " 'chrUn_KI270748v1',\n",
       " 'chrUn_KI270749v1',\n",
       " 'chrUn_KI270750v1',\n",
       " 'chrUn_KI270751v1',\n",
       " 'chrUn_KI270752v1',\n",
       " 'chrUn_KI270753v1',\n",
       " 'chrUn_KI270754v1',\n",
       " 'chrUn_KI270755v1',\n",
       " 'chrUn_KI270756v1',\n",
       " 'chrUn_KI270757v1',\n",
       " 'chrUn_GL000214v1',\n",
       " 'chrUn_KI270742v1',\n",
       " 'chrUn_GL000216v2',\n",
       " 'chrUn_GL000218v1',\n",
       " 'chrEBV']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenized_data.keys()) #lots of these unknown ones, but we only care about the main ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so let's make a quick test to see if we get the same results\n",
    "tokenized_data['chr1'].dtype #this is kind of slow for some reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    tokenized_data['chr1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(output_file, allow_pickle=True) as data:\n",
    "    # Convert each array in the .npz file to a regular NumPy array\n",
    "    tokenized_records = {key: np.array(data[key]) for key in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#that loads it into memory in an uncompressed form, now we can see how quickly we can access it\n",
    "#and we see it's near instant, now let's test it! if it works, we save a rc version of this too!!\n",
    "for i in range(1000):\n",
    "    tokenized_records['chr1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 272/1000 [01:01<02:43,  4.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m seq \u001b[38;5;241m=\u001b[39m genome[chrom][start:end]\u001b[38;5;241m.\u001b[39mseq\n\u001b[1;32m      7\u001b[0m seqnp \u001b[38;5;241m=\u001b[39m tokenized_records[chrom][start:end]\n\u001b[0;32m----> 8\u001b[0m seq_tokenized \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtoken_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(seqnp,seq_tokenized):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m seq \u001b[38;5;241m=\u001b[39m genome[chrom][start:end]\u001b[38;5;241m.\u001b[39mseq\n\u001b[1;32m      7\u001b[0m seqnp \u001b[38;5;241m=\u001b[39m tokenized_records[chrom][start:end]\n\u001b[0;32m----> 8\u001b[0m seq_tokenized \u001b[38;5;241m=\u001b[39m [token_mapping\u001b[38;5;241m.\u001b[39mget(base, \u001b[38;5;241m11\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m seq]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(seqnp,seq_tokenized):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    seqnp = tokenized_records[chrom][start:end]\n",
    "    seq_tokenized = [token_mapping.get(base, 11) for base in seq]\n",
    "    if not np.allclose(seqnp,seq_tokenized):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTGCCTCCCAAAGTGCTGGGATGACAGGCGTGAGCCACCACACTTGGCTAATTTTTGCATTGTTAGTTGAGACGGGGCTTCTCCATGTTGGCCAGGCTGGTCTTGGACTCCTGACATCAGGTGATCCAAGAAAGGGACCTTCTTCTAAGACCATCATGAGCCTGAAGCTTCCCCGAATGGCCCAAGAGGGGCTCTCCACCCCGCCCTTCCTCAGGTAACACCTCTGATCCTCTCTGAAGCAAAGGCAGCAGCTCCCATCACAGAACACGGGACTCACCGTGGGGGTCAGCAGTGCCGGTGTCCAGCTTAAGAGGAGCTCTTTGGAAGTAGGGGATTCTCAGACCCACATGGCTGTGCTTCAGGGCAGAGACAAGGTCACCTGAAGCTGCACTCAATCTCCTTACACCCTGCCGGCTCAGCCTGAGTCCCCACCGCCGGCCAGCATCTGGCCTGGTCTGGGTCAGGAGCGTCTTGGACTTGGTTGCTCTATAAGGGCCTCAACTTCCCCGACAGACAAATGCTTCTGGACCGGCCTCCCTTCAGGCCCTCTTAGCCTCCTCACGGCTCCCACAAGGGGATGCTGCGGGGGCAGGGTGGGGGGGTTCCTCCTTCAGCCCCTTCCCCTCCAAAGCTCAAGCGAGAATCTAGAATTCAGCCCCTTCCCATCCAAAGCTCAACCAAGAATCTAGAAGAGGGAAGGATGGGAGGGAGAGGAAGGCACCGGGCCTGGTCTACTTGAGTGTGATTTGAGGCAAAAAAAAAAAAAAACAAACAGGGAAAAGGAAAGAAAAGTAGAAACGCGAAGTTCAATACCGAGAGCCCTGGCTGAACCCACAAACCCAGATGCCACGTGTCTCCCAGCTAAACACTCGCACAAAAAGCTGACTGCCAAGTCAGGGGCTGGAATGAGGCTTTGGCCCCTGAACTCTCCTCCGGTGACAAGAACAGCCCAACCACAGGGACCCCCGCTGGTACCCAAAGGATCCTCCGTGAGTGTCCC\n",
      "(1000,)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#seems good! let's test a random thing\n",
    "chrom = 'chr17'\n",
    "start = 1028349\n",
    "end = start + 1000\n",
    "seq = genome[chrom][start:end].seq\n",
    "seqnp = tokenized_records[chrom][start:end]\n",
    "seq_tokenized = [token_mapping.get(base, 11) for base in seq]\n",
    "print(seq)\n",
    "print(seqnp.shape) # a 1d array, I think this works officially!!\n",
    "print(len(seq_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sequences: 100%|| 195/195 [05:22<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "#in that case, we can create another one for the reverse complement\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the mapping\n",
    "token_mapping = {\n",
    "    'A': 7,\n",
    "    'C': 8,\n",
    "    'G': 9,\n",
    "    'T': 10\n",
    "}\n",
    "\n",
    "def tokenize_sequence(sequence):\n",
    "    # Tokenize the sequence using the mapping, defaulting to 11 for any non-ACGT characters\n",
    "    return np.array([token_mapping.get(base, 11) for base in sequence], dtype=np.int8) #changed it to int8, no need to go further\n",
    "\n",
    "def tokenize_fasta(input_fasta, output_file):\n",
    "    # Read the input FASTA file\n",
    "    records = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    \n",
    "    # Create a dictionary to hold tokenized sequences\n",
    "    tokenized_records = {}\n",
    "    \n",
    "    # Process each record with progress tracking\n",
    "    for record in tqdm(records, desc=\"Tokenizing sequences\"):\n",
    "        tokenized_seq = tokenize_sequence(record.seq)\n",
    "        \n",
    "        # Store the tokenized sequence in the dictionary\n",
    "        tokenized_records[record.id] = tokenized_seq\n",
    "    \n",
    "    # Save the tokenized sequences to a .npz file\n",
    "    np.savez(output_file, **tokenized_records) #also save it out not compressed, it's not even a huge amount of data, it should be fine.\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa\"\n",
    "output_file = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_rc.npz\"\n",
    "\n",
    "# Run the tokenization function\n",
    "tokenize_fasta(input_fasta, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_rc.npz' with keys: chr1, chr2, chr3, chr4, chr5...\n"
     ]
    }
   ],
   "source": [
    "tokenized_rc = np.load(output_file)\n",
    "print(tokenized_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    tokenized_rc['chr1'].shape #so here it's not compressed but still it takes a while to load because loads it every time we call it, so in this case let's just "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(output_file, allow_pickle=True) as data:\n",
    "    # Convert each array in the .npz file to a regular NumPy array\n",
    "    tokenized_rc = {key: np.array(data[key]) for key in data} #so this ends up loading much faster, so better to do it this way as int8 and not compressed\n",
    "    #compressed saves 2 GB (of 3) but it is much much slower! Actuallys aves 2 gb when was int32, now it's int8 tho so it's fine to leave as is I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing sequences: 100%|| 195/195 [05:16<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\" #do it again for the normal one\n",
    "output_file = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz\"\n",
    "tokenize_fasta(input_fasta, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final tests to ensure this np array approach works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first load in both\n",
    "rc_np = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized_rc.npz'\n",
    "npz = '/data/leslie/sarthak/data/chrombpnet_test/hg38_tokenized.npz'\n",
    "with np.load(rc_np, allow_pickle=True) as data:\n",
    "    tokenized_rc = {key: np.array(data[key]) for key in data}\n",
    "with np.load(npz, allow_pickle=True) as data:\n",
    "    tokenized = {key: np.array(data[key]) for key in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's also load in the genome\n",
    "import pyfaidx\n",
    "input_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38.fa\"\n",
    "genome = pyfaidx.Fasta(input_fasta)\n",
    "rc_fasta = \"/data/leslie/sarthak/data/chrombpnet_test/hg38_rc.fa\"\n",
    "genome_rc = pyfaidx.Fasta(rc_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:25<00:00,  3.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#now let's call 10000 random sequences\n",
    "for i in tqdm(range(100)):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    seq = genome[chrom][start:end].seq\n",
    "    seqnp = tokenized[chrom][start:end]\n",
    "    seq_tokenized = [token_mapping.get(base, 11) for base in seq]\n",
    "    if not np.allclose(seqnp,seq_tokenized):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:26<00:00,  3.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#same for the rc one\n",
    "for i in tqdm(range(100)):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    seq = genome_rc[chrom][start:end].seq\n",
    "    seqnp = tokenized_rc[chrom][start:end]\n",
    "    seq_tokenized = [token_mapping.get(base, 11) for base in seq]\n",
    "    if not np.allclose(seqnp,seq_tokenized):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 76/100 [02:16<00:43,  1.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m seqrc \u001b[38;5;241m=\u001b[39m tokenized_rc[chrom][chromlen\u001b[38;5;241m-\u001b[39mend:chromlen\u001b[38;5;241m-\u001b[39mstart]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#now rc the rc again\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m seqnp_rc \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mRC_mapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnuc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnuc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mseqnp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(seqrc,seqnp_rc):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[98], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m seqrc \u001b[38;5;241m=\u001b[39m tokenized_rc[chrom][chromlen\u001b[38;5;241m-\u001b[39mend:chromlen\u001b[38;5;241m-\u001b[39mstart]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#now rc the rc again\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m seqnp_rc \u001b[38;5;241m=\u001b[39m [RC_mapping\u001b[38;5;241m.\u001b[39mget(nuc, \u001b[38;5;241m11\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m nuc \u001b[38;5;129;01min\u001b[39;00m seqnp[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(seqrc,seqnp_rc):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#now the final test is ensuring that the RC is indeed the reverse complement\n",
    "RC_mapping = {7: 10, 8: 9, 9: 8, 10: 7, 11: 11}\n",
    "for i in tqdm(range(100)):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    chromlen = len(genome[chrom])\n",
    "    seqnp = tokenized[chrom][start:end]\n",
    "    seqrc = tokenized_rc[chrom][chromlen-end:chromlen-start]\n",
    "    #now rc the rc again\n",
    "    seqnp_rc = [RC_mapping.get(nuc, 11) for nuc in seqnp[::-1]]\n",
    "    if not np.allclose(seqrc,seqnp_rc):\n",
    "        print('error')\n",
    "        break\n",
    "#yeah now this works perfectly! Finally let's just do a test to see the speed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    #get a random chromosome\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000))\n",
    "    end = int(start + 1e6)\n",
    "    seqnp = tokenized[chrom][start:end]\n",
    "#literally instant, the slow is now the np.random.choice part!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n",
      "7\n",
      "11\n",
      "7\n",
      "10\n",
      "7\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized.keys():\n",
    "    temp = tokenized[key]\n",
    "    print(temp.min())\n",
    "    print(temp.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there's any nan\n",
    "for key in tokenized.keys():\n",
    "    temp = tokenized[key]\n",
    "    if np.isnan(temp).any():\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's reset the thing so we can load in the proper updated thing and see how fast it can really be!\n",
    "#the next slowdown might be the bigwig file but make that a np array too, that way no nans, can easily just leave it as it is!\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 32768, tokenizer_name = 'char', rc_aug = True, jitter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:04<00:00, 227.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #should be near instant as the tokenized data is already loaded in, and it's small sequences anyways\n",
    "    out = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = profile_atac_long.ProfileATACLong('train', 1_000_000, tokenizer_name = 'char', rc_aug = True, jitter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 100/1000 [00:18<02:49,  5.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m)): \u001b[38;5;66;03m#should be near instant as the tokenized data is already loaded in, and it's small sequences anyways\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets/profile_atac_long.py:276\u001b[0m, in \u001b[0;36mProfileATACLong.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    274\u001b[0m center \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(peak[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    275\u001b[0m offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjitter\n\u001b[0;32m--> 276\u001b[0m cts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcts_bw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchrom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m#first we see if we do RC, if it is RC get it from the other file\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrc_aug \u001b[38;5;129;01mand\u001b[39;00m coin_flip():\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# seq = [self.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #should be near instant as the tokenized data is already loaded in, and it's small sequences anyways\n",
    "    out = dataset[0]\n",
    "    \n",
    "#much faster but still quite slow, let's see with the cprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         34 function calls in 0.146 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.015    0.015    0.146    0.146 3033894889.py:10(main)\n",
      "        1    0.000    0.000    0.000    0.000 3033894889.py:5(coin_flip)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1207(_handle_fromlist)\n",
      "        1    0.000    0.000    0.146    0.146 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2172(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:2177(sum)\n",
      "        1    0.000    0.000    0.001    0.001 fromnumeric.py:71(_wrapreduction)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:72(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 getlimits.py:484(__new__)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:1080(copyto)\n",
      "        1    0.000    0.000    0.000    0.000 type_check.py:393(_getmaxmin)\n",
      "        1    0.000    0.000    0.000    0.000 type_check.py:399(_nan_to_num_dispatcher)\n",
      "        1    0.005    0.005    0.059    0.059 type_check.py:403(nan_to_num)\n",
      "        2    0.000    0.000    0.000    0.000 ufunclike.py:14(_dispatcher)\n",
      "        1    0.002    0.002    0.002    0.002 ufunclike.py:142(isneginf)\n",
      "        1    0.002    0.002    0.002    0.002 ufunclike.py:71(isposinf)\n",
      "        1    0.000    0.000    0.146    0.146 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.050    0.050    0.050    0.050 {built-in method numpy.array}\n",
      "        1    0.002    0.002    0.002    0.002 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'random' of '_random.Random' objects}\n",
      "        1    0.001    0.001    0.001    0.001 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        1    0.069    0.069    0.069    0.069 {method 'values' of 'pyBigWig.bigWigFile' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from random import random\n",
    "idx = 0\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "#now use the profiler to determine slowdown\n",
    "import cProfile\n",
    "def main():\n",
    "    peak = dataset.peak_coords[idx]\n",
    "    chrom = peak[0]\n",
    "    center = int(peak[1])\n",
    "    offset = dataset.max_length//2 + dataset.jitter\n",
    "    cts = np.nan_to_num(dataset.cts_bw.values(chrom, center-offset, center+offset))\n",
    "    #first we see if we do RC, if it is RC get it from the other file\n",
    "    if dataset.rc_aug and coin_flip():\n",
    "        # seq = [dataset.RC_mapping.get(nuc, 11) for nuc in seq[::-1]]\n",
    "        lenchrom = len(dataset.genome[chrom])\n",
    "        seq = dataset.genome_rc[chrom][lenchrom-(center+offset):lenchrom-(center-offset)]\n",
    "        cts = cts[::-1]\n",
    "    else:\n",
    "        seq = dataset.genome[chrom][center-offset:center+offset]\n",
    "    if dataset.one_hot:\n",
    "        raise NotImplementedError('Not implemented yet, we have it tokenized, how to go from tokenized to that?')\n",
    "        one_hot_seq = dna_to_one_hot([seq])[0]\n",
    "    else:\n",
    "        one_hot_seq = None\n",
    "    # offset = dataset.max_length//2 + dataset.jitter\n",
    "    \n",
    "    \n",
    "    \n",
    "    if dataset.jitter:\n",
    "        start = np.random.choice(range(dataset.jitter*2+1)) #because range excludes the last one\n",
    "        seq = seq[start:start+dataset.max_length]\n",
    "        cts = cts[start:start+dataset.max_length]\n",
    "        if dataset.one_hot:\n",
    "            one_hot_seq = one_hot_seq[start:start+dataset.max_length]\n",
    "    \n",
    "    counts = np.log(1+np.sum(cts))\n",
    "    seq = torch.LongTensor(seq)\n",
    "    # print(counts)\n",
    "    counts = torch.FloatTensor([counts])\n",
    "    cts = torch.FloatTensor(cts.copy())\n",
    "    if dataset.one_hot:\n",
    "        one_hot_seq = torch.FloatTensor(one_hot_seq.copy()) #is a numpy array so can do .copy()\n",
    "\n",
    "cProfile.run('main()')\n",
    "\n",
    "#yeah so it's that bigwig thing, the final step is to make sure that the bigwig will be loaded in as a numpy array, that way we can avoid the nan issue and it will bve very fast!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final thing we will do is make it so we load in the bigwig and save it as a np array\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "bigwigfile = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.bw'\n",
    "#now load it\n",
    "bw = pyBigWig.open(bigwigfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#now we see it's properties\n",
    "print(len(bw.values('chr1', 0, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr1': 248956422,\n",
       " 'chr10': 133797422,\n",
       " 'chr11': 135086622,\n",
       " 'chr11_KI270721v1_random': 100316,\n",
       " 'chr12': 133275309,\n",
       " 'chr13': 114364328,\n",
       " 'chr14': 107043718,\n",
       " 'chr14_GL000009v2_random': 201709,\n",
       " 'chr14_GL000194v1_random': 191469,\n",
       " 'chr14_GL000225v1_random': 211173,\n",
       " 'chr14_KI270722v1_random': 194050,\n",
       " 'chr14_KI270723v1_random': 38115,\n",
       " 'chr14_KI270724v1_random': 39555,\n",
       " 'chr14_KI270725v1_random': 172810,\n",
       " 'chr14_KI270726v1_random': 43739,\n",
       " 'chr15': 101991189,\n",
       " 'chr15_KI270727v1_random': 448248,\n",
       " 'chr16': 90338345,\n",
       " 'chr16_KI270728v1_random': 1872759,\n",
       " 'chr17': 83257441,\n",
       " 'chr17_GL000205v2_random': 185591,\n",
       " 'chr17_KI270729v1_random': 280839,\n",
       " 'chr17_KI270730v1_random': 112551,\n",
       " 'chr18': 80373285,\n",
       " 'chr19': 58617616,\n",
       " 'chr1_KI270706v1_random': 175055,\n",
       " 'chr1_KI270707v1_random': 32032,\n",
       " 'chr1_KI270708v1_random': 127682,\n",
       " 'chr1_KI270709v1_random': 66860,\n",
       " 'chr1_KI270710v1_random': 40176,\n",
       " 'chr1_KI270711v1_random': 42210,\n",
       " 'chr1_KI270712v1_random': 176043,\n",
       " 'chr1_KI270713v1_random': 40745,\n",
       " 'chr1_KI270714v1_random': 41717,\n",
       " 'chr2': 242193529,\n",
       " 'chr20': 64444167,\n",
       " 'chr21': 46709983,\n",
       " 'chr22': 50818468,\n",
       " 'chr22_KI270731v1_random': 150754,\n",
       " 'chr22_KI270732v1_random': 41543,\n",
       " 'chr22_KI270733v1_random': 179772,\n",
       " 'chr22_KI270734v1_random': 165050,\n",
       " 'chr22_KI270735v1_random': 42811,\n",
       " 'chr22_KI270736v1_random': 181920,\n",
       " 'chr22_KI270737v1_random': 103838,\n",
       " 'chr22_KI270738v1_random': 99375,\n",
       " 'chr22_KI270739v1_random': 73985,\n",
       " 'chr2_KI270715v1_random': 161471,\n",
       " 'chr2_KI270716v1_random': 153799,\n",
       " 'chr3': 198295559,\n",
       " 'chr3_GL000221v1_random': 155397,\n",
       " 'chr4': 190214555,\n",
       " 'chr4_GL000008v2_random': 209709,\n",
       " 'chr5': 181538259,\n",
       " 'chr5_GL000208v1_random': 92689,\n",
       " 'chr6': 170805979,\n",
       " 'chr7': 159345973,\n",
       " 'chr8': 145138636,\n",
       " 'chr9': 138394717,\n",
       " 'chr9_KI270717v1_random': 40062,\n",
       " 'chr9_KI270718v1_random': 38054,\n",
       " 'chr9_KI270719v1_random': 176845,\n",
       " 'chr9_KI270720v1_random': 39050,\n",
       " 'chrEBV': 171823,\n",
       " 'chrUn_GL000195v1': 182896,\n",
       " 'chrUn_GL000213v1': 164239,\n",
       " 'chrUn_GL000214v1': 137718,\n",
       " 'chrUn_GL000216v2': 176608,\n",
       " 'chrUn_GL000218v1': 161147,\n",
       " 'chrUn_GL000219v1': 179198,\n",
       " 'chrUn_GL000220v1': 161802,\n",
       " 'chrUn_GL000224v1': 179693,\n",
       " 'chrUn_GL000226v1': 15008,\n",
       " 'chrUn_KI270303v1': 1942,\n",
       " 'chrUn_KI270304v1': 2165,\n",
       " 'chrUn_KI270310v1': 1201,\n",
       " 'chrUn_KI270312v1': 998,\n",
       " 'chrUn_KI270315v1': 2276,\n",
       " 'chrUn_KI270317v1': 37690,\n",
       " 'chrUn_KI270320v1': 4416,\n",
       " 'chrUn_KI270322v1': 21476,\n",
       " 'chrUn_KI270330v1': 1652,\n",
       " 'chrUn_KI270333v1': 2699,\n",
       " 'chrUn_KI270336v1': 1026,\n",
       " 'chrUn_KI270337v1': 1121,\n",
       " 'chrUn_KI270362v1': 3530,\n",
       " 'chrUn_KI270363v1': 1803,\n",
       " 'chrUn_KI270364v1': 2855,\n",
       " 'chrUn_KI270366v1': 8320,\n",
       " 'chrUn_KI270371v1': 2805,\n",
       " 'chrUn_KI270372v1': 1650,\n",
       " 'chrUn_KI270373v1': 1451,\n",
       " 'chrUn_KI270374v1': 2656,\n",
       " 'chrUn_KI270375v1': 2378,\n",
       " 'chrUn_KI270376v1': 1136,\n",
       " 'chrUn_KI270378v1': 1048,\n",
       " 'chrUn_KI270384v1': 1658,\n",
       " 'chrUn_KI270386v1': 1788,\n",
       " 'chrUn_KI270411v1': 2646,\n",
       " 'chrUn_KI270412v1': 1179,\n",
       " 'chrUn_KI270417v1': 2043,\n",
       " 'chrUn_KI270418v1': 2145,\n",
       " 'chrUn_KI270420v1': 2321,\n",
       " 'chrUn_KI270422v1': 1445,\n",
       " 'chrUn_KI270423v1': 981,\n",
       " 'chrUn_KI270424v1': 2140,\n",
       " 'chrUn_KI270425v1': 1884,\n",
       " 'chrUn_KI270429v1': 1361,\n",
       " 'chrUn_KI270435v1': 92983,\n",
       " 'chrUn_KI270438v1': 112505,\n",
       " 'chrUn_KI270442v1': 392061,\n",
       " 'chrUn_KI270448v1': 7992,\n",
       " 'chrUn_KI270465v1': 1774,\n",
       " 'chrUn_KI270466v1': 1233,\n",
       " 'chrUn_KI270467v1': 3920,\n",
       " 'chrUn_KI270468v1': 4055,\n",
       " 'chrUn_KI270507v1': 5353,\n",
       " 'chrUn_KI270508v1': 1951,\n",
       " 'chrUn_KI270509v1': 2318,\n",
       " 'chrUn_KI270510v1': 2415,\n",
       " 'chrUn_KI270511v1': 8127,\n",
       " 'chrUn_KI270512v1': 22689,\n",
       " 'chrUn_KI270515v1': 6361,\n",
       " 'chrUn_KI270516v1': 1300,\n",
       " 'chrUn_KI270517v1': 3253,\n",
       " 'chrUn_KI270518v1': 2186,\n",
       " 'chrUn_KI270519v1': 138126,\n",
       " 'chrUn_KI270521v1': 7642,\n",
       " 'chrUn_KI270522v1': 5674,\n",
       " 'chrUn_KI270528v1': 2983,\n",
       " 'chrUn_KI270530v1': 2168,\n",
       " 'chrUn_KI270538v1': 91309,\n",
       " 'chrUn_KI270539v1': 993,\n",
       " 'chrUn_KI270544v1': 1202,\n",
       " 'chrUn_KI270579v1': 31033,\n",
       " 'chrUn_KI270580v1': 1553,\n",
       " 'chrUn_KI270581v1': 7046,\n",
       " 'chrUn_KI270582v1': 6504,\n",
       " 'chrUn_KI270583v1': 1400,\n",
       " 'chrUn_KI270584v1': 4513,\n",
       " 'chrUn_KI270587v1': 2969,\n",
       " 'chrUn_KI270588v1': 6158,\n",
       " 'chrUn_KI270589v1': 44474,\n",
       " 'chrUn_KI270590v1': 4685,\n",
       " 'chrUn_KI270591v1': 5796,\n",
       " 'chrUn_KI270593v1': 3041,\n",
       " 'chrUn_KI270741v1': 157432,\n",
       " 'chrUn_KI270742v1': 186739,\n",
       " 'chrUn_KI270743v1': 210658,\n",
       " 'chrUn_KI270744v1': 168472,\n",
       " 'chrUn_KI270745v1': 41891,\n",
       " 'chrUn_KI270746v1': 66486,\n",
       " 'chrUn_KI270747v1': 198735,\n",
       " 'chrUn_KI270748v1': 93321,\n",
       " 'chrUn_KI270749v1': 158759,\n",
       " 'chrUn_KI270750v1': 148850,\n",
       " 'chrUn_KI270751v1': 150742,\n",
       " 'chrUn_KI270752v1': 27745,\n",
       " 'chrUn_KI270753v1': 62944,\n",
       " 'chrUn_KI270754v1': 40191,\n",
       " 'chrUn_KI270755v1': 36723,\n",
       " 'chrUn_KI270756v1': 79590,\n",
       " 'chrUn_KI270757v1': 71251,\n",
       " 'chrX': 156040895,\n",
       " 'chrY': 57227415}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bw.chroms() #shows the length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248956422\n"
     ]
    }
   ],
   "source": [
    "values = bw.values('chr1',0,248956422) #if go one further, it doesn't work\n",
    "print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "#see the default data type\n",
    "a = np.nan_to_num(bw.values('chr1',0,100000))\n",
    "print(a.dtype) #defaults to float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "source": [
    "print(a.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mod(1.5,1) #the remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mod(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(np.equal(np.mod(a, 1), 0)) #yes the mod with 1 is true, I think they're all integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check if these are all ints\n",
    "for i in range(1000):\n",
    "    start = np.random.choice(range(1000))\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    a = np.nan_to_num(bw.values(chrom,start*10,start*10+1000))\n",
    "    if not np.all(np.equal(np.mod(a, 1), 0)):\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chr1\n",
      "Processing chr10\n",
      "Processing chr11\n",
      "Processing chr11_KI270721v1_random\n",
      "Processing chr12\n",
      "Processing chr13\n",
      "Processing chr14\n",
      "Processing chr14_GL000009v2_random\n",
      "Processing chr14_GL000194v1_random\n",
      "Processing chr14_GL000225v1_random\n",
      "Processing chr14_KI270722v1_random\n",
      "Processing chr14_KI270723v1_random\n",
      "Processing chr14_KI270724v1_random\n",
      "Processing chr14_KI270725v1_random\n",
      "Processing chr14_KI270726v1_random\n",
      "Processing chr15\n",
      "Processing chr15_KI270727v1_random\n",
      "Processing chr16\n",
      "Processing chr16_KI270728v1_random\n",
      "Processing chr17\n",
      "Processing chr17_GL000205v2_random\n",
      "Processing chr17_KI270729v1_random\n",
      "Processing chr17_KI270730v1_random\n",
      "Processing chr18\n",
      "Processing chr19\n",
      "Processing chr1_KI270706v1_random\n",
      "Processing chr1_KI270707v1_random\n",
      "Processing chr1_KI270708v1_random\n",
      "Processing chr1_KI270709v1_random\n",
      "Processing chr1_KI270710v1_random\n",
      "Processing chr1_KI270711v1_random\n",
      "Processing chr1_KI270712v1_random\n",
      "Processing chr1_KI270713v1_random\n",
      "Processing chr1_KI270714v1_random\n",
      "Processing chr2\n",
      "Processing chr20\n",
      "Processing chr21\n",
      "Processing chr22\n",
      "Processing chr22_KI270731v1_random\n",
      "Processing chr22_KI270732v1_random\n",
      "Processing chr22_KI270733v1_random\n",
      "Processing chr22_KI270734v1_random\n",
      "Processing chr22_KI270735v1_random\n",
      "Processing chr22_KI270736v1_random\n",
      "Processing chr22_KI270737v1_random\n",
      "Processing chr22_KI270738v1_random\n",
      "Processing chr22_KI270739v1_random\n",
      "Processing chr2_KI270715v1_random\n",
      "Processing chr2_KI270716v1_random\n",
      "Processing chr3\n",
      "Processing chr3_GL000221v1_random\n",
      "Processing chr4\n",
      "Processing chr4_GL000008v2_random\n",
      "Processing chr5\n",
      "Processing chr5_GL000208v1_random\n",
      "Processing chr6\n",
      "Processing chr7\n",
      "Processing chr8\n",
      "Processing chr9\n",
      "Processing chr9_KI270717v1_random\n",
      "Processing chr9_KI270718v1_random\n",
      "Processing chr9_KI270719v1_random\n",
      "Processing chr9_KI270720v1_random\n",
      "Processing chrEBV\n",
      "Processing chrUn_GL000195v1\n",
      "Processing chrUn_GL000213v1\n",
      "Processing chrUn_GL000214v1\n",
      "Processing chrUn_GL000216v2\n",
      "Processing chrUn_GL000218v1\n",
      "Processing chrUn_GL000219v1\n",
      "Processing chrUn_GL000220v1\n",
      "Processing chrUn_GL000224v1\n",
      "Processing chrUn_GL000226v1\n",
      "Processing chrUn_KI270303v1\n",
      "Processing chrUn_KI270304v1\n",
      "Processing chrUn_KI270310v1\n",
      "Processing chrUn_KI270312v1\n",
      "Processing chrUn_KI270315v1\n",
      "Processing chrUn_KI270317v1\n",
      "Processing chrUn_KI270320v1\n",
      "Processing chrUn_KI270322v1\n",
      "Processing chrUn_KI270330v1\n",
      "Processing chrUn_KI270333v1\n",
      "Processing chrUn_KI270336v1\n",
      "Processing chrUn_KI270337v1\n",
      "Processing chrUn_KI270362v1\n",
      "Processing chrUn_KI270363v1\n",
      "Processing chrUn_KI270364v1\n",
      "Processing chrUn_KI270366v1\n",
      "Processing chrUn_KI270371v1\n",
      "Processing chrUn_KI270372v1\n",
      "Processing chrUn_KI270373v1\n",
      "Processing chrUn_KI270374v1\n",
      "Processing chrUn_KI270375v1\n",
      "Processing chrUn_KI270376v1\n",
      "Processing chrUn_KI270378v1\n",
      "Processing chrUn_KI270384v1\n",
      "Processing chrUn_KI270386v1\n",
      "Processing chrUn_KI270411v1\n",
      "Processing chrUn_KI270412v1\n",
      "Processing chrUn_KI270417v1\n",
      "Processing chrUn_KI270418v1\n",
      "Processing chrUn_KI270420v1\n",
      "Processing chrUn_KI270422v1\n",
      "Processing chrUn_KI270423v1\n",
      "Processing chrUn_KI270424v1\n",
      "Processing chrUn_KI270425v1\n",
      "Processing chrUn_KI270429v1\n",
      "Processing chrUn_KI270435v1\n",
      "Processing chrUn_KI270438v1\n",
      "Processing chrUn_KI270442v1\n",
      "Processing chrUn_KI270448v1\n",
      "Processing chrUn_KI270465v1\n",
      "Processing chrUn_KI270466v1\n",
      "Processing chrUn_KI270467v1\n",
      "Processing chrUn_KI270468v1\n",
      "Processing chrUn_KI270507v1\n",
      "Processing chrUn_KI270508v1\n",
      "Processing chrUn_KI270509v1\n",
      "Processing chrUn_KI270510v1\n",
      "Processing chrUn_KI270511v1\n",
      "Processing chrUn_KI270512v1\n",
      "Processing chrUn_KI270515v1\n",
      "Processing chrUn_KI270516v1\n",
      "Processing chrUn_KI270517v1\n",
      "Processing chrUn_KI270518v1\n",
      "Processing chrUn_KI270519v1\n",
      "Processing chrUn_KI270521v1\n",
      "Processing chrUn_KI270522v1\n",
      "Processing chrUn_KI270528v1\n",
      "Processing chrUn_KI270530v1\n",
      "Processing chrUn_KI270538v1\n",
      "Processing chrUn_KI270539v1\n",
      "Processing chrUn_KI270544v1\n",
      "Processing chrUn_KI270579v1\n",
      "Processing chrUn_KI270580v1\n",
      "Processing chrUn_KI270581v1\n",
      "Processing chrUn_KI270582v1\n",
      "Processing chrUn_KI270583v1\n",
      "Processing chrUn_KI270584v1\n",
      "Processing chrUn_KI270587v1\n",
      "Processing chrUn_KI270588v1\n",
      "Processing chrUn_KI270589v1\n",
      "Processing chrUn_KI270590v1\n",
      "Processing chrUn_KI270591v1\n",
      "Processing chrUn_KI270593v1\n",
      "Processing chrUn_KI270741v1\n",
      "Processing chrUn_KI270742v1\n",
      "Processing chrUn_KI270743v1\n",
      "Processing chrUn_KI270744v1\n",
      "Processing chrUn_KI270745v1\n",
      "Processing chrUn_KI270746v1\n",
      "Processing chrUn_KI270747v1\n",
      "Processing chrUn_KI270748v1\n",
      "Processing chrUn_KI270749v1\n",
      "Processing chrUn_KI270750v1\n",
      "Processing chrUn_KI270751v1\n",
      "Processing chrUn_KI270752v1\n",
      "Processing chrUn_KI270753v1\n",
      "Processing chrUn_KI270754v1\n",
      "Processing chrUn_KI270755v1\n",
      "Processing chrUn_KI270756v1\n",
      "Processing chrUn_KI270757v1\n",
      "Processing chrX\n",
      "Processing chrY\n"
     ]
    }
   ],
   "source": [
    "def extract_chromosome_data(bw, chrom):\n",
    "    # Get the length of the chromosome\n",
    "    chrom_length = bw.chroms()[chrom]\n",
    "    \n",
    "    # Retrieve values for the entire chromosome\n",
    "    values = np.nan_to_num(bw.values(chrom, 0, chrom_length))\n",
    "    #check to ensure it's all integers\n",
    "    if not np.all(np.equal(np.mod(values, 1), 0)):\n",
    "        print('error')\n",
    "    if not np.all(values >= 0):\n",
    "        print('error')\n",
    "    return values.astype(np.uint16) #no counts should be above \n",
    "\n",
    "def save_bigwig_to_npz(bigwigfile, output_file):\n",
    "    # Open the BigWig file\n",
    "    bw = pyBigWig.open(bigwigfile)\n",
    "    \n",
    "    # Dictionary to hold the data for each chromosome\n",
    "    chrom_data = {}\n",
    "    \n",
    "    # Extract data for each chromosome\n",
    "    for chrom in bw.chroms().keys():\n",
    "        print(f\"Processing {chrom}\")\n",
    "        chrom_data[chrom] = extract_chromosome_data(bw, chrom)\n",
    "    \n",
    "    # Save the data to a .npz file\n",
    "    np.savez(output_file, **chrom_data)\n",
    "    \n",
    "    # Close the BigWig file\n",
    "    bw.close()\n",
    "\n",
    "# Define the input and output file paths\n",
    "bigwigfile = '/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.bw' #takes roughly 6gb!\n",
    "output_file = \"/data/leslie/sarthak/data/chrombpnet_test/chrombpnet_model_1000/auxiliary/data_unstranded.npz\"\n",
    "\n",
    "# Run the function to save BigWig data to .npz\n",
    "save_bigwig_to_npz(bigwigfile, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [03:30<00:00,  4.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#that should save it out to a bigwig file!\n",
    "bw_np = np.load(output_file, allow_pickle=True)\n",
    "#but now we need to preload it to a dict so it's in memory\n",
    "bw_np = {key: np.array(bw_np[key]) for key in bw_np}\n",
    "\n",
    "#now let's see how hard it is to load some random sequences\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)):\n",
    "    chrom = np.random.choice(['chr1','chr2','chr3','chr4','chr5','chr6','chr7','chr8','chr9','chr10','chr11','chr12','chr13','chr14','chr15','chr16','chr17','chr18','chr19','chr20','chr21','chr22','chrX','chrY'])\n",
    "    start = np.random.choice(range(1000000))\n",
    "    end = int(start + 1e6)\n",
    "    seq = bw_np[chrom][start:end]\n",
    "    seq2 = np.nan_to_num(bw.values(chrom, start, end))\n",
    "    if not np.allclose(seq, seq2):\n",
    "        print('error')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#this is great, we can see that it clearly produces the same things! so now we can use this\n",
    "start = 539685\n",
    "print(seq[start:start+10])\n",
    "print(seq2[start:start+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we modify the dataset and try again!\n",
    "#let's load it in\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "import src.dataloaders.datasets.profile_atac_long as profile_atac_long\n",
    "dataset = profile_atac_long.ProfileATACLong('train', 1_000_000, tokenizer_name = 'char', rc_aug = True, jitter = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:03<00:00, 301.85it/s]\n"
     ]
    }
   ],
   "source": [
    "#and finally we test it's speed\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(1000)): #should be near instant as the tokenized data is already loaded in, and have the bigwig file\n",
    "    out = dataset[0]\n",
    "#so much faster dude! the whole genome when broken into chunks of 1 million is just 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3_000_000_000/1_000_000 #yeah see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing it with one other file\n",
    "import numpy as np\n",
    "import pyBigWig\n",
    "bw = pyBigWig.open('/data/leslie/sarthak/data/enformer/data/K562_DNase_ENCODE.bigWig')\n",
    "chrom_length = bw.chroms()['chr1']\n",
    "values = np.nan_to_num(bw.values('chr1', 0, chrom_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248956422,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape #this is the legnth of the chromosome of course``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2265.18994140625\n",
      "0.050518875527088114\n"
     ]
    }
   ],
   "source": [
    "#find the min, max, mean\n",
    "print(values.min())\n",
    "print(values.max())\n",
    "print(values.mean())\n",
    "#so we can see that the values are not at all integers, so I am quite unclear how they were processed\n",
    "#this is not just tn5 insertions but is rather"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
