{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the code\n",
    "\n",
    "This is a binary prediction problem, but we can use the fasta files for some stuff like the sequences, and that's the real goal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test the \n",
    "\"\"\"\n",
    "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
    "There are 8 datasets to choose from.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "os.chdir('/data/leslie/sarthak/environments/hyena-dna')\n",
    "\n",
    "#located at /data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/genomic_benchmarks/loc2seq/loc2seq.py\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "\n",
    "#was able to load all of them in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test the dataloader they use first\n",
    "\"\"\"\n",
    "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
    "There are 8 datasets to choose from.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import os\n",
    "os.chdir('/data/leslie/sarthak/environments/hyena-dna')\n",
    "\n",
    "#located at /data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/genomic_benchmarks/loc2seq/loc2seq.py\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "# augmentation\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "\n",
    "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
    "    Returns a generator that retrieves the sequence.\n",
    "\n",
    "    Genomic Benchmarks Dataset, from:\n",
    "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        max_length,\n",
    "        dataset_name='human_enhancers_cohn',\n",
    "        d_output=2, # default binary classification\n",
    "        dest_path=\"/content\", # default for colab\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        use_padding=None,\n",
    "        add_eos=False,\n",
    "        rc_aug=False,\n",
    "        return_augs=False,\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.d_output = d_output  # needed for decoder to grab\n",
    "        self.rc_aug = rc_aug\n",
    "\n",
    "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
    "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
    "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
    "        else:\n",
    "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
    "\n",
    "        # use Path object\n",
    "        base_path = Path(dest_path) / dataset_name / split\n",
    "\n",
    "        self.all_paths = []\n",
    "        self.all_labels = []\n",
    "        label_mapper = {}\n",
    "\n",
    "        for i, x in enumerate(base_path.iterdir()):\n",
    "            label_mapper[x.stem] = i\n",
    "\n",
    "        for label_type in label_mapper.keys():\n",
    "            for x in (base_path / label_type).iterdir():\n",
    "                self.all_paths.append(x)\n",
    "                self.all_labels.append(label_mapper[label_type])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_path = self.all_paths[idx]\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        x = content\n",
    "        y = self.all_labels[idx]\n",
    "\n",
    "        # apply rc_aug here if using\n",
    "        if self.rc_aug and coin_flip():\n",
    "            x = string_reverse_complement(x)\n",
    "\n",
    "        seq = self.tokenizer(x,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\" if self.use_padding else None,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )  # add cls and eos token (+2)\n",
    "        seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        # need to handle eos here\n",
    "        if self.add_eos:\n",
    "            # append list seems to be faster than append tensor\n",
    "            seq.append(self.tokenizer.sep_token_id)\n",
    "\n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)\n",
    "\n",
    "        # need to wrap in list\n",
    "        target = torch.LongTensor([y])\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading human_enhancers_cohn to /data/leslie/sarthak/data/genomic_benchmark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=176563cDPQ5Y094WyoSBF02QjoVQhWuCh \n",
      "\n"
     ]
    },
    {
     "ename": "ReadError",
     "evalue": "/data/leslie/sarthak/data/genomic_benchmark/human_enhancers_cohn.zip is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#only need split and max length\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGenomicBenchmarkDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/leslie/sarthak/data/genomic_benchmark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 80\u001b[0m, in \u001b[0;36mGenomicBenchmarkDataset.__init__\u001b[0;34m(self, split, max_length, dataset_name, d_output, dest_path, tokenizer, tokenizer_name, use_padding, add_eos, rc_aug, return_augs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_downloaded(dataset_name, cache_path\u001b[38;5;241m=\u001b[39mdest_path):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_name, dest_path))\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malready downloaded \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(split, dataset_name))\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/genomic_benchmarks/loc2seq/loc2seq.py:55\u001b[0m, in \u001b[0;36mdownload_dataset\u001b[0;34m(interval_list_dataset, version, dest_path, cache_path, force_download, use_cloud_cache, local_repo)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cloud_cache \u001b[38;5;129;01mand\u001b[39;00m ((dataset_name, version) \u001b[38;5;129;01min\u001b[39;00m CLOUD_CACHE):\n\u001b[1;32m     54\u001b[0m     Path(dest_path)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# to be sure \"./.genomic_benchmarks\" exists\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdownload_from_cloud_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m refs \u001b[38;5;241m=\u001b[39m _download_references(metadata, cache_path\u001b[38;5;241m=\u001b[39mcache_path, force\u001b[38;5;241m=\u001b[39mforce_download)\n\u001b[1;32m     58\u001b[0m fastas \u001b[38;5;241m=\u001b[39m _load_fastas_into_memory(refs, cache_path\u001b[38;5;241m=\u001b[39mcache_path)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/genomic_benchmarks/loc2seq/cloud_caching.py:37\u001b[0m, in \u001b[0;36mdownload_from_cloud_cache\u001b[0;34m(file_key, dest_path, cloud_cache, force_download)\u001b[0m\n\u001b[1;32m     30\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mrmtree(\u001b[38;5;28mstr\u001b[39m(dest_path))\n\u001b[1;32m     32\u001b[0m gdown\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m cloud_cache[file_key],\n\u001b[1;32m     34\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(dest_path) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m Path(\u001b[38;5;28mstr\u001b[39m(dest_path) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39munlink()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Path(dest_path)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/shutil.py:1327\u001b[0m, in \u001b[0;36munpack_archive\u001b[0;34m(filename, extract_dir, format, filter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m func \u001b[38;5;241m=\u001b[39m _UNPACK_FORMATS[\u001b[38;5;28mformat\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1326\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_UNPACK_FORMATS[\u001b[38;5;28mformat\u001b[39m][\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m|\u001b[39m filter_kwargs\n\u001b[0;32m-> 1327\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/shutil.py:1214\u001b[0m, in \u001b[0;36m_unpack_zipfile\u001b[0;34m(filename, extract_dir)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m  \u001b[38;5;66;03m# late import for breaking circular dependency\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(filename):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a zip file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28mzip\u001b[39m \u001b[38;5;241m=\u001b[39m zipfile\u001b[38;5;241m.\u001b[39mZipFile(filename)\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mReadError\u001b[0m: /data/leslie/sarthak/data/genomic_benchmark/human_enhancers_cohn.zip is not a zip file"
     ]
    }
   ],
   "source": [
    "#only need split and max length\n",
    "generator = GenomicBenchmarkDataset(split='train', max_length=1000, dest_path='/data/leslie/sarthak/data/genomic_benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded train-human_enhancers_cohn\n"
     ]
    }
   ],
   "source": [
    "generator = GenomicBenchmarkDataset(split='train', max_length=1000, dest_path='/data/leslie/sarthak/data/genomic_benchmark')\n",
    "#it is already downloaded, perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GenomicBenchmarkDataset at 0x2b966c963190>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/leslie/sarthak/data/genomic_benchmark/human_enhancers_cohn/train/positive/train_positive_4511.txt\n",
      "20843\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#so we have the generator class now, i'm unsure how to use it\n",
    "print(generator.all_paths[0])\n",
    "print(len(generator.all_paths))\n",
    "print(generator.all_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTCAAACCTTCCGTTCTTTCAAGGAAAGACAATTTTTGAAACTGTATCTTTTCCTTATTATTCTTTTACTTTATTTTCTCAGCATGCCCACTCAAAGGGCCTAAAACCAATGTCCCCACAGAAGCAAGAAACATACCTAATGCCCAGATCTTGGTTGTTAAATATTGTTCCTCACTAATTGTACTGAAGCTCTTTGGTGAAATGGCTAATTCTAAGTCTGTGACCATGCATATCTATCTATGGCCACTGAGAGACACCACTGACCTGCCTAACCAGAGTACAGAGTCCTAAAGCTTTAGTTCTGTTTGTTGCCGCTCGTTGCAAATTTCCCCCCGATCCTGATATAGACAGGCCGTGTATTTTTGTAAATAGTTTCCTGGGAGCACAGACATACCCATTTGTTTAAACACTGTCTAGAGCTGTTTTCATGCTACTATGGCATTGTTAAGTAGTTGCCACTGAGACCATAAGGCACACAAAGGCTAAAATATCACTAACTA\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#let's test it now\n",
    "idx = 0\n",
    "txt_path = generator.all_paths[idx]\n",
    "with open(txt_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "x = content\n",
    "y = generator.all_labels[idx]\n",
    "print(x,y, sep='\\n')\n",
    "\n",
    "# # apply rc_aug here if using\n",
    "# if generator.rc_aug and coin_flip():\n",
    "#     x = string_reverse_complement(x)\n",
    "\n",
    "# seq = generator.tokenizer(x,\n",
    "#     add_special_tokens=False,\n",
    "#     padding=\"max_length\" if generator.use_padding else None,\n",
    "#     max_length=generator.max_length,\n",
    "#     truncation=True,\n",
    "# )  # add cls and eos token (+2)\n",
    "# seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "# # need to handle eos here\n",
    "# if generator.add_eos:\n",
    "#     # append list seems to be faster than append tensor\n",
    "#     seq.append(generator.tokenizer.sep_token_id)\n",
    "\n",
    "# # convert to tensor\n",
    "# seq = torch.LongTensor(seq)\n",
    "\n",
    "# # need to wrap in list\n",
    "# target = torch.LongTensor([y])\n",
    "\n",
    "# return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have our sequence, let's ignore the reverse complement for now\n",
    "\n",
    "#the tokenizer is the none thing for some reason, where is it defined? Oh we need to get the character tokenizer too!!\n",
    "\n",
    "#oh we need to add the tokenizer ourself, lets use the character tokenizer\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "    \n",
    "tokenizer = CharacterTokenizer(characters=['A', 'C', 'G', 'T','N'], model_max_length=1000+2) #the plus 2 for the bos and eos tokens\n",
    "#since hyena is causal, pad to the elft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 7, 1], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [0, 7, 6, 8, 1], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [0, 7, 8, 9, 10, 11, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('A'))\n",
    "print(tokenizer('ABC'))\n",
    "print(tokenizer('ACGTN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded train-human_enhancers_cohn\n"
     ]
    }
   ],
   "source": [
    "generator = GenomicBenchmarkDataset(split='train', max_length=1000, dest_path='/data/leslie/sarthak/data/genomic_benchmark', tokenizer=tokenizer, tokenizer_name='character',\n",
    "                                    use_padding=True, add_eos=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n",
      "tensor(4)\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "print(generator[0][0].shape)\n",
    "print(generator[0][0][0], generator[0][0][-1], sep='\\n')\n",
    "#i have no clue what that 4 is... is it bos? I don't think so because exaclty 1000??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 10, 8, 7, 7, 7, 8, 8, 10, 10, 8, 8, 9, 10, 10, 8, 10, 10, 10, 8, 7, 7, 9, 9, 7, 7, 7, 9, 7, 8, 7, 7, 10, 10, 10, 10, 10, 9, 7, 7, 7, 8, 10, 9, 10, 7, 10, 8, 10, 10, 10, 10, 8, 8, 10, 10, 7, 10, 10, 7, 10, 10, 8, 10, 10, 10, 10, 7, 8, 10, 10, 10, 7, 10, 10, 10, 10, 8, 10, 8, 7, 9, 8, 7, 10, 9, 8, 8, 8, 7, 8, 10, 8, 7, 7, 7, 9, 9, 9, 8, 8, 10, 7, 7, 7, 7, 8, 8, 7, 7, 10, 9, 10, 8, 8, 8, 8, 7, 8, 7, 9, 7, 7, 9, 8, 7, 7, 9, 7, 7, 7, 8, 7, 10, 7, 8, 8, 10, 7, 7, 10, 9, 8, 8, 8, 7, 9, 7, 10, 8, 10, 10, 9, 9, 10, 10, 9, 10, 10, 7, 7, 7, 10, 7, 10, 10, 9, 10, 10, 8, 8, 10, 8, 7, 8, 10, 7, 7, 10, 10, 9, 10, 7, 8, 10, 9, 7, 7, 9, 8, 10, 8, 10, 10, 10, 9, 9, 10, 9, 7, 7, 7, 10, 9, 9, 8, 10, 7, 7, 10, 10, 8, 10, 7, 7, 9, 10, 8, 10, 9, 10, 9, 7, 8, 8, 7, 10, 9, 8, 7, 10, 7, 10, 8, 10, 7, 10, 8, 10, 7, 10, 9, 9, 8, 8, 7, 8, 10, 9, 7, 9, 7, 9, 7, 8, 7, 8, 8, 7, 8, 10, 9, 7, 8, 8, 10, 9, 8, 8, 10, 7, 7, 8, 8, 7, 9, 7, 9, 10, 7, 8, 7, 9, 7, 9, 10, 8, 8, 10, 7, 7, 7, 9, 8, 10, 10, 10, 7, 9, 10, 10, 8, 10, 9, 10, 10, 10, 9, 10, 10, 9, 8, 8, 9, 8, 10, 8, 9, 10, 10, 9, 8, 7, 7, 7, 10, 10, 10, 8, 8, 8, 8, 8, 8, 9, 7, 10, 8, 8, 10, 9, 7, 10, 7, 10, 7, 9, 7, 8, 7, 9, 9, 8, 8, 9, 10, 9, 10, 7, 10, 10, 10, 10, 10, 9, 10, 7, 7, 7, 10, 7, 9, 10, 10, 10, 8, 8, 10, 9, 9, 9, 7, 9, 8, 7, 8, 7, 9, 7, 8, 7, 10, 7, 8, 8, 8, 7, 10, 10, 10, 9, 10, 10, 10, 7, 7, 7, 8, 7, 8, 10, 9, 10, 8, 10, 7, 9, 7, 9, 8, 10, 9, 10, 10, 10, 10, 8, 7, 10, 9, 8, 10, 7, 8, 10, 7, 10, 9, 9, 8, 7, 10, 10, 9, 10, 10, 7, 7, 9, 10, 7, 9, 10, 10, 9, 8, 8, 7, 8, 10, 9, 7, 9, 7, 8, 8, 7, 10, 7, 7, 9, 9, 8, 7, 8, 7, 8, 7, 7, 7, 9, 9, 8, 10, 7, 7, 7, 7, 10, 7, 10, 8, 7, 8, 10, 7, 7, 8, 10, 7, 1]\n",
      "502\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(x)['input_ids'])\n",
    "print(len(tokenizer(x)['input_ids']))\n",
    "#see this is what we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.sep_token_id) #that's the sep token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "seq = tokenizer(x,\n",
    "    add_special_tokens=False)  # add cls and eos token (+2)\n",
    "print(len(seq['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "#oh that's one strange thing, it's 500, so the stuff at the beginning, could it just be the pading??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(generator[0][0][0:5])\n",
    "#yessir, the 4 is just the padding that was added to the left since the sequence is not long enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "#let's make sure and see if the first 500 elements are the same\n",
    "print(min(generator[0][0][0:500]))\n",
    "#so no bos token, which is a little strange, but should be fine.\n",
    "\n",
    "#but the key is when we call the tokenizer later can still add the bos and eos tokens, but choose to pad on the elft and eos on the right, that should be fine.\n",
    "\n",
    "#so if we say yes to the eos token, then we should get 1001, and if we say no, we should get 1000, and that's exaclty what happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will implement their next token prediction class\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from pyfaidx import Fasta\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "from random import randrange, random\n",
    "import numpy as np\n",
    "\n",
    "class FastaInterval():\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        fasta_file,\n",
    "        # max_length = None,\n",
    "        return_seq_indices = False,\n",
    "        shift_augs = None,\n",
    "        rc_aug = False,\n",
    "        pad_interval = False,\n",
    "    ):\n",
    "        fasta_file = Path(fasta_file)\n",
    "        assert fasta_file.exists(), 'path to fasta file must exist'\n",
    "\n",
    "        self.seqs = Fasta(str(fasta_file))\n",
    "        self.return_seq_indices = return_seq_indices\n",
    "        # self.max_length = max_length # -1 for adding sos or eos token\n",
    "        self.shift_augs = shift_augs\n",
    "        self.rc_aug = rc_aug\n",
    "        self.pad_interval = pad_interval        \n",
    "\n",
    "        # calc len of each chromosome in fasta file, store in dict\n",
    "        self.chr_lens = {}\n",
    "\n",
    "        for chr_name in self.seqs.keys():\n",
    "            # remove tail end, might be gibberish code\n",
    "            # truncate_len = int(len(self.seqs[chr_name]) * 0.9)\n",
    "            # self.chr_lens[chr_name] = truncate_len\n",
    "            self.chr_lens[chr_name] = len(self.seqs[chr_name])\n",
    "\n",
    "\n",
    "    def __call__(self, chr_name, start, end, max_length, return_augs = False):\n",
    "        \"\"\"\n",
    "        max_length passed from dataset, not from init\n",
    "        \"\"\"\n",
    "        interval_length = end - start\n",
    "        chromosome = self.seqs[chr_name]\n",
    "        # chromosome_length = len(chromosome)\n",
    "        chromosome_length = self.chr_lens[chr_name]\n",
    "\n",
    "        if exists(self.shift_augs):\n",
    "            min_shift, max_shift = self.shift_augs\n",
    "            max_shift += 1\n",
    "\n",
    "            min_shift = max(start + min_shift, 0) - start\n",
    "            max_shift = min(end + max_shift, chromosome_length) - end\n",
    "\n",
    "            rand_shift = randrange(min_shift, max_shift)\n",
    "            start += rand_shift\n",
    "            end += rand_shift\n",
    "\n",
    "        left_padding = right_padding = 0\n",
    "\n",
    "        # checks if not enough sequence to fill up the start to end\n",
    "        if interval_length < max_length:\n",
    "            extra_seq = max_length - interval_length\n",
    "\n",
    "            extra_left_seq = extra_seq // 2\n",
    "            extra_right_seq = extra_seq - extra_left_seq\n",
    "\n",
    "            start -= extra_left_seq\n",
    "            end += extra_right_seq\n",
    "\n",
    "        if start < 0:\n",
    "            left_padding = -start\n",
    "            start = 0\n",
    "\n",
    "        if end > chromosome_length:\n",
    "            right_padding = end - chromosome_length\n",
    "            end = chromosome_length\n",
    "\n",
    "        # Added support!  need to allow shorter seqs\n",
    "        if interval_length > max_length:\n",
    "            end = start + max_length\n",
    "\n",
    "        seq = str(chromosome[start:end])\n",
    "\n",
    "        if self.rc_aug and coin_flip():\n",
    "            seq = string_reverse_complement(seq)\n",
    "\n",
    "        if self.pad_interval:\n",
    "            seq = ('.' * left_padding) + seq + ('.' * right_padding)\n",
    "\n",
    "        return seq\n",
    "\n",
    "class HG38Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        bed_file,\n",
    "        fasta_file,\n",
    "        max_length,\n",
    "        pad_max_length=None,\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        add_eos=False,\n",
    "        return_seq_indices=False,\n",
    "        shift_augs=None,\n",
    "        rc_aug=False,\n",
    "        return_augs=False,\n",
    "        replace_N_token=False,  # replace N token with pad token\n",
    "        pad_interval = False,  # options for different padding\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.replace_N_token = replace_N_token  \n",
    "        self.pad_interval = pad_interval         \n",
    "\n",
    "        bed_path = Path(bed_file)\n",
    "        assert bed_path.exists(), 'path to .bed file must exist'\n",
    "\n",
    "        # read bed file\n",
    "        df_raw = pd.read_csv(str(bed_path), sep = '\\t', names=['chr_name', 'start', 'end', 'id1', 'id2', 'annotation'])\n",
    "        # select only split df\n",
    "        # self.df = df_raw[df_raw['split'] == split]\n",
    "        #drop the other columns\n",
    "        self.df = df_raw[['chr_name', 'start', 'end']]\n",
    "\n",
    "        self.fasta = FastaInterval(\n",
    "            fasta_file = fasta_file,\n",
    "            # max_length = max_length,\n",
    "            return_seq_indices = return_seq_indices,\n",
    "            shift_augs = shift_augs,\n",
    "            rc_aug = rc_aug,\n",
    "            pad_interval = pad_interval,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def replace_value(self, x, old_value, new_value):\n",
    "        return torch.where(x == old_value, new_value, x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a sequence of specified len\"\"\"\n",
    "        # sample a random row from df\n",
    "        row = self.df.iloc[idx]\n",
    "        # row = (chr, start, end, split)\n",
    "        chr_name, start, end = (row[0], row[1], row[2])\n",
    "\n",
    "        seq = self.fasta(chr_name, start, end, max_length=self.max_length, return_augs=self.return_augs)\n",
    "\n",
    "        if self.tokenizer_name == 'char':\n",
    "\n",
    "            seq = self.tokenizer(seq,\n",
    "                add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        elif self.tokenizer_name == 'bpe':\n",
    "            seq = self.tokenizer(seq, \n",
    "                # add_special_tokens=False, \n",
    "                padding=\"max_length\",\n",
    "                max_length=self.pad_max_length,\n",
    "                truncation=True,\n",
    "            ) \n",
    "            # get input_ids\n",
    "            if self.add_eos:\n",
    "                seq = seq[\"input_ids\"][1:]  # remove the bos, keep the eos token\n",
    "            else:\n",
    "                seq = seq[\"input_ids\"][1:-1]  # remove both special tokens\n",
    "        \n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)  # hack, remove the initial cls tokens for now\n",
    "\n",
    "        if self.replace_N_token:\n",
    "            # replace N token with a pad token, so we can ignore it in the loss\n",
    "            seq = self.replace_value(seq, self.tokenizer._vocab_str_to_int['N'], self.tokenizer.pad_token_id)\n",
    "\n",
    "        data = seq[:-1].clone()  # remove eos\n",
    "        target = seq[1:].clone()  # offset by 1, includes eos\n",
    "\n",
    "        return data, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now let's test their class and the fasta interval class, but it seems quite straightforward\n",
    "#let's see the output of this compared to the output of my function, so then we can just easily load it in\n",
    "fasta = FastaInterval(\n",
    "            fasta_file = '/data/leslie/sarthak/data/ncbi_dataset/data/GCF_000001405.26/GCF_000001405.26_GRCh38_genomic.fna',\n",
    "            # max_length = max_length,\n",
    "            return_seq_indices =False,\n",
    "            shift_augs =None,\n",
    "            rc_aug =False,\n",
    "            pad_interval =False,\n",
    "        )\n",
    "fasta('NC_000001.11', 1000, 2000, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAAG\n",
      "atttcagatctctagaactatccatcagtgaaatggattgcaaatacaaagagtaataccatgtcacttaagaatagaatcatggacgaggctgccacctgctgttgggggccactgcagaagaaattccagaacactggactggagagcacc\n",
      "1000\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "#now compare to mine\n",
    "#use the actual first one from the bed file\n",
    "#chr1\t104896\t105048\n",
    "seq_theirs = fasta('NC_000001.11', 104896, 105048, max_length=1000)\n",
    "#now compare to using the pysam approach\n",
    "import requests\n",
    "\n",
    "def fetch_sequence(chromosome, start, end, genome=\"hg38\"):\n",
    "    \"\"\"\n",
    "    Fetches a DNA sequence from the UCSC Genome Browser.\n",
    "\n",
    "    Parameters:\n",
    "    chromosome (str): Chromosome number (e.g., 'chr1', 'chr2')\n",
    "    start (int): Starting position of the sequence\n",
    "    end (int): Ending position of the sequence\n",
    "    genome (str): Genome build (default is hg38, the latest human genome version)\n",
    "\n",
    "    Returns:\n",
    "    str: DNA sequence\n",
    "    \"\"\"\n",
    "    url = f\"http://genome.ucsc.edu/cgi-bin/das/{genome}/dna?segment={chromosome}:{start},{end}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parsing the response to extract the sequence\n",
    "        from xml.etree import ElementTree as ET\n",
    "        root = ET.fromstring(response.text)\n",
    "        sequence = next(root.iter('DNA')).text\n",
    "        sequence = sequence.replace('\\n', '').strip()\n",
    "        return sequence\n",
    "    else:\n",
    "        return \"Error: Unable to fetch the sequence.\"\n",
    "    \n",
    "seq_mine = fetch_sequence('chr1', 104896, 105048)\n",
    "\n",
    "#now compare\n",
    "print(seq_theirs)\n",
    "print(seq_mine)\n",
    "print(len(seq_theirs))\n",
    "print(len(seq_mine))\n",
    "#Their approach also extends it on both sides, so it is a decent alternative approach.\n",
    "#there is one more on the right than on th eleft, but probably doesn't matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's see their output\n",
    "\n",
    "hg38 = HG38Dataset(split='train', bed_file='/data/leslie/sarthak/data/GRCh38-cCREs.bed', fasta_file='/data/leslie/sarthak/data/ncbi_dataset/data/GCF_000001405.26/GCF_000001405.26_GRCh38_genomic.fna', max_length=1000, pad_max_length=None, tokenizer=tokenizer, tokenizer_name='character', add_eos=False, return_seq_indices=False, shift_augs=None, rc_aug=False, return_augs=False, replace_N_token=False, pad_interval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>104896</td>\n",
       "      <td>105048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>138866</td>\n",
       "      <td>139134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>181289</td>\n",
       "      <td>181639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>267925</td>\n",
       "      <td>268171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>586036</td>\n",
       "      <td>586264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063873</th>\n",
       "      <td>chrY</td>\n",
       "      <td>21252996</td>\n",
       "      <td>21253278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063874</th>\n",
       "      <td>chrY</td>\n",
       "      <td>21598449</td>\n",
       "      <td>21598656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063875</th>\n",
       "      <td>chrY</td>\n",
       "      <td>21839503</td>\n",
       "      <td>21839853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063876</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26352857</td>\n",
       "      <td>26353207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063877</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26353315</td>\n",
       "      <td>26353520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063878 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        chr_name     start       end\n",
       "0           chr1    104896    105048\n",
       "1           chr1    138866    139134\n",
       "2           chr1    181289    181639\n",
       "3           chr1    267925    268171\n",
       "4           chr1    586036    586264\n",
       "...          ...       ...       ...\n",
       "1063873     chrY  21252996  21253278\n",
       "1063874     chrY  21598449  21598656\n",
       "1063875     chrY  21839503  21839853\n",
       "1063876     chrY  26352857  26353207\n",
       "1063877     chrY  26353315  26353520\n",
       "\n",
       "[1063878 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg38.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'NC_000001.11',\n",
       " '2': 'NC_000002.12',\n",
       " '3': 'NC_000003.12',\n",
       " '4': 'NC_000004.12',\n",
       " '5': 'NC_000005.10',\n",
       " '6': 'NC_000006.12',\n",
       " '7': 'NC_000007.14',\n",
       " '8': 'NC_000008.11',\n",
       " '9': 'NC_000009.12',\n",
       " '10': 'NC_000010.11',\n",
       " '11': 'NC_000011.10',\n",
       " '12': 'NC_000012.12',\n",
       " '13': 'NC_000013.11',\n",
       " '14': 'NC_000014.9',\n",
       " '15': 'NC_000015.10',\n",
       " '16': 'NC_000016.10',\n",
       " '17': 'NC_000017.11',\n",
       " '18': 'NC_000018.10',\n",
       " '19': 'NC_000019.10',\n",
       " '20': 'NC_000020.11',\n",
       " '21': 'NC_000021.9',\n",
       " '22': 'NC_000022.11',\n",
       " '23': 'NC_000023.11',\n",
       " '24': 'NC_000024.10',\n",
       " 'X': 'NC_000023.11',\n",
       " 'Y': 'NC_000024.10'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#swap the name with the actual name\n",
    "chromosome_names = ['NC_000001.11','NC_000002.12','NC_000003.12','NC_000004.12','NC_000005.10','NC_000006.12','NC_000007.14',\n",
    "                    'NC_000008.11','NC_000009.12','NC_000010.11','NC_000011.10','NC_000012.12','NC_000013.11','NC_000014.9',\n",
    "                    'NC_000015.10','NC_000016.10','NC_000017.11','NC_000018.10','NC_000019.10','NC_000020.11','NC_000021.9',\n",
    "                    'NC_000022.11','NC_000023.11','NC_000024.10']\n",
    "\n",
    "#first make a dict that translates the chromosome name to the index in the list\n",
    "# chromosome_names = dict(zip(range(1,25), chromosome_names))\n",
    "translation_dict = {}\n",
    "for i in range(1,25):\n",
    "    translation_dict[str(i)] = chromosome_names[i-1]\n",
    "translation_dict['X'] = 'NC_000023.11'\n",
    "translation_dict['Y'] = 'NC_000024.10'\n",
    "translation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr_name</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>104896</td>\n",
       "      <td>105048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>138866</td>\n",
       "      <td>139134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>181289</td>\n",
       "      <td>181639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>267925</td>\n",
       "      <td>268171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>586036</td>\n",
       "      <td>586264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063873</th>\n",
       "      <td>NC_000024.10</td>\n",
       "      <td>21252996</td>\n",
       "      <td>21253278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063874</th>\n",
       "      <td>NC_000024.10</td>\n",
       "      <td>21598449</td>\n",
       "      <td>21598656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063875</th>\n",
       "      <td>NC_000024.10</td>\n",
       "      <td>21839503</td>\n",
       "      <td>21839853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063876</th>\n",
       "      <td>NC_000024.10</td>\n",
       "      <td>26352857</td>\n",
       "      <td>26353207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063877</th>\n",
       "      <td>NC_000024.10</td>\n",
       "      <td>26353315</td>\n",
       "      <td>26353520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063878 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             chr_name     start       end\n",
       "0        NC_000001.11    104896    105048\n",
       "1        NC_000001.11    138866    139134\n",
       "2        NC_000001.11    181289    181639\n",
       "3        NC_000001.11    267925    268171\n",
       "4        NC_000001.11    586036    586264\n",
       "...               ...       ...       ...\n",
       "1063873  NC_000024.10  21252996  21253278\n",
       "1063874  NC_000024.10  21598449  21598656\n",
       "1063875  NC_000024.10  21839503  21839853\n",
       "1063876  NC_000024.10  26352857  26353207\n",
       "1063877  NC_000024.10  26353315  26353520\n",
       "\n",
       "[1063878 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now translate the df\n",
    "def translate_chromosome_names(chr_name):\n",
    "    num = chr_name[3:]  # This gets the part after 'chr'\n",
    "    if num == 'X':\n",
    "        num = '23'\n",
    "    elif num == 'Y':\n",
    "        num = '24'\n",
    "    return chromosome_names.get(num, chr_name)\n",
    "\n",
    "hg38.df['chr_name'] = hg38.df['chr_name'].str.replace('chr', '').map(translation_dict)\n",
    "hg38.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lsftmp/2982455.tmpdir/ipykernel_91926/2161617610.py:161: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  chr_name, start, end = (row[0], row[1], row[2])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhg38\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 189\u001b[0m, in \u001b[0;36mHG38Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    186\u001b[0m         seq \u001b[38;5;241m=\u001b[39m seq[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# remove both special tokens\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# convert to tensor\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m seq \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# hack, remove the initial cls tokens for now\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_N_token:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# replace N token with a pad token, so we can ignore it in the loss\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_value(seq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39m_vocab_str_to_int[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "hg38[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NC_000001.11\n",
      "104896\n",
      "105048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/lsftmp/2982455.tmpdir/ipykernel_91926/2811297280.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  chr_name, start, end = (row[0], row[1], row[2])\n"
     ]
    }
   ],
   "source": [
    "row = hg38.df.iloc[idx]\n",
    "# row = (chr, start, end, split)\n",
    "chr_name, start, end = (row[0], row[1], row[2])\n",
    "print(chr_name, start, end, sep='\\n')\n",
    "# seq = self.fasta(chr_name, start, end, max_length=self.max_length, return_augs=self.return_augs)\n",
    "\n",
    "# if self.tokenizer_name == 'char':\n",
    "\n",
    "#     seq = self.tokenizer(seq,\n",
    "#         add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=self.max_length,\n",
    "#         truncation=True,\n",
    "#     )\n",
    "#     seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "# elif self.tokenizer_name == 'bpe':\n",
    "#     seq = self.tokenizer(seq, \n",
    "#         # add_special_tokens=False, \n",
    "#         padding=\"max_length\",\n",
    "#         max_length=self.pad_max_length,\n",
    "#         truncation=True,\n",
    "#     ) \n",
    "#     # get input_ids\n",
    "#     if self.add_eos:\n",
    "#         seq = seq[\"input_ids\"][1:]  # remove the bos, keep the eos token\n",
    "#     else:\n",
    "#         seq = seq[\"input_ids\"][1:-1]  # remove both special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAAG\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "seq = hg38.fasta(chr_name, start, end, max_length=hg38.max_length, return_augs=hg38.return_augs)\n",
    "print(seq)\n",
    "print(len(seq))\n",
    "#works as expected! since idx is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then they tokenize the sequence\n",
    "seq = hg38.tokenizer(seq,\n",
    "        add_special_tokens=True if hg38.add_eos else False,  # this is what controls adding eos\n",
    "        padding=\"max_length\",\n",
    "        max_length=hg38.max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg38.add_eos\n",
    "#no bos or eos, but can add them\n",
    "#regardless they remove eos, but keep bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [7, 7, 10, 8, 10, 9, 9, 10, 9, 9, 9, 9, 7, 7, 9, 8, 7, 7, 9, 8, 7, 7, 7, 10, 9, 8, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 7, 9, 7, 9, 8, 9, 7, 8, 10, 8, 7, 9, 7, 10, 9, 8, 10, 7, 10, 7, 7, 7, 7, 8, 10, 10, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 8, 10, 8, 7, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 8, 7, 10, 7, 8, 7, 7, 10, 8, 8, 7, 9, 9, 10, 10, 10, 10, 7, 7, 10, 8, 7, 10, 8, 7, 9, 7, 7, 7, 10, 8, 7, 8, 7, 9, 10, 8, 8, 10, 7, 10, 10, 9, 10, 8, 10, 10, 8, 10, 9, 8, 7, 8, 7, 9, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 10, 8, 7, 10, 9, 10, 10, 8, 7, 7, 10, 7, 10, 9, 7, 7, 10, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 8, 9, 8, 9, 7, 9, 7, 7, 9, 10, 7, 8, 7, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 9, 8, 8, 7, 9, 8, 10, 9, 9, 8, 7, 10, 9, 10, 8, 7, 7, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 7, 8, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 9, 7, 7, 9, 7, 8, 10, 8, 7, 9, 9, 7, 7, 9, 7, 8, 7, 7, 9, 10, 7, 10, 9, 7, 7, 9, 8, 7, 10, 7, 7, 10, 8, 10, 9, 10, 9, 7, 8, 7, 10, 10, 8, 8, 7, 10, 9, 8, 9, 9, 8, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 7, 8, 7, 10, 7, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 9, 8, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 8, 10, 9, 7, 9, 8, 10, 7, 7, 7, 10, 8, 10, 8, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 7, 10, 8, 10, 8, 10, 7, 9, 7, 7, 8, 10, 7, 10, 8, 8, 7, 10, 8, 7, 9, 10, 9, 7, 7, 7, 10, 9, 9, 7, 10, 10, 9, 8, 7, 7, 7, 10, 7, 8, 7, 7, 7, 9, 7, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 9, 10, 8, 7, 8, 10, 10, 7, 7, 9, 7, 7, 10, 7, 9, 7, 7, 10, 8, 7, 10, 9, 9, 7, 8, 9, 7, 9, 9, 8, 10, 9, 8, 8, 7, 8, 8, 10, 9, 8, 10, 9, 10, 10, 9, 9, 9, 9, 9, 8, 8, 7, 8, 10, 9, 8, 7, 9, 7, 7, 9, 7, 7, 7, 10, 10, 8, 8, 7, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 8, 10, 9, 9, 7, 9, 7, 9, 8, 7, 8, 8, 10, 8, 7, 8, 10, 10, 10, 8, 8, 10, 10, 7, 8, 7, 9, 8, 10, 8, 10, 7, 7, 9, 10, 10, 10, 8, 10, 9, 7, 8, 10, 8, 7, 9, 10, 9, 7, 8, 8, 10, 9, 7, 10, 10, 8, 7, 8, 10, 7, 8, 8, 7, 10, 7, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 8, 7, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 10, 9, 7, 8, 10, 9, 10, 10, 8, 10, 10, 8, 7, 8, 7, 8, 10, 7, 9, 9, 8, 8, 8, 7, 10, 9, 9, 7, 9, 7, 8, 7, 9, 9, 9, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 10, 7, 8, 8, 10, 10, 8, 10, 8, 8, 9, 8, 10, 7, 8, 10, 9, 7, 8, 7, 10, 8, 10, 7, 9, 9, 8, 7, 10, 10, 7, 8, 7, 8, 7, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 7, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 9, 7, 7, 9, 10, 10, 10, 9, 8, 10, 7, 8, 10, 10, 8, 10, 8, 7, 9, 7, 9, 7, 9, 7, 8, 10, 7, 7, 10, 9, 7, 9, 10, 7, 9, 10, 9, 7, 9, 8, 7, 7, 7, 10, 7, 10, 8, 8, 10, 9, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 7, 7, 8, 9, 9, 7, 7, 10, 7, 10, 10, 8, 7, 10, 8, 7, 7, 7, 7, 8, 7, 8, 7, 9, 8, 7, 9, 10, 10, 8, 10, 9, 8, 7, 8, 10, 10, 7, 7, 8, 10, 10, 10, 7, 9, 9, 8, 8, 10, 10, 10, 10, 8, 10, 7, 7, 8, 7, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 9, 10, 7, 7, 8, 10, 9, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 8, 8, 7, 8, 7, 9, 7, 10, 7, 7, 7, 9, 9, 7, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 9, 9, 7, 10, 7, 9, 9, 9, 10, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 7, 9], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(seq['input_ids']))\n",
    "#no eos or bos token or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [7, 7, 10, 8, 10, 9, 9, 10, 9, 9, 9, 9, 7, 7, 9, 8, 7, 7, 9, 8, 7, 7, 7, 10, 9, 8, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 7, 9, 7, 9, 8, 9, 7, 8, 10, 8, 7, 9, 7, 10, 9, 8, 10, 7, 10, 7, 7, 7, 7, 8, 10, 10, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 8, 10, 8, 7, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 8, 7, 10, 7, 8, 7, 7, 10, 8, 8, 7, 9, 9, 10, 10, 10, 10, 7, 7, 10, 8, 7, 10, 8, 7, 9, 7, 7, 7, 10, 8, 7, 8, 7, 9, 10, 8, 8, 10, 7, 10, 10, 9, 10, 8, 10, 10, 8, 10, 9, 8, 7, 8, 7, 9, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 10, 8, 7, 10, 9, 10, 10, 8, 7, 7, 10, 7, 10, 9, 7, 7, 10, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 8, 9, 8, 9, 7, 9, 7, 7, 9, 10, 7, 8, 7, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 9, 8, 8, 7, 9, 8, 10, 9, 9, 8, 7, 10, 9, 10, 8, 7, 7, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 7, 8, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 9, 7, 7, 9, 7, 8, 10, 8, 7, 9, 9, 7, 7, 9, 7, 8, 7, 7, 9, 10, 7, 10, 9, 7, 7, 9, 8, 7, 10, 7, 7, 10, 8, 10, 9, 10, 9, 7, 8, 7, 10, 10, 8, 8, 7, 10, 9, 8, 9, 9, 8, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 7, 8, 7, 10, 7, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 9, 8, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 8, 10, 9, 7, 9, 8, 10, 7, 7, 7, 10, 8, 10, 8, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 7, 10, 8, 10, 8, 10, 7, 9, 7, 7, 8, 10, 7, 10, 8, 8, 7, 10, 8, 7, 9, 10, 9, 7, 7, 7, 10, 9, 9, 7, 10, 10, 9, 8, 7, 7, 7, 10, 7, 8, 7, 7, 7, 9, 7, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 9, 10, 8, 7, 8, 10, 10, 7, 7, 9, 7, 7, 10, 7, 9, 7, 7, 10, 8, 7, 10, 9, 9, 7, 8, 9, 7, 9, 9, 8, 10, 9, 8, 8, 7, 8, 8, 10, 9, 8, 10, 9, 10, 10, 9, 9, 9, 9, 9, 8, 8, 7, 8, 10, 9, 8, 7, 9, 7, 7, 9, 7, 7, 7, 10, 10, 8, 8, 7, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 8, 10, 9, 9, 7, 9, 7, 9, 8, 7, 8, 8, 10, 8, 7, 8, 10, 10, 10, 8, 8, 10, 10, 7, 8, 7, 9, 8, 10, 8, 10, 7, 7, 9, 10, 10, 10, 8, 10, 9, 7, 8, 10, 8, 7, 9, 10, 9, 7, 8, 8, 10, 9, 7, 10, 10, 8, 7, 8, 10, 7, 8, 8, 7, 10, 7, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 8, 7, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 10, 9, 7, 8, 10, 9, 10, 10, 8, 10, 10, 8, 7, 8, 7, 8, 10, 7, 9, 9, 8, 8, 8, 7, 10, 9, 9, 7, 9, 7, 8, 7, 9, 9, 9, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 10, 7, 8, 8, 10, 10, 8, 10, 8, 8, 9, 8, 10, 7, 8, 10, 9, 7, 8, 7, 10, 8, 10, 7, 9, 9, 8, 7, 10, 10, 7, 8, 7, 8, 7, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 7, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 9, 7, 7, 9, 10, 10, 10, 9, 8, 10, 7, 8, 10, 10, 8, 10, 8, 7, 9, 7, 9, 7, 9, 7, 8, 10, 7, 7, 10, 9, 7, 9, 10, 7, 9, 10, 9, 7, 9, 8, 7, 7, 7, 10, 7, 10, 8, 8, 10, 9, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 7, 7, 8, 9, 9, 7, 7, 10, 7, 10, 10, 8, 7, 10, 8, 7, 7, 7, 7, 8, 7, 8, 7, 9, 8, 7, 9, 10, 10, 8, 10, 9, 8, 7, 8, 10, 10, 7, 7, 8, 10, 10, 10, 7, 9, 9, 8, 8, 10, 10, 10, 10, 8, 10, 7, 7, 8, 7, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 9, 10, 7, 7, 8, 10, 9, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 8, 8, 7, 8, 7, 9, 7, 10, 7, 7, 7, 9, 9, 7, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 9, 9, 7, 10, 7, 9, 9, 9, 10, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 7, 9], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "1000\n",
      "{'input_ids': [0, 7, 7, 10, 8, 10, 9, 9, 10, 9, 9, 9, 9, 7, 7, 9, 8, 7, 7, 9, 8, 7, 7, 7, 10, 9, 8, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 7, 9, 7, 9, 8, 9, 7, 8, 10, 8, 7, 9, 7, 10, 9, 8, 10, 7, 10, 7, 7, 7, 7, 8, 10, 10, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 8, 10, 8, 7, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 8, 7, 10, 7, 8, 7, 7, 10, 8, 8, 7, 9, 9, 10, 10, 10, 10, 7, 7, 10, 8, 7, 10, 8, 7, 9, 7, 7, 7, 10, 8, 7, 8, 7, 9, 10, 8, 8, 10, 7, 10, 10, 9, 10, 8, 10, 10, 8, 10, 9, 8, 7, 8, 7, 9, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 10, 8, 7, 10, 9, 10, 10, 8, 7, 7, 10, 7, 10, 9, 7, 7, 10, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 8, 9, 8, 9, 7, 9, 7, 7, 9, 10, 7, 8, 7, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 9, 8, 8, 7, 9, 8, 10, 9, 9, 8, 7, 10, 9, 10, 8, 7, 7, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 7, 8, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 9, 7, 7, 9, 7, 8, 10, 8, 7, 9, 9, 7, 7, 9, 7, 8, 7, 7, 9, 10, 7, 10, 9, 7, 7, 9, 8, 7, 10, 7, 7, 10, 8, 10, 9, 10, 9, 7, 8, 7, 10, 10, 8, 8, 7, 10, 9, 8, 9, 9, 8, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 7, 8, 7, 10, 7, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 9, 8, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 8, 10, 9, 7, 9, 8, 10, 7, 7, 7, 10, 8, 10, 8, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 7, 10, 8, 10, 8, 10, 7, 9, 7, 7, 8, 10, 7, 10, 8, 8, 7, 10, 8, 7, 9, 10, 9, 7, 7, 7, 10, 9, 9, 7, 10, 10, 9, 8, 7, 7, 7, 10, 7, 8, 7, 7, 7, 9, 7, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 9, 10, 8, 7, 8, 10, 10, 7, 7, 9, 7, 7, 10, 7, 9, 7, 7, 10, 8, 7, 10, 9, 9, 7, 8, 9, 7, 9, 9, 8, 10, 9, 8, 8, 7, 8, 8, 10, 9, 8, 10, 9, 10, 10, 9, 9, 9, 9, 9, 8, 8, 7, 8, 10, 9, 8, 7, 9, 7, 7, 9, 7, 7, 7, 10, 10, 8, 8, 7, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 8, 10, 9, 9, 7, 9, 7, 9, 8, 7, 8, 8, 10, 8, 7, 8, 10, 10, 10, 8, 8, 10, 10, 7, 8, 7, 9, 8, 10, 8, 10, 7, 7, 9, 10, 10, 10, 8, 10, 9, 7, 8, 10, 8, 7, 9, 10, 9, 7, 8, 8, 10, 9, 7, 10, 10, 8, 7, 8, 10, 7, 8, 8, 7, 10, 7, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 8, 7, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 10, 9, 7, 8, 10, 9, 10, 10, 8, 10, 10, 8, 7, 8, 7, 8, 10, 7, 9, 9, 8, 8, 8, 7, 10, 9, 9, 7, 9, 7, 8, 7, 9, 9, 9, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 10, 7, 8, 8, 10, 10, 8, 10, 8, 8, 9, 8, 10, 7, 8, 10, 9, 7, 8, 7, 10, 8, 10, 7, 9, 9, 8, 7, 10, 10, 7, 8, 7, 8, 7, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 7, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 9, 7, 7, 9, 10, 10, 10, 9, 8, 10, 7, 8, 10, 10, 8, 10, 8, 7, 9, 7, 9, 7, 9, 7, 8, 10, 7, 7, 10, 9, 7, 9, 10, 7, 9, 10, 9, 7, 9, 8, 7, 7, 7, 10, 7, 10, 8, 8, 10, 9, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 7, 7, 8, 9, 9, 7, 7, 10, 7, 10, 10, 8, 7, 10, 8, 7, 7, 7, 7, 8, 7, 8, 7, 9, 8, 7, 9, 10, 10, 8, 10, 9, 8, 7, 8, 10, 10, 7, 7, 8, 10, 10, 10, 7, 9, 9, 8, 8, 10, 10, 10, 10, 8, 10, 7, 7, 8, 7, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 9, 10, 7, 7, 8, 10, 9, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 8, 8, 7, 8, 7, 9, 7, 10, 7, 7, 7, 9, 9, 7, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 9, 9, 7, 10, 7, 9, 9, 9, 10, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "1000\n",
      "{'input_ids': [0, 7, 7, 10, 8, 10, 9, 9, 10, 9, 9, 9, 9, 7, 7, 9, 8, 7, 7, 9, 8, 7, 7, 7, 10, 9, 8, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 7, 9, 7, 9, 8, 9, 7, 8, 10, 8, 7, 9, 7, 10, 9, 8, 10, 7, 10, 7, 7, 7, 7, 8, 10, 10, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 8, 10, 8, 7, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 8, 7, 10, 7, 8, 7, 7, 10, 8, 8, 7, 9, 9, 10, 10, 10, 10, 7, 7, 10, 8, 7, 10, 8, 7, 9, 7, 7, 7, 10, 8, 7, 8, 7, 9, 10, 8, 8, 10, 7, 10, 10, 9, 10, 8, 10, 10, 8, 10, 9, 8, 7, 8, 7, 9, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 10, 8, 7, 10, 9, 10, 10, 8, 7, 7, 10, 7, 10, 9, 7, 7, 10, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 8, 9, 8, 9, 7, 9, 7, 7, 9, 10, 7, 8, 7, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 9, 8, 8, 7, 9, 8, 10, 9, 9, 8, 7, 10, 9, 10, 8, 7, 7, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 7, 8, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 9, 7, 7, 9, 7, 8, 10, 8, 7, 9, 9, 7, 7, 9, 7, 8, 7, 7, 9, 10, 7, 10, 9, 7, 7, 9, 8, 7, 10, 7, 7, 10, 8, 10, 9, 10, 9, 7, 8, 7, 10, 10, 8, 8, 7, 10, 9, 8, 9, 9, 8, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 7, 8, 7, 10, 7, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 9, 8, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 8, 10, 9, 7, 9, 8, 10, 7, 7, 7, 10, 8, 10, 8, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 7, 10, 8, 10, 8, 10, 7, 9, 7, 7, 8, 10, 7, 10, 8, 8, 7, 10, 8, 7, 9, 10, 9, 7, 7, 7, 10, 9, 9, 7, 10, 10, 9, 8, 7, 7, 7, 10, 7, 8, 7, 7, 7, 9, 7, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 9, 10, 8, 7, 8, 10, 10, 7, 7, 9, 7, 7, 10, 7, 9, 7, 7, 10, 8, 7, 10, 9, 9, 7, 8, 9, 7, 9, 9, 8, 10, 9, 8, 8, 7, 8, 8, 10, 9, 8, 10, 9, 10, 10, 9, 9, 9, 9, 9, 8, 8, 7, 8, 10, 9, 8, 7, 9, 7, 7, 9, 7, 7, 7, 10, 10, 8, 8, 7, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 8, 10, 9, 9, 7, 9, 7, 9, 8, 7, 8, 8, 10, 8, 7, 8, 10, 10, 10, 8, 8, 10, 10, 7, 8, 7, 9, 8, 10, 8, 10, 7, 7, 9, 10, 10, 10, 8, 10, 9, 7, 8, 10, 8, 7, 9, 10, 9, 7, 8, 8, 10, 9, 7, 10, 10, 8, 7, 8, 10, 7, 8, 8, 7, 10, 7, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 8, 7, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 10, 9, 7, 8, 10, 9, 10, 10, 8, 10, 10, 8, 7, 8, 7, 8, 10, 7, 9, 9, 8, 8, 8, 7, 10, 9, 9, 7, 9, 7, 8, 7, 9, 9, 9, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 10, 7, 8, 8, 10, 10, 8, 10, 8, 8, 9, 8, 10, 7, 8, 10, 9, 7, 8, 7, 10, 8, 10, 7, 9, 9, 8, 7, 10, 10, 7, 8, 7, 8, 7, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 7, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 9, 7, 7, 9, 10, 10, 10, 9, 8, 10, 7, 8, 10, 10, 8, 10, 8, 7, 9, 7, 9, 7, 9, 7, 8, 10, 7, 7, 10, 9, 7, 9, 10, 7, 9, 10, 9, 7, 9, 8, 7, 7, 7, 10, 7, 10, 8, 8, 10, 9, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 7, 7, 8, 9, 9, 7, 7, 10, 7, 10, 10, 8, 7, 10, 8, 7, 7, 7, 7, 8, 7, 8, 7, 9, 8, 7, 9, 10, 10, 8, 10, 9, 8, 7, 8, 10, 10, 7, 7, 8, 10, 10, 10, 7, 9, 9, 8, 8, 10, 10, 10, 10, 8, 10, 7, 7, 8, 7, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 9, 10, 7, 7, 8, 10, 9, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 8, 8, 7, 8, 7, 9, 7, 10, 7, 7, 7, 9, 9, 7, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 9, 9, 7, 10, 7, 9, 9, 9, 10, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 7, 9, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "#let's try it with special tokens\n",
    "pure_seq = hg38.fasta(chr_name, start, end, max_length=hg38.max_length, return_augs=hg38.return_augs)\n",
    "seq = hg38.tokenizer(pure_seq,\n",
    "        add_special_tokens=False,  # this is what controls adding eos\n",
    "        padding=\"max_length\",\n",
    "        max_length=hg38.max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "print(seq, len(seq['input_ids']), sep='\\n')\n",
    "seq2 = hg38.tokenizer(pure_seq,\n",
    "        add_special_tokens=True,  # this is what controls adding eos\n",
    "        padding=\"max_length\",\n",
    "        max_length=hg38.max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "print(seq2, len(seq2['input_ids']), sep='\\n')\n",
    "seq3 = hg38.tokenizer(pure_seq,\n",
    "        add_special_tokens=True,  # this is what controls adding eos\n",
    "        padding=\"max_length\",\n",
    "        max_length=hg38.max_length,\n",
    "        truncation=False,\n",
    "    )\n",
    "print(seq3, len(seq3['input_ids']), sep='\\n')\n",
    "#so adds bos and eos token as we expect, but the only options in the fucntion are to remove the bos or to remove both, so basically just the pure sequence is fed in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n"
     ]
    }
   ],
   "source": [
    "print(len(seq['input_ids']))\n",
    "#still not 1002, didn't do anythning unless we turn trucnation off. If truncation is on it truncates the sequence to 1000 and then \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7,  7, 10,  8, 10,  9,  9, 10,  9,  9,  9,  9,  7,  7,  9,  8,  7,  7,\n",
      "         9,  8,  7,  7,  7, 10,  9,  8,  8,  8,  7, 10,  8,  7,  8,  7, 10,  9,\n",
      "         8,  7,  8, 10, 10, 10,  8,  8, 10,  8,  8,  7,  7,  8,  7,  9,  7,  9,\n",
      "         8,  9,  7,  8, 10,  8,  7,  9,  7, 10,  9,  8, 10,  7, 10,  7,  7,  7,\n",
      "         7,  8, 10, 10,  9,  8, 10,  7,  7,  8,  7,  8,  7,  9, 10,  8, 10,  8,\n",
      "         7,  9,  9,  9, 10,  8, 10,  9,  7, 10,  8,  7,  8,  7,  9, 10,  7,  7,\n",
      "         8,  7, 10,  7,  8,  7,  7, 10,  8,  8,  7,  9,  9, 10, 10, 10, 10,  7,\n",
      "         7, 10,  8,  7, 10,  8,  7,  9,  7,  7,  7, 10,  8,  7,  8,  7,  9, 10,\n",
      "         8,  8, 10,  7, 10, 10,  9, 10,  8, 10, 10,  8, 10,  9,  8,  7,  8,  7,\n",
      "         9,  7,  8,  8,  8,  7,  7,  7,  8,  7,  8,  7,  8, 10, 10,  9,  9,  7,\n",
      "         9,  9, 10,  8,  7, 10,  9, 10, 10,  8,  7,  7, 10,  7, 10,  9,  7,  7,\n",
      "        10,  7,  8,  8,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  8,  7,  8,  9,  8,  9,  7,  9,  7,  7,  9, 10,  7,\n",
      "         8,  7, 10,  8, 10,  9,  8,  7,  9,  7,  7,  7,  9,  8,  8,  7,  9,  8,\n",
      "        10,  9,  9,  8,  7, 10,  9, 10,  8,  7,  7,  8,  8,  7, 10, 10,  8,  7,\n",
      "         7,  7,  7,  7,  8, 10,  8,  7,  9,  9,  9, 10,  9, 10, 10,  8, 10,  9,\n",
      "         9,  7, 10,  7,  7,  7,  9,  7,  7,  9,  7,  8, 10,  8,  7,  9,  9,  7,\n",
      "         7,  9,  7,  8,  7,  7,  9, 10,  7, 10,  9,  7,  7,  9,  8,  7, 10,  7,\n",
      "         7, 10,  8, 10,  9, 10,  9,  7,  8,  7, 10, 10,  8,  8,  7, 10,  9,  8,\n",
      "         9,  9,  8,  7,  9,  7,  8,  7, 10, 10,  7,  9,  7,  8,  7,  8,  7, 10,\n",
      "         7,  8,  7,  7,  9,  7,  9,  7,  9, 10, 10,  9, 10, 10,  9,  9,  7,  7,\n",
      "         7,  9,  8,  9,  9,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  7,  8,  7,  8, 10,  9,  7,  9,  8, 10,  7,\n",
      "         7,  7, 10,  8, 10,  8,  7,  7, 10,  7, 10, 10, 10,  8,  7,  9,  7, 10,\n",
      "         8, 10,  8, 10,  7,  9,  7,  7,  8, 10,  7, 10,  8,  8,  7, 10,  8,  7,\n",
      "         9, 10,  9,  7,  7,  7, 10,  9,  9,  7, 10, 10,  9,  8,  7,  7,  7, 10,\n",
      "         7,  8,  7,  7,  7,  9,  7,  9, 10,  7,  7, 10,  7,  8,  8,  7, 10,  9,\n",
      "        10,  8,  7,  8, 10, 10,  7,  7,  9,  7,  7, 10,  7,  9,  7,  7, 10,  8,\n",
      "         7, 10,  9,  9,  7,  8,  9,  7,  9,  9,  8, 10,  9,  8,  8,  7,  8,  8,\n",
      "        10,  9,  8, 10,  9, 10, 10,  9,  9,  9,  9,  9,  8,  8,  7,  8, 10,  9,\n",
      "         8,  7,  9,  7,  7,  9,  7,  7,  7, 10, 10,  8,  8,  7,  9,  7,  7,  8,\n",
      "         7,  8, 10,  9,  9,  7,  8, 10,  9,  9,  7,  9,  7,  9,  8,  7,  8,  8,\n",
      "        10,  8,  7,  8, 10, 10, 10,  8,  8, 10, 10,  7,  8,  7,  9,  8, 10,  8,\n",
      "        10,  7,  7,  9, 10, 10, 10,  8, 10,  9,  7,  8, 10,  8,  7,  9, 10,  9,\n",
      "         7,  8,  8, 10,  9,  7, 10, 10,  8,  7,  8, 10,  7,  8,  8,  7, 10,  7,\n",
      "        10,  7,  8,  7,  8,  7,  7,  7,  9,  7,  8,  8,  8,  7,  8, 10, 10,  7,\n",
      "         8,  7,  8,  7,  7,  7, 10,  9,  7,  8, 10,  9, 10, 10,  8, 10, 10,  8,\n",
      "         7,  8,  7,  8, 10,  7,  9,  9,  8,  8,  8,  7, 10,  9,  9,  7,  9,  7,\n",
      "         8,  7,  9,  9,  9,  7, 10,  7,  7,  7,  7, 10, 10,  8, 10,  9,  7,  7,\n",
      "        10, 10, 10,  9,  8, 10,  8,  7,  9,  7, 10,  7,  8,  8, 10, 10,  8, 10,\n",
      "         8,  8,  9,  8, 10,  7,  8, 10,  9,  7,  8,  7, 10,  8, 10,  7,  9,  9,\n",
      "         8,  7, 10, 10,  7,  8,  7,  8,  7,  7, 10, 10,  8,  7, 10,  8, 10,  8,\n",
      "        10, 10,  8,  7, 10,  7, 10, 10, 10,  7,  7,  8,  8, 10, 10, 10,  9,  7,\n",
      "         7,  9, 10, 10, 10,  9,  8, 10,  7,  8, 10, 10,  8, 10,  8,  7,  9,  7,\n",
      "         9,  7,  9,  7,  8, 10,  7,  7, 10,  9,  7,  9, 10,  7,  9, 10,  9,  7,\n",
      "         9,  8,  7,  7,  7, 10,  7, 10,  8,  8, 10,  9,  7,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8, 10,\n",
      "         8, 10,  8,  7,  7,  7,  7,  8,  7,  7,  8,  9,  9,  7,  7, 10,  7, 10,\n",
      "        10,  8,  7, 10,  8,  7,  7,  7,  7,  8,  7,  8,  7,  9,  8,  7,  9, 10,\n",
      "        10,  8, 10,  9,  8,  7,  8, 10, 10,  7,  7,  8, 10, 10, 10,  7,  9,  9,\n",
      "         8,  8, 10, 10, 10, 10,  8, 10,  7,  7,  8,  7,  8,  8, 10, 10,  9, 10,\n",
      "        10, 10,  8, 10, 10,  9,  9,  8,  7,  9, 10,  7,  7,  8, 10,  9, 10,  9,\n",
      "         9,  8,  8,  7,  9,  7,  7, 10,  7,  9,  8, 10,  8, 10, 10, 10,  8,  8,\n",
      "         7,  8,  7,  9,  7, 10,  7,  7,  7,  9,  9,  7,  8,  8, 10, 10, 10, 10,\n",
      "         9,  7,  7,  7,  9,  9,  7, 10,  7,  9,  9,  9, 10,  8, 10,  8, 10,  7,\n",
      "         9,  7, 10,  7,  9,  7,  7,  7,  7,  9])\n"
     ]
    }
   ],
   "source": [
    "seq = seq['input_ids']\n",
    "seq = torch.LongTensor(seq)  # hack, remove the initial cls tokens for now\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([999])\n",
      "torch.Size([999])\n"
     ]
    }
   ],
   "source": [
    "data = seq[:-1].clone()  # remove eos\n",
    "target = seq[1:].clone()  # offset by 1, includes eos\n",
    "print(data.shape, target.shape, sep='\\n')\n",
    "#each is 999??\n",
    "#so i think you iterate, you use target up to the set index, then are predicting that index of target\n",
    "#but it seems their data loader simply returns the full array, and there's something else that handles the next token process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing own dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need something that returns it with the eos and bos, can reuse much of their code, but need to have my own way to load in the data and we can use my preloaded sequences!\n",
    "#it needs to have a split and options for bos and eos tokens, can copy lots of the skeleton, but split it just which file we load, so if anything it's better!\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class cCRE():\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        max_length,\n",
    "        pad_max_length=None,\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        add_eos=False,\n",
    "        # return_seq_indices=False,\n",
    "        # shift_augs=None,\n",
    "        # rc_aug=False,\n",
    "        return_augs=False,\n",
    "        replace_N_token=False,  # replace N token with pad token\n",
    "        pad_interval = False,  # options for different padding\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.pad_max_length = pad_max_length if pad_max_length is not None else max_length\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.replace_N_token = replace_N_token  \n",
    "        self.pad_interval = pad_interval         \n",
    "\n",
    "        #we load in based on the split\n",
    "        data_path = f'/data/leslie/sarthak/data/{split}.csv'\n",
    "        #load in csv\n",
    "        df_raw = pd.read_csv(data_path)\n",
    "        #now only take the column titled sequence\n",
    "        self.df = df_raw[['sequence']]\n",
    "        #turn to numpy array\n",
    "        self.array = self.df.to_numpy()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def replace_value(self, x, old_value, new_value):\n",
    "        return torch.where(x == old_value, new_value, x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a sequence of specified len\"\"\"\n",
    "        # sample a random row from df\n",
    "        seq = self.array[idx][0]\n",
    "        # row = (chr, start, end, split)\n",
    "        # chr_name, start, end = (row[0], row[1], row[2])\n",
    "\n",
    "        # seq = self.fasta(chr_name, start, end, max_length=self.max_length, return_augs=self.return_augs)\n",
    "\n",
    "        if self.tokenizer_name == 'char': #will stick with this for sure\n",
    "            seq = self.tokenizer(seq,\n",
    "                add_special_tokens=True if self.add_eos else False,  # this is what controls adding eos\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        elif self.tokenizer_name == 'bpe':\n",
    "            seq = self.tokenizer(seq, \n",
    "                # add_special_tokens=False, \n",
    "                padding=\"max_length\",\n",
    "                max_length=self.pad_max_length,\n",
    "                truncation=True,\n",
    "            ) \n",
    "            # get input_ids\n",
    "            if self.add_eos:\n",
    "                seq = seq[\"input_ids\"][1:]  # remove the bos, keep the eos token\n",
    "            else:\n",
    "                seq = seq[\"input_ids\"][1:-1]  # remove both special tokens\n",
    "        # print(seq)\n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)  # hack, remove the initial cls tokens for now\n",
    "\n",
    "        if self.replace_N_token:\n",
    "            # replace N token with a pad token, so we can ignore it in the loss\n",
    "            seq = self.replace_value(seq, self.tokenizer._vocab_str_to_int['N'], self.tokenizer.pad_token_id)\n",
    "\n",
    "        data = seq[:-1].clone()  # remove eos\n",
    "        target = seq[1:].clone()  # offset by 1, includes eos\n",
    "\n",
    "        return data, target\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so when they call their function they use this appraoch, I can just call it exactly the same way\n",
    "tokenizer = CharacterTokenizer(characters=['A', 'C', 'G', 'T','N'], model_max_length=1000+2) #the plus 2 for the bos and eos tokens\n",
    "ccre = cCRE(split='train', max_length=1000, pad_max_length=None, tokenizer=tokenizer, tokenizer_name='char', add_eos=False, return_seq_indices=False, shift_augs=None, rc_aug=False, return_augs=False, replace_N_token=False, pad_interval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GAATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAA']\n",
      "GAATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAA\n"
     ]
    }
   ],
   "source": [
    "print(ccre.array[0])\n",
    "print(ccre.array[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9, 7, 7, 1], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "character\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('GAA'))\n",
    "print(ccre.tokenizer_name)\n",
    "#ah have to change to char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9,  7,  7, 10,  8])\ttensor([7, 9, 7, 7, 7])\n",
      "tensor([ 7,  7, 10,  8, 10])\ttensor([9, 7, 7, 7, 7])\n",
      "torch.Size([999])\n",
      "torch.Size([999])\n",
      "GAATCT\tAGAAAA\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(ccre[0][0][0:5], ccre[0][0][-5:], sep='\\t')\n",
    "print(ccre[0][1][0:5], ccre[0][1][-5:], sep='\\t')\n",
    "print(ccre[0][0].shape)\n",
    "print(ccre[0][1].shape)\n",
    "#target is second, ahve one extra sequence, and the information is top, so start with one extra. here there's no eos or bos token, have to specify that...\n",
    "print(ccre.array[0][0][0:6], ccre.array[0][0][-6:], sep='\\t')\n",
    "#the tokenizer and the way the data is formatted works exactly as expected, I think a big thing again is the eos token if it's needed or not.\n",
    "print(ccre.add_eos) #this is why we have no eos token, the bos is removed regardless it seems...\n",
    "\n",
    "#also changed it os that the useless arguments are no longer used\n",
    "#we see that the torch size is 999, so that shoudl fit in the model which I think is actually 1024, in which case we may have to redo the way we do the padding, but we can do that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standalone with hugging face instead\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchvision.ops import StochasticDepth\n",
    "from collections import namedtuple\n",
    "\n",
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # we only need a head if doing classification, otherwise we'll use the\n",
    "        # hidden states as embeddings\n",
    "        if self.use_head:\n",
    "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "        # if self.use_head:\n",
    "        #     self.tie_weights()\n",
    "\n",
    "    # def tie_weights(self):\n",
    "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json'))) #defaults to the config in the same folder\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n",
    "    \n",
    "#@title Hyena layer\n",
    "\n",
    "\n",
    "def fftconv(u, k, D):\n",
    "    \"\"\"\n",
    "    We apply a convolution through the fourier domain (from the Convolution Theorem)\n",
    "\n",
    "    \"\"\"\n",
    "    seqlen = u.shape[-1]\n",
    "    fft_size = 2 * seqlen\n",
    "\n",
    "    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n",
    "    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n",
    "\n",
    "    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n",
    "    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n",
    "\n",
    "    out = y + u * D.unsqueeze(-1)\n",
    "    return out.to(dtype=u.dtype)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mul_sum(q, y):\n",
    "    return (q * y).sum(dim=1)\n",
    "\n",
    "class OptimModule(nn.Module):\n",
    "    \"\"\" Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters \"\"\"\n",
    "\n",
    "    def register(self, name, tensor, lr=None, wd=0.0):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            if wd is not None: optim[\"weight_decay\"] = wd\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"The Sin activation function for the Hyena Filter function.\"\"\"\n",
    "    def __init__(self, dim, w=10, train_freq=True):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.freq * x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(OptimModule):\n",
    "    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):\n",
    "        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        # The time embedding fed to the filteres is normalized so that t_f = 1\n",
    "        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n",
    "\n",
    "        if emb_dim > 1:\n",
    "            bands = (emb_dim - 1) // 2\n",
    "        # To compute the right embeddings we use the \"proper\" linspace\n",
    "        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n",
    "        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1\n",
    "\n",
    "        f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n",
    "        z = torch.exp(-1j * f * w)\n",
    "        z = torch.cat([t, z.real, z.imag], dim=-1)\n",
    "        self.register(\"z\", z, lr=lr_pos_emb)\n",
    "        self.register(\"t\", t, lr=0.0)\n",
    "\n",
    "    def forward(self, L):\n",
    "        return self.z[:, :L], self.t[:, :L]\n",
    "\n",
    "\n",
    "class ExponentialModulation(OptimModule):\n",
    "    \"\"\"The window function applied to the output of the (MLP) filter function.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        fast_decay_pct=0.3,\n",
    "        slow_decay_pct=1.5,\n",
    "        target=1e-2,\n",
    "        modulation_lr=0.0,\n",
    "        modulate: bool=True,\n",
    "        shift: float = 0.05,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.modulate = modulate\n",
    "        self.shift = shift\n",
    "        max_decay = math.log(target) / fast_decay_pct\n",
    "        min_decay = math.log(target) / slow_decay_pct\n",
    "        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n",
    "        self.register(\"deltas\", deltas, lr=modulation_lr)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.modulate:\n",
    "            decay = torch.exp(-t * self.deltas.abs())\n",
    "            x = x * (decay + self.shift)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HyenaFilter(OptimModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            emb_dim=3, # dim of input to MLP, augments with positional encoding\n",
    "            order=16, # width of the implicit MLP\n",
    "            fused_fft_conv=False,\n",
    "            seq_len=1024,\n",
    "            lr=1e-3,\n",
    "            lr_pos_emb=1e-5,\n",
    "            dropout=0.0,\n",
    "            w=1, # frequency of periodic activations\n",
    "            wd=0, # weight decay of kernel parameters\n",
    "            bias=True,\n",
    "            num_inner_mlps=2,\n",
    "            normalized=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Implicit long filter with modulation.\n",
    "\n",
    "        Args:\n",
    "            d_model: number of channels in the input\n",
    "            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n",
    "            order: width of the FFN\n",
    "            num_inner_mlps: number of inner linear layers inside filter MLP\n",
    "\n",
    "        Note:\n",
    "            filter_dropout is not implemented\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.use_bias = bias\n",
    "        self.fused_fft_conv = fused_fft_conv\n",
    "        self.bias = nn.Parameter(torch.randn(self.d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        act = Sin(dim=order, w=w)\n",
    "        self.emb_dim = emb_dim\n",
    "        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n",
    "\n",
    "        self.implicit_filter = nn.Sequential(\n",
    "            nn.Linear(emb_dim, order),\n",
    "            act,\n",
    "        )\n",
    "        for i in range(num_inner_mlps):\n",
    "            self.implicit_filter.append(nn.Linear(order, order))\n",
    "            self.implicit_filter.append(act)\n",
    "\n",
    "        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n",
    "\n",
    "        self.modulation = ExponentialModulation(d_model, **kwargs)\n",
    "\n",
    "        self.normalized = normalized\n",
    "        for c in self.implicit_filter.children():\n",
    "            for name, v in c.state_dict().items():\n",
    "                optim = {\"weight_decay\": wd, \"lr\": lr}\n",
    "                setattr(getattr(c, name), \"_optim\", optim)\n",
    "\n",
    "    def filter(self, L, *args, **kwargs):\n",
    "        z, t = self.pos_emb(L)\n",
    "        h = self.implicit_filter(z)\n",
    "        h = self.modulation(t, h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n",
    "        if k is None: k = self.filter(L)\n",
    "\n",
    "        # Ensure compatibility with filters that return a tuple\n",
    "        k = k[0] if type(k) is tuple else k\n",
    "\n",
    "        y = fftconv(x, k, bias)\n",
    "        return y\n",
    "\n",
    "\n",
    "class HyenaOperator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            l_max,\n",
    "            order=2,\n",
    "            filter_order=64,\n",
    "            dropout=0.0,\n",
    "            filter_dropout=0.0,\n",
    "            **filter_args,\n",
    "        ):\n",
    "        r\"\"\"\n",
    "        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the input and output embeddings (width of the layer)\n",
    "            l_max: (int): Maximum input sequence length. Defaults to None\n",
    "            order: (int): Depth of the Hyena recurrence. Defaults to 2\n",
    "            dropout: (float): Dropout probability. Defaults to 0.0\n",
    "            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.l_max = l_max\n",
    "        self.order = order\n",
    "        inner_width = d_model * (order + 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.in_proj = nn.Linear(d_model, inner_width)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.short_filter = nn.Conv1d(\n",
    "            inner_width,\n",
    "            inner_width,\n",
    "            3,\n",
    "            padding=2,\n",
    "            groups=inner_width\n",
    "        )\n",
    "        self.filter_fn = HyenaFilter(\n",
    "            d_model * (order - 1),\n",
    "            order=filter_order,\n",
    "            seq_len=l_max,\n",
    "            channels=1,\n",
    "            dropout=filter_dropout,\n",
    "            **filter_args\n",
    "        )\n",
    "\n",
    "    def forward(self, u, *args, **kwargs):\n",
    "        l = u.size(-2)\n",
    "        l_filter = min(l, self.l_max)\n",
    "        u = self.in_proj(u)\n",
    "        u = rearrange(u, 'b l d -> b d l')\n",
    "\n",
    "        uc = self.short_filter(u)[...,:l_filter]\n",
    "        *x, v = uc.split(self.d_model, dim=1)\n",
    "\n",
    "        k = self.filter_fn.filter(l_filter)[0]\n",
    "        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)\n",
    "        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)\n",
    "\n",
    "        for o, x_i in enumerate(reversed(x[1:])):\n",
    "            v = self.dropout(v * x_i)\n",
    "            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])\n",
    "\n",
    "        y = rearrange(v * x[0], 'b d l -> b l d')\n",
    "\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "#@title Self-Attention (alternative)\n",
    "\n",
    "\"\"\"\n",
    "If you'd like to try the HyenaDNA model using attention instead, you can. ie,\n",
    "use a regular decoder only Transformer.\n",
    "\n",
    "Borrowed from the FlashAttention library by Tri Dao.\n",
    "\"\"\"\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Implement the scaled dot product attention with softmax.\n",
    "    Arguments\n",
    "    ---------\n",
    "        softmax_scale: The temperature to use for the softmax attention.\n",
    "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
    "                      runtime)\n",
    "        attention_dropout: The dropout rate to apply to the attention\n",
    "                           (default: 0.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.dropout_p = attention_dropout\n",
    "\n",
    "    def forward(self, qkv, causal=None, key_padding_mask=None):\n",
    "        \"\"\"Implements the multihead softmax attention.\n",
    "        Arguments\n",
    "        ---------\n",
    "            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)\n",
    "            causal: if passed, will override self.causal\n",
    "            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,\n",
    "                False means to mask out. (B, S)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        causal = self.causal if causal is None else causal\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype,\n",
    "                                      device=scores.device)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, 'b s -> b 1 1 s')\n",
    "        if causal:\n",
    "            # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "            # So we have to construct the mask in float\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
    "        attention_drop = F.dropout(attention, self.dropout_p if self.training else 0.0)\n",
    "        output = torch.einsum('bhts,bshd->bthd', attention_drop, v)\n",
    "        return output\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi-head self-attention and cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, bias=True, dropout=0.0,\n",
    "                 softmax_scale=None, causal=False, layer_idx=None, dwconv=False,return_residual=False,device=None, dtype=None) -> None:\n",
    "        \"\"\"\n",
    "            return_residual: whether to return the input x along with the output. This is for\n",
    "                performance reason: for post-norm architecture, returning the input allows us\n",
    "                to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.causal = causal\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dwconv = dwconv\n",
    "        self.return_residual = return_residual\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embed_dim // num_heads\n",
    "\n",
    "        linear_cls = nn.Linear\n",
    "        linear_resid_cls = LinearResidual\n",
    "        inner_attn_cls =  SelfAttention\n",
    "\n",
    "        if not self.return_residual:\n",
    "            self.Wqkv = linear_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        else:\n",
    "            self.Wqkv = linear_resid_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        if self.dwconv:\n",
    "            self.dwconv_qkv = nn.Conv1d(3 * embed_dim, 3 * embed_dim, kernel_size=3, padding=2,\n",
    "                                        groups=3 * embed_dim)\n",
    "\n",
    "        self.inner_attn = inner_attn_cls(causal=causal, softmax_scale=softmax_scale,\n",
    "                                         attention_dropout=dropout)\n",
    "\n",
    "        # output projection always have the bias (for now)\n",
    "        self.out_proj = linear_cls(embed_dim, embed_dim, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
    "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
    "                is the is the sum of the sequence lengths in the batch.\n",
    "            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
    "                of the sequences in the batch, used to index into x. Only applicable when using\n",
    "                FlashAttention.\n",
    "            max_seqlen: int. Maximum sequence length in the batch.\n",
    "            key_padding_mask: boolean mask, True means to keep, False means to mask out.\n",
    "                (batch, seqlen). Only applicable when not using FlashAttention.\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
    "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs = ({'key_padding_mask': key_padding_mask, **kwargs})\n",
    "\n",
    "        if not self.return_residual:\n",
    "            qkv = self.Wqkv(x)\n",
    "        else:\n",
    "            qkv, x = self.Wqkv(x)\n",
    "        if self.dwconv:\n",
    "            qkv = rearrange(self.dwconv_qkv(rearrange(qkv, 'b s d -> b d s'))[..., :-2],\n",
    "                            'b d s -> b s d').contiguous()\n",
    "        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, d=self.head_dim)\n",
    "\n",
    "        context = self.inner_attn(qkv, **kwargs)\n",
    "\n",
    "        out = self.out_proj(rearrange(context, '... h d -> ... (h d)'))\n",
    "        return out if not self.return_residual else (out, x)\n",
    "\n",
    "#@title MLP layer\n",
    "\n",
    "\"\"\"\n",
    "The MLP layer after the mixer layer (HyenaOperator).\n",
    "\"\"\"\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,\n",
    "                 return_residual=False, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.return_residual = return_residual\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.fc2(y)\n",
    "        return y if not self.return_residual else (y, x)\n",
    "\n",
    "#@title Block layer (Hyena + MLP layers)\n",
    "\n",
    "\"\"\"\n",
    "A block consists of a Mixer layer (Hyena or attention), and a MLP layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LinearResidual(nn.Linear):\n",
    "    \"\"\"Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(input), input\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,\n",
    "                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,\n",
    "                 drop_path1=0., drop_path2=0.,\n",
    "                 return_residual=False,\n",
    "                 residual_in_fp32=False):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py\n",
    "        For prenorm=True, this Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both\n",
    "        the hidden_states (output of the MLP) and the residual.\n",
    "        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        For prenorm=False, this Block has the same structure as a regular postnorm Transformer\n",
    "        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.\n",
    "        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.\n",
    "        This is for performance reason: for post-norm architecture, returning the input allows us\n",
    "        to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prenorm = prenorm\n",
    "        self.return_residual = return_residual\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        if self.residual_in_fp32:\n",
    "            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'\n",
    "        if mixer_cls is None:\n",
    "            mixer_cls = partial(MHA, num_heads=dim // 64)\n",
    "        if mlp_cls is None:\n",
    "            mlp_cls = partial(Mlp, hidden_features=4 * dim)\n",
    "        self.mixer = mixer_cls()\n",
    "        self.dropout1 = dropout_cls(resid_dropout1)\n",
    "        self.drop_path1 = StochasticDepth(drop_path1, mode='row')\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.mlp = mlp_cls(dim)\n",
    "        if not isinstance(self.mlp, nn.Identity):\n",
    "            self.dropout2 = dropout_cls(resid_dropout2)\n",
    "            self.drop_path2 = StochasticDepth(drop_path2, mode='row')\n",
    "            self.norm2 = norm_cls(dim)\n",
    "\n",
    "    def forward(self, hidden_states, residual = None,\n",
    "                mixer_subset=None, mixer_kwargs=None):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "        \"\"\"\n",
    "        if self.prenorm:\n",
    "            dropped = self.drop_path1(self.dropout1(hidden_states))\n",
    "            residual = (dropped + residual) if residual is not None else dropped\n",
    "            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "            if mixer_kwargs is None:\n",
    "                mixer_kwargs = {}\n",
    "            if mixer_subset is not None:\n",
    "                mixer_kwargs['mixer_subset'] = mixer_subset\n",
    "            hidden_states = self.mixer(hidden_states, **mixer_kwargs)\n",
    "            if mixer_subset is not None:\n",
    "                residual = residual[:, mixer_subset]\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                dropped = self.drop_path2(self.dropout2(hidden_states))\n",
    "                residual = (dropped + residual) if residual is not None else dropped\n",
    "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
    "                if self.residual_in_fp32:\n",
    "                    residual = residual.to(torch.float32)\n",
    "\n",
    "                hidden_states = self.mlp(hidden_states)\n",
    "            return hidden_states, residual\n",
    "        else:\n",
    "            assert residual is None\n",
    "            mixer_out = self.mixer(\n",
    "                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})\n",
    "            )\n",
    "            if self.return_residual:  # mixer out is actually a pair here\n",
    "                mixer_out, hidden_states = mixer_out\n",
    "\n",
    "            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))\n",
    "                                        + hidden_states).to(dtype=self.norm1.weight.dtype))\n",
    "\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                mlp_out = self.mlp(hidden_states)\n",
    "                if self.return_residual:  # mlp out is actually a pair here\n",
    "                    mlp_out, hidden_states = mlp_out\n",
    "\n",
    "                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))\n",
    "                                            + hidden_states).to(dtype=self.norm2.weight.dtype))\n",
    "\n",
    "            return hidden_states\n",
    "\n",
    "def create_mixer_cls(layer=None,\n",
    "                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,\n",
    "                     device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    if attn_layer_idx is not None and layer_idx in attn_layer_idx:\n",
    "        causal = True if attn_cfg is None else attn_cfg.pop('causal', True)\n",
    "\n",
    "        mha_cls = MHA\n",
    "\n",
    "        mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,\n",
    "                            **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)\n",
    "    else:\n",
    "        # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)\n",
    "\n",
    "        mixer_cls = partial(HyenaOperator, **layer)\n",
    "\n",
    "    return mixer_cls\n",
    "\n",
    "def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    inner_dim = d_inner if d_inner is not None else 4 * d_model\n",
    "\n",
    "    mlp_cls = partial(Mlp, hidden_features=inner_dim,\n",
    "                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)\n",
    "\n",
    "    return mlp_cls\n",
    "\n",
    "\n",
    "def create_block(d_model, d_inner=None,\n",
    "                 layer=None, attn_layer_idx=None,\n",
    "                 attn_cfg=None, layer_norm_epsilon=1e-5,\n",
    "                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,\n",
    "                 layer_idx=None,\n",
    "                 device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    mixer_cls = create_mixer_cls(layer=layer,\n",
    "                                 attn_layer_idx=attn_layer_idx,\n",
    "                                 attn_cfg=attn_cfg, layer_idx=layer_idx,\n",
    "                                 **factory_kwargs)\n",
    "    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,\n",
    "                             **factory_kwargs)\n",
    "    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,\n",
    "                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)\n",
    "    block.layer_idx = layer_idx\n",
    "    return block\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,\n",
    "                  glu_act=False):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "            # If using GLU activation for now, we scale the std by 2\n",
    "            elif name in [\"output_linear.0.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                if not glu_act:\n",
    "                    nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    out_features = p.shape[0]\n",
    "                    # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5\n",
    "                    # on average.\n",
    "                    nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)\n",
    "\n",
    "\n",
    "\n",
    "#@title Backbone model (stack of blocks)\n",
    "\n",
    "\"\"\"\n",
    "A backbone model consists of a stack of blocks. If you use attention, then\n",
    "positional embeddings are included. When using Hyena, then the pos emb\n",
    "revert to doing nothing.\n",
    "\"\"\"\n",
    "\n",
    "class GPT2Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,\n",
    "                 word_embed_proj_dim=None, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "            If max_position_embeddings <= 0, there's no position embeddings\n",
    "            If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n",
    "                the project up to embed_dim\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if word_embed_proj_dim is None:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,\n",
    "                                                **factory_kwargs)\n",
    "            self.project_in = None\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,\n",
    "                                                padding_idx=padding_idx, **factory_kwargs)\n",
    "            self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,\n",
    "                                        **factory_kwargs)\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        if self.max_position_embeddings > 0:\n",
    "            self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,\n",
    "                                                    **factory_kwargs)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: (batch, seqlen)\n",
    "            position_ids: (batch, seqlen)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = input_ids.shape\n",
    "        embeddings = self.word_embeddings(input_ids)\n",
    "        if self.project_in is not None:\n",
    "            embeddings = self.project_in(embeddings)\n",
    "        if self.max_position_embeddings > 0:\n",
    "            if position_ids is None:\n",
    "                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings = embeddings + position_embeddings\n",
    "        return embeddings\n",
    "\n",
    "class LMBackbone(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 process_group=None, layer=None,\n",
    "                 attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.process_group = process_group\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        # note max_position_embeddings is 0 for Hyena, and therefore isn't used\n",
    "        self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,\n",
    "                                             **factory_kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList([create_block(\n",
    "            d_model, d_inner=d_inner,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx,\n",
    "            attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,\n",
    "            resid_dropout1=embed_dropout if i == 0 else resid_dropout,\n",
    "            resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,\n",
    "            **factory_kwargs,\n",
    "        ) for i in range(n_layer)])\n",
    "\n",
    "        self.drop_f = nn.Dropout(resid_dropout)\n",
    "        self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        hidden_states = self.embeddings(input_ids, position_ids=position_ids,)\n",
    "        residual = None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(hidden_states, residual)\n",
    "\n",
    "        dropped = self.drop_f(hidden_states)\n",
    "        residual = (dropped + residual) if residual is not None else dropped\n",
    "        hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "#@title Decoder head layer\n",
    "\n",
    "\"\"\"\n",
    "A simple decoder head (using MLP) to predict a sequence level classification.\n",
    "You have the option to average across all the tokens in a sequence or using the\n",
    "\"last\" token to classify.  At least, those 2 worked best for us, but we provide\n",
    "other \"modes\" as well.\n",
    "\n",
    "We only need this for classification.  Otherwise we'll use the hidden\n",
    "states of the backbone as embeddings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SequenceDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, d_output=None, l_output=None, use_lengths=False, mode=\"last\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)\n",
    "\n",
    "        if l_output is None:\n",
    "            self.l_output = None\n",
    "            self.squeeze = False\n",
    "        elif l_output == 0:\n",
    "            # Equivalent to getting an output of length 1 and then squeezing\n",
    "            self.l_output = 1\n",
    "            self.squeeze = True\n",
    "        else:\n",
    "            assert l_output > 0\n",
    "            self.l_output = l_output\n",
    "            self.squeeze = False\n",
    "\n",
    "        self.use_lengths = use_lengths\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'ragged':\n",
    "            assert not use_lengths\n",
    "\n",
    "    def forward(self, x, state=None, lengths=None, l_output=None):\n",
    "        \"\"\"\n",
    "        x: (n_batch, l_seq, d_model)\n",
    "        Returns: (n_batch, l_output, d_output)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.l_output is None:\n",
    "            if l_output is not None:\n",
    "                assert isinstance(l_output, int)  # Override by pass in\n",
    "            else:\n",
    "                # Grab entire output\n",
    "                l_output = x.size(-2)\n",
    "            squeeze = False\n",
    "        else:\n",
    "            l_output = self.l_output\n",
    "            squeeze = self.squeeze\n",
    "\n",
    "        if self.mode == \"last\":\n",
    "            restrict = lambda x: x[..., -l_output:, :]\n",
    "        elif self.mode == \"first\":\n",
    "            restrict = lambda x: x[..., :l_output, :]\n",
    "        elif self.mode == \"pool\":\n",
    "            restrict = lambda x: (\n",
    "                torch.cumsum(x, dim=-2)\n",
    "                / torch.arange(\n",
    "                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                ).unsqueeze(-1)\n",
    "            )[..., -l_output:, :]\n",
    "\n",
    "            def restrict(x):\n",
    "                L = x.size(-2)\n",
    "                s = x.sum(dim=-2, keepdim=True)\n",
    "                if l_output > 1:\n",
    "                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n",
    "                    c = F.pad(c, (0, 0, 1, 0))\n",
    "                    s = s - c  # (B, l_output, D)\n",
    "                    s = s.flip(-2)\n",
    "                denom = torch.arange(\n",
    "                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n",
    "                )\n",
    "                s = s / denom\n",
    "                return s\n",
    "\n",
    "        elif self.mode == \"sum\":\n",
    "            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n",
    "            # TODO use same restrict function as pool case\n",
    "        elif self.mode == 'ragged':\n",
    "            assert lengths is not None, \"lengths must be provided for ragged mode\"\n",
    "            # remove any additional padding (beyond max length of any sequence in the batch)\n",
    "            restrict = lambda x: x[..., : max(lengths), :]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Mode must be ['last' | 'first' | 'pool' | 'sum']\"\n",
    "            )\n",
    "\n",
    "        # Restrict to actual length of sequence\n",
    "        if self.use_lengths:\n",
    "            assert lengths is not None\n",
    "            x = torch.stack(\n",
    "                [\n",
    "                    restrict(out[..., :length, :])\n",
    "                    for out, length in zip(torch.unbind(x, dim=0), lengths)\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            x = restrict(x)\n",
    "\n",
    "        if squeeze:\n",
    "            assert x.size(-2) == 1\n",
    "            x = x.squeeze(-2)\n",
    "\n",
    "        x = self.output_transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def step(self, x, state=None):\n",
    "        # Ignore all length logic\n",
    "        return self.output_transform(x)\n",
    "\n",
    "#@title Model (backbone + head)\n",
    "\n",
    "\"\"\"\n",
    "Putting it all together, the model consists of a backbone model\n",
    "and a decoder head (you can turn off head for embeddings only too).\n",
    "\n",
    "Here we use a simple head to do multi-classification, but\n",
    "can also swap the head to do next token prediction too.  We defer to the main\n",
    "HyenaDNA for that code, since pretraining with next token prediction isn't quite\n",
    "feasible on colab.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # we only need a head if doing classification, otherwise we'll use the\n",
    "        # hidden states as embeddings\n",
    "        if self.use_head:\n",
    "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "        # if self.use_head:\n",
    "        #     self.tie_weights()\n",
    "\n",
    "    # def tie_weights(self):\n",
    "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "    \n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n",
    "\n",
    "\n",
    "# Data pipeline\n",
    "\n",
    "\n",
    "\n",
    "#@title Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "    \n",
    "#@title GenomicBenchmark dataset\n",
    "\n",
    "\"\"\"\n",
    "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
    "There are 8 datasets to choose from.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "# augmentation\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "\n",
    "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
    "    Returns a generator that retrieves the sequence.\n",
    "\n",
    "    Genomic Benchmarks Dataset, from:\n",
    "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        max_length,\n",
    "        dataset_name='human_enhancers_cohn',\n",
    "        d_output=2, # default binary classification\n",
    "        dest_path=\"/content\", # default for colab\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        use_padding=None,\n",
    "        add_eos=False,\n",
    "        rc_aug=False,\n",
    "        return_augs=False,\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.d_output = d_output  # needed for decoder to grab\n",
    "        self.rc_aug = rc_aug\n",
    "\n",
    "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
    "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
    "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
    "        else:\n",
    "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
    "\n",
    "        # use Path object\n",
    "        base_path = Path(dest_path) / dataset_name / split\n",
    "\n",
    "        self.all_paths = []\n",
    "        self.all_labels = []\n",
    "        label_mapper = {}\n",
    "\n",
    "        for i, x in enumerate(base_path.iterdir()):\n",
    "            label_mapper[x.stem] = i\n",
    "\n",
    "        for label_type in label_mapper.keys():\n",
    "            for x in (base_path / label_type).iterdir():\n",
    "                self.all_paths.append(x)\n",
    "                self.all_labels.append(label_mapper[label_type])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_path = self.all_paths[idx]\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        x = content\n",
    "        y = self.all_labels[idx]\n",
    "\n",
    "        # apply rc_aug here if using\n",
    "        if self.rc_aug and coin_flip():\n",
    "            x = string_reverse_complement(x)\n",
    "\n",
    "        seq = self.tokenizer(x,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\" if self.use_padding else None,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )  # add cls and eos token (+2)\n",
    "        seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        # need to handle eos here\n",
    "        if self.add_eos:\n",
    "            # append list seems to be faster than append tensor\n",
    "            seq.append(self.tokenizer.sep_token_id)\n",
    "\n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)\n",
    "\n",
    "        # need to wrap in list\n",
    "        target = torch.LongTensor([y])\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "#now load the weights\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-tiny-1k-seqlen', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1774, -0.1000, -0.1742,  ..., -0.8039, -0.5348,  0.2469],\n",
      "         [-0.5091, -0.1538, -0.0706,  ..., -0.8190, -0.7371,  0.3698],\n",
      "         [-0.5304, -0.1182, -0.2418,  ..., -0.6155, -0.7445,  0.6163],\n",
      "         ...,\n",
      "         [ 0.0863, -0.4831, -0.7076,  ..., -0.2147, -0.3556,  0.6491],\n",
      "         [ 0.2488, -0.4462, -0.6233,  ..., -0.2467,  0.0415,  0.5895],\n",
      "         [ 0.3003, -0.4419, -0.7078,  ..., -0.1258, -0.2273,  0.5688]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 1000])\n",
      "tensor([[3, 1, 2, 4, 4, 4, 2, 4, 2, 0, 0, 3, 4, 0, 1, 2, 4, 0, 3, 4, 0, 4, 2, 2,\n",
      "         0, 0, 1, 0, 4, 4, 2, 0, 0, 4, 4, 0, 2, 0, 0, 3, 3, 2, 4, 2, 4, 3, 2, 2,\n",
      "         1, 3, 2, 0, 0, 4, 2, 2, 3, 0, 4, 1, 1, 0, 3, 0, 3, 1, 4, 4, 4, 1, 4, 2,\n",
      "         4, 1, 1, 2, 2, 0, 3, 0, 4, 2, 0, 2, 4, 3, 4, 2, 2, 0, 0, 1, 3, 4, 3, 2,\n",
      "         4, 1, 1, 1, 3, 4, 2, 3, 3, 3, 4, 3, 4, 1, 3, 0, 2, 1, 0, 3, 0, 0, 4, 2,\n",
      "         0, 2, 0, 2, 3, 2, 0, 2, 2, 2, 2, 1, 2, 0, 4, 2, 1, 0, 3, 1, 2, 1, 1, 2,\n",
      "         2, 2, 0, 0, 2, 4, 3, 4, 0, 1, 4, 3, 1, 1, 0, 4, 4, 0, 2, 0, 4, 3, 0, 0,\n",
      "         3, 1, 2, 1, 4, 1, 3, 3, 2, 0, 4, 2, 3, 2, 3, 2, 1, 1, 0, 0, 1, 3, 3, 4,\n",
      "         0, 3, 0, 1, 0, 3, 0, 0, 2, 0, 4, 1, 4, 1, 1, 3, 4, 3, 2, 3, 1, 1, 1, 4,\n",
      "         2, 1, 3, 0, 4, 2, 3, 2, 0, 2, 0, 4, 4, 2, 0, 0, 1, 4, 3, 1, 4, 0, 2, 2,\n",
      "         1, 1, 3, 0, 0, 4, 3, 2, 0, 3, 3, 0, 3, 2, 0, 1, 0, 0, 2, 1, 0, 1, 0, 2,\n",
      "         3, 4, 1, 4, 2, 4, 3, 1, 3, 4, 4, 2, 3, 3, 0, 2, 3, 2, 2, 2, 0, 4, 0, 0,\n",
      "         4, 3, 3, 1, 2, 0, 3, 2, 4, 0, 0, 0, 2, 2, 0, 1, 4, 0, 2, 2, 4, 2, 1, 2,\n",
      "         1, 0, 0, 1, 1, 4, 0, 0, 4, 0, 3, 1, 2, 1, 1, 3, 1, 0, 4, 4, 2, 0, 4, 0,\n",
      "         1, 1, 3, 1, 2, 3, 1, 4, 3, 0, 2, 1, 2, 2, 3, 1, 1, 4, 4, 1, 4, 3, 0, 0,\n",
      "         1, 2, 3, 2, 4, 4, 4, 4, 0, 3, 4, 2, 2, 4, 3, 3, 0, 1, 4, 4, 3, 4, 1, 0,\n",
      "         0, 0, 3, 4, 4, 4, 4, 3, 1, 4, 3, 0, 1, 2, 1, 3, 1, 4, 2, 1, 3, 4, 1, 3,\n",
      "         4, 3, 4, 0, 1, 1, 1, 3, 2, 3, 0, 4, 4, 3, 4, 3, 2, 4, 2, 2, 2, 2, 2, 0,\n",
      "         1, 1, 4, 4, 3, 1, 3, 4, 1, 0, 3, 0, 1, 4, 4, 0, 1, 1, 1, 3, 1, 0, 1, 2,\n",
      "         3, 2, 2, 3, 4, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 3, 4, 1, 3, 3, 0, 3, 0, 4,\n",
      "         4, 2, 4, 3, 0, 2, 1, 0, 0, 4, 2, 1, 2, 4, 4, 3, 2, 1, 3, 4, 1, 2, 3, 0,\n",
      "         1, 3, 3, 0, 2, 1, 2, 4, 2, 3, 2, 4, 0, 2, 1, 4, 0, 1, 4, 2, 4, 3, 3, 0,\n",
      "         3, 2, 1, 4, 0, 1, 2, 2, 1, 0, 1, 4, 4, 3, 4, 2, 2, 4, 2, 2, 4, 2, 0, 1,\n",
      "         4, 1, 0, 3, 4, 0, 4, 4, 3, 2, 3, 2, 4, 2, 0, 1, 3, 2, 1, 2, 2, 0, 0, 1,\n",
      "         3, 1, 2, 1, 1, 4, 0, 2, 0, 2, 0, 3, 3, 1, 1, 1, 2, 0, 0, 0, 2, 1, 2, 1,\n",
      "         4, 3, 3, 0, 4, 0, 2, 2, 1, 2, 3, 3, 2, 0, 0, 2, 1, 1, 3, 2, 3, 0, 4, 1,\n",
      "         2, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 4, 1, 0, 0, 2, 3, 0, 0, 1, 3, 3, 1, 0,\n",
      "         3, 0, 1, 0, 2, 1, 1, 3, 1, 1, 1, 2, 4, 0, 4, 4, 2, 1, 1, 2, 2, 2, 4, 4,\n",
      "         3, 0, 3, 4, 3, 3, 2, 1, 3, 1, 2, 2, 0, 0, 1, 1, 1, 1, 3, 0, 1, 1, 2, 0,\n",
      "         2, 1, 2, 2, 0, 2, 4, 2, 0, 0, 2, 1, 0, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 1,\n",
      "         3, 2, 4, 0, 3, 2, 4, 2, 3, 2, 3, 4, 0, 2, 4, 1, 0, 2, 1, 0, 2, 1, 4, 1,\n",
      "         4, 1, 0, 0, 3, 1, 4, 3, 2, 0, 4, 4, 2, 1, 0, 4, 1, 3, 0, 0, 0, 4, 1, 0,\n",
      "         0, 0, 3, 1, 0, 1, 4, 0, 0, 4, 3, 1, 3, 1, 2, 2, 4, 0, 0, 2, 3, 0, 2, 0,\n",
      "         3, 2, 3, 4, 3, 3, 1, 4, 3, 2, 4, 2, 1, 3, 0, 2, 1, 0, 4, 2, 4, 2, 0, 3,\n",
      "         1, 2, 1, 4, 0, 0, 1, 4, 2, 3, 2, 3, 3, 3, 0, 4, 0, 0, 0, 3, 1, 2, 4, 1,\n",
      "         4, 2, 2, 2, 0, 0, 0, 1, 2, 4, 0, 3, 4, 1, 2, 2, 4, 2, 1, 4, 4, 0, 1, 1,\n",
      "         3, 4, 3, 3, 1, 2, 2, 3, 1, 2, 0, 4, 1, 1, 2, 4, 3, 3, 2, 2, 0, 2, 3, 4,\n",
      "         4, 0, 2, 1, 0, 1, 1, 0, 2, 0, 1, 0, 3, 3, 4, 4, 3, 1, 4, 2, 1, 4, 4, 2,\n",
      "         1, 2, 2, 0, 2, 3, 3, 4, 1, 3, 0, 2, 1, 3, 3, 1, 3, 1, 1, 1, 3, 1, 1, 3,\n",
      "         4, 3, 1, 3, 3, 4, 2, 4, 2, 4, 1, 0, 1, 0, 4, 1, 3, 0, 3, 3, 2, 3, 1, 2,\n",
      "         2, 4, 3, 4, 1, 1, 2, 1, 1, 4, 3, 0, 3, 2, 3, 4, 3, 2, 1, 0, 4, 1, 3, 1,\n",
      "         0, 4, 0, 3, 2, 2, 4, 2, 3, 3, 2, 1, 0, 4, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#now let's input this into our model\n",
    "print(model(torch.randint(0, 5, (1, 1000))))\n",
    "print(torch.randint(0,5,(1,1000)).shape)\n",
    "print(torch.randint(0,5,(1,1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([999])\n",
      "tensor([ 9,  7,  7, 10,  8, 10,  9,  9, 10,  9,  9,  9,  9,  7,  7,  9,  8,  7,\n",
      "         7,  9,  8,  7,  7,  7, 10,  9,  8,  8,  8,  7, 10,  8,  7,  8,  7, 10,\n",
      "         9,  8,  7,  8, 10, 10, 10,  8,  8, 10,  8,  8,  7,  7,  8,  7,  9,  7,\n",
      "         9,  8,  9,  7,  8, 10,  8,  7,  9,  7, 10,  9,  8, 10,  7, 10,  7,  7,\n",
      "         7,  7,  8, 10, 10,  9,  8, 10,  7,  7,  8,  7,  8,  7,  9, 10,  8, 10,\n",
      "         8,  7,  9,  9,  9, 10,  8, 10,  9,  7, 10,  8,  7,  8,  7,  9, 10,  7,\n",
      "         7,  8,  7, 10,  7,  8,  7,  7, 10,  8,  8,  7,  9,  9, 10, 10, 10, 10,\n",
      "         7,  7, 10,  8,  7, 10,  8,  7,  9,  7,  7,  7, 10,  8,  7,  8,  7,  9,\n",
      "        10,  8,  8, 10,  7, 10, 10,  9, 10,  8, 10, 10,  8, 10,  9,  8,  7,  8,\n",
      "         7,  9,  7,  8,  8,  8,  7,  7,  7,  8,  7,  8,  7,  8, 10, 10,  9,  9,\n",
      "         7,  9,  9, 10,  8,  7, 10,  9, 10, 10,  8,  7,  7, 10,  7, 10,  9,  7,\n",
      "         7, 10,  7,  8,  8,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  8,  7,  8,  9,  8,  9,  7,  9,  7,  7,  9, 10,\n",
      "         7,  8,  7, 10,  8, 10,  9,  8,  7,  9,  7,  7,  7,  9,  8,  8,  7,  9,\n",
      "         8, 10,  9,  9,  8,  7, 10,  9, 10,  8,  7,  7,  8,  8,  7, 10, 10,  8,\n",
      "         7,  7,  7,  7,  7,  8, 10,  8,  7,  9,  9,  9, 10,  9, 10, 10,  8, 10,\n",
      "         9,  9,  7, 10,  7,  7,  7,  9,  7,  7,  9,  7,  8, 10,  8,  7,  9,  9,\n",
      "         7,  7,  9,  7,  8,  7,  7,  9, 10,  7, 10,  9,  7,  7,  9,  8,  7, 10,\n",
      "         7,  7, 10,  8, 10,  9, 10,  9,  7,  8,  7, 10, 10,  8,  8,  7, 10,  9,\n",
      "         8,  9,  9,  8,  7,  9,  7,  8,  7, 10, 10,  7,  9,  7,  8,  7,  8,  7,\n",
      "        10,  7,  8,  7,  7,  9,  7,  9,  7,  9, 10, 10,  9, 10, 10,  9,  9,  7,\n",
      "         7,  7,  9,  8,  9,  9,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  7,  8, 10,  9,  7,  9,  8, 10,\n",
      "         7,  7,  7, 10,  8, 10,  8,  7,  7, 10,  7, 10, 10, 10,  8,  7,  9,  7,\n",
      "        10,  8, 10,  8, 10,  7,  9,  7,  7,  8, 10,  7, 10,  8,  8,  7, 10,  8,\n",
      "         7,  9, 10,  9,  7,  7,  7, 10,  9,  9,  7, 10, 10,  9,  8,  7,  7,  7,\n",
      "        10,  7,  8,  7,  7,  7,  9,  7,  9, 10,  7,  7, 10,  7,  8,  8,  7, 10,\n",
      "         9, 10,  8,  7,  8, 10, 10,  7,  7,  9,  7,  7, 10,  7,  9,  7,  7, 10,\n",
      "         8,  7, 10,  9,  9,  7,  8,  9,  7,  9,  9,  8, 10,  9,  8,  8,  7,  8,\n",
      "         8, 10,  9,  8, 10,  9, 10, 10,  9,  9,  9,  9,  9,  8,  8,  7,  8, 10,\n",
      "         9,  8,  7,  9,  7,  7,  9,  7,  7,  7, 10, 10,  8,  8,  7,  9,  7,  7,\n",
      "         8,  7,  8, 10,  9,  9,  7,  8, 10,  9,  9,  7,  9,  7,  9,  8,  7,  8,\n",
      "         8, 10,  8,  7,  8, 10, 10, 10,  8,  8, 10, 10,  7,  8,  7,  9,  8, 10,\n",
      "         8, 10,  7,  7,  9, 10, 10, 10,  8, 10,  9,  7,  8, 10,  8,  7,  9, 10,\n",
      "         9,  7,  8,  8, 10,  9,  7, 10, 10,  8,  7,  8, 10,  7,  8,  8,  7, 10,\n",
      "         7, 10,  7,  8,  7,  8,  7,  7,  7,  9,  7,  8,  8,  8,  7,  8, 10, 10,\n",
      "         7,  8,  7,  8,  7,  7,  7, 10,  9,  7,  8, 10,  9, 10, 10,  8, 10, 10,\n",
      "         8,  7,  8,  7,  8, 10,  7,  9,  9,  8,  8,  8,  7, 10,  9,  9,  7,  9,\n",
      "         7,  8,  7,  9,  9,  9,  7, 10,  7,  7,  7,  7, 10, 10,  8, 10,  9,  7,\n",
      "         7, 10, 10, 10,  9,  8, 10,  8,  7,  9,  7, 10,  7,  8,  8, 10, 10,  8,\n",
      "        10,  8,  8,  9,  8, 10,  7,  8, 10,  9,  7,  8,  7, 10,  8, 10,  7,  9,\n",
      "         9,  8,  7, 10, 10,  7,  8,  7,  8,  7,  7, 10, 10,  8,  7, 10,  8, 10,\n",
      "         8, 10, 10,  8,  7, 10,  7, 10, 10, 10,  7,  7,  8,  8, 10, 10, 10,  9,\n",
      "         7,  7,  9, 10, 10, 10,  9,  8, 10,  7,  8, 10, 10,  8, 10,  8,  7,  9,\n",
      "         7,  9,  7,  9,  7,  8, 10,  7,  7, 10,  9,  7,  9, 10,  7,  9, 10,  9,\n",
      "         7,  9,  8,  7,  7,  7, 10,  7, 10,  8,  8, 10,  9,  7,  6,  6,  6,  6,\n",
      "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8,\n",
      "        10,  8, 10,  8,  7,  7,  7,  7,  8,  7,  7,  8,  9,  9,  7,  7, 10,  7,\n",
      "        10, 10,  8,  7, 10,  8,  7,  7,  7,  7,  8,  7,  8,  7,  9,  8,  7,  9,\n",
      "        10, 10,  8, 10,  9,  8,  7,  8, 10, 10,  7,  7,  8, 10, 10, 10,  7,  9,\n",
      "         9,  8,  8, 10, 10, 10, 10,  8, 10,  7,  7,  8,  7,  8,  8, 10, 10,  9,\n",
      "        10, 10, 10,  8, 10, 10,  9,  9,  8,  7,  9, 10,  7,  7,  8, 10,  9, 10,\n",
      "         9,  9,  8,  8,  7,  9,  7,  7, 10,  7,  9,  8, 10,  8, 10, 10, 10,  8,\n",
      "         8,  7,  8,  7,  9,  7, 10,  7,  7,  7,  9,  9,  7,  8,  8, 10, 10, 10,\n",
      "        10,  9,  7,  7,  7,  9,  9,  7, 10,  7,  9,  9,  9, 10,  8, 10,  8, 10,\n",
      "         7,  9,  7, 10,  7,  9,  7,  7,  7])\n"
     ]
    }
   ],
   "source": [
    "print(ccre[0][0].shape)\n",
    "print(ccre[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 999])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#now let's make the shape 1x1000 when we input it to the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ccre[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mccre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 1003\u001b[0m, in \u001b[0;36mHyenaDNAModel.forward\u001b[0;34m(self, input_ids, position_ids, state)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# state for the repo interface\u001b[39;00m\n\u001b[0;32m-> 1003\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_head:\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 813\u001b[0m, in \u001b[0;36mLMBackbone.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 813\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 770\u001b[0m, in \u001b[0;36mGPT2Embeddings.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    766\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m        input_ids: (batch, seqlen)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m        position_ids: (batch, seqlen)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 770\u001b[0m     batch_size, seqlen \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m    771\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#now let's make the shape 1x1000 when we input it to the model\n",
    "print(ccre[0][0].unsqueeze(0).shape)\n",
    "print(model(ccre[0][0].unsqueeze))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.Size([1, 1000])\n",
      "tensor([[[-0.1711, -0.0438, -0.1490,  ..., -0.6019, -0.6615,  0.2640],\n",
      "         [-0.3871,  0.0754, -0.2123,  ..., -0.7412, -0.5472,  0.4276],\n",
      "         [-0.1986,  0.8799, -0.2828,  ...,  0.5297, -0.1062, -0.2779],\n",
      "         ...,\n",
      "         [ 0.1266, -0.6398, -0.4388,  ..., -0.1927, -0.0620,  0.6233],\n",
      "         [ 0.1075, -0.5597, -0.6213,  ..., -0.1531, -0.1889,  0.5921],\n",
      "         [ 0.0437, -0.3788, -0.8403,  ...,  0.0180, -0.3103,  0.5746]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(mytensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(model(torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1001\u001b[39m))))\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmytensor\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 1003\u001b[0m, in \u001b[0;36mHyenaDNAModel.forward\u001b[0;34m(self, input_ids, position_ids, state)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# state for the repo interface\u001b[39;00m\n\u001b[0;32m-> 1003\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_head:\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 813\u001b[0m, in \u001b[0;36mLMBackbone.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 813\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 771\u001b[0m, in \u001b[0;36mGPT2Embeddings.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    input_ids: (batch, seqlen)\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m    position_ids: (batch, seqlen)\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    770\u001b[0m batch_size, seqlen \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 771\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_in(embeddings)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "print(type(ccre[0][0].unsqueeze(0)))\n",
    "print(type(torch.randint(0,5,(1,1000))))\n",
    "#no dtype\n",
    "print(ccre[0][0].unsqueeze(0).dtype)\n",
    "print(torch.randint(0,5,(1,1000)).dtype)\n",
    "#append a 0 at the end\n",
    "mytensor = ccre[0][0].unsqueeze(0)\n",
    "mytensor = torch.cat((mytensor,torch.zeros(1,1)),1)\n",
    "print(mytensor.shape)\n",
    "print(model(torch.randint(0, 5, (1, 1001))))\n",
    "print(model(mytensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5208,  0.2889, -0.3443,  ...,  0.6271, -0.2119,  0.2367],\n",
      "         [-0.5477,  0.3131, -0.7037,  ...,  0.8036, -0.2931,  0.2496],\n",
      "         [-0.4796,  0.1738, -0.4642,  ...,  0.7316, -0.3880,  0.1677],\n",
      "         ...,\n",
      "         [-0.7524,  0.4155, -0.2061,  ...,  0.9488, -0.1626, -0.1463],\n",
      "         [-0.6259,  0.5373, -0.4277,  ...,  1.2585, -0.4374,  0.0760],\n",
      "         [-0.5506,  0.2984, -0.1218,  ...,  0.7830, -0.3950,  0.2254]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 999, 128])\n"
     ]
    }
   ],
   "source": [
    "out = model(ccre[0][0].unsqueeze(0)) #have to do unsqueeze(0) not unsqueeze, that's the issue, but yes it does go through the model\n",
    "print(out)\n",
    "#oh it worked... ok\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing dataset module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lila/data/leslie/sarthak'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so far all we've done is the dataset, dataloader is not that much harder, but let's test it now that it's a module\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "    \n",
    "#and now the dataset\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/src/dataloaders/datasets')\n",
    "from ccre_dataset import CcreDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now test if it's imported properly\n",
    "#so when they call their function they use this appraoch, I can just call it exactly the same way\n",
    "tokenizer = CharacterTokenizer(characters=['A', 'C', 'G', 'T','N'], model_max_length=1000+2) #the plus 2 for the bos and eos tokens\n",
    "ccre = CcreDataset(split='train', max_length=1000, pad_max_length=None, tokenizer=tokenizer, tokenizer_name='char', add_eos=False, rc_aug=False, return_augs=False, replace_N_token=False, pad_interval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9,  7,  7, 10,  8, 10,  9,  9, 10,  9,  9,  9,  9,  7,  7,  9,  8,  7,\n",
       "          7,  9,  8,  7,  7,  7, 10,  9,  8,  8,  8,  7, 10,  8,  7,  8,  7, 10,\n",
       "          9,  8,  7,  8, 10, 10, 10,  8,  8, 10,  8,  8,  7,  7,  8,  7,  9,  7,\n",
       "          9,  8,  9,  7,  8, 10,  8,  7,  9,  7, 10,  9,  8, 10,  7, 10,  7,  7,\n",
       "          7,  7,  8, 10, 10,  9,  8, 10,  7,  7,  8,  7,  8,  7,  9, 10,  8, 10,\n",
       "          8,  7,  9,  9,  9, 10,  8, 10,  9,  7, 10,  8,  7,  8,  7,  9, 10,  7,\n",
       "          7,  8,  7, 10,  7,  8,  7,  7, 10,  8,  8,  7,  9,  9, 10, 10, 10, 10,\n",
       "          7,  7, 10,  8,  7, 10,  8,  7,  9,  7,  7,  7, 10,  8,  7,  8,  7,  9,\n",
       "         10,  8,  8, 10,  7, 10, 10,  9, 10,  8, 10, 10,  8, 10,  9,  8,  7,  8,\n",
       "          7,  9,  7,  8,  8,  8,  7,  7,  7,  8,  7,  8,  7,  8, 10, 10,  9,  9,\n",
       "          7,  9,  9, 10,  8,  7, 10,  9, 10, 10,  8,  7,  7, 10,  7, 10,  9,  7,\n",
       "          7, 10,  7,  8,  8,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  8,  7,  8,  9,  8,  9,  7,  9,  7,  7,  9, 10,\n",
       "          7,  8,  7, 10,  8, 10,  9,  8,  7,  9,  7,  7,  7,  9,  8,  8,  7,  9,\n",
       "          8, 10,  9,  9,  8,  7, 10,  9, 10,  8,  7,  7,  8,  8,  7, 10, 10,  8,\n",
       "          7,  7,  7,  7,  7,  8, 10,  8,  7,  9,  9,  9, 10,  9, 10, 10,  8, 10,\n",
       "          9,  9,  7, 10,  7,  7,  7,  9,  7,  7,  9,  7,  8, 10,  8,  7,  9,  9,\n",
       "          7,  7,  9,  7,  8,  7,  7,  9, 10,  7, 10,  9,  7,  7,  9,  8,  7, 10,\n",
       "          7,  7, 10,  8, 10,  9, 10,  9,  7,  8,  7, 10, 10,  8,  8,  7, 10,  9,\n",
       "          8,  9,  9,  8,  7,  9,  7,  8,  7, 10, 10,  7,  9,  7,  8,  7,  8,  7,\n",
       "         10,  7,  8,  7,  7,  9,  7,  9,  7,  9, 10, 10,  9, 10, 10,  9,  9,  7,\n",
       "          7,  7,  9,  8,  9,  9,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  7,  8,  7,  8, 10,  9,  7,  9,  8, 10,\n",
       "          7,  7,  7, 10,  8, 10,  8,  7,  7, 10,  7, 10, 10, 10,  8,  7,  9,  7,\n",
       "         10,  8, 10,  8, 10,  7,  9,  7,  7,  8, 10,  7, 10,  8,  8,  7, 10,  8,\n",
       "          7,  9, 10,  9,  7,  7,  7, 10,  9,  9,  7, 10, 10,  9,  8,  7,  7,  7,\n",
       "         10,  7,  8,  7,  7,  7,  9,  7,  9, 10,  7,  7, 10,  7,  8,  8,  7, 10,\n",
       "          9, 10,  8,  7,  8, 10, 10,  7,  7,  9,  7,  7, 10,  7,  9,  7,  7, 10,\n",
       "          8,  7, 10,  9,  9,  7,  8,  9,  7,  9,  9,  8, 10,  9,  8,  8,  7,  8,\n",
       "          8, 10,  9,  8, 10,  9, 10, 10,  9,  9,  9,  9,  9,  8,  8,  7,  8, 10,\n",
       "          9,  8,  7,  9,  7,  7,  9,  7,  7,  7, 10, 10,  8,  8,  7,  9,  7,  7,\n",
       "          8,  7,  8, 10,  9,  9,  7,  8, 10,  9,  9,  7,  9,  7,  9,  8,  7,  8,\n",
       "          8, 10,  8,  7,  8, 10, 10, 10,  8,  8, 10, 10,  7,  8,  7,  9,  8, 10,\n",
       "          8, 10,  7,  7,  9, 10, 10, 10,  8, 10,  9,  7,  8, 10,  8,  7,  9, 10,\n",
       "          9,  7,  8,  8, 10,  9,  7, 10, 10,  8,  7,  8, 10,  7,  8,  8,  7, 10,\n",
       "          7, 10,  7,  8,  7,  8,  7,  7,  7,  9,  7,  8,  8,  8,  7,  8, 10, 10,\n",
       "          7,  8,  7,  8,  7,  7,  7, 10,  9,  7,  8, 10,  9, 10, 10,  8, 10, 10,\n",
       "          8,  7,  8,  7,  8, 10,  7,  9,  9,  8,  8,  8,  7, 10,  9,  9,  7,  9,\n",
       "          7,  8,  7,  9,  9,  9,  7, 10,  7,  7,  7,  7, 10, 10,  8, 10,  9,  7,\n",
       "          7, 10, 10, 10,  9,  8, 10,  8,  7,  9,  7, 10,  7,  8,  8, 10, 10,  8,\n",
       "         10,  8,  8,  9,  8, 10,  7,  8, 10,  9,  7,  8,  7, 10,  8, 10,  7,  9,\n",
       "          9,  8,  7, 10, 10,  7,  8,  7,  8,  7,  7, 10, 10,  8,  7, 10,  8, 10,\n",
       "          8, 10, 10,  8,  7, 10,  7, 10, 10, 10,  7,  7,  8,  8, 10, 10, 10,  9,\n",
       "          7,  7,  9, 10, 10, 10,  9,  8, 10,  7,  8, 10, 10,  8, 10,  8,  7,  9,\n",
       "          7,  9,  7,  9,  7,  8, 10,  7,  7, 10,  9,  7,  9, 10,  7,  9, 10,  9,\n",
       "          7,  9,  8,  7,  7,  7, 10,  7, 10,  8,  8, 10,  9,  7,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8,\n",
       "         10,  8, 10,  8,  7,  7,  7,  7,  8,  7,  7,  8,  9,  9,  7,  7, 10,  7,\n",
       "         10, 10,  8,  7, 10,  8,  7,  7,  7,  7,  8,  7,  8,  7,  9,  8,  7,  9,\n",
       "         10, 10,  8, 10,  9,  8,  7,  8, 10, 10,  7,  7,  8, 10, 10, 10,  7,  9,\n",
       "          9,  8,  8, 10, 10, 10, 10,  8, 10,  7,  7,  8,  7,  8,  8, 10, 10,  9,\n",
       "         10, 10, 10,  8, 10, 10,  9,  9,  8,  7,  9, 10,  7,  7,  8, 10,  9, 10,\n",
       "          9,  9,  8,  8,  7,  9,  7,  7, 10,  7,  9,  8, 10,  8, 10, 10, 10,  8,\n",
       "          8,  7,  8,  7,  9,  7, 10,  7,  7,  7,  9,  9,  7,  8,  8, 10, 10, 10,\n",
       "         10,  9,  7,  7,  7,  9,  9,  7, 10,  7,  9,  9,  9, 10,  8, 10,  8, 10,\n",
       "          7,  9,  7, 10,  7,  9,  7,  7,  7]),\n",
       " tensor([ 7,  7, 10,  8, 10,  9,  9, 10,  9,  9,  9,  9,  7,  7,  9,  8,  7,  7,\n",
       "          9,  8,  7,  7,  7, 10,  9,  8,  8,  8,  7, 10,  8,  7,  8,  7, 10,  9,\n",
       "          8,  7,  8, 10, 10, 10,  8,  8, 10,  8,  8,  7,  7,  8,  7,  9,  7,  9,\n",
       "          8,  9,  7,  8, 10,  8,  7,  9,  7, 10,  9,  8, 10,  7, 10,  7,  7,  7,\n",
       "          7,  8, 10, 10,  9,  8, 10,  7,  7,  8,  7,  8,  7,  9, 10,  8, 10,  8,\n",
       "          7,  9,  9,  9, 10,  8, 10,  9,  7, 10,  8,  7,  8,  7,  9, 10,  7,  7,\n",
       "          8,  7, 10,  7,  8,  7,  7, 10,  8,  8,  7,  9,  9, 10, 10, 10, 10,  7,\n",
       "          7, 10,  8,  7, 10,  8,  7,  9,  7,  7,  7, 10,  8,  7,  8,  7,  9, 10,\n",
       "          8,  8, 10,  7, 10, 10,  9, 10,  8, 10, 10,  8, 10,  9,  8,  7,  8,  7,\n",
       "          9,  7,  8,  8,  8,  7,  7,  7,  8,  7,  8,  7,  8, 10, 10,  9,  9,  7,\n",
       "          9,  9, 10,  8,  7, 10,  9, 10, 10,  8,  7,  7, 10,  7, 10,  9,  7,  7,\n",
       "         10,  7,  8,  8,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  8,  7,  8,  9,  8,  9,  7,  9,  7,  7,  9, 10,  7,\n",
       "          8,  7, 10,  8, 10,  9,  8,  7,  9,  7,  7,  7,  9,  8,  8,  7,  9,  8,\n",
       "         10,  9,  9,  8,  7, 10,  9, 10,  8,  7,  7,  8,  8,  7, 10, 10,  8,  7,\n",
       "          7,  7,  7,  7,  8, 10,  8,  7,  9,  9,  9, 10,  9, 10, 10,  8, 10,  9,\n",
       "          9,  7, 10,  7,  7,  7,  9,  7,  7,  9,  7,  8, 10,  8,  7,  9,  9,  7,\n",
       "          7,  9,  7,  8,  7,  7,  9, 10,  7, 10,  9,  7,  7,  9,  8,  7, 10,  7,\n",
       "          7, 10,  8, 10,  9, 10,  9,  7,  8,  7, 10, 10,  8,  8,  7, 10,  9,  8,\n",
       "          9,  9,  8,  7,  9,  7,  8,  7, 10, 10,  7,  9,  7,  8,  7,  8,  7, 10,\n",
       "          7,  8,  7,  7,  9,  7,  9,  7,  9, 10, 10,  9, 10, 10,  9,  9,  7,  7,\n",
       "          7,  9,  8,  9,  9,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  7,  8,  7,  8, 10,  9,  7,  9,  8, 10,  7,\n",
       "          7,  7, 10,  8, 10,  8,  7,  7, 10,  7, 10, 10, 10,  8,  7,  9,  7, 10,\n",
       "          8, 10,  8, 10,  7,  9,  7,  7,  8, 10,  7, 10,  8,  8,  7, 10,  8,  7,\n",
       "          9, 10,  9,  7,  7,  7, 10,  9,  9,  7, 10, 10,  9,  8,  7,  7,  7, 10,\n",
       "          7,  8,  7,  7,  7,  9,  7,  9, 10,  7,  7, 10,  7,  8,  8,  7, 10,  9,\n",
       "         10,  8,  7,  8, 10, 10,  7,  7,  9,  7,  7, 10,  7,  9,  7,  7, 10,  8,\n",
       "          7, 10,  9,  9,  7,  8,  9,  7,  9,  9,  8, 10,  9,  8,  8,  7,  8,  8,\n",
       "         10,  9,  8, 10,  9, 10, 10,  9,  9,  9,  9,  9,  8,  8,  7,  8, 10,  9,\n",
       "          8,  7,  9,  7,  7,  9,  7,  7,  7, 10, 10,  8,  8,  7,  9,  7,  7,  8,\n",
       "          7,  8, 10,  9,  9,  7,  8, 10,  9,  9,  7,  9,  7,  9,  8,  7,  8,  8,\n",
       "         10,  8,  7,  8, 10, 10, 10,  8,  8, 10, 10,  7,  8,  7,  9,  8, 10,  8,\n",
       "         10,  7,  7,  9, 10, 10, 10,  8, 10,  9,  7,  8, 10,  8,  7,  9, 10,  9,\n",
       "          7,  8,  8, 10,  9,  7, 10, 10,  8,  7,  8, 10,  7,  8,  8,  7, 10,  7,\n",
       "         10,  7,  8,  7,  8,  7,  7,  7,  9,  7,  8,  8,  8,  7,  8, 10, 10,  7,\n",
       "          8,  7,  8,  7,  7,  7, 10,  9,  7,  8, 10,  9, 10, 10,  8, 10, 10,  8,\n",
       "          7,  8,  7,  8, 10,  7,  9,  9,  8,  8,  8,  7, 10,  9,  9,  7,  9,  7,\n",
       "          8,  7,  9,  9,  9,  7, 10,  7,  7,  7,  7, 10, 10,  8, 10,  9,  7,  7,\n",
       "         10, 10, 10,  9,  8, 10,  8,  7,  9,  7, 10,  7,  8,  8, 10, 10,  8, 10,\n",
       "          8,  8,  9,  8, 10,  7,  8, 10,  9,  7,  8,  7, 10,  8, 10,  7,  9,  9,\n",
       "          8,  7, 10, 10,  7,  8,  7,  8,  7,  7, 10, 10,  8,  7, 10,  8, 10,  8,\n",
       "         10, 10,  8,  7, 10,  7, 10, 10, 10,  7,  7,  8,  8, 10, 10, 10,  9,  7,\n",
       "          7,  9, 10, 10, 10,  9,  8, 10,  7,  8, 10, 10,  8, 10,  8,  7,  9,  7,\n",
       "          9,  7,  9,  7,  8, 10,  7,  7, 10,  9,  7,  9, 10,  7,  9, 10,  9,  7,\n",
       "          9,  8,  7,  7,  7, 10,  7, 10,  8,  8, 10,  9,  7,  6,  6,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8, 10,\n",
       "          8, 10,  8,  7,  7,  7,  7,  8,  7,  7,  8,  9,  9,  7,  7, 10,  7, 10,\n",
       "         10,  8,  7, 10,  8,  7,  7,  7,  7,  8,  7,  8,  7,  9,  8,  7,  9, 10,\n",
       "         10,  8, 10,  9,  8,  7,  8, 10, 10,  7,  7,  8, 10, 10, 10,  7,  9,  9,\n",
       "          8,  8, 10, 10, 10, 10,  8, 10,  7,  7,  8,  7,  8,  8, 10, 10,  9, 10,\n",
       "         10, 10,  8, 10, 10,  9,  9,  8,  7,  9, 10,  7,  7,  8, 10,  9, 10,  9,\n",
       "          9,  8,  8,  7,  9,  7,  7, 10,  7,  9,  8, 10,  8, 10, 10, 10,  8,  8,\n",
       "          7,  8,  7,  9,  7, 10,  7,  7,  7,  9,  9,  7,  8,  8, 10, 10, 10, 10,\n",
       "          9,  7,  7,  7,  9,  9,  7, 10,  7,  9,  9,  9, 10,  8, 10,  8, 10,  7,\n",
       "          9,  7, 10,  7,  9,  7,  7,  7,  7]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccre[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ccre.max_length #so no append anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccre = CcreDataset(split='train', max_length=1005, pad_max_length=None, tokenizer=tokenizer, tokenizer_name='char', add_eos=True, rc_aug=False, return_augs=False, replace_N_token=False, pad_interval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005\n",
      "(tensor([4, 4, 4,  ..., 7, 7, 7]), tensor([4, 4, 0,  ..., 7, 7, 1]))\n",
      "tensor([4, 4, 4, 0, 9])\n"
     ]
    }
   ],
   "source": [
    "print(ccre.max_length)\n",
    "print(ccre[0])\n",
    "\n",
    "#the 4 is the padding at the beginning, and the 1 is the eos token\n",
    "print(ccre[0][0][0:5])\n",
    "#seems no padding is done on the other end, that's incorrect, let's modify it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 6, 6, 7, 7, 10, 10, 6, 6, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('..AATT..') #has bos and eos token, seems period is a 6 here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "seq = ccre.array[0][0]\n",
    "templen = len(seq)\n",
    "interval_length = len(seq)\n",
    "# row = (chr, start, end, split)\n",
    "# chr_name, start, end = (row[0], row[1], row[2])\n",
    "\n",
    "# seq = self.fasta(chr_name, start, end, max_length=self.max_length, return_augs=self.return_augs)\n",
    "\n",
    "# left_padding = right_padding = 0\n",
    "\n",
    "if interval_length < ccre.max_length:\n",
    "    extra_seq = ccre.max_length - interval_length\n",
    "\n",
    "    extra_left_seq = extra_seq // 2\n",
    "    extra_right_seq = extra_seq - extra_left_seq\n",
    "\n",
    "if ccre.rc_aug and coin_flip():\n",
    "    seq = string_reverse_complement(seq)\n",
    "\n",
    "if ccre.pad_interval:\n",
    "    seq = ('.' * extra_left_seq) + seq + ('.' * extra_right_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 7, 8, 9, 10, 6, 6, 6, 6, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('ACGTacgt')\n",
    "#wait no it can't be reverse complement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GAATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAA']\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(seq)\n",
    "print(ccre.pad_interval)\n",
    "#oh we aren't doing padding... that's why!\n",
    "#the difference is the reverse complement... no where is the 4 from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 0, 9, 7, 7, 10, 8, 10, 9, 9, 10, 9, 9, 9, 9, 7, 7, 9, 8, 7, 7, 9, 8, 7, 7, 7, 10, 9, 8, 8, 8, 7, 10, 8, 7, 8, 7, 10, 9, 8, 7, 8, 10, 10, 10, 8, 8, 10, 8, 8, 7, 7, 8, 7, 9, 7, 9, 8, 9, 7, 8, 10, 8, 7, 9, 7, 10, 9, 8, 10, 7, 10, 7, 7, 7, 7, 8, 10, 10, 9, 8, 10, 7, 7, 8, 7, 8, 7, 9, 10, 8, 10, 8, 7, 9, 9, 9, 10, 8, 10, 9, 7, 10, 8, 7, 8, 7, 9, 10, 7, 7, 8, 7, 10, 7, 8, 7, 7, 10, 8, 8, 7, 9, 9, 10, 10, 10, 10, 7, 7, 10, 8, 7, 10, 8, 7, 9, 7, 7, 7, 10, 8, 7, 8, 7, 9, 10, 8, 8, 10, 7, 10, 10, 9, 10, 8, 10, 10, 8, 10, 9, 8, 7, 8, 7, 9, 7, 8, 8, 8, 7, 7, 7, 8, 7, 8, 7, 8, 10, 10, 9, 9, 7, 9, 9, 10, 8, 7, 10, 9, 10, 10, 8, 7, 7, 10, 7, 10, 9, 7, 7, 10, 7, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 7, 8, 9, 8, 9, 7, 9, 7, 7, 9, 10, 7, 8, 7, 10, 8, 10, 9, 8, 7, 9, 7, 7, 7, 9, 8, 8, 7, 9, 8, 10, 9, 9, 8, 7, 10, 9, 10, 8, 7, 7, 8, 8, 7, 10, 10, 8, 7, 7, 7, 7, 7, 8, 10, 8, 7, 9, 9, 9, 10, 9, 10, 10, 8, 10, 9, 9, 7, 10, 7, 7, 7, 9, 7, 7, 9, 7, 8, 10, 8, 7, 9, 9, 7, 7, 9, 7, 8, 7, 7, 9, 10, 7, 10, 9, 7, 7, 9, 8, 7, 10, 7, 7, 10, 8, 10, 9, 10, 9, 7, 8, 7, 10, 10, 8, 8, 7, 10, 9, 8, 9, 9, 8, 7, 9, 7, 8, 7, 10, 10, 7, 9, 7, 8, 7, 8, 7, 10, 7, 8, 7, 7, 9, 7, 9, 7, 9, 10, 10, 9, 10, 10, 9, 9, 7, 7, 7, 9, 8, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 8, 7, 8, 10, 9, 7, 9, 8, 10, 7, 7, 7, 10, 8, 10, 8, 7, 7, 10, 7, 10, 10, 10, 8, 7, 9, 7, 10, 8, 10, 8, 10, 7, 9, 7, 7, 8, 10, 7, 10, 8, 8, 7, 10, 8, 7, 9, 10, 9, 7, 7, 7, 10, 9, 9, 7, 10, 10, 9, 8, 7, 7, 7, 10, 7, 8, 7, 7, 7, 9, 7, 9, 10, 7, 7, 10, 7, 8, 8, 7, 10, 9, 10, 8, 7, 8, 10, 10, 7, 7, 9, 7, 7, 10, 7, 9, 7, 7, 10, 8, 7, 10, 9, 9, 7, 8, 9, 7, 9, 9, 8, 10, 9, 8, 8, 7, 8, 8, 10, 9, 8, 10, 9, 10, 10, 9, 9, 9, 9, 9, 8, 8, 7, 8, 10, 9, 8, 7, 9, 7, 7, 9, 7, 7, 7, 10, 10, 8, 8, 7, 9, 7, 7, 8, 7, 8, 10, 9, 9, 7, 8, 10, 9, 9, 7, 9, 7, 9, 8, 7, 8, 8, 10, 8, 7, 8, 10, 10, 10, 8, 8, 10, 10, 7, 8, 7, 9, 8, 10, 8, 10, 7, 7, 9, 10, 10, 10, 8, 10, 9, 7, 8, 10, 8, 7, 9, 10, 9, 7, 8, 8, 10, 9, 7, 10, 10, 8, 7, 8, 10, 7, 8, 8, 7, 10, 7, 10, 7, 8, 7, 8, 7, 7, 7, 9, 7, 8, 8, 8, 7, 8, 10, 10, 7, 8, 7, 8, 7, 7, 7, 10, 9, 7, 8, 10, 9, 10, 10, 8, 10, 10, 8, 7, 8, 7, 8, 10, 7, 9, 9, 8, 8, 8, 7, 10, 9, 9, 7, 9, 7, 8, 7, 9, 9, 9, 7, 10, 7, 7, 7, 7, 10, 10, 8, 10, 9, 7, 7, 10, 10, 10, 9, 8, 10, 8, 7, 9, 7, 10, 7, 8, 8, 10, 10, 8, 10, 8, 8, 9, 8, 10, 7, 8, 10, 9, 7, 8, 7, 10, 8, 10, 7, 9, 9, 8, 7, 10, 10, 7, 8, 7, 8, 7, 7, 10, 10, 8, 7, 10, 8, 10, 8, 10, 10, 8, 7, 10, 7, 10, 10, 10, 7, 7, 8, 8, 10, 10, 10, 9, 7, 7, 9, 10, 10, 10, 9, 8, 10, 7, 8, 10, 10, 8, 10, 8, 7, 9, 7, 9, 7, 9, 7, 8, 10, 7, 7, 10, 9, 7, 9, 10, 7, 9, 10, 9, 7, 9, 8, 7, 7, 7, 10, 7, 10, 8, 8, 10, 9, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 10, 8, 10, 8, 7, 7, 7, 7, 8, 7, 7, 8, 9, 9, 7, 7, 10, 7, 10, 10, 8, 7, 10, 8, 7, 7, 7, 7, 8, 7, 8, 7, 9, 8, 7, 9, 10, 10, 8, 10, 9, 8, 7, 8, 10, 10, 7, 7, 8, 10, 10, 10, 7, 9, 9, 8, 8, 10, 10, 10, 10, 8, 10, 7, 7, 8, 7, 8, 8, 10, 10, 9, 10, 10, 10, 8, 10, 10, 9, 9, 8, 7, 9, 10, 7, 7, 8, 10, 9, 10, 9, 9, 8, 8, 7, 9, 7, 7, 10, 7, 9, 8, 10, 8, 10, 10, 10, 8, 8, 7, 8, 7, 9, 7, 10, 7, 7, 7, 9, 9, 7, 8, 8, 10, 10, 10, 10, 9, 7, 7, 7, 9, 9, 7, 10, 7, 9, 9, 9, 10, 8, 10, 8, 10, 7, 9, 7, 10, 7, 9, 7, 7, 7, 7, 1]\n",
      "tensor([4, 4, 4,  ..., 7, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "seq = ccre.tokenizer(seq,\n",
    "    add_special_tokens=True if ccre.add_eos else False,  # this is what controls adding eos\n",
    "    padding=\"max_length\",\n",
    "    max_length=ccre.max_length,\n",
    "    truncation=True,\n",
    ")\n",
    "seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "print(seq)\n",
    "\n",
    "#ahh it is padding from the tokenizer itself!\n",
    "seq = torch.LongTensor(seq)\n",
    "print(seq)\n",
    "\n",
    "#no clue why it only added it to the left.\n",
    "#because that's the default and on top of that, the 4 is added because that is the default pad token as well! Glad we are figuring this out!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCtcacagagaaggaaatttaCACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGaatttatcttcatataaacaACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAagctgagaatgcttctacctCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAA\n",
      "GAATCTGGTGGGGAAGCAAGCAAATGCCCATCACATGCACTTTCCTCCAACAGAGCGACTCAGATGCTATAAAACTTGCTAACACAGTCTCAGGGTCTGATCACAGTAACATACAATCCAGGTTTTAATCATCAGAAATCACAGTCCTATTGTCTTCTGCACAGACCCAAACACACTTGGAGGTCATGTTCAATATGAATACCTCACAGAGAAGGAAATTTACACGCGAGAAGTACATCTGCAGAAAGCCAGCTGGCATGTCAACCATTCAAAAACTCAGGGTGTTCTGGATAAAGAAGACTCAGGAAGACAAGTATGAAGCATAATCTGTGACATTCCATGCGGCAGACATTAGACACATACAAGAGAGTTGTTGGAAAGCGGAATTTATCTTCATATAAACAACACTGAGCTAAATCTCAATATTTCAGATCTCTAGAACTATCCATCAGTGAAATGGATTGCAAATACAAAGAGTAATACCATGTCACTTAAGAATAGAATCATGGACGAGGCTGCCACCTGCTGTTGGGGGCCACTGCAGAAGAAATTCCAGAACACTGGACTGGAGAGCACCTCACTTTCCTTACAGCTCTAAGTTTCTGACTCAGTGACCTGATTCACTACCATATACACAAAGACCCACTTACACAAATGACTGTTCTTCACACTAGGCCCATGGAGACAGGGATAAAATTCTGAATTTGCTCAGATACCTTCTCCGCTACTGACATCTAGGCATTACACAATTCATCTCTTCATATTTAACCTTTGAAGTTTGCTACTTCTCAGAGAGACTAATGAGTAGTGAGCAAATATCCTGAAGCTGAGAATGCTTCTACCTCCTCTCAAAACAACGGAATATTCATCAAAACACAGCAGTTCTGCACTTAACTTTAGGCCTTTTCTAACACCTTGTTTCTTGGCAGTAACTGTGGCCAGAATAGCTCTTTCCACAGATAAAGGACCTTTTGAAAGGATAGGGTCTCTAGATAGAAAA\n"
     ]
    }
   ],
   "source": [
    "#test .upper\n",
    "seq = ccre.array[0][0]\n",
    "print(seq)\n",
    "print(seq.upper()) #works like a charm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing DNase loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna/')\n",
    "from src.dataloaders.datasets.DNase_dataset import DNaseDataset\n",
    "from src.tasks.decoders import SequenceDecoder\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "# from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "from src.models.sequence.dna_embedding import DNAEmbeddingModel\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# d_output = 161\n",
    "\n",
    "tokenizer = CharacterTokenizer( #make sure to fix the tokenizer too\n",
    "                characters=['A', 'C', 'G', 'T', 'N', 'S', 'U', 'V', 'W', 'X', 'Y', 'Z'],\n",
    "                model_max_length=1024 + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "                padding_side='left'\n",
    "            )\n",
    "ccre = DNaseDataset(max_length = 1024, split = 'test', tokenizer=tokenizer, rc_aug = False, tokenizer_name='char', add_eos='True', filter = True)\n",
    "data, target = ccre[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
