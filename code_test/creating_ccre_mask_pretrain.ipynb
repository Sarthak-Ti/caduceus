{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chr1': 248956422,\n",
       " 'chr2': 242193529,\n",
       " 'chr3': 198295559,\n",
       " 'chr4': 190214555,\n",
       " 'chr5': 181538259,\n",
       " 'chr6': 170805979,\n",
       " 'chr7': 159345973,\n",
       " 'chr8': 145138636,\n",
       " 'chr9': 138394717,\n",
       " 'chr10': 133797422,\n",
       " 'chr11': 135086622,\n",
       " 'chr12': 133275309,\n",
       " 'chr13': 114364328,\n",
       " 'chr14': 107043718,\n",
       " 'chr15': 101991189,\n",
       " 'chr16': 90338345,\n",
       " 'chr17': 83257441,\n",
       " 'chr18': 80373285,\n",
       " 'chr19': 58617616,\n",
       " 'chr20': 64444167,\n",
       " 'chr21': 46709983,\n",
       " 'chr22': 50818468,\n",
       " 'chrX': 156040895,\n",
       " 'chrY': 57227415}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to create a token mask that we can easily access that is as smalla s possible, will be Binary 0 and 1s, so can multiply by the input, and the 0s are masked\n",
    "#first load in the genome\n",
    "import numpy as np\n",
    "\n",
    "genome_sizes = {'chr1': 248956422,\n",
    "'chr2':\t242193529,\n",
    "'chr3':\t198295559,\n",
    "'chr4':\t190214555,\n",
    "'chr5':\t181538259,\n",
    "'chr6':\t170805979,\n",
    "'chr7':\t159345973,\n",
    "'chr8':\t145138636,\n",
    "'chr9':\t138394717,\n",
    "'chr10':\t133797422,\n",
    "'chr11':\t135086622,\n",
    "'chr12':\t133275309,\n",
    "'chr13':\t114364328,\n",
    "'chr14':\t107043718,\n",
    "'chr15':\t101991189,\n",
    "'chr16':\t90338345,\n",
    "'chr17':\t83257441,\n",
    "'chr18':\t80373285,\n",
    "'chr19':\t58617616,\n",
    "'chr20':\t64444167,\n",
    "'chr21':\t46709983,\n",
    "'chr22':\t50818468,\n",
    "'chrX':\t156040895,\n",
    "'chrY':\t57227415}\n",
    "genome_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr1 (248956422,)\n",
      "chr2 (242193529,)\n",
      "chr3 (198295559,)\n",
      "chr4 (190214555,)\n",
      "chr5 (181538259,)\n",
      "chr6 (170805979,)\n",
      "chr7 (159345973,)\n",
      "chr8 (145138636,)\n",
      "chr9 (138394717,)\n",
      "chr10 (133797422,)\n",
      "chr11 (135086622,)\n",
      "chr12 (133275309,)\n",
      "chr13 (114364328,)\n",
      "chr14 (107043718,)\n",
      "chr15 (101991189,)\n",
      "chr16 (90338345,)\n",
      "chr17 (83257441,)\n",
      "chr18 (80373285,)\n",
      "chr19 (58617616,)\n",
      "chr20 (64444167,)\n",
      "chr21 (46709983,)\n",
      "chr22 (50818468,)\n",
      "chrX (156040895,)\n",
      "chrY (57227415,)\n"
     ]
    }
   ],
   "source": [
    "#now create a dictionary of numpy arrays\n",
    "genome = {}\n",
    "for chrom in genome_sizes.keys():\n",
    "    genome[chrom] = np.zeros(genome_sizes[chrom], dtype ='bool')\n",
    "    print(chrom, genome[chrom].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genome['chr1'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.088269832"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbytes = 0\n",
    "for chrom in genome.keys():\n",
    "    nbytes += genome[chrom].nbytes\n",
    "nbytes/1e9 #about 3 GB as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>104896</td>\n",
       "      <td>105048</td>\n",
       "      <td>EH38D4327509</td>\n",
       "      <td>EH38E2776520</td>\n",
       "      <td>CTCF-only,CTCF-bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>138866</td>\n",
       "      <td>139134</td>\n",
       "      <td>EH38D4327520</td>\n",
       "      <td>EH38E2776521</td>\n",
       "      <td>pELS,CTCF-bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>181289</td>\n",
       "      <td>181639</td>\n",
       "      <td>EH38D4327525</td>\n",
       "      <td>EH38E2776524</td>\n",
       "      <td>DNase-H3K4me3,CTCF-bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>267925</td>\n",
       "      <td>268171</td>\n",
       "      <td>EH38D4327544</td>\n",
       "      <td>EH38E2776528</td>\n",
       "      <td>CTCF-only,CTCF-bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>586036</td>\n",
       "      <td>586264</td>\n",
       "      <td>EH38D4327554</td>\n",
       "      <td>EH38E2776532</td>\n",
       "      <td>CTCF-only,CTCF-bound</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0       1       2             3             4                         5\n",
       "0  chr1  104896  105048  EH38D4327509  EH38E2776520      CTCF-only,CTCF-bound\n",
       "1  chr1  138866  139134  EH38D4327520  EH38E2776521           pELS,CTCF-bound\n",
       "2  chr1  181289  181639  EH38D4327525  EH38E2776524  DNase-H3K4me3,CTCF-bound\n",
       "3  chr1  267925  268171  EH38D4327544  EH38E2776528      CTCF-only,CTCF-bound\n",
       "4  chr1  586036  586264  EH38D4327554  EH38E2776532      CTCF-only,CTCF-bound"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok now we go through the bed file and we will turn it to ones/True if it's in the bed\n",
    "import pandas as pd\n",
    "file_path = '/data/leslie/sarthak/data/GRCh38-cCREs.bed'\n",
    "df = pd.read_csv(file_path, sep = '\\t', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063878/1063878 [00:48<00:00, 21840.03it/s]\n"
     ]
    }
   ],
   "source": [
    "#now go through each row\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(df.shape[0]), total = df.shape[0]):\n",
    "    chrom = df.iloc[i, 0]\n",
    "    start = df.iloc[i, 1]\n",
    "    end = df.iloc[i, 2]\n",
    "    genome[chrom][start:end] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is relatively quite fast, and now we can save this mask\n",
    "np.savez('/data/leslie/sarthak/data/GRCh38-cCREs_mask.npz', **genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and now altering mlm masking by adding this ccre mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196608])\n"
     ]
    }
   ],
   "source": [
    "#this way can easily work with kmers or others\n",
    "#so we load it in and get a sequence of 196608, let's test it\n",
    "import torch\n",
    "sample_seq = torch.ones((196608), dtype = torch.float16)\n",
    "print(sample_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196608])\n"
     ]
    }
   ],
   "source": [
    "mlm_probability = 0.15\n",
    "data = sample_seq.clone()\n",
    "target = data.clone()\n",
    "probability_matrix = torch.full(target.shape, mlm_probability)\n",
    "print(probability_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.1500, 0.1500,  ..., 0.1500, 0.1500, 0.1500])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_matrix #ahh so just makes it the same size but the values are 0.15 each\n",
    "#this is where we can apply our mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False, False,  ..., False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.bernoulli(probability_matrix).bool()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.0000, 0.0000,  ..., 0.0000, 0.1500, 0.0000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = mask * probability_matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dtype #keeps the float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False,  ..., False, False, False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli = torch.bernoulli(a).bool()\n",
    "bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4376)\n",
      "tensor(0.0223)\n",
      "tensor(0.1488)\n"
     ]
    }
   ],
   "source": [
    "print(sum(bernoulli)) #4376 out of 196608 is too low\n",
    "print(sum(bernoulli)/bernoulli.shape[0]) #clearly below the 0.15\n",
    "print(sum(bernoulli)/sum(a.bool())) #around the 15% that we expect, so this masking approach works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROPE testing too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have the mask for cCREs now let's test ROPE\n",
    "import torch.nn as nn\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "  def __init__(self, d: int, base: int = 10_000):\n",
    "\n",
    "    super().__init__()\n",
    "    self.base = base\n",
    "    self.d = d\n",
    "    self.cos_cached = None\n",
    "    self.sin_cached = None\n",
    "\n",
    "  def _build_cache(self, x: torch.Tensor):\n",
    "\n",
    "    if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "      return\n",
    "\n",
    "    seq_len = x.shape[0]\n",
    "\n",
    "    theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device) # THETA = 10,000^(-2*i/d) or 1/10,000^(2i/d)\n",
    "\n",
    "    seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device) #Position Index -> [0,1,2...seq-1]\n",
    "\n",
    "    idx_theta = torch.einsum('n,d->nd', seq_idx, theta)  #Calculates m*(THETA) = [ [0, 0...], [THETA_1, THETA_2...THETA_d/2], ... [seq-1*(THETA_1), seq-1*(THETA_2)...] ]\n",
    "\n",
    "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1) # [THETA_1, THETA_2...THETA_d/2] -> [THETA_1, THETA_2...THETA_d]\n",
    "\n",
    "\n",
    "    self.cos_cached = idx_theta2.cos()[:, None, None, :] #Cache [cosTHETA_1, cosTHETA_2...cosTHETA_d]\n",
    "    self.sin_cached = idx_theta2.sin()[:, None, None, :] #cache [sinTHETA_1, sinTHETA_2...sinTHETA_d]\n",
    "\n",
    "  def _neg_half(self, x: torch.Tensor):\n",
    "\n",
    "    d_2 = self.d // 2 #\n",
    "\n",
    "    return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1) # [x_1, x_2,...x_d] -> [-x_d/2, ... -x_d, x_1, ... x_d/2]\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "\n",
    "    self._build_cache(x)\n",
    "\n",
    "    neg_half_x = self._neg_half(x)\n",
    "\n",
    "    x_rope = (x * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]]) # [x_1*cosTHETA_1 - x_d/2*sinTHETA_d/2, ....]\n",
    "\n",
    "    return x_rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196608, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what happens when we do this\n",
    "x = torch.rand((196608, 512), dtype = torch.float16)\n",
    "rope = RotaryPositionalEmbeddings(512)\n",
    "out = rope(x[:,None,None,:])[:,0,0,:]\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2612, 0.0322, 0.9404,  ..., 0.0190, 0.5605, 0.0083],\n",
      "        [0.4565, 0.1807, 0.9087,  ..., 0.5752, 0.4297, 0.9282],\n",
      "        [0.5483, 0.5825, 0.9116,  ..., 0.3164, 0.5000, 0.1167],\n",
      "        ...,\n",
      "        [0.4546, 0.3862, 0.5557,  ..., 0.2559, 0.4946, 0.3105],\n",
      "        [0.6841, 0.8496, 0.5161,  ..., 0.0098, 0.3491, 0.5835],\n",
      "        [0.1152, 0.3120, 0.1182,  ..., 0.7456, 0.2114, 0.0273]],\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2612,  0.0322,  0.9404,  ...,  0.0190,  0.5605,  0.0083],\n",
      "        [ 0.0684, -0.1788,  0.1877,  ...,  0.5753,  0.4298,  0.9282],\n",
      "        [-0.8298, -0.4477, -0.9684,  ...,  0.3166,  0.5000,  0.1168],\n",
      "        ...,\n",
      "        [ 0.3846,  0.7037,  0.1268,  ..., -0.1932,  0.4112,  0.4675],\n",
      "        [ 0.8043,  0.7064, -0.9518,  ...,  0.0035, -0.2216,  0.2719],\n",
      "        [ 0.2510, -0.1667, -0.0868,  ..., -0.6952,  0.3114,  0.5583]])\n"
     ]
    }
   ],
   "source": [
    "print(out) #first row the same, rotated by 0, see it rotates as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(x: torch.Tensor, seq_len: int, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply Rotary Position Embedding to input tensor.\n",
    "    \n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim)\n",
    "    seq_len (int): Sequence length\n",
    "    dim (int): Dimension of the model (should be divisible by 2)\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Tensor with RoPE applied\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    \n",
    "    # Precompute RoPE matrix\n",
    "    half_dim = dim // 2\n",
    "    inv_freq = 1.0 / (10000 ** (torch.arange(0, half_dim, 2).float().to(device) / half_dim))\n",
    "    t = torch.arange(seq_len, device=device).type_as(inv_freq)\n",
    "    freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "    emb = torch.cat((freqs, freqs), dim=-1)\n",
    "    cos = emb.cos().view(seq_len, 1, dim)\n",
    "    sin = emb.sin().view(seq_len, 1, dim)\n",
    "    \n",
    "    # Rotate half of the dimensions\n",
    "    x_rope = x.clone()\n",
    "    x_pass, x_rot = x_rope.chunk(2, dim=-1)\n",
    "    x_rot = torch.stack([-x_rot[..., 1::2], x_rot[..., ::2]], dim=-1).flatten(-2)\n",
    "    x_rope = torch.cat((x_pass, x_rot), dim=-1)\n",
    "    \n",
    "    # Apply rotation\n",
    "    return x_rope * cos + torch.roll(x_rope, shifts=1, dims=-1) * sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[196608, 1, 512]' is invalid for input of size 50331648",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#see the output based on RoPEMamba\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mapply_rope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m196608\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m, in \u001b[0;36mapply_rope\u001b[0;34m(x, seq_len, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m freqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi,j->ij\u001b[39m\u001b[38;5;124m'\u001b[39m, t, inv_freq)\n\u001b[1;32m     20\u001b[0m emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((freqs, freqs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m cos \u001b[38;5;241m=\u001b[39m \u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m sin \u001b[38;5;241m=\u001b[39m emb\u001b[38;5;241m.\u001b[39msin()\u001b[38;5;241m.\u001b[39mview(seq_len, \u001b[38;5;241m1\u001b[39m, dim)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Rotate half of the dimensions\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[196608, 1, 512]' is invalid for input of size 50331648"
     ]
    }
   ],
   "source": [
    "#see the output based on RoPEMamba\n",
    "apply_rope(x[None,:,:], seq_len=196608, dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and now lucid rains implementation\n",
    "from __future__ import annotations\n",
    "from math import pi, log\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.amp import autocast\n",
    "from torch import nn, einsum, broadcast_tensors, Tensor\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# broadcat, as tortoise-tts was using it\n",
    "\n",
    "def broadcat(tensors, dim = -1):\n",
    "    broadcasted_tensors = broadcast_tensors(*tensors)\n",
    "    return torch.cat(broadcasted_tensors, dim = dim)\n",
    "\n",
    "# rotary embedding helper functions\n",
    "\n",
    "def rotate_half(x):\n",
    "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
    "    x1, x2 = x.unbind(dim = -1)\n",
    "    x = torch.stack((-x2, x1), dim = -1)\n",
    "    return rearrange(x, '... d r -> ... (d r)')\n",
    "\n",
    "@autocast('cuda', enabled = False)\n",
    "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1., seq_dim = -2):\n",
    "    dtype = t.dtype\n",
    "\n",
    "    if t.ndim == 3:\n",
    "        seq_len = t.shape[seq_dim]\n",
    "        freqs = freqs[-seq_len:]\n",
    "\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    end_index = start_index + rot_dim\n",
    "\n",
    "    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n",
    "\n",
    "    # Split t into three parts: left, middle (to be transformed), and right\n",
    "    t_left = t[..., :start_index]\n",
    "    t_middle = t[..., start_index:end_index]\n",
    "    t_right = t[..., end_index:]\n",
    "\n",
    "    # Apply rotary embeddings without modifying t in place    \n",
    "    t_transformed = (t_middle * freqs.cos() * scale) + (rotate_half(t_middle) * freqs.sin() * scale)\n",
    "        \n",
    "    out = torch.cat((t_left, t_transformed, t_right), dim=-1)\n",
    "\n",
    "    return out.type(dtype)\n",
    "\n",
    "# learned rotation helpers\n",
    "\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    if exists(freq_ranges):\n",
    "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
    "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
    "\n",
    "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)\n",
    "\n",
    "# classes\n",
    "\n",
    "class RotaryEmbedding(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        custom_freqs: Tensor | None = None,\n",
    "        freqs_for:  Literal['lang', 'pixel', 'constant'] = 'lang',\n",
    "        theta = 10000,\n",
    "        max_freq = 10,\n",
    "        num_freqs = 1,\n",
    "        learned_freq = False,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = 512,\n",
    "        interpolate_factor = 1.,\n",
    "        theta_rescale_factor = 1.,\n",
    "        seq_before_head_dim = False,\n",
    "        cache_if_possible = True,\n",
    "        cache_max_seq_len = 8192\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n",
    "        # has some connection to NTK literature\n",
    "        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n",
    "\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "        self.freqs_for = freqs_for\n",
    "\n",
    "        if exists(custom_freqs):\n",
    "            freqs = custom_freqs\n",
    "        elif freqs_for == 'lang':\n",
    "            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "        elif freqs_for == 'pixel':\n",
    "            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n",
    "        elif freqs_for == 'constant':\n",
    "            freqs = torch.ones(num_freqs).float()\n",
    "\n",
    "        self.cache_if_possible = cache_if_possible\n",
    "        self.cache_max_seq_len = cache_max_seq_len\n",
    "\n",
    "        self.register_buffer('cached_freqs', torch.zeros(cache_max_seq_len, dim), persistent = False)\n",
    "        self.register_buffer('cached_freqs_seq_len', torch.tensor(0), persistent = False)\n",
    "\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
    "\n",
    "        self.learned_freq = learned_freq\n",
    "\n",
    "        # dummy for device\n",
    "\n",
    "        self.register_buffer('dummy', torch.tensor(0), persistent = False)\n",
    "\n",
    "        # default sequence dimension\n",
    "\n",
    "        self.seq_before_head_dim = seq_before_head_dim\n",
    "        self.default_seq_dim = -3 if seq_before_head_dim else -2\n",
    "\n",
    "        # interpolation factors\n",
    "\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "\n",
    "        # xpos\n",
    "\n",
    "        self.use_xpos = use_xpos\n",
    "\n",
    "        if not use_xpos:\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.scale_base = xpos_scale_base\n",
    "\n",
    "        self.register_buffer('scale', scale, persistent = False)\n",
    "        self.register_buffer('cached_scales', torch.zeros(cache_max_seq_len, dim), persistent = False)\n",
    "        self.register_buffer('cached_scales_seq_len', torch.tensor(0), persistent = False)\n",
    "\n",
    "        # add apply_rotary_emb as static method\n",
    "\n",
    "        self.apply_rotary_emb = staticmethod(apply_rotary_emb)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.dummy.device\n",
    "\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = None, offset = 0, scale = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert not self.use_xpos or exists(scale), 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n",
    "\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "\n",
    "        seq = self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset)\n",
    "\n",
    "        freqs = self.forward(seq, seq_len = seq_len, offset = offset)\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "\n",
    "        return apply_rotary_emb(freqs, t, scale = default(scale, 1.), seq_dim = seq_dim)\n",
    "\n",
    "    def rotate_queries_with_cached_keys(self, q, k, seq_dim = None, offset = 0):\n",
    "        dtype, device, seq_dim = q.dtype, q.device, default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        q_len, k_len = q.shape[seq_dim], k.shape[seq_dim]\n",
    "        assert q_len <= k_len\n",
    "\n",
    "        q_scale = k_scale = 1.\n",
    "\n",
    "        if self.use_xpos:\n",
    "            seq = self.get_seq_pos(k_len, dtype = dtype, device = device)\n",
    "\n",
    "            q_scale = self.get_scale(seq[-q_len:]).type(dtype)\n",
    "            k_scale = self.get_scale(seq).type(dtype)\n",
    "\n",
    "        rotated_q = self.rotate_queries_or_keys(q, seq_dim = seq_dim, scale = q_scale, offset = k_len - q_len + offset)\n",
    "        rotated_k = self.rotate_queries_or_keys(k, seq_dim = seq_dim, scale = k_scale ** -1)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def rotate_queries_and_keys(self, q, k, seq_dim = None):\n",
    "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
    "\n",
    "        assert self.use_xpos\n",
    "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
    "\n",
    "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
    "\n",
    "        freqs = self.forward(seq, seq_len = seq_len)\n",
    "        scale = self.get_scale(seq, seq_len = seq_len).to(dtype)\n",
    "\n",
    "        if seq_dim == -3:\n",
    "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
    "            scale = rearrange(scale, 'n d -> n 1 d')\n",
    "\n",
    "        rotated_q = apply_rotary_emb(freqs, q, scale = scale, seq_dim = seq_dim)\n",
    "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1, seq_dim = seq_dim)\n",
    "\n",
    "        rotated_q = rotated_q.type(q.dtype)\n",
    "        rotated_k = rotated_k.type(k.dtype)\n",
    "\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def get_scale(\n",
    "        self,\n",
    "        t: Tensor,\n",
    "        seq_len: int | None = None,\n",
    "        offset = 0\n",
    "    ):\n",
    "        assert self.use_xpos\n",
    "\n",
    "        should_cache = (\n",
    "            self.cache_if_possible and\n",
    "            exists(seq_len) and\n",
    "            (offset + seq_len) <= self.cache_max_seq_len\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            should_cache and \\\n",
    "            exists(self.cached_scales) and \\\n",
    "            (seq_len + offset) <= self.cached_scales_seq_len.item()\n",
    "        ):\n",
    "            return self.cached_scales[offset:(offset + seq_len)]\n",
    "\n",
    "        scale = 1.\n",
    "        if self.use_xpos:\n",
    "            power = (t - len(t) // 2) / self.scale_base\n",
    "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "            scale = repeat(scale, 'n d -> n (d r)', r = 2)\n",
    "\n",
    "        if should_cache and offset == 0:\n",
    "            self.cached_scales[:seq_len] = scale.detach()\n",
    "            self.cached_scales_seq_len.copy_(seq_len)\n",
    "\n",
    "        return scale\n",
    "\n",
    "    def get_axial_freqs(self, *dims):\n",
    "        Colon = slice(None)\n",
    "        all_freqs = []\n",
    "\n",
    "        for ind, dim in enumerate(dims):\n",
    "            if self.freqs_for == 'pixel':\n",
    "                pos = torch.linspace(-1, 1, steps = dim, device = self.device)\n",
    "            else:\n",
    "                pos = torch.arange(dim, device = self.device)\n",
    "\n",
    "            freqs = self.forward(pos, seq_len = dim)\n",
    "\n",
    "            all_axis = [None] * len(dims)\n",
    "            all_axis[ind] = Colon\n",
    "\n",
    "            new_axis_slice = (Ellipsis, *all_axis, Colon)\n",
    "            all_freqs.append(freqs[new_axis_slice])\n",
    "\n",
    "        all_freqs = broadcast_tensors(*all_freqs)\n",
    "        return torch.cat(all_freqs, dim = -1)\n",
    "\n",
    "    @autocast('cuda', enabled = False)\n",
    "    def forward(\n",
    "        self,\n",
    "        t: Tensor,\n",
    "        seq_len = None,\n",
    "        offset = 0\n",
    "    ):\n",
    "        should_cache = (\n",
    "            self.cache_if_possible and\n",
    "            not self.learned_freq and\n",
    "            exists(seq_len) and\n",
    "            self.freqs_for != 'pixel' and\n",
    "            (offset + seq_len) <= self.cache_max_seq_len\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            should_cache and \\\n",
    "            exists(self.cached_freqs) and \\\n",
    "            (offset + seq_len) <= self.cached_freqs_seq_len.item()\n",
    "        ):\n",
    "            return self.cached_freqs[offset:(offset + seq_len)].detach()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if should_cache and offset == 0:\n",
    "            self.cached_freqs[:seq_len] = freqs.detach()\n",
    "            self.cached_freqs_seq_len.copy_(seq_len)\n",
    "\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_emb = RotaryEmbedding(dim = 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I can do this, but this really seems to be for attention, why don't we just do the good old absolute embeddings, we already have relative\n",
    "def positionalencoding1d(d_model, length):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param length: length of positions\n",
    "    :return: length*d_model position matrix\n",
    "    \"\"\"\n",
    "    if d_model % 2 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dim (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(length, d_model)\n",
    "    position = torch.arange(0, length).unsqueeze(1)\n",
    "    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n",
    "                         -(torch.log(torch.tensor(10000)) / d_model)))\n",
    "    pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "pos_enc = positionalencoding1d(512, 196608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196608, 512])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [-8.3611e-01, -5.4857e-01, -6.0648e-01,  ..., -6.4953e-01,\n",
       "          9.9922e-01,  3.9614e-02],\n",
       "        [-9.1335e-01,  4.0717e-01,  3.1184e-01,  ..., -6.4961e-01,\n",
       "          9.9922e-01,  3.9509e-02],\n",
       "        [-1.5087e-01,  9.8855e-01,  9.5518e-01,  ..., -6.4970e-01,\n",
       "          9.9922e-01,  3.9406e-02]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ok so it's the right size\n",
    "pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using kmer genome with length 6\n",
      "4675\n"
     ]
    }
   ],
   "source": [
    "#I think this is better and makes more sense\n",
    "#So let's see what the part after the embedding looks like\n",
    "ckpt_path = '/data/leslie/sarthak/caduceus/outputs/2024-09-18/13-52-33-428792/checkpoints/last.ckpt'\n",
    "# we have lots of model outputs, now let's test it to see what works\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/caduceus')\n",
    "import evals.evals_utils_enformer as e\n",
    "#now let's load the model anad we can plot some things\n",
    "import numpy as np\n",
    "split = 'test'\n",
    "\n",
    "labels = np.load(f'/data/leslie/sarthak/data/enformer/data/{split}_label.npy')\n",
    "\n",
    "evals = e.Evals(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAEmbeddingModelCaduceus(\n",
       "  (caduceus): Caduceus(\n",
       "    (backbone): CaduceusMixerModel(\n",
       "      (embeddings): CaduceusEmbeddings(\n",
       "        (word_embeddings): RCPSEmbedding(\n",
       "          (embedding): Embedding(15632, 256)\n",
       "        )\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-9): 10 x RCPSMambaBlock(\n",
       "          (mixer): RCPSWrapper(\n",
       "            (submodule): BiMambaWrapper(\n",
       "              (mamba_fwd): Mamba(\n",
       "                (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "                (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "                (act): SiLU()\n",
       "                (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "                (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              )\n",
       "              (mamba_rev): Mamba(\n",
       "                (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "                (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "                (act): SiLU()\n",
       "                (x_proj): Linear(in_features=512, out_features=48, bias=False)\n",
       "                (dt_proj): Linear(in_features=16, out_features=512, bias=True)\n",
       "                (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaduceusEmbeddings(\n",
       "  (word_embeddings): RCPSEmbedding(\n",
       "    (embedding): Embedding(15632, 256)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.backbone.caduceus.backbone.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15632, 256)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.backbone.caduceus.backbone.embeddings.word_embeddings.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196608])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we see that the embedding is just d_model...\n",
    "#so let's use random integers of length 196608\n",
    "input = torch.randint(0, 21, (1,196608))\n",
    "input.shape\n",
    "# evals.backbone.caduceus.backbone.embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 196608, 512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.backbone.caduceus.backbone.embeddings(input.to('cuda')).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so we can see that the embedding already doubles it, so we have to set it up so that the RCPS Embedding which we use takes into account the positional embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
