{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading and testing hyena model\n",
    "see if we can get it on one a100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'einops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'einops'"
     ]
    }
   ],
   "source": [
    "#first load and save the hyena model\n",
    "# import transformers\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchvision.ops import StochasticDepth\n",
    "from collections import namedtuple\n",
    "\n",
    "#this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 97.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#test basic tqdm loop\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "for i in tqdm(range(100)):\n",
    "    #pause for .1 s\n",
    "    time.sleep(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from functools import partial, wraps\n",
    "from typing import Callable, List, Sequence\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from hydra.utils import get_original_cwd\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.utilities import rank_zero_only, rank_zero_warn\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
    "\n",
    "os.chdir('/data/leslie/sarthak/hyena/hyena-dna')\n",
    "\n",
    "import src.models.nn.utils as U\n",
    "import src.utils as utils\n",
    "import src.utils.train\n",
    "from src.dataloaders import SequenceDataset  # TODO make registry\n",
    "from src.tasks import decoders, encoders, tasks\n",
    "from src.utils import registry\n",
    "from src.utils.optim_groups import add_optimizer_hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can download the model from the huggingface model hub, which we did\n",
    "\n",
    "#now we can load the model\n",
    "\n",
    "#first take the model from test.py\n",
    "\n",
    "class SequenceLightningModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        # Disable profiling executor. This reduces memory and increases speed.\n",
    "        try:\n",
    "            torch._C._jit_set_profiling_executor(False)\n",
    "            torch._C._jit_set_profiling_mode(False)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        super().__init__()\n",
    "        # Passing in config expands it one level, so can access by self.hparams.train instead of self.hparams.config.train\n",
    "        self.save_hyperparameters(config, logger=False)\n",
    "\n",
    "        # Dataset arguments\n",
    "        self.dataset = SequenceDataset.registry[self.hparams.dataset._name_](\n",
    "            **self.hparams.dataset\n",
    "        )\n",
    "\n",
    "        # Check hparams\n",
    "        self._check_config()\n",
    "\n",
    "        # PL has some bugs, so add hooks and make sure they're only called once\n",
    "        self._has_setup = False\n",
    "\n",
    "        self.setup()  ## Added by KS\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if not self.hparams.train.disable_dataset:\n",
    "            self.dataset.setup()\n",
    "\n",
    "        # We need to set up the model in setup() because for some reason when training with DDP, one GPU uses much more memory than the others\n",
    "        # In order to not overwrite the model multiple times during different stages, we need this hack\n",
    "        # TODO PL 1.5 seems to have an option to skip hooks to avoid this\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/issues/5410#issuecomment-762257024\n",
    "        if self._has_setup:\n",
    "            return\n",
    "        else:\n",
    "            self._has_setup = True\n",
    "\n",
    "        # Convenience feature: if model specifies encoder, combine it with main encoder\n",
    "        encoder_cfg = utils.to_list(self.hparams.encoder) + utils.to_list(\n",
    "            self.hparams.model.pop(\"encoder\", None)\n",
    "        )\n",
    "        decoder_cfg = utils.to_list(\n",
    "            self.hparams.model.pop(\"decoder\", None)\n",
    "        ) + utils.to_list(self.hparams.decoder)\n",
    "\n",
    "        # Instantiate model\n",
    "        self.model = utils.instantiate(registry.model, self.hparams.model)\n",
    "        if (name := self.hparams.train.post_init_hook['_name_']) is not None:\n",
    "            kwargs = self.hparams.train.post_init_hook.copy()\n",
    "            del kwargs['_name_']\n",
    "            for module in self.modules():\n",
    "                if hasattr(module, name):\n",
    "                    getattr(module, name)(**kwargs)\n",
    "\n",
    "        # Instantiate the task\n",
    "        self.task = utils.instantiate(\n",
    "            tasks.registry, self.hparams.task, dataset=self.dataset, model=self.model\n",
    "        )\n",
    "\n",
    "        # Create encoders and decoders\n",
    "        encoder = encoders.instantiate(\n",
    "            encoder_cfg, dataset=self.dataset, model=self.model\n",
    "        )\n",
    "        decoder = decoders.instantiate(\n",
    "            decoder_cfg, model=self.model, dataset=self.dataset\n",
    "        )\n",
    "\n",
    "        # Extract the modules so they show up in the top level parameter count\n",
    "        self.encoder = U.PassthroughSequential(self.task.encoder, encoder)\n",
    "        self.decoder = U.PassthroughSequential(decoder, self.task.decoder)\n",
    "        self.loss = self.task.loss\n",
    "        self.loss_val = self.task.loss\n",
    "        if hasattr(self.task, 'loss_val'):\n",
    "            self.loss_val = self.task.loss_val\n",
    "        self.metrics = self.task.metrics\n",
    "        self.train_torchmetrics = self.task.train_torchmetrics\n",
    "        self.val_torchmetrics = self.task.val_torchmetrics\n",
    "        self.test_torchmetrics = self.task.test_torchmetrics\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=False):\n",
    "        if self.hparams.train.pretrained_model_state_hook['_name_'] is not None:\n",
    "            model_state_hook = utils.instantiate(\n",
    "                registry.model_state_hook,\n",
    "                self.hparams.train.pretrained_model_state_hook.copy(),\n",
    "                partial=True,\n",
    "            )\n",
    "            state_dict = model_state_hook(self.model, state_dict)\n",
    "\n",
    "        print(\"Custom load_state_dict function is running.\")\n",
    "\n",
    "        # strict==True will require all modules to match\n",
    "        # strict==False can allow encoder/decoder to be loaded from scratch too\n",
    "        return super().load_state_dict(state_dict, strict=strict)\n",
    "\n",
    "    def _check_config(self):\n",
    "        assert self.hparams.train.state.mode in [None, \"none\", \"null\", \"reset\", \"bptt\", \"tbptt\"]\n",
    "        assert (\n",
    "            (n := self.hparams.train.state.n_context) is None\n",
    "            or isinstance(n, int)\n",
    "            and n >= 0\n",
    "        )\n",
    "        assert (\n",
    "            (n := self.hparams.train.state.n_context_eval) is None\n",
    "            or isinstance(n, int)\n",
    "            and n >= 0\n",
    "        )\n",
    "\n",
    "    def _initialize_state(self):\n",
    "        \"\"\"Called at model setup and start of epoch to completely reset state\"\"\"\n",
    "        self._state = None\n",
    "        self._memory_chunks = []\n",
    "\n",
    "    def _reset_state(self, batch, device=None):\n",
    "        \"\"\"Called to construct default_state when necessary, e.g. during BPTT\"\"\"\n",
    "        device = device or batch[0].device\n",
    "        self._state = self.model.default_state(*batch[0].shape[:1], device=device)\n",
    "\n",
    "    def _detach_state(self, state):\n",
    "        if isinstance(state, torch.Tensor):\n",
    "            return state.detach()\n",
    "        elif isinstance(state, tuple):\n",
    "            return tuple(self._detach_state(s) for s in state)\n",
    "        elif isinstance(state, list):\n",
    "            return [self._detach_state(s) for s in state]\n",
    "        elif isinstance(state, dict):\n",
    "            return {k: self._detach_state(v) for k, v in state.items()}\n",
    "        elif state is None:\n",
    "            return None\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _process_state(self, batch, batch_idx, train=True):\n",
    "        \"\"\"Handle logic for state context.\"\"\"\n",
    "        # Number of context steps\n",
    "        key = \"n_context\" if train else \"n_context_eval\"\n",
    "        n_context = self.hparams.train.state.get(key)\n",
    "\n",
    "        # Don't need to do anything if 0 context steps. Make sure there is no state\n",
    "        if n_context == 0 and self.hparams.train.state.mode not in ['tbptt']:\n",
    "            self._initialize_state()\n",
    "            return\n",
    "\n",
    "        # Reset state if needed\n",
    "        if self.hparams.train.state.mode == \"reset\":\n",
    "            if batch_idx % (n_context + 1) == 0:\n",
    "                self._reset_state(batch)\n",
    "\n",
    "        # Pass through memory chunks\n",
    "        elif self.hparams.train.state.mode == \"bptt\":\n",
    "            self._reset_state(batch)\n",
    "            with torch.no_grad():  # should be unnecessary because individual modules should handle this\n",
    "                for _batch in self._memory_chunks:\n",
    "                    self.forward(_batch)\n",
    "            # Prepare for next step\n",
    "            self._memory_chunks.append(batch)\n",
    "            self._memory_chunks = self._memory_chunks[-n_context:]\n",
    "\n",
    "        elif self.hparams.train.state.mode == 'tbptt':\n",
    "            _, _, z = batch\n",
    "            reset = z[\"reset\"]\n",
    "            if reset:\n",
    "                self._reset_state(batch)\n",
    "            else:\n",
    "                self._state = self._detach_state(self._state)\n",
    "\n",
    "    # def forward(self, batch):\n",
    "    #     \"\"\"Passes a batch through the encoder, backbone, and decoder\"\"\"\n",
    "    #     # z holds arguments such as sequence length\n",
    "    #     x, y, *z = batch # z holds extra dataloader info such as resolution\n",
    "    #     if len(z) == 0:\n",
    "    #         z = {}\n",
    "    #     else:\n",
    "    #         assert len(z) == 1 and isinstance(z[0], dict), \"Dataloader must return dictionary of extra arguments\"\n",
    "    #         z = z[0]\n",
    "\n",
    "    #     x, w = self.encoder(x, **z) # w can model-specific constructions such as key_padding_mask for transformers or state for RNNs\n",
    "    #     x, state = self.model(x, **w, state=self._state)\n",
    "    #     self._state = state\n",
    "    #     x, w = self.decoder(x, state=state, **z)\n",
    "    #     return x, y, w\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.task.forward(batch, self.encoder, self.model, self.decoder, self._state)\n",
    "\n",
    "    def step(self, x_t):\n",
    "        x_t, *_ = self.encoder(x_t) # Potential edge case for encoders that expect (B, L, H)?\n",
    "        x_t, state = self.model.step(x_t, state=self._state)\n",
    "        self._state = state\n",
    "        # x_t = x_t[:, None, ...] # Dummy length\n",
    "        # x_t, *_ = self.decoder(x_t, state=state)\n",
    "        # x_t = x_t[:, 0, ...]\n",
    "        x_t, *_ = self.decoder.step(x_t, state=state)\n",
    "        return x_t\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx, prefix=\"train\"):\n",
    "\n",
    "        self._process_state(batch, batch_idx, train=(prefix == \"train\"))\n",
    "        x, y, w = self.forward(batch)\n",
    "\n",
    "        # Loss\n",
    "        if prefix == 'train':\n",
    "            loss = self.loss(x, y, **w)\n",
    "        else:\n",
    "            loss = self.loss_val(x, y, **w)\n",
    "\n",
    "        # Metrics\n",
    "        metrics = self.metrics(x, y, **w)\n",
    "        metrics[\"loss\"] = loss\n",
    "        metrics = {f\"{prefix}/{k}\": v for k, v in metrics.items()}\n",
    "\n",
    "        # Calculate torchmetrics\n",
    "        torchmetrics = getattr(self, f'{prefix}_torchmetrics')\n",
    "        torchmetrics(x, y, loss=loss)\n",
    "        \n",
    "        log_on_step = 'eval' in self.hparams and self.hparams.eval.get('log_on_step', False) and prefix == 'train'\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            on_step=log_on_step,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        # log the whole dict, otherwise lightning takes the mean to reduce it\n",
    "        # https://pytorch-lightning.readthedocs.io/en/stable/visualize/logging_advanced.html#enable-metrics-for-distributed-training\n",
    "        self.log_dict(\n",
    "            torchmetrics,\n",
    "            on_step=log_on_step,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            add_dataloader_idx=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        # Reset training torchmetrics\n",
    "        self.task._reset_torchmetrics(\"train\")\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # Log training torchmetrics\n",
    "        super().training_epoch_end(outputs)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        # Reset all validation torchmetrics\n",
    "        for name in self.val_loader_names:\n",
    "            self.task._reset_torchmetrics(name)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # Log all validation torchmetrics\n",
    "        super().validation_epoch_end(outputs)\n",
    "\n",
    "    def on_test_epoch_start(self):\n",
    "        # Reset all test torchmetrics\n",
    "        for name in self.test_loader_names:\n",
    "            self.task._reset_torchmetrics(name)\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        # Log all test torchmetrics\n",
    "        super().test_epoch_end(outputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        loss = self._shared_step(batch, batch_idx, prefix=\"train\")\n",
    "\n",
    "        # Log the loss explicitly so it shows up in WandB\n",
    "        # Note that this currently runs into a bug in the progress bar with ddp (as of 1.4.6)\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/pull/9142\n",
    "        # We additionally log the epochs under 'trainer' to get a consistent prefix with 'global_step'\n",
    "        loss_epoch = {\"trainer/loss\": loss, \"trainer/epoch\": self.current_epoch}\n",
    "        self.log_dict(\n",
    "            loss_epoch,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        # Log any extra info that the models want to expose (e.g. output norms)\n",
    "        metrics = {}\n",
    "        for module in list(self.modules())[1:]:\n",
    "            if hasattr(module, \"metrics\"):\n",
    "                metrics.update(module.metrics)\n",
    "\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            on_step=True,\n",
    "            on_epoch=False,\n",
    "            prog_bar=False,\n",
    "            add_dataloader_idx=False,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        ema = (\n",
    "            self.val_loader_names[dataloader_idx].endswith(\"/ema\")\n",
    "            and self.optimizers().optimizer.stepped\n",
    "        )  # There's a bit of an annoying edge case with the first (0-th) epoch; it has to be excluded due to the initial sanity check\n",
    "        if ema:\n",
    "            self.optimizers().swap_ema()\n",
    "        loss = self._shared_step(\n",
    "            batch, batch_idx, prefix=self.val_loader_names[dataloader_idx]\n",
    "        )\n",
    "        if ema:\n",
    "            self.optimizers().swap_ema()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self._shared_step(\n",
    "            batch, batch_idx, prefix=self.test_loader_names[dataloader_idx]\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Set zero weight decay for some params\n",
    "        if 'optimizer_param_grouping' in self.hparams.train:\n",
    "            add_optimizer_hooks(self.model, **self.hparams.train.optimizer_param_grouping)\n",
    "\n",
    "        # Normal parameters\n",
    "        all_params = list(self.parameters())\n",
    "        params = [p for p in all_params if not hasattr(p, \"_optim\")]\n",
    "\n",
    "        optimizer = utils.instantiate(registry.optimizer, self.hparams.optimizer, params)\n",
    "\n",
    "        del self.hparams.optimizer._name_\n",
    "\n",
    "        # Add parameters with special hyperparameters\n",
    "        hps = [getattr(p, \"_optim\") for p in all_params if hasattr(p, \"_optim\")]\n",
    "        hps = [\n",
    "            # dict(s) for s in set(frozenset(hp.items()) for hp in hps)\n",
    "            dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "            # dict(s) for s in dict.fromkeys(frozenset(hp.items()) for hp in hps)\n",
    "        ]  # Unique dicts\n",
    "        print(\"Hyperparameter groups\", hps)\n",
    "        for hp in hps:\n",
    "            params = [p for p in all_params if getattr(p, \"_optim\", None) == hp]\n",
    "            optimizer.add_param_group(\n",
    "                {\"params\": params, **self.hparams.optimizer, **hp}\n",
    "            )\n",
    "\n",
    "        ### Layer Decay ###\n",
    "\n",
    "        if self.hparams.train.layer_decay['_name_'] is not None:\n",
    "            get_num_layer = utils.instantiate(\n",
    "                registry.layer_decay,\n",
    "                self.hparams.train.layer_decay['_name_'],\n",
    "                partial=True,\n",
    "            )\n",
    "\n",
    "            # Go through all parameters and get num layer\n",
    "            layer_wise_groups = {}\n",
    "            num_max_layers = 0\n",
    "            for name, p in self.named_parameters():\n",
    "                # Get layer id for each parameter in the model\n",
    "                layer_id = get_num_layer(name)\n",
    "\n",
    "                # Add to layer wise group\n",
    "                if layer_id not in layer_wise_groups:\n",
    "                    layer_wise_groups[layer_id] = {\n",
    "                        'params': [],\n",
    "                        'lr': None,\n",
    "                        'weight_decay': self.hparams.optimizer.weight_decay\n",
    "                    }\n",
    "                layer_wise_groups[layer_id]['params'].append(p)\n",
    "\n",
    "                if layer_id > num_max_layers: num_max_layers = layer_id\n",
    "\n",
    "            # Update lr for each layer\n",
    "            for layer_id, group in layer_wise_groups.items():\n",
    "                group['lr'] = self.hparams.optimizer.lr * (self.hparams.train.layer_decay.decay ** (num_max_layers - layer_id))\n",
    "\n",
    "            # Reset the torch optimizer's param groups\n",
    "            optimizer.param_groups = []\n",
    "            for layer_id, group in layer_wise_groups.items():\n",
    "                optimizer.add_param_group(group)\n",
    "\n",
    "        # Print optimizer info for debugging\n",
    "        keys = set([k for hp in hps for k in hp.keys()])  # Special hparams\n",
    "        utils.train.log_optimizer(log, optimizer, keys)\n",
    "        # Configure scheduler\n",
    "        if \"scheduler\" not in self.hparams:\n",
    "            return optimizer\n",
    "        lr_scheduler = utils.instantiate(\n",
    "            registry.scheduler, self.hparams.scheduler, optimizer\n",
    "        )\n",
    "        scheduler = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": self.hparams.train.interval,  # 'epoch' or 'step'\n",
    "            \"monitor\": self.hparams.train.monitor,\n",
    "            \"name\": \"trainer/lr\",  # default is e.g. 'lr-AdamW'\n",
    "        }\n",
    "        # See documentation for how to configure the return\n",
    "        # https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.dataset.train_dataloader(**self.hparams.loader)\n",
    "\n",
    "    def _eval_dataloaders_names(self, loaders, prefix):\n",
    "        \"\"\"Process loaders into a list of names and loaders\"\"\"\n",
    "        if utils.is_dict(loaders):\n",
    "            return [\n",
    "                f\"{prefix}/{k}\" if k is not None else prefix for k in loaders.keys()\n",
    "            ], list(loaders.values())\n",
    "        elif utils.is_list(loaders):\n",
    "            return [f\"{prefix}/{i}\" for i in range(len(loaders))], loaders\n",
    "        else:\n",
    "            return [prefix], [loaders]\n",
    "\n",
    "    def _eval_dataloaders(self):\n",
    "        # Return all val + test loaders\n",
    "        val_loaders = self.dataset.val_dataloader(**self.hparams.loader)\n",
    "        test_loaders = self.dataset.test_dataloader(**self.hparams.loader)\n",
    "        val_loader_names, val_loaders = self._eval_dataloaders_names(val_loaders, \"val\")\n",
    "        test_loader_names, test_loaders = self._eval_dataloaders_names(\n",
    "            test_loaders, \"test\"\n",
    "        )\n",
    "\n",
    "        # Duplicate datasets for ema\n",
    "        if self.hparams.train.ema > 0.0:\n",
    "            val_loader_names += [name + \"/ema\" for name in val_loader_names]\n",
    "            val_loaders = val_loaders + val_loaders\n",
    "            test_loader_names += [name + \"/ema\" for name in test_loader_names]\n",
    "            test_loaders = test_loaders + test_loaders\n",
    "\n",
    "        # adding option to only have val loader at eval (eg if test is duplicate)\n",
    "        if self.hparams.train.get(\"remove_test_loader_in_eval\", False):\n",
    "            return val_loader_names, val_loaders\n",
    "        # adding option to only have test loader at eval\n",
    "        elif self.hparams.train.get(\"remove_val_loader_in_eval\", False):\n",
    "            return test_loader_names, test_loaders\n",
    "        # default behavior is to add test loaders in eval\n",
    "        else:\n",
    "            return val_loader_names + test_loader_names, val_loaders + test_loaders\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader_names, val_loaders = self._eval_dataloaders()\n",
    "        self.val_loader_names = val_loader_names\n",
    "        return val_loaders\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_loader_names, test_loaders = self._eval_dataloaders()\n",
    "        self.test_loader_names = [\"final/\" + name for name in test_loader_names]\n",
    "        return test_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SequenceLightningModule.__init__() got an unexpected keyword argument 'd_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/hyena/hyena-dna/hyenadna-medium-450k-seqlen/config.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequenceLightningModule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/leslie/sarthak/hyena/hyena-dna/hyenadna-medium-450k-seqlen/weights.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SequenceLightningModule.__init__() got an unexpected keyword argument 'd_model'"
     ]
    }
   ],
   "source": [
    "#now load the checkpoint using pytorch lightning\n",
    "import json\n",
    "#open checkpoint first\n",
    "with open('/data/leslie/sarthak/hyena/hyena-dna/hyenadna-medium-450k-seqlen/config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "model = SequenceLightningModule(**config)\n",
    "\n",
    "model = model.load_from_checkpoint('/data/leslie/sarthak/hyena/hyena-dna/hyenadna-medium-450k-seqlen/weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standalone hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchvision.ops import StochasticDepth\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Hyena layer\n",
    "\n",
    "\n",
    "def fftconv(u, k, D):\n",
    "    \"\"\"\n",
    "    We apply a convolution through the fourier domain (from the Convolution Theorem)\n",
    "\n",
    "    \"\"\"\n",
    "    seqlen = u.shape[-1]\n",
    "    fft_size = 2 * seqlen\n",
    "\n",
    "    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n",
    "    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n",
    "\n",
    "    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n",
    "    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n",
    "\n",
    "    out = y + u * D.unsqueeze(-1)\n",
    "    return out.to(dtype=u.dtype)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mul_sum(q, y):\n",
    "    return (q * y).sum(dim=1)\n",
    "\n",
    "class OptimModule(nn.Module):\n",
    "    \"\"\" Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters \"\"\"\n",
    "\n",
    "    def register(self, name, tensor, lr=None, wd=0.0):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            if wd is not None: optim[\"weight_decay\"] = wd\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"The Sin activation function for the Hyena Filter function.\"\"\"\n",
    "    def __init__(self, dim, w=10, train_freq=True):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.freq * x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(OptimModule):\n",
    "    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):\n",
    "        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        # The time embedding fed to the filteres is normalized so that t_f = 1\n",
    "        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n",
    "\n",
    "        if emb_dim > 1:\n",
    "            bands = (emb_dim - 1) // 2\n",
    "        # To compute the right embeddings we use the \"proper\" linspace\n",
    "        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n",
    "        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1\n",
    "\n",
    "        f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n",
    "        z = torch.exp(-1j * f * w)\n",
    "        z = torch.cat([t, z.real, z.imag], dim=-1)\n",
    "        self.register(\"z\", z, lr=lr_pos_emb)\n",
    "        self.register(\"t\", t, lr=0.0)\n",
    "\n",
    "    def forward(self, L):\n",
    "        return self.z[:, :L], self.t[:, :L]\n",
    "\n",
    "\n",
    "class ExponentialModulation(OptimModule):\n",
    "    \"\"\"The window function applied to the output of the (MLP) filter function.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        fast_decay_pct=0.3,\n",
    "        slow_decay_pct=1.5,\n",
    "        target=1e-2,\n",
    "        modulation_lr=0.0,\n",
    "        modulate: bool=True,\n",
    "        shift: float = 0.05,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.modulate = modulate\n",
    "        self.shift = shift\n",
    "        max_decay = math.log(target) / fast_decay_pct\n",
    "        min_decay = math.log(target) / slow_decay_pct\n",
    "        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n",
    "        self.register(\"deltas\", deltas, lr=modulation_lr)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.modulate:\n",
    "            decay = torch.exp(-t * self.deltas.abs())\n",
    "            x = x * (decay + self.shift)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HyenaFilter(OptimModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            emb_dim=3, # dim of input to MLP, augments with positional encoding\n",
    "            order=16, # width of the implicit MLP\n",
    "            fused_fft_conv=False,\n",
    "            seq_len=1024,\n",
    "            lr=1e-3,\n",
    "            lr_pos_emb=1e-5,\n",
    "            dropout=0.0,\n",
    "            w=1, # frequency of periodic activations\n",
    "            wd=0, # weight decay of kernel parameters\n",
    "            bias=True,\n",
    "            num_inner_mlps=2,\n",
    "            normalized=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Implicit long filter with modulation.\n",
    "\n",
    "        Args:\n",
    "            d_model: number of channels in the input\n",
    "            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n",
    "            order: width of the FFN\n",
    "            num_inner_mlps: number of inner linear layers inside filter MLP\n",
    "\n",
    "        Note:\n",
    "            filter_dropout is not implemented\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.use_bias = bias\n",
    "        self.fused_fft_conv = fused_fft_conv\n",
    "        self.bias = nn.Parameter(torch.randn(self.d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        act = Sin(dim=order, w=w)\n",
    "        self.emb_dim = emb_dim\n",
    "        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n",
    "\n",
    "        self.implicit_filter = nn.Sequential(\n",
    "            nn.Linear(emb_dim, order),\n",
    "            act,\n",
    "        )\n",
    "        for i in range(num_inner_mlps):\n",
    "            self.implicit_filter.append(nn.Linear(order, order))\n",
    "            self.implicit_filter.append(act)\n",
    "\n",
    "        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n",
    "\n",
    "        self.modulation = ExponentialModulation(d_model, **kwargs)\n",
    "\n",
    "        self.normalized = normalized\n",
    "        for c in self.implicit_filter.children():\n",
    "            for name, v in c.state_dict().items():\n",
    "                optim = {\"weight_decay\": wd, \"lr\": lr}\n",
    "                setattr(getattr(c, name), \"_optim\", optim)\n",
    "\n",
    "    def filter(self, L, *args, **kwargs):\n",
    "        z, t = self.pos_emb(L)\n",
    "        h = self.implicit_filter(z)\n",
    "        h = self.modulation(t, h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n",
    "        if k is None: k = self.filter(L)\n",
    "\n",
    "        # Ensure compatibility with filters that return a tuple\n",
    "        k = k[0] if type(k) is tuple else k\n",
    "\n",
    "        y = fftconv(x, k, bias)\n",
    "        return y\n",
    "\n",
    "\n",
    "class HyenaOperator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            l_max,\n",
    "            order=2,\n",
    "            filter_order=64,\n",
    "            dropout=0.0,\n",
    "            filter_dropout=0.0,\n",
    "            **filter_args,\n",
    "        ):\n",
    "        r\"\"\"\n",
    "        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the input and output embeddings (width of the layer)\n",
    "            l_max: (int): Maximum input sequence length. Defaults to None\n",
    "            order: (int): Depth of the Hyena recurrence. Defaults to 2\n",
    "            dropout: (float): Dropout probability. Defaults to 0.0\n",
    "            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.l_max = l_max\n",
    "        self.order = order\n",
    "        inner_width = d_model * (order + 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.in_proj = nn.Linear(d_model, inner_width)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.short_filter = nn.Conv1d(\n",
    "            inner_width,\n",
    "            inner_width,\n",
    "            3,\n",
    "            padding=2,\n",
    "            groups=inner_width\n",
    "        )\n",
    "        self.filter_fn = HyenaFilter(\n",
    "            d_model * (order - 1),\n",
    "            order=filter_order,\n",
    "            seq_len=l_max,\n",
    "            channels=1,\n",
    "            dropout=filter_dropout,\n",
    "            **filter_args\n",
    "        )\n",
    "\n",
    "    def forward(self, u, *args, **kwargs):\n",
    "        l = u.size(-2)\n",
    "        l_filter = min(l, self.l_max)\n",
    "        u = self.in_proj(u)\n",
    "        u = rearrange(u, 'b l d -> b d l')\n",
    "\n",
    "        uc = self.short_filter(u)[...,:l_filter]\n",
    "        *x, v = uc.split(self.d_model, dim=1)\n",
    "\n",
    "        k = self.filter_fn.filter(l_filter)[0]\n",
    "        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)\n",
    "        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)\n",
    "\n",
    "        for o, x_i in enumerate(reversed(x[1:])):\n",
    "            v = self.dropout(v * x_i)\n",
    "            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])\n",
    "\n",
    "        y = rearrange(v * x[0], 'b d l -> b l d')\n",
    "\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "#@title Self-Attention (alternative)\n",
    "\n",
    "\"\"\"\n",
    "If you'd like to try the HyenaDNA model using attention instead, you can. ie,\n",
    "use a regular decoder only Transformer.\n",
    "\n",
    "Borrowed from the FlashAttention library by Tri Dao.\n",
    "\"\"\"\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Implement the scaled dot product attention with softmax.\n",
    "    Arguments\n",
    "    ---------\n",
    "        softmax_scale: The temperature to use for the softmax attention.\n",
    "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
    "                      runtime)\n",
    "        attention_dropout: The dropout rate to apply to the attention\n",
    "                           (default: 0.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.dropout_p = attention_dropout\n",
    "\n",
    "    def forward(self, qkv, causal=None, key_padding_mask=None):\n",
    "        \"\"\"Implements the multihead softmax attention.\n",
    "        Arguments\n",
    "        ---------\n",
    "            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)\n",
    "            causal: if passed, will override self.causal\n",
    "            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,\n",
    "                False means to mask out. (B, S)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        causal = self.causal if causal is None else causal\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype,\n",
    "                                      device=scores.device)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, 'b s -> b 1 1 s')\n",
    "        if causal:\n",
    "            # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "            # So we have to construct the mask in float\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
    "        attention_drop = F.dropout(attention, self.dropout_p if self.training else 0.0)\n",
    "        output = torch.einsum('bhts,bshd->bthd', attention_drop, v)\n",
    "        return output\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi-head self-attention and cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, bias=True, dropout=0.0,\n",
    "                 softmax_scale=None, causal=False, layer_idx=None, dwconv=False,return_residual=False,device=None, dtype=None) -> None:\n",
    "        \"\"\"\n",
    "            return_residual: whether to return the input x along with the output. This is for\n",
    "                performance reason: for post-norm architecture, returning the input allows us\n",
    "                to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.causal = causal\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dwconv = dwconv\n",
    "        self.return_residual = return_residual\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embed_dim // num_heads\n",
    "\n",
    "        linear_cls = nn.Linear\n",
    "        linear_resid_cls = LinearResidual\n",
    "        inner_attn_cls =  SelfAttention\n",
    "\n",
    "        if not self.return_residual:\n",
    "            self.Wqkv = linear_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        else:\n",
    "            self.Wqkv = linear_resid_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        if self.dwconv:\n",
    "            self.dwconv_qkv = nn.Conv1d(3 * embed_dim, 3 * embed_dim, kernel_size=3, padding=2,\n",
    "                                        groups=3 * embed_dim)\n",
    "\n",
    "        self.inner_attn = inner_attn_cls(causal=causal, softmax_scale=softmax_scale,\n",
    "                                         attention_dropout=dropout)\n",
    "\n",
    "        # output projection always have the bias (for now)\n",
    "        self.out_proj = linear_cls(embed_dim, embed_dim, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
    "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
    "                is the is the sum of the sequence lengths in the batch.\n",
    "            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
    "                of the sequences in the batch, used to index into x. Only applicable when using\n",
    "                FlashAttention.\n",
    "            max_seqlen: int. Maximum sequence length in the batch.\n",
    "            key_padding_mask: boolean mask, True means to keep, False means to mask out.\n",
    "                (batch, seqlen). Only applicable when not using FlashAttention.\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
    "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs = ({'key_padding_mask': key_padding_mask, **kwargs})\n",
    "\n",
    "        if not self.return_residual:\n",
    "            qkv = self.Wqkv(x)\n",
    "        else:\n",
    "            qkv, x = self.Wqkv(x)\n",
    "        if self.dwconv:\n",
    "            qkv = rearrange(self.dwconv_qkv(rearrange(qkv, 'b s d -> b d s'))[..., :-2],\n",
    "                            'b d s -> b s d').contiguous()\n",
    "        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, d=self.head_dim)\n",
    "\n",
    "        context = self.inner_attn(qkv, **kwargs)\n",
    "\n",
    "        out = self.out_proj(rearrange(context, '... h d -> ... (h d)'))\n",
    "        return out if not self.return_residual else (out, x)\n",
    "\n",
    "#@title MLP layer\n",
    "\n",
    "\"\"\"\n",
    "The MLP layer after the mixer layer (HyenaOperator).\n",
    "\"\"\"\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,\n",
    "                 return_residual=False, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.return_residual = return_residual\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.fc2(y)\n",
    "        return y if not self.return_residual else (y, x)\n",
    "\n",
    "#@title Block layer (Hyena + MLP layers)\n",
    "\n",
    "\"\"\"\n",
    "A block consists of a Mixer layer (Hyena or attention), and a MLP layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LinearResidual(nn.Linear):\n",
    "    \"\"\"Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(input), input\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,\n",
    "                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,\n",
    "                 drop_path1=0., drop_path2=0.,\n",
    "                 return_residual=False,\n",
    "                 residual_in_fp32=False):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py\n",
    "        For prenorm=True, this Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both\n",
    "        the hidden_states (output of the MLP) and the residual.\n",
    "        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        For prenorm=False, this Block has the same structure as a regular postnorm Transformer\n",
    "        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.\n",
    "        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.\n",
    "        This is for performance reason: for post-norm architecture, returning the input allows us\n",
    "        to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prenorm = prenorm\n",
    "        self.return_residual = return_residual\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        if self.residual_in_fp32:\n",
    "            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'\n",
    "        if mixer_cls is None:\n",
    "            mixer_cls = partial(MHA, num_heads=dim // 64)\n",
    "        if mlp_cls is None:\n",
    "            mlp_cls = partial(Mlp, hidden_features=4 * dim)\n",
    "        self.mixer = mixer_cls()\n",
    "        self.dropout1 = dropout_cls(resid_dropout1)\n",
    "        self.drop_path1 = StochasticDepth(drop_path1, mode='row')\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.mlp = mlp_cls(dim)\n",
    "        if not isinstance(self.mlp, nn.Identity):\n",
    "            self.dropout2 = dropout_cls(resid_dropout2)\n",
    "            self.drop_path2 = StochasticDepth(drop_path2, mode='row')\n",
    "            self.norm2 = norm_cls(dim)\n",
    "\n",
    "    def forward(self, hidden_states, residual = None,\n",
    "                mixer_subset=None, mixer_kwargs=None):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "        \"\"\"\n",
    "        if self.prenorm:\n",
    "            dropped = self.drop_path1(self.dropout1(hidden_states))\n",
    "            residual = (dropped + residual) if residual is not None else dropped\n",
    "            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "            if mixer_kwargs is None:\n",
    "                mixer_kwargs = {}\n",
    "            if mixer_subset is not None:\n",
    "                mixer_kwargs['mixer_subset'] = mixer_subset\n",
    "            hidden_states = self.mixer(hidden_states, **mixer_kwargs)\n",
    "            if mixer_subset is not None:\n",
    "                residual = residual[:, mixer_subset]\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                dropped = self.drop_path2(self.dropout2(hidden_states))\n",
    "                residual = (dropped + residual) if residual is not None else dropped\n",
    "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
    "                if self.residual_in_fp32:\n",
    "                    residual = residual.to(torch.float32)\n",
    "\n",
    "                hidden_states = self.mlp(hidden_states)\n",
    "            return hidden_states, residual\n",
    "        else:\n",
    "            assert residual is None\n",
    "            mixer_out = self.mixer(\n",
    "                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})\n",
    "            )\n",
    "            if self.return_residual:  # mixer out is actually a pair here\n",
    "                mixer_out, hidden_states = mixer_out\n",
    "\n",
    "            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))\n",
    "                                        + hidden_states).to(dtype=self.norm1.weight.dtype))\n",
    "\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                mlp_out = self.mlp(hidden_states)\n",
    "                if self.return_residual:  # mlp out is actually a pair here\n",
    "                    mlp_out, hidden_states = mlp_out\n",
    "\n",
    "                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))\n",
    "                                            + hidden_states).to(dtype=self.norm2.weight.dtype))\n",
    "\n",
    "            return hidden_states\n",
    "\n",
    "def create_mixer_cls(layer=None,\n",
    "                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,\n",
    "                     device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    if attn_layer_idx is not None and layer_idx in attn_layer_idx:\n",
    "        causal = True if attn_cfg is None else attn_cfg.pop('causal', True)\n",
    "\n",
    "        mha_cls = MHA\n",
    "\n",
    "        mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,\n",
    "                            **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)\n",
    "    else:\n",
    "        # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)\n",
    "\n",
    "        mixer_cls = partial(HyenaOperator, **layer)\n",
    "\n",
    "    return mixer_cls\n",
    "\n",
    "def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    inner_dim = d_inner if d_inner is not None else 4 * d_model\n",
    "\n",
    "    mlp_cls = partial(Mlp, hidden_features=inner_dim,\n",
    "                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)\n",
    "\n",
    "    return mlp_cls\n",
    "\n",
    "\n",
    "def create_block(d_model, d_inner=None,\n",
    "                 layer=None, attn_layer_idx=None,\n",
    "                 attn_cfg=None, layer_norm_epsilon=1e-5,\n",
    "                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,\n",
    "                 layer_idx=None,\n",
    "                 device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    mixer_cls = create_mixer_cls(layer=layer,\n",
    "                                 attn_layer_idx=attn_layer_idx,\n",
    "                                 attn_cfg=attn_cfg, layer_idx=layer_idx,\n",
    "                                 **factory_kwargs)\n",
    "    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,\n",
    "                             **factory_kwargs)\n",
    "    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,\n",
    "                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)\n",
    "    block.layer_idx = layer_idx\n",
    "    return block\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,\n",
    "                  glu_act=False):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "            # If using GLU activation for now, we scale the std by 2\n",
    "            elif name in [\"output_linear.0.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                if not glu_act:\n",
    "                    nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    out_features = p.shape[0]\n",
    "                    # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5\n",
    "                    # on average.\n",
    "                    nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)\n",
    "\n",
    "\n",
    "\n",
    "#@title Backbone model (stack of blocks)\n",
    "\n",
    "\"\"\"\n",
    "A backbone model consists of a stack of blocks. If you use attention, then\n",
    "positional embeddings are included. When using Hyena, then the pos emb\n",
    "revert to doing nothing.\n",
    "\"\"\"\n",
    "\n",
    "class GPT2Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,\n",
    "                 word_embed_proj_dim=None, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "            If max_position_embeddings <= 0, there's no position embeddings\n",
    "            If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n",
    "                the project up to embed_dim\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if word_embed_proj_dim is None:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,\n",
    "                                                **factory_kwargs)\n",
    "            self.project_in = None\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,\n",
    "                                                padding_idx=padding_idx, **factory_kwargs)\n",
    "            self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,\n",
    "                                        **factory_kwargs)\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        if self.max_position_embeddings > 0:\n",
    "            self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,\n",
    "                                                    **factory_kwargs)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: (batch, seqlen)\n",
    "            position_ids: (batch, seqlen)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = input_ids.shape\n",
    "        embeddings = self.word_embeddings(input_ids)\n",
    "        if self.project_in is not None:\n",
    "            embeddings = self.project_in(embeddings)\n",
    "        if self.max_position_embeddings > 0:\n",
    "            if position_ids is None:\n",
    "                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings = embeddings + position_embeddings\n",
    "        return embeddings\n",
    "\n",
    "class LMBackbone(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 process_group=None, layer=None,\n",
    "                 attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.process_group = process_group\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        # note max_position_embeddings is 0 for Hyena, and therefore isn't used\n",
    "        self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,\n",
    "                                             **factory_kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList([create_block(\n",
    "            d_model, d_inner=d_inner,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx,\n",
    "            attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,\n",
    "            resid_dropout1=embed_dropout if i == 0 else resid_dropout,\n",
    "            resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,\n",
    "            **factory_kwargs,\n",
    "        ) for i in range(n_layer)])\n",
    "\n",
    "        self.drop_f = nn.Dropout(resid_dropout)\n",
    "        self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        hidden_states = self.embeddings(input_ids, position_ids=position_ids,)\n",
    "        residual = None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(hidden_states, residual)\n",
    "\n",
    "        dropped = self.drop_f(hidden_states)\n",
    "        residual = (dropped + residual) if residual is not None else dropped\n",
    "        hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "#@title Decoder head layer\n",
    "\n",
    "\"\"\"\n",
    "A simple decoder head (using MLP) to predict a sequence level classification.\n",
    "You have the option to average across all the tokens in a sequence or using the\n",
    "\"last\" token to classify.  At least, those 2 worked best for us, but we provide\n",
    "other \"modes\" as well.\n",
    "\n",
    "We only need this for classification.  Otherwise we'll use the hidden\n",
    "states of the backbone as embeddings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SequenceDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, d_output=None, l_output=None, use_lengths=False, mode=\"last\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)\n",
    "\n",
    "        if l_output is None:\n",
    "            self.l_output = None\n",
    "            self.squeeze = False\n",
    "        elif l_output == 0:\n",
    "            # Equivalent to getting an output of length 1 and then squeezing\n",
    "            self.l_output = 1\n",
    "            self.squeeze = True\n",
    "        else:\n",
    "            assert l_output > 0\n",
    "            self.l_output = l_output\n",
    "            self.squeeze = False\n",
    "\n",
    "        self.use_lengths = use_lengths\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'ragged':\n",
    "            assert not use_lengths\n",
    "\n",
    "    def forward(self, x, state=None, lengths=None, l_output=None):\n",
    "        \"\"\"\n",
    "        x: (n_batch, l_seq, d_model)\n",
    "        Returns: (n_batch, l_output, d_output)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.l_output is None:\n",
    "            if l_output is not None:\n",
    "                assert isinstance(l_output, int)  # Override by pass in\n",
    "            else:\n",
    "                # Grab entire output\n",
    "                l_output = x.size(-2)\n",
    "            squeeze = False\n",
    "        else:\n",
    "            l_output = self.l_output\n",
    "            squeeze = self.squeeze\n",
    "\n",
    "        if self.mode == \"last\":\n",
    "            restrict = lambda x: x[..., -l_output:, :]\n",
    "        elif self.mode == \"first\":\n",
    "            restrict = lambda x: x[..., :l_output, :]\n",
    "        elif self.mode == \"pool\":\n",
    "            restrict = lambda x: (\n",
    "                torch.cumsum(x, dim=-2)\n",
    "                / torch.arange(\n",
    "                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                ).unsqueeze(-1)\n",
    "            )[..., -l_output:, :]\n",
    "\n",
    "            def restrict(x):\n",
    "                L = x.size(-2)\n",
    "                s = x.sum(dim=-2, keepdim=True)\n",
    "                if l_output > 1:\n",
    "                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n",
    "                    c = F.pad(c, (0, 0, 1, 0))\n",
    "                    s = s - c  # (B, l_output, D)\n",
    "                    s = s.flip(-2)\n",
    "                denom = torch.arange(\n",
    "                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n",
    "                )\n",
    "                s = s / denom\n",
    "                return s\n",
    "\n",
    "        elif self.mode == \"sum\":\n",
    "            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n",
    "            # TODO use same restrict function as pool case\n",
    "        elif self.mode == 'ragged':\n",
    "            assert lengths is not None, \"lengths must be provided for ragged mode\"\n",
    "            # remove any additional padding (beyond max length of any sequence in the batch)\n",
    "            restrict = lambda x: x[..., : max(lengths), :]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Mode must be ['last' | 'first' | 'pool' | 'sum']\"\n",
    "            )\n",
    "\n",
    "        # Restrict to actual length of sequence\n",
    "        if self.use_lengths:\n",
    "            assert lengths is not None\n",
    "            x = torch.stack(\n",
    "                [\n",
    "                    restrict(out[..., :length, :])\n",
    "                    for out, length in zip(torch.unbind(x, dim=0), lengths)\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            x = restrict(x)\n",
    "\n",
    "        if squeeze:\n",
    "            assert x.size(-2) == 1\n",
    "            x = x.squeeze(-2)\n",
    "\n",
    "        x = self.output_transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def step(self, x, state=None):\n",
    "        # Ignore all length logic\n",
    "        return self.output_transform(x)\n",
    "\n",
    "#@title Model (backbone + head)\n",
    "\n",
    "\"\"\"\n",
    "Putting it all together, the model consists of a backbone model\n",
    "and a decoder head (you can turn off head for embeddings only too).\n",
    "\n",
    "Here we use a simple head to do multi-classification, but\n",
    "can also swap the head to do next token prediction too.  We defer to the main\n",
    "HyenaDNA for that code, since pretraining with next token prediction isn't quite\n",
    "feasible on colab.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # we only need a head if doing classification, otherwise we'll use the\n",
    "        # hidden states as embeddings\n",
    "        if self.use_head:\n",
    "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "        # if self.use_head:\n",
    "        #     self.tie_weights()\n",
    "\n",
    "    # def tie_weights(self):\n",
    "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "    \n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n",
    "\n",
    "\n",
    "# Data pipeline\n",
    "\n",
    "\n",
    "\n",
    "#@title Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "    \n",
    "#@title GenomicBenchmark dataset\n",
    "\n",
    "\"\"\"\n",
    "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
    "There are 8 datasets to choose from.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "# augmentation\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "\n",
    "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
    "    Returns a generator that retrieves the sequence.\n",
    "\n",
    "    Genomic Benchmarks Dataset, from:\n",
    "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        max_length,\n",
    "        dataset_name='human_enhancers_cohn',\n",
    "        d_output=2, # default binary classification\n",
    "        dest_path=\"/content\", # default for colab\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        use_padding=None,\n",
    "        add_eos=False,\n",
    "        rc_aug=False,\n",
    "        return_augs=False,\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.d_output = d_output  # needed for decoder to grab\n",
    "        self.rc_aug = rc_aug\n",
    "\n",
    "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
    "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
    "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
    "        else:\n",
    "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
    "\n",
    "        # use Path object\n",
    "        base_path = Path(dest_path) / dataset_name / split\n",
    "\n",
    "        self.all_paths = []\n",
    "        self.all_labels = []\n",
    "        label_mapper = {}\n",
    "\n",
    "        for i, x in enumerate(base_path.iterdir()):\n",
    "            label_mapper[x.stem] = i\n",
    "\n",
    "        for label_type in label_mapper.keys():\n",
    "            for x in (base_path / label_type).iterdir():\n",
    "                self.all_paths.append(x)\n",
    "                self.all_labels.append(label_mapper[label_type])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_path = self.all_paths[idx]\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        x = content\n",
    "        y = self.all_labels[idx]\n",
    "\n",
    "        # apply rc_aug here if using\n",
    "        if self.rc_aug and coin_flip():\n",
    "            x = string_reverse_complement(x)\n",
    "\n",
    "        seq = self.tokenizer(x,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\" if self.use_padding else None,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )  # add cls and eos token (+2)\n",
    "        seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        # need to handle eos here\n",
    "        if self.add_eos:\n",
    "            # append list seems to be faster than append tensor\n",
    "            seq.append(self.tokenizer.sep_token_id)\n",
    "\n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)\n",
    "\n",
    "        # need to wrap in list\n",
    "        target = torch.LongTensor([y])\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\"\"\"\n",
    "We provide simple training code for the GenomicBenchmark datasets.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, loss_fn, log_interval=10):\n",
    "    \"\"\"Training loop.\"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader, loss_fn):\n",
    "    \"\"\"Test loop.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target.squeeze()).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "\n",
    "def run_train():\n",
    "\n",
    "    '''\n",
    "    Main entry point for training.  Select the dataset name and metadata, as\n",
    "    well as model and training args, and you're off to the genomic races!\n",
    "\n",
    "    ### GenomicBenchmarks Metadata\n",
    "    # there are 8 datasets in this suite, choose 1 at a time, with their corresponding settings\n",
    "    # name                                num_seqs        num_classes     median len    std\n",
    "    # dummy_mouse_enhancers_ensembl       1210            2               2381          984.4\n",
    "    # demo_coding_vs_intergenomic_seqs    100_000         2               200           0\n",
    "    # demo_human_or_worm                  100_000         2               200           0\n",
    "    # human_enhancers_cohn                27791           2               500           0\n",
    "    # human_enhancers_ensembl             154842          2               269           122.6\n",
    "    # human_ensembl_regulatory            289061          3               401           184.3\n",
    "    # human_nontata_promoters             36131           2               251           0\n",
    "    # human_ocr_ensembl                   174756          2               315           108.1\n",
    "\n",
    "    '''\n",
    "    # experiment settings:\n",
    "    num_epochs = 100  # ~100 seems fine\n",
    "    max_length = 500  # max len of sequence of dataset (of what you want)\n",
    "    use_padding = True\n",
    "    dataset_name = 'human_enhancers_cohn'\n",
    "    batch_size = 256\n",
    "    learning_rate = 6e-4  # good default for Hyena\n",
    "    rc_aug = True  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "    weight_decay = 0.1\n",
    "\n",
    "    # for fine-tuning, only the 'tiny' model can fit on colab\n",
    "    pretrained_model_name = 'hyenadna-tiny-1k-seqlen'  # use None if training from scratch\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = True\n",
    "    n_classes = 2\n",
    "\n",
    "    # you can override with your own backbone config here if you want,\n",
    "    # otherwise we'll load the HF one by default\n",
    "    backbone_cfg = None\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # instantiate the model (pretrained here)\n",
    "    if pretrained_model_name in ['hyenadna-tiny-1k-seqlen']:\n",
    "        # use the pretrained Huggingface wrapper instead\n",
    "        model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "            './checkpoints',\n",
    "            pretrained_model_name,\n",
    "            download=True,\n",
    "            config=backbone_cfg,\n",
    "            device=device,\n",
    "            use_head=use_head,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "\n",
    "    # from scratch\n",
    "    else:\n",
    "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n",
    "\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    # create datasets\n",
    "    ds_train = GenomicBenchmarkDataset(\n",
    "        max_length = max_length,\n",
    "        use_padding = use_padding,\n",
    "        split = 'train',\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=dataset_name,\n",
    "        rc_aug=rc_aug,\n",
    "        add_eos=add_eos,\n",
    "    )\n",
    "\n",
    "    ds_test = GenomicBenchmarkDataset(\n",
    "        max_length = max_length,\n",
    "        use_padding = use_padding,\n",
    "        split = 'test',\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=dataset_name,\n",
    "        rc_aug=rc_aug,\n",
    "        add_eos=add_eos,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # create optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, device, train_loader, optimizer, epoch, loss_fn)\n",
    "        test(model, device, test_loader, loss_fn)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Single example\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "\n",
    "def inference_single():\n",
    "\n",
    "    '''\n",
    "    this selects which backbone to use, and grabs weights/ config from HF\n",
    "    4 options:\n",
    "      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok\n",
    "      'hyenadna-small-32k-seqlen'\n",
    "      'hyenadna-medium-160k-seqlen'  # inference only on colab\n",
    "      'hyenadna-medium-450k-seqlen'  # inference only on colab\n",
    "      'hyenadna-large-1m-seqlen'  # inference only on colab\n",
    "    '''\n",
    "\n",
    "    # you only need to select which model to use here, we'll do the rest!\n",
    "    pretrained_model_name = 'hyenadna-small-32k-seqlen'\n",
    "\n",
    "    max_lengths = {\n",
    "        'hyenadna-tiny-1k-seqlen': 1024,\n",
    "        'hyenadna-small-32k-seqlen': 32768,\n",
    "        'hyenadna-medium-160k-seqlen': 160000,\n",
    "        'hyenadna-medium-450k-seqlen': 450000,  # T4 up to here\n",
    "        'hyenadna-large-1m-seqlen': 1_000_000,  # only A100 (paid tier)\n",
    "    }\n",
    "\n",
    "    max_length = max_lengths[pretrained_model_name]  # auto selects\n",
    "\n",
    "    # data settings:\n",
    "    use_padding = True\n",
    "    rc_aug = False  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = False\n",
    "    n_classes = 2  # not used for embeddings only\n",
    "\n",
    "    # you can override with your own backbone config here if you want,\n",
    "    # otherwise we'll load the HF one in None\n",
    "    backbone_cfg = None\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # instantiate the model (pretrained here)\n",
    "    if pretrained_model_name in ['hyenadna-tiny-1k-seqlen',\n",
    "                                 'hyenadna-small-32k-seqlen',\n",
    "                                 'hyenadna-medium-160k-seqlen',\n",
    "                                 'hyenadna-medium-450k-seqlen',\n",
    "                                 'hyenadna-large-1m-seqlen']:\n",
    "        # use the pretrained Huggingface wrapper instead\n",
    "        model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "            './checkpoints',\n",
    "            pretrained_model_name,\n",
    "            download=True,\n",
    "            config=backbone_cfg,\n",
    "            device=device,\n",
    "            use_head=use_head,\n",
    "            n_classes=n_classes,\n",
    "        )\n",
    "\n",
    "    # from scratch\n",
    "    elif pretrained_model_name is None:\n",
    "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n",
    "\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    #### Single embedding example ####\n",
    "\n",
    "    # create a sample 450k long, prepare\n",
    "    sequence = 'ACTG' * int(max_length/4)\n",
    "    tok_seq = tokenizer(sequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    # prep model and forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    print(embeddings.shape)  # embeddings here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standalone with hugging face instead\n",
    "\n",
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # we only need a head if doing classification, otherwise we'll use the\n",
    "        # hidden states as embeddings\n",
    "        if self.use_head:\n",
    "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "        # if self.use_head:\n",
    "        #     self.tie_weights()\n",
    "\n",
    "    # def tie_weights(self):\n",
    "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json'))) #defaults to the config in the same folder\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "#now load the model\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-medium-450k-seqlen', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyenaDNAModel(\n",
       "  (backbone): LMBackbone(\n",
       "    (embeddings): GPT2Embeddings(\n",
       "      (word_embeddings): Embedding(16, 256)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-7): 7 x Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (drop_f): Dropout(p=0.0, inplace=False)\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 12 15:18:20 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.60.13    Driver Version: 525.60.13    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA A40          On   | 00000000:65:00.0 Off |                    0 |\n",
      "|  0%   31C    P0    76W / 300W |    935MiB / 46068MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     38104      C   ...ents/hyena-dna/bin/python      932MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 112610688 bytes, 107.39 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def model_size_in_bytes(model):\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        total_size += param.nelement() * param.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        total_size += buffer.nelement() * buffer.element_size()\n",
    "    return total_size\n",
    "\n",
    "# Assuming 'model' is your PyTorch Lightning model\n",
    "size_bytes = model_size_in_bytes(model)\n",
    "size_megabytes = size_bytes / (1024 ** 2)  # Convert to megabytes\n",
    "\n",
    "print(f\"Model size: {size_bytes} bytes, {size_megabytes:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 450002, 256])\n"
     ]
    }
   ],
   "source": [
    "#now we will try to see if we can do inference\n",
    "\n",
    "# torch.cuda.is_available()\n",
    "\n",
    "def inference(model):\n",
    "    #define some things\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_length = 450000\n",
    "    use_padding = False\n",
    "    rc_aug = False  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = False\n",
    "    n_classes = 2  # not used for embeddings only\n",
    "    backbone_cfg = None\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    #### Single embedding example ####\n",
    "\n",
    "    # create a sample 450k long, prepare\n",
    "    sequence = 'ACTG' * int(max_length/4)\n",
    "    tok_seq = tokenizer(sequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    # prep model and forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    print(embeddings.shape)  # embeddings here!\n",
    "    return embeddings, tok_seq, tokenizer\n",
    "\n",
    "embed,seq,tokenizer = inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  7,  8,  ..., 10,  9,  1], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.shape\n",
    "seq[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 7, 1], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [0, 6, 1], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [0, 8, 1], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}\n",
      "{'input_ids': [0, 7, 6, 8, 1], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "{'input_ids': [0, 7, 8, 9, 10, 8, 9, 7, 10, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('A'))\n",
    "print(tokenizer('B'))\n",
    "print(tokenizer('C'))\n",
    "print(tokenizer('ABC'))\n",
    "#does each token at a time, and puts a 0 at the beginning for bos token and 1 at the end for eos token\n",
    "print(tokenizer('ACGTCGAT'))\n",
    "#pays attention to all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "#repeat but say use head = True\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-medium-450k-seqlen', device='cuda', use_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HyenaDNAModel(\n",
       "  (backbone): LMBackbone(\n",
       "    (embeddings): GPT2Embeddings(\n",
       "      (word_embeddings): Embedding(16, 256)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-7): 7 x Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (drop_f): Dropout(p=0.0, inplace=False)\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): SequenceDecoder(\n",
       "    (output_transform): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def model_size_in_bytes(model):\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        total_size += param.nelement() * param.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        total_size += buffer.nelement() * buffer.element_size()\n",
    "    return total_size\n",
    "\n",
    "# Assuming 'model' is your PyTorch Lightning model\n",
    "size_bytes = model_size_in_bytes(model)\n",
    "size_megabytes = size_bytes / (1024 ** 2)  # Convert to megabytes\n",
    "\n",
    "print(f\"Model size: {size_bytes} bytes, {size_megabytes:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacty of 10.75 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 7.41 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 452.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# embeddings here!\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, tok_seq, tokenizer\n\u001b[0;32m---> 41\u001b[0m embed,seq,tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36minference\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 36\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(embeddings\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# embeddings here!\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings, tok_seq, tokenizer\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mHyenaDNAModel.forward\u001b[0;34m(self, input_ids, position_ids, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m): \u001b[38;5;66;03m# state for the repo interface\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_head:\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 690\u001b[0m, in \u001b[0;36mLMBackbone.forward\u001b[0;34m(self, input_ids, position_ids)\u001b[0m\n\u001b[1;32m    687\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 690\u001b[0m     hidden_states, residual \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_f(hidden_states)\n\u001b[1;32m    693\u001b[0m residual \u001b[38;5;241m=\u001b[39m (dropped \u001b[38;5;241m+\u001b[39m residual) \u001b[38;5;28;01mif\u001b[39;00m residual \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dropped\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 490\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, hidden_states, residual, mixer_subset, mixer_kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixer_subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     mixer_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixer_subset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mixer_subset\n\u001b[0;32m--> 490\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmixer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mixer_subset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    492\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual[:, mixer_subset]\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 247\u001b[0m, in \u001b[0;36mHyenaOperator.forward\u001b[0;34m(self, u, *args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o, x_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(x[\u001b[38;5;241m1\u001b[39m:])):\n\u001b[1;32m    246\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(v \u001b[38;5;241m*\u001b[39m x_i)\n\u001b[0;32m--> 247\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[43mo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m y \u001b[38;5;241m=\u001b[39m rearrange(v \u001b[38;5;241m*\u001b[39m x[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb d l -> b l d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    251\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(y)\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena-dna/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 181\u001b[0m, in \u001b[0;36mHyenaFilter.forward\u001b[0;34m(self, x, L, k, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Ensure compatibility with filters that return a tuple\u001b[39;00m\n\u001b[1;32m    179\u001b[0m k \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(k) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m k\n\u001b[0;32m--> 181\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mfftconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mfftconv\u001b[0;34m(u, k, D)\u001b[0m\n\u001b[1;32m      9\u001b[0m seqlen \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m fft_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m seqlen\n\u001b[0;32m---> 12\u001b[0m k_f \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfft_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m fft_size\n\u001b[1;32m     13\u001b[0m u_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mrfft(u\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m.\u001b[39mdtype), n\u001b[38;5;241m=\u001b[39mfft_size)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m: k_f \u001b[38;5;241m=\u001b[39m k_f\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacty of 10.75 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 7.41 GiB memory in use. Of the allocated memory 6.13 GiB is allocated by PyTorch, and 452.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def inference(model):\n",
    "    #define some things\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    max_length = 450000\n",
    "    use_padding = False\n",
    "    rc_aug = False  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = False\n",
    "    n_classes = 2  # not used for embeddings only\n",
    "    backbone_cfg = None\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    #### Single embedding example ####\n",
    "\n",
    "    # create a sample 450k long, prepare\n",
    "    sequence = 'ACTG' * int(max_length/4)\n",
    "    tok_seq = tokenizer(sequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    # prep model and forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    print(embeddings.shape)  # embeddings here!\n",
    "    return embeddings, tok_seq, tokenizer\n",
    "\n",
    "embed,seq,tokenizer = inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-tiny-1k-seqlen', device='cuda', use_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436354\n",
      "436354\n"
     ]
    }
   ],
   "source": [
    "#now find number of parameters of the pytorch lightning model\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "6551042\n",
      "6551042\n"
     ]
    }
   ],
   "source": [
    "#again but for the 450k model\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-medium-450k-seqlen', device='cuda', use_head=True)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HyenaDNAModel(\n",
      "  (backbone): LMBackbone(\n",
      "    (embeddings): GPT2Embeddings(\n",
      "      (word_embeddings): Embedding(16, 256)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1-7): 7 x Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (drop_f): Dropout(p=0.0, inplace=False)\n",
      "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): SequenceDecoder(\n",
      "    (output_transform): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "+------------------------------------------------------------+------------+\n",
      "|                          Modules                           | Parameters |\n",
      "+------------------------------------------------------------+------------+\n",
      "|         backbone.embeddings.word_embeddings.weight         |    2048    |\n",
      "|           backbone.layers.0.mixer.in_proj.weight           |   49152    |\n",
      "|            backbone.layers.0.mixer.in_proj.bias            |    384     |\n",
      "|          backbone.layers.0.mixer.out_proj.weight           |   16384    |\n",
      "|           backbone.layers.0.mixer.out_proj.bias            |    128     |\n",
      "|        backbone.layers.0.mixer.short_filter.weight         |    1152    |\n",
      "|         backbone.layers.0.mixer.short_filter.bias          |    384     |\n",
      "|           backbone.layers.0.mixer.filter_fn.bias           |    128     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight |    8192    |\n",
      "|               backbone.layers.0.norm1.weight               |    128     |\n",
      "|                backbone.layers.0.norm1.bias                |    128     |\n",
      "|              backbone.layers.0.mlp.fc1.weight              |   65536    |\n",
      "|               backbone.layers.0.mlp.fc1.bias               |    512     |\n",
      "|              backbone.layers.0.mlp.fc2.weight              |   65536    |\n",
      "|               backbone.layers.0.mlp.fc2.bias               |    128     |\n",
      "|               backbone.layers.0.norm2.weight               |    128     |\n",
      "|                backbone.layers.0.norm2.bias                |    128     |\n",
      "|           backbone.layers.1.mixer.in_proj.weight           |   49152    |\n",
      "|            backbone.layers.1.mixer.in_proj.bias            |    384     |\n",
      "|          backbone.layers.1.mixer.out_proj.weight           |   16384    |\n",
      "|           backbone.layers.1.mixer.out_proj.bias            |    128     |\n",
      "|        backbone.layers.1.mixer.short_filter.weight         |    1152    |\n",
      "|         backbone.layers.1.mixer.short_filter.bias          |    384     |\n",
      "|           backbone.layers.1.mixer.filter_fn.bias           |    128     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight |    8192    |\n",
      "|               backbone.layers.1.norm1.weight               |    128     |\n",
      "|                backbone.layers.1.norm1.bias                |    128     |\n",
      "|              backbone.layers.1.mlp.fc1.weight              |   65536    |\n",
      "|               backbone.layers.1.mlp.fc1.bias               |    512     |\n",
      "|              backbone.layers.1.mlp.fc2.weight              |   65536    |\n",
      "|               backbone.layers.1.mlp.fc2.bias               |    128     |\n",
      "|               backbone.layers.1.norm2.weight               |    128     |\n",
      "|                backbone.layers.1.norm2.bias                |    128     |\n",
      "|                    backbone.ln_f.weight                    |    128     |\n",
      "|                     backbone.ln_f.bias                     |    128     |\n",
      "|                head.output_transform.weight                |    256     |\n",
      "|                 head.output_transform.bias                 |     2      |\n",
      "+------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 436354\n",
      "Loaded pretrained weights ok!\n",
      "+------------------------------------------------------------+------------+\n",
      "|                          Modules                           | Parameters |\n",
      "+------------------------------------------------------------+------------+\n",
      "|         backbone.embeddings.word_embeddings.weight         |    4096    |\n",
      "|           backbone.layers.0.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.0.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.0.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.0.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.0.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.0.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.0.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.0.norm1.weight               |    256     |\n",
      "|                backbone.layers.0.norm1.bias                |    256     |\n",
      "|              backbone.layers.0.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.0.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.0.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.0.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.0.norm2.weight               |    256     |\n",
      "|                backbone.layers.0.norm2.bias                |    256     |\n",
      "|           backbone.layers.1.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.1.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.1.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.1.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.1.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.1.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.1.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.1.norm1.weight               |    256     |\n",
      "|                backbone.layers.1.norm1.bias                |    256     |\n",
      "|              backbone.layers.1.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.1.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.1.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.1.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.1.norm2.weight               |    256     |\n",
      "|                backbone.layers.1.norm2.bias                |    256     |\n",
      "|           backbone.layers.2.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.2.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.2.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.2.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.2.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.2.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.2.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.2.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.2.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.2.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.2.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.2.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.2.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.2.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.2.norm1.weight               |    256     |\n",
      "|                backbone.layers.2.norm1.bias                |    256     |\n",
      "|              backbone.layers.2.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.2.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.2.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.2.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.2.norm2.weight               |    256     |\n",
      "|                backbone.layers.2.norm2.bias                |    256     |\n",
      "|           backbone.layers.3.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.3.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.3.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.3.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.3.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.3.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.3.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.3.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.3.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.3.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.3.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.3.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.3.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.3.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.3.norm1.weight               |    256     |\n",
      "|                backbone.layers.3.norm1.bias                |    256     |\n",
      "|              backbone.layers.3.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.3.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.3.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.3.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.3.norm2.weight               |    256     |\n",
      "|                backbone.layers.3.norm2.bias                |    256     |\n",
      "|           backbone.layers.4.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.4.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.4.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.4.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.4.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.4.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.4.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.4.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.4.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.4.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.4.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.4.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.4.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.4.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.4.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.4.norm1.weight               |    256     |\n",
      "|                backbone.layers.4.norm1.bias                |    256     |\n",
      "|              backbone.layers.4.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.4.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.4.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.4.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.4.norm2.weight               |    256     |\n",
      "|                backbone.layers.4.norm2.bias                |    256     |\n",
      "|           backbone.layers.5.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.5.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.5.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.5.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.5.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.5.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.5.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.5.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.5.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.5.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.5.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.5.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.5.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.5.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.5.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.5.norm1.weight               |    256     |\n",
      "|                backbone.layers.5.norm1.bias                |    256     |\n",
      "|              backbone.layers.5.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.5.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.5.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.5.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.5.norm2.weight               |    256     |\n",
      "|                backbone.layers.5.norm2.bias                |    256     |\n",
      "|           backbone.layers.6.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.6.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.6.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.6.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.6.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.6.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.6.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.6.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.6.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.6.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.6.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.6.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.6.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.6.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.6.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.6.norm1.weight               |    256     |\n",
      "|                backbone.layers.6.norm1.bias                |    256     |\n",
      "|              backbone.layers.6.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.6.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.6.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.6.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.6.norm2.weight               |    256     |\n",
      "|                backbone.layers.6.norm2.bias                |    256     |\n",
      "|           backbone.layers.7.mixer.in_proj.weight           |   196608   |\n",
      "|            backbone.layers.7.mixer.in_proj.bias            |    768     |\n",
      "|          backbone.layers.7.mixer.out_proj.weight           |   65536    |\n",
      "|           backbone.layers.7.mixer.out_proj.bias            |    256     |\n",
      "|        backbone.layers.7.mixer.short_filter.weight         |    2304    |\n",
      "|         backbone.layers.7.mixer.short_filter.bias          |    768     |\n",
      "|           backbone.layers.7.mixer.filter_fn.bias           |    256     |\n",
      "| backbone.layers.7.mixer.filter_fn.implicit_filter.0.weight |    320     |\n",
      "|  backbone.layers.7.mixer.filter_fn.implicit_filter.0.bias  |     64     |\n",
      "|  backbone.layers.7.mixer.filter_fn.implicit_filter.1.freq  |     64     |\n",
      "| backbone.layers.7.mixer.filter_fn.implicit_filter.2.weight |    4096    |\n",
      "|  backbone.layers.7.mixer.filter_fn.implicit_filter.2.bias  |     64     |\n",
      "| backbone.layers.7.mixer.filter_fn.implicit_filter.4.weight |    4096    |\n",
      "|  backbone.layers.7.mixer.filter_fn.implicit_filter.4.bias  |     64     |\n",
      "| backbone.layers.7.mixer.filter_fn.implicit_filter.6.weight |   16384    |\n",
      "|               backbone.layers.7.norm1.weight               |    256     |\n",
      "|                backbone.layers.7.norm1.bias                |    256     |\n",
      "|              backbone.layers.7.mlp.fc1.weight              |   262144   |\n",
      "|               backbone.layers.7.mlp.fc1.bias               |    1024    |\n",
      "|              backbone.layers.7.mlp.fc2.weight              |   262144   |\n",
      "|               backbone.layers.7.mlp.fc2.bias               |    256     |\n",
      "|               backbone.layers.7.norm2.weight               |    256     |\n",
      "|                backbone.layers.7.norm2.bias                |    256     |\n",
      "|                    backbone.ln_f.weight                    |    256     |\n",
      "|                     backbone.ln_f.bias                     |    256     |\n",
      "|                head.output_transform.weight                |    512     |\n",
      "|                 head.output_transform.bias                 |     2      |\n",
      "+------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 6551042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6551042"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make it look nice\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-tiny-1k-seqlen', device='cuda', use_head=True)\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(model)\n",
    "\n",
    "model_big = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-medium-450k-seqlen', device='cuda', use_head=True)\n",
    "count_parameters(model_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minimally defining model and loading it\n",
    "\n",
    "Just no need to make it super complex, let's do the basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchvision.ops import StochasticDepth\n",
    "from collections import namedtuple\n",
    "#@title Hyena layer\n",
    "\n",
    "\n",
    "def fftconv(u, k, D):\n",
    "    \"\"\"\n",
    "    We apply a convolution through the fourier domain (from the Convolution Theorem)\n",
    "\n",
    "    \"\"\"\n",
    "    seqlen = u.shape[-1]\n",
    "    fft_size = 2 * seqlen\n",
    "\n",
    "    k_f = torch.fft.rfft(k, n=fft_size) / fft_size\n",
    "    u_f = torch.fft.rfft(u.to(dtype=k.dtype), n=fft_size)\n",
    "\n",
    "    if len(u.shape) > 3: k_f = k_f.unsqueeze(1)\n",
    "    y = torch.fft.irfft(u_f * k_f, n=fft_size, norm='forward')[..., :seqlen]\n",
    "\n",
    "    out = y + u * D.unsqueeze(-1)\n",
    "    return out.to(dtype=u.dtype)\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mul_sum(q, y):\n",
    "    return (q * y).sum(dim=1)\n",
    "\n",
    "class OptimModule(nn.Module):\n",
    "    \"\"\" Interface for Module that allows registering buffers/parameters with configurable optimizer hyperparameters \"\"\"\n",
    "\n",
    "    def register(self, name, tensor, lr=None, wd=0.0):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            if wd is not None: optim[\"weight_decay\"] = wd\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    \"\"\"The Sin activation function for the Hyena Filter function.\"\"\"\n",
    "    def __init__(self, dim, w=10, train_freq=True):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(w * torch.ones(1, dim)) if train_freq else w * torch.ones(1, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.freq * x)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(OptimModule):\n",
    "    def __init__(self, emb_dim: int, seq_len: int, lr_pos_emb: float=1e-5, **kwargs):\n",
    "        \"\"\"Complex exponential positional embeddings for Hyena filters.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        # The time embedding fed to the filteres is normalized so that t_f = 1\n",
    "        t = torch.linspace(0, 1, self.seq_len)[None, :, None] # 1, L, 1\n",
    "\n",
    "        if emb_dim > 1:\n",
    "            bands = (emb_dim - 1) // 2\n",
    "        # To compute the right embeddings we use the \"proper\" linspace\n",
    "        t_rescaled = torch.linspace(0, seq_len - 1, seq_len)[None, :, None]\n",
    "        w = 2 * math.pi * t_rescaled / seq_len # 1, L, 1\n",
    "\n",
    "        f = torch.linspace(1e-4, bands - 1, bands)[None, None]\n",
    "        z = torch.exp(-1j * f * w)\n",
    "        z = torch.cat([t, z.real, z.imag], dim=-1)\n",
    "        self.register(\"z\", z, lr=lr_pos_emb)\n",
    "        self.register(\"t\", t, lr=0.0)\n",
    "\n",
    "    def forward(self, L):\n",
    "        return self.z[:, :L], self.t[:, :L]\n",
    "\n",
    "\n",
    "class ExponentialModulation(OptimModule):\n",
    "    \"\"\"The window function applied to the output of the (MLP) filter function.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        fast_decay_pct=0.3,\n",
    "        slow_decay_pct=1.5,\n",
    "        target=1e-2,\n",
    "        modulation_lr=0.0,\n",
    "        modulate: bool=True,\n",
    "        shift: float = 0.05,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.modulate = modulate\n",
    "        self.shift = shift\n",
    "        max_decay = math.log(target) / fast_decay_pct\n",
    "        min_decay = math.log(target) / slow_decay_pct\n",
    "        deltas = torch.linspace(min_decay, max_decay, d_model)[None, None]\n",
    "        self.register(\"deltas\", deltas, lr=modulation_lr)\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if self.modulate:\n",
    "            decay = torch.exp(-t * self.deltas.abs())\n",
    "            x = x * (decay + self.shift)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HyenaFilter(OptimModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            emb_dim=3, # dim of input to MLP, augments with positional encoding\n",
    "            order=16, # width of the implicit MLP\n",
    "            fused_fft_conv=False,\n",
    "            seq_len=1024,\n",
    "            lr=1e-3,\n",
    "            lr_pos_emb=1e-5,\n",
    "            dropout=0.0,\n",
    "            w=1, # frequency of periodic activations\n",
    "            wd=0, # weight decay of kernel parameters\n",
    "            bias=True,\n",
    "            num_inner_mlps=2,\n",
    "            normalized=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Implicit long filter with modulation.\n",
    "\n",
    "        Args:\n",
    "            d_model: number of channels in the input\n",
    "            emb_dim: dimension of the positional encoding (`emb_dim` - 1) // 2 is the number of bands\n",
    "            order: width of the FFN\n",
    "            num_inner_mlps: number of inner linear layers inside filter MLP\n",
    "\n",
    "        Note:\n",
    "            filter_dropout is not implemented\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.use_bias = bias\n",
    "        self.fused_fft_conv = fused_fft_conv\n",
    "        self.bias = nn.Parameter(torch.randn(self.d_model))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        act = Sin(dim=order, w=w)\n",
    "        self.emb_dim = emb_dim\n",
    "        assert emb_dim % 2 != 0 and emb_dim >= 3, \"emb_dim must be odd and greater or equal to 3 (time, sine and cosine)\"\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding(emb_dim, seq_len, lr_pos_emb)\n",
    "\n",
    "        self.implicit_filter = nn.Sequential(\n",
    "            nn.Linear(emb_dim, order),\n",
    "            act,\n",
    "        )\n",
    "        for i in range(num_inner_mlps):\n",
    "            self.implicit_filter.append(nn.Linear(order, order))\n",
    "            self.implicit_filter.append(act)\n",
    "\n",
    "        self.implicit_filter.append(nn.Linear(order, d_model, bias=False))\n",
    "\n",
    "        self.modulation = ExponentialModulation(d_model, **kwargs)\n",
    "\n",
    "        self.normalized = normalized\n",
    "        for c in self.implicit_filter.children():\n",
    "            for name, v in c.state_dict().items():\n",
    "                optim = {\"weight_decay\": wd, \"lr\": lr}\n",
    "                setattr(getattr(c, name), \"_optim\", optim)\n",
    "\n",
    "    def filter(self, L, *args, **kwargs):\n",
    "        z, t = self.pos_emb(L)\n",
    "        h = self.implicit_filter(z)\n",
    "        h = self.modulation(t, h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x, L, k=None, bias=None, *args, **kwargs):\n",
    "        if k is None: k = self.filter(L)\n",
    "\n",
    "        # Ensure compatibility with filters that return a tuple\n",
    "        k = k[0] if type(k) is tuple else k\n",
    "\n",
    "        y = fftconv(x, k, bias)\n",
    "        return y\n",
    "\n",
    "\n",
    "class HyenaOperator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model,\n",
    "            l_max,\n",
    "            order=2,\n",
    "            filter_order=64,\n",
    "            dropout=0.0,\n",
    "            filter_dropout=0.0,\n",
    "            **filter_args,\n",
    "        ):\n",
    "        r\"\"\"\n",
    "        Hyena operator described in the paper https://arxiv.org/pdf/2302.10866.pdf\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the input and output embeddings (width of the layer)\n",
    "            l_max: (int): Maximum input sequence length. Defaults to None\n",
    "            order: (int): Depth of the Hyena recurrence. Defaults to 2\n",
    "            dropout: (float): Dropout probability. Defaults to 0.0\n",
    "            filter_dropout: (float): Dropout probability for the filter. Defaults to 0.0\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.l_max = l_max\n",
    "        self.order = order\n",
    "        inner_width = d_model * (order + 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.in_proj = nn.Linear(d_model, inner_width)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.short_filter = nn.Conv1d(\n",
    "            inner_width,\n",
    "            inner_width,\n",
    "            3,\n",
    "            padding=2,\n",
    "            groups=inner_width\n",
    "        )\n",
    "        self.filter_fn = HyenaFilter(\n",
    "            d_model * (order - 1),\n",
    "            order=filter_order,\n",
    "            seq_len=l_max,\n",
    "            channels=1,\n",
    "            dropout=filter_dropout,\n",
    "            **filter_args\n",
    "        )\n",
    "\n",
    "    def forward(self, u, *args, **kwargs):\n",
    "        l = u.size(-2)\n",
    "        l_filter = min(l, self.l_max)\n",
    "        u = self.in_proj(u)\n",
    "        u = rearrange(u, 'b l d -> b d l')\n",
    "\n",
    "        uc = self.short_filter(u)[...,:l_filter]\n",
    "        *x, v = uc.split(self.d_model, dim=1)\n",
    "\n",
    "        k = self.filter_fn.filter(l_filter)[0]\n",
    "        k = rearrange(k, 'l (o d) -> o d l', o=self.order - 1)\n",
    "        bias = rearrange(self.filter_fn.bias, '(o d) -> o d', o=self.order - 1)\n",
    "\n",
    "        for o, x_i in enumerate(reversed(x[1:])):\n",
    "            v = self.dropout(v * x_i)\n",
    "            v = self.filter_fn(v, l_filter, k=k[o], bias=bias[o])\n",
    "\n",
    "        y = rearrange(v * x[0], 'b d l -> b l d')\n",
    "\n",
    "        y = self.out_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "#@title Self-Attention (alternative)\n",
    "\n",
    "\"\"\"\n",
    "If you'd like to try the HyenaDNA model using attention instead, you can. ie,\n",
    "use a regular decoder only Transformer.\n",
    "\n",
    "Borrowed from the FlashAttention library by Tri Dao.\n",
    "\"\"\"\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Implement the scaled dot product attention with softmax.\n",
    "    Arguments\n",
    "    ---------\n",
    "        softmax_scale: The temperature to use for the softmax attention.\n",
    "                      (default: 1/sqrt(d_keys) where d_keys is computed at\n",
    "                      runtime)\n",
    "        attention_dropout: The dropout rate to apply to the attention\n",
    "                           (default: 0.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, causal=False, softmax_scale=None, attention_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.softmax_scale = softmax_scale\n",
    "        self.dropout_p = attention_dropout\n",
    "\n",
    "    def forward(self, qkv, causal=None, key_padding_mask=None):\n",
    "        \"\"\"Implements the multihead softmax attention.\n",
    "        Arguments\n",
    "        ---------\n",
    "            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)\n",
    "            causal: if passed, will override self.causal\n",
    "            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,\n",
    "                False means to mask out. (B, S)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = qkv.shape[0], qkv.shape[1]\n",
    "        causal = self.causal if causal is None else causal\n",
    "        q, k, v = qkv.unbind(dim=2)\n",
    "        softmax_scale = self.softmax_scale or 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum('bthd,bshd->bhts', q, k * softmax_scale)\n",
    "        if key_padding_mask is not None:\n",
    "            padding_mask = torch.full((batch_size, seqlen), -10000.0, dtype=scores.dtype,\n",
    "                                      device=scores.device)\n",
    "            padding_mask.masked_fill_(key_padding_mask, 0.0)\n",
    "            scores = scores + rearrange(padding_mask, 'b s -> b 1 1 s')\n",
    "        if causal:\n",
    "            # \"triu_tril_cuda_template\" not implemented for 'BFloat16'\n",
    "            # So we have to construct the mask in float\n",
    "            causal_mask = torch.triu(torch.full((seqlen, seqlen), -10000.0, device=scores.device), 1)\n",
    "            scores = scores + causal_mask.to(dtype=scores.dtype)\n",
    "        attention = torch.softmax(scores, dim=-1, dtype=v.dtype)\n",
    "        attention_drop = F.dropout(attention, self.dropout_p if self.training else 0.0)\n",
    "        output = torch.einsum('bhts,bshd->bthd', attention_drop, v)\n",
    "        return output\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    \"\"\"Multi-head self-attention and cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, bias=True, dropout=0.0,\n",
    "                 softmax_scale=None, causal=False, layer_idx=None, dwconv=False,return_residual=False,device=None, dtype=None) -> None:\n",
    "        \"\"\"\n",
    "            return_residual: whether to return the input x along with the output. This is for\n",
    "                performance reason: for post-norm architecture, returning the input allows us\n",
    "                to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.causal = causal\n",
    "        self.layer_idx = layer_idx\n",
    "        self.dwconv = dwconv\n",
    "        self.return_residual = return_residual\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        assert self.embed_dim % num_heads == 0, \"self.kdim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embed_dim // num_heads\n",
    "\n",
    "        linear_cls = nn.Linear\n",
    "        linear_resid_cls = LinearResidual\n",
    "        inner_attn_cls =  SelfAttention\n",
    "\n",
    "        if not self.return_residual:\n",
    "            self.Wqkv = linear_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        else:\n",
    "            self.Wqkv = linear_resid_cls(embed_dim, 3 * embed_dim, bias=bias, **factory_kwargs)\n",
    "        if self.dwconv:\n",
    "            self.dwconv_qkv = nn.Conv1d(3 * embed_dim, 3 * embed_dim, kernel_size=3, padding=2,\n",
    "                                        groups=3 * embed_dim)\n",
    "\n",
    "        self.inner_attn = inner_attn_cls(causal=causal, softmax_scale=softmax_scale,\n",
    "                                         attention_dropout=dropout)\n",
    "\n",
    "        # output projection always have the bias (for now)\n",
    "        self.out_proj = linear_cls(embed_dim, embed_dim, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n",
    "                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n",
    "                is the is the sum of the sequence lengths in the batch.\n",
    "            cu_seqlens: (batch_size + 1,), dtype torch.int32. The cumulative sequence lengths\n",
    "                of the sequences in the batch, used to index into x. Only applicable when using\n",
    "                FlashAttention.\n",
    "            max_seqlen: int. Maximum sequence length in the batch.\n",
    "            key_padding_mask: boolean mask, True means to keep, False means to mask out.\n",
    "                (batch, seqlen). Only applicable when not using FlashAttention.\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n",
    "            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs = ({'key_padding_mask': key_padding_mask, **kwargs})\n",
    "\n",
    "        if not self.return_residual:\n",
    "            qkv = self.Wqkv(x)\n",
    "        else:\n",
    "            qkv, x = self.Wqkv(x)\n",
    "        if self.dwconv:\n",
    "            qkv = rearrange(self.dwconv_qkv(rearrange(qkv, 'b s d -> b d s'))[..., :-2],\n",
    "                            'b d s -> b s d').contiguous()\n",
    "        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, d=self.head_dim)\n",
    "\n",
    "        context = self.inner_attn(qkv, **kwargs)\n",
    "\n",
    "        out = self.out_proj(rearrange(context, '... h d -> ... (h d)'))\n",
    "        return out if not self.return_residual else (out, x)\n",
    "\n",
    "#@title MLP layer\n",
    "\n",
    "\"\"\"\n",
    "The MLP layer after the mixer layer (HyenaOperator).\n",
    "\"\"\"\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, activation=F.gelu,\n",
    "                 return_residual=False, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/mlp.py\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.return_residual = return_residual\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, **factory_kwargs)\n",
    "        self.activation = activation\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, **factory_kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.fc2(y)\n",
    "        return y if not self.return_residual else (y, x)\n",
    "\n",
    "#@title Block layer (Hyena + MLP layers)\n",
    "\n",
    "\"\"\"\n",
    "A block consists of a Mixer layer (Hyena or attention), and a MLP layer.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class LinearResidual(nn.Linear):\n",
    "    \"\"\"Wrap nn.Linear to return the residual as well. For compatibility with FusedDense.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(input), input\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mixer_cls=None, mlp_cls=None, norm_cls=nn.LayerNorm,\n",
    "                 dropout_cls=nn.Dropout, prenorm=True, resid_dropout1=0., resid_dropout2=0.,\n",
    "                 drop_path1=0., drop_path2=0.,\n",
    "                 return_residual=False,\n",
    "                 residual_in_fp32=False):\n",
    "        \"\"\"\n",
    "        From https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/modules/block.py\n",
    "        For prenorm=True, this Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Dropout -> Add -> LN -> MHA -> Dropout -> Add -> LN -> MLP, returning both\n",
    "        the hidden_states (output of the MLP) and the residual.\n",
    "        This is for performance reasons, as we can fuse the dropout, add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        For prenorm=False, this Block has the same structure as a regular postnorm Transformer\n",
    "        block: MHA -> Dropout -> Add -> LN -> MLP -> Dropout -> Add -> LN.\n",
    "        return_residual: whether each of the sub-layers (mixer and mlp) will return the residual.\n",
    "        This is for performance reason: for post-norm architecture, returning the input allows us\n",
    "        to fuse the backward of nn.Linear with the residual connection.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.prenorm = prenorm\n",
    "        self.return_residual = return_residual\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        if self.residual_in_fp32:\n",
    "            assert self.prenorm, 'residual_in_fp32 is only compatible with prenorm=True'\n",
    "        if mixer_cls is None:\n",
    "            mixer_cls = partial(MHA, num_heads=dim // 64)\n",
    "        if mlp_cls is None:\n",
    "            mlp_cls = partial(Mlp, hidden_features=4 * dim)\n",
    "        self.mixer = mixer_cls()\n",
    "        self.dropout1 = dropout_cls(resid_dropout1)\n",
    "        self.drop_path1 = StochasticDepth(drop_path1, mode='row')\n",
    "        self.norm1 = norm_cls(dim)\n",
    "        self.mlp = mlp_cls(dim)\n",
    "        if not isinstance(self.mlp, nn.Identity):\n",
    "            self.dropout2 = dropout_cls(resid_dropout2)\n",
    "            self.drop_path2 = StochasticDepth(drop_path2, mode='row')\n",
    "            self.norm2 = norm_cls(dim)\n",
    "\n",
    "    def forward(self, hidden_states, residual = None,\n",
    "                mixer_subset=None, mixer_kwargs=None):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: if postnorm, residual=None, If prenorm, hidden_states = Attn/MLP(LN(residual))\n",
    "            mixer_subset: for cross-attention only. If not None, will take a subset of x\n",
    "                before applying the query projection. Useful for e.g., ViT where we only care\n",
    "                about the CLS token in the last layer.\n",
    "        \"\"\"\n",
    "        if self.prenorm:\n",
    "            dropped = self.drop_path1(self.dropout1(hidden_states))\n",
    "            residual = (dropped + residual) if residual is not None else dropped\n",
    "            hidden_states = self.norm1(residual.to(dtype=self.norm1.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "            if mixer_kwargs is None:\n",
    "                mixer_kwargs = {}\n",
    "            if mixer_subset is not None:\n",
    "                mixer_kwargs['mixer_subset'] = mixer_subset\n",
    "            hidden_states = self.mixer(hidden_states, **mixer_kwargs)\n",
    "            if mixer_subset is not None:\n",
    "                residual = residual[:, mixer_subset]\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                dropped = self.drop_path2(self.dropout2(hidden_states))\n",
    "                residual = (dropped + residual) if residual is not None else dropped\n",
    "                hidden_states = self.norm2(residual.to(dtype=self.norm2.weight.dtype))\n",
    "                if self.residual_in_fp32:\n",
    "                    residual = residual.to(torch.float32)\n",
    "\n",
    "                hidden_states = self.mlp(hidden_states)\n",
    "            return hidden_states, residual\n",
    "        else:\n",
    "            assert residual is None\n",
    "            mixer_out = self.mixer(\n",
    "                hidden_states, **(mixer_kwargs if mixer_kwargs is not None else {})\n",
    "            )\n",
    "            if self.return_residual:  # mixer out is actually a pair here\n",
    "                mixer_out, hidden_states = mixer_out\n",
    "\n",
    "            hidden_states = self.norm1((self.drop_path1(self.dropout1(mixer_out))\n",
    "                                        + hidden_states).to(dtype=self.norm1.weight.dtype))\n",
    "\n",
    "            if not isinstance(self.mlp, nn.Identity):\n",
    "                mlp_out = self.mlp(hidden_states)\n",
    "                if self.return_residual:  # mlp out is actually a pair here\n",
    "                    mlp_out, hidden_states = mlp_out\n",
    "\n",
    "                hidden_states = self.norm2((self.drop_path2(self.dropout2(mlp_out))\n",
    "                                            + hidden_states).to(dtype=self.norm2.weight.dtype))\n",
    "\n",
    "            return hidden_states\n",
    "\n",
    "def create_mixer_cls(layer=None,\n",
    "                     attn_layer_idx=None, attn_cfg=None, layer_idx=None,\n",
    "                     device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    if attn_layer_idx is not None and layer_idx in attn_layer_idx:\n",
    "        causal = True if attn_cfg is None else attn_cfg.pop('causal', True)\n",
    "\n",
    "        mha_cls = MHA\n",
    "\n",
    "        mixer_cls = partial(mha_cls, causal=causal, layer_idx=layer_idx,\n",
    "                            **(attn_cfg if attn_cfg is not None else {}),**factory_kwargs)\n",
    "    else:\n",
    "        # mixer_cls = instantiate(registry.layer, layer, partial=True, layer_idx=layer_idx, **factory_kwargs)\n",
    "\n",
    "        mixer_cls = partial(HyenaOperator, **layer)\n",
    "\n",
    "    return mixer_cls\n",
    "\n",
    "def create_mlp_cls(d_model, d_inner=None, device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    inner_dim = d_inner if d_inner is not None else 4 * d_model\n",
    "\n",
    "    mlp_cls = partial(Mlp, hidden_features=inner_dim,\n",
    "                          activation=partial(F.gelu, approximate='tanh'), **factory_kwargs)\n",
    "\n",
    "    return mlp_cls\n",
    "\n",
    "\n",
    "def create_block(d_model, d_inner=None,\n",
    "                 layer=None, attn_layer_idx=None,\n",
    "                 attn_cfg=None, layer_norm_epsilon=1e-5,\n",
    "                 resid_dropout1=0.0, resid_dropout2=0.0, residual_in_fp32=False,\n",
    "                 layer_idx=None,\n",
    "                 device=None, dtype=None):\n",
    "    factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "    mixer_cls = create_mixer_cls(layer=layer,\n",
    "                                 attn_layer_idx=attn_layer_idx,\n",
    "                                 attn_cfg=attn_cfg, layer_idx=layer_idx,\n",
    "                                 **factory_kwargs)\n",
    "    mlp_cls = create_mlp_cls(d_model, d_inner=d_inner,\n",
    "                             **factory_kwargs)\n",
    "    norm_cls = partial(nn.LayerNorm, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "    block = Block(d_model, mixer_cls, mlp_cls, norm_cls=norm_cls,\n",
    "                  prenorm=True, resid_dropout1=resid_dropout1, resid_dropout2=resid_dropout2,residual_in_fp32=residual_in_fp32)\n",
    "    block.layer_idx = layer_idx\n",
    "    return block\n",
    "\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(module, n_layer, initializer_range=0.02, rescale_prenorm_residual=True,\n",
    "                  glu_act=False):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "            # If using GLU activation for now, we scale the std by 2\n",
    "            elif name in [\"output_linear.0.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                if not glu_act:\n",
    "                    nn.init.normal_(p, mean=0.0, std=initializer_range / math.sqrt(2 * n_layer))\n",
    "                else:\n",
    "                    out_features = p.shape[0]\n",
    "                    # Multiplying the first half of the matrix by 2 since sigmoid scales it down by 0.5\n",
    "                    # on average.\n",
    "                    nn.init.normal_(p[:out_features // 2], mean=0.0, std=initializer_range / math.sqrt(2 * n_layer) * 2)\n",
    "\n",
    "\n",
    "\n",
    "#@title Backbone model (stack of blocks)\n",
    "\n",
    "\"\"\"\n",
    "A backbone model consists of a stack of blocks. If you use attention, then\n",
    "positional embeddings are included. When using Hyena, then the pos emb\n",
    "revert to doing nothing.\n",
    "\"\"\"\n",
    "\n",
    "class GPT2Embeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, vocab_size, max_position_embeddings, padding_idx=None,\n",
    "                 word_embed_proj_dim=None, device=None, dtype=None):\n",
    "        \"\"\"\n",
    "            If max_position_embeddings <= 0, there's no position embeddings\n",
    "            If word_embe_proj_dim is not None (e.g., OPT-350m), we embed to that dimension\n",
    "                the project up to embed_dim\n",
    "        \"\"\"\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if word_embed_proj_dim is None:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx,\n",
    "                                                **factory_kwargs)\n",
    "            self.project_in = None\n",
    "        else:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, word_embed_proj_dim,\n",
    "                                                padding_idx=padding_idx, **factory_kwargs)\n",
    "            self.project_in = nn.Linear(word_embed_proj_dim, embed_dim, bias=False,\n",
    "                                        **factory_kwargs)\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        if self.max_position_embeddings > 0:\n",
    "            self.position_embeddings = nn.Embedding(max_position_embeddings, embed_dim,\n",
    "                                                    **factory_kwargs)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        \"\"\"\n",
    "            input_ids: (batch, seqlen)\n",
    "            position_ids: (batch, seqlen)\n",
    "        \"\"\"\n",
    "        batch_size, seqlen = input_ids.shape\n",
    "        embeddings = self.word_embeddings(input_ids)\n",
    "        if self.project_in is not None:\n",
    "            embeddings = self.project_in(embeddings)\n",
    "        if self.max_position_embeddings > 0:\n",
    "            if position_ids is None:\n",
    "                position_ids = torch.arange(seqlen, dtype=torch.long, device=input_ids.device)\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings = embeddings + position_embeddings\n",
    "        return embeddings\n",
    "\n",
    "class LMBackbone(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 process_group=None, layer=None,\n",
    "                 attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.process_group = process_group\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        # note max_position_embeddings is 0 for Hyena, and therefore isn't used\n",
    "        self.embeddings = GPT2Embeddings(d_model, vocab_size, max_position_embeddings,\n",
    "                                             **factory_kwargs)\n",
    "\n",
    "        self.layers = nn.ModuleList([create_block(\n",
    "            d_model, d_inner=d_inner,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx,\n",
    "            attn_cfg=attn_cfg, layer_norm_epsilon=layer_norm_epsilon,\n",
    "            resid_dropout1=embed_dropout if i == 0 else resid_dropout,\n",
    "            resid_dropout2=resid_dropout, residual_in_fp32=residual_in_fp32,layer_idx=i,\n",
    "            **factory_kwargs,\n",
    "        ) for i in range(n_layer)])\n",
    "\n",
    "        self.drop_f = nn.Dropout(resid_dropout)\n",
    "        self.ln_f = nn.LayerNorm(d_model, eps=layer_norm_epsilon, **factory_kwargs)\n",
    "\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None):\n",
    "        hidden_states = self.embeddings(input_ids, position_ids=position_ids,)\n",
    "        residual = None\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states, residual = layer(hidden_states, residual)\n",
    "\n",
    "        dropped = self.drop_f(hidden_states)\n",
    "        residual = (dropped + residual) if residual is not None else dropped\n",
    "        hidden_states = self.ln_f(residual.to(dtype=self.ln_f.weight.dtype))\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "#@title Decoder head layer\n",
    "\n",
    "\"\"\"\n",
    "A simple decoder head (using MLP) to predict a sequence level classification.\n",
    "You have the option to average across all the tokens in a sequence or using the\n",
    "\"last\" token to classify.  At least, those 2 worked best for us, but we provide\n",
    "other \"modes\" as well.\n",
    "\n",
    "We only need this for classification.  Otherwise we'll use the hidden\n",
    "states of the backbone as embeddings.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SequenceDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model, d_output=None, l_output=None, use_lengths=False, mode=\"last\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_transform = nn.Identity() if d_output is None else nn.Linear(d_model, d_output)\n",
    "\n",
    "        if l_output is None:\n",
    "            self.l_output = None\n",
    "            self.squeeze = False\n",
    "        elif l_output == 0:\n",
    "            # Equivalent to getting an output of length 1 and then squeezing\n",
    "            self.l_output = 1\n",
    "            self.squeeze = True\n",
    "        else:\n",
    "            assert l_output > 0\n",
    "            self.l_output = l_output\n",
    "            self.squeeze = False\n",
    "\n",
    "        self.use_lengths = use_lengths\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'ragged':\n",
    "            assert not use_lengths\n",
    "\n",
    "    def forward(self, x, state=None, lengths=None, l_output=None):\n",
    "        \"\"\"\n",
    "        x: (n_batch, l_seq, d_model)\n",
    "        Returns: (n_batch, l_output, d_output)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.l_output is None:\n",
    "            if l_output is not None:\n",
    "                assert isinstance(l_output, int)  # Override by pass in\n",
    "            else:\n",
    "                # Grab entire output\n",
    "                l_output = x.size(-2)\n",
    "            squeeze = False\n",
    "        else:\n",
    "            l_output = self.l_output\n",
    "            squeeze = self.squeeze\n",
    "\n",
    "        if self.mode == \"last\":\n",
    "            restrict = lambda x: x[..., -l_output:, :]\n",
    "        elif self.mode == \"first\":\n",
    "            restrict = lambda x: x[..., :l_output, :]\n",
    "        elif self.mode == \"pool\":\n",
    "            restrict = lambda x: (\n",
    "                torch.cumsum(x, dim=-2)\n",
    "                / torch.arange(\n",
    "                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n",
    "                ).unsqueeze(-1)\n",
    "            )[..., -l_output:, :]\n",
    "\n",
    "            def restrict(x):\n",
    "                L = x.size(-2)\n",
    "                s = x.sum(dim=-2, keepdim=True)\n",
    "                if l_output > 1:\n",
    "                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n",
    "                    c = F.pad(c, (0, 0, 1, 0))\n",
    "                    s = s - c  # (B, l_output, D)\n",
    "                    s = s.flip(-2)\n",
    "                denom = torch.arange(\n",
    "                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n",
    "                )\n",
    "                s = s / denom\n",
    "                return s\n",
    "\n",
    "        elif self.mode == \"sum\":\n",
    "            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n",
    "            # TODO use same restrict function as pool case\n",
    "        elif self.mode == 'ragged':\n",
    "            assert lengths is not None, \"lengths must be provided for ragged mode\"\n",
    "            # remove any additional padding (beyond max length of any sequence in the batch)\n",
    "            restrict = lambda x: x[..., : max(lengths), :]\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Mode must be ['last' | 'first' | 'pool' | 'sum']\"\n",
    "            )\n",
    "\n",
    "        # Restrict to actual length of sequence\n",
    "        if self.use_lengths:\n",
    "            assert lengths is not None\n",
    "            x = torch.stack(\n",
    "                [\n",
    "                    restrict(out[..., :length, :])\n",
    "                    for out, length in zip(torch.unbind(x, dim=0), lengths)\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            x = restrict(x)\n",
    "\n",
    "        if squeeze:\n",
    "            assert x.size(-2) == 1\n",
    "            x = x.squeeze(-2)\n",
    "\n",
    "        x = self.output_transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def step(self, x, state=None):\n",
    "        # Ignore all length logic\n",
    "        return self.output_transform(x)\n",
    "\n",
    "#@title Model (backbone + head)\n",
    "\n",
    "\"\"\"\n",
    "Putting it all together, the model consists of a backbone model\n",
    "and a decoder head (you can turn off head for embeddings only too).\n",
    "\n",
    "Here we use a simple head to do multi-classification, but\n",
    "can also swap the head to do next token prediction too.  We defer to the main\n",
    "HyenaDNA for that code, since pretraining with next token prediction isn't quite\n",
    "feasible on colab.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HyenaDNAModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_layer: int, d_inner: int, vocab_size: int,\n",
    "                 layer=None, attn_layer_idx=None, attn_cfg=None, max_position_embeddings=0,\n",
    "                 resid_dropout: float = 0.0, embed_dropout: float = 0.1,\n",
    "                 layer_norm_epsilon: float = 1e-5, initializer_cfg=None,residual_in_fp32=False,\n",
    "                 pad_vocab_size_multiple: int = 1, use_head=False, n_classes: int = 2,\n",
    "                 device=None, dtype=None, **kwargs) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        if vocab_size % pad_vocab_size_multiple != 0:\n",
    "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
    "\n",
    "        self.use_head = use_head\n",
    "\n",
    "        # check if layer (config) has d_model (HF code differs from main Safari code)\n",
    "        if 'd_model' not in layer:\n",
    "            layer['d_model'] = d_model\n",
    "\n",
    "        self.backbone = LMBackbone(\n",
    "            d_model=d_model, n_layer=n_layer, d_inner=d_inner, vocab_size=vocab_size,\n",
    "            layer=layer, attn_layer_idx=attn_layer_idx, attn_cfg=attn_cfg,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            resid_dropout=resid_dropout, embed_dropout=embed_dropout,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            initializer_cfg=initializer_cfg, residual_in_fp32=residual_in_fp32,\n",
    "            **factory_kwargs, **kwargs\n",
    "        )\n",
    "\n",
    "        # we only need a head if doing classification, otherwise we'll use the\n",
    "        # hidden states as embeddings\n",
    "        if self.use_head:\n",
    "            self.head = SequenceDecoder(d_model=d_model, d_output=n_classes, l_output=0, mode='pool')\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.apply(partial(_init_weights, n_layer=n_layer,\n",
    "                           **(initializer_cfg if initializer_cfg is not None else {})))\n",
    "\n",
    "        # if self.use_head:\n",
    "        #     self.tie_weights()\n",
    "\n",
    "    # def tie_weights(self):\n",
    "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, state=None): # state for the repo interface\n",
    "        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n",
    "\n",
    "        if self.use_head:\n",
    "            return self.head(hidden_states)\n",
    "        else:\n",
    "            return hidden_states\n",
    "\n",
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "    \n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n",
    "\n",
    "\n",
    "# Data pipeline\n",
    "\n",
    "\n",
    "\n",
    "#@title Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    "    \n",
    "#@title GenomicBenchmark dataset\n",
    "\n",
    "\"\"\"\n",
    "The GenomicBenchmarks dataset will automatically download to /contents on colab.\n",
    "There are 8 datasets to choose from.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from random import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def coin_flip():\n",
    "    return random() > 0.5\n",
    "\n",
    "\n",
    "string_complement_map = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'a': 't', 'c': 'g', 'g': 'c', 't': 'a'}\n",
    "# augmentation\n",
    "def string_reverse_complement(seq):\n",
    "    rev_comp = ''\n",
    "    for base in seq[::-1]:\n",
    "        if base in string_complement_map:\n",
    "            rev_comp += string_complement_map[base]\n",
    "        # if bp not complement map, use the same bp\n",
    "        else:\n",
    "            rev_comp += base\n",
    "    return rev_comp\n",
    "\n",
    "\n",
    "class GenomicBenchmarkDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    '''\n",
    "    Loop thru bed file, retrieve (chr, start, end), query fasta file for sequence.\n",
    "    Returns a generator that retrieves the sequence.\n",
    "\n",
    "    Genomic Benchmarks Dataset, from:\n",
    "    https://github.com/ML-Bioinfo-CEITEC/genomic_benchmarks\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split,\n",
    "        max_length,\n",
    "        dataset_name='human_enhancers_cohn',\n",
    "        d_output=2, # default binary classification\n",
    "        dest_path=\"/content\", # default for colab\n",
    "        tokenizer=None,\n",
    "        tokenizer_name=None,\n",
    "        use_padding=None,\n",
    "        add_eos=False,\n",
    "        rc_aug=False,\n",
    "        return_augs=False,\n",
    "    ):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.use_padding = use_padding\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.return_augs = return_augs\n",
    "        self.add_eos = add_eos\n",
    "        self.d_output = d_output  # needed for decoder to grab\n",
    "        self.rc_aug = rc_aug\n",
    "\n",
    "        if not is_downloaded(dataset_name, cache_path=dest_path):\n",
    "            print(\"downloading {} to {}\".format(dataset_name, dest_path))\n",
    "            download_dataset(dataset_name, version=0, dest_path=dest_path)\n",
    "        else:\n",
    "            print(\"already downloaded {}-{}\".format(split, dataset_name))\n",
    "\n",
    "        # use Path object\n",
    "        base_path = Path(dest_path) / dataset_name / split\n",
    "\n",
    "        self.all_paths = []\n",
    "        self.all_labels = []\n",
    "        label_mapper = {}\n",
    "\n",
    "        for i, x in enumerate(base_path.iterdir()):\n",
    "            label_mapper[x.stem] = i\n",
    "\n",
    "        for label_type in label_mapper.keys():\n",
    "            for x in (base_path / label_type).iterdir():\n",
    "                self.all_paths.append(x)\n",
    "                self.all_labels.append(label_mapper[label_type])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt_path = self.all_paths[idx]\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        x = content\n",
    "        y = self.all_labels[idx]\n",
    "\n",
    "        # apply rc_aug here if using\n",
    "        if self.rc_aug and coin_flip():\n",
    "            x = string_reverse_complement(x)\n",
    "\n",
    "        seq = self.tokenizer(x,\n",
    "            add_special_tokens=False,\n",
    "            padding=\"max_length\" if self.use_padding else None,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "        )  # add cls and eos token (+2)\n",
    "        seq = seq[\"input_ids\"]  # get input_ids\n",
    "\n",
    "        # need to handle eos here\n",
    "        if self.add_eos:\n",
    "            # append list seems to be faster than append tensor\n",
    "            seq.append(self.tokenizer.sep_token_id)\n",
    "\n",
    "        # convert to tensor\n",
    "        seq = torch.LongTensor(seq)\n",
    "\n",
    "        # need to wrap in list\n",
    "        target = torch.LongTensor([y])\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next the tokenizer\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)\n",
    " \n",
    " \n",
    " #pretty long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "HyenaDNAModel(\n",
      "  (backbone): LMBackbone(\n",
      "    (embeddings): GPT2Embeddings(\n",
      "      (word_embeddings): Embedding(16, 128)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=128, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (short_filter): Conv1d(384, 384, kernel_size=(3,), stride=(1,), padding=(2,), groups=384)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=128, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (drop_f): Dropout(p=0.0, inplace=False)\n",
      "    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (head): SequenceDecoder(\n",
      "    (output_transform): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#now the model\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-tiny-1k-seqlen', device='cpu', use_head=True)\n",
    "\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.Size([1, 2])\n",
      "tensor([[-0.1258,  0.1310]])\n"
     ]
    }
   ],
   "source": [
    "def inference(model):\n",
    "    #define some things\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    max_length = 1024 #has to be less than 1028, which makes sense\n",
    "    use_padding = False\n",
    "    rc_aug = False  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = False\n",
    "    n_classes = 2  # not used for embeddings only\n",
    "    backbone_cfg = None\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    #### Single embedding example ####\n",
    "\n",
    "    # create a sample 450k long, prepare\n",
    "    sequence = 'ACTG' * int(max_length/4)\n",
    "    tok_seq = tokenizer(sequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    # prep model and forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    print(embeddings.shape)  # embeddings here!\n",
    "    return embeddings, tok_seq, tokenizer\n",
    "\n",
    "embed,seq,tokenizer = inference(model)\n",
    "print(embed)\n",
    "#clearly we see that this is doing multiclass classification..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.backbone.embeddings.word_embeddings.weight', 'model.backbone.layers.0.mixer.out_proj.weight', 'model.backbone.layers.0.mixer.out_proj.bias', 'model.backbone.layers.0.mixer.in_proj.weight', 'model.backbone.layers.0.mixer.in_proj.bias', 'model.backbone.layers.0.mixer.short_filter.weight', 'model.backbone.layers.0.mixer.short_filter.bias', 'model.backbone.layers.0.mixer.filter_fn.bias', 'model.backbone.layers.0.mixer.filter_fn.pos_emb.z', 'model.backbone.layers.0.mixer.filter_fn.pos_emb.t', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', 'model.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', 'model.backbone.layers.0.mixer.filter_fn.modulation.deltas', 'model.backbone.layers.0.norm1.weight', 'model.backbone.layers.0.norm1.bias', 'model.backbone.layers.0.mlp.fc1.weight', 'model.backbone.layers.0.mlp.fc1.bias', 'model.backbone.layers.0.mlp.fc2.weight', 'model.backbone.layers.0.mlp.fc2.bias', 'model.backbone.layers.0.norm2.weight', 'model.backbone.layers.0.norm2.bias', 'model.backbone.layers.1.mixer.out_proj.weight', 'model.backbone.layers.1.mixer.out_proj.bias', 'model.backbone.layers.1.mixer.in_proj.weight', 'model.backbone.layers.1.mixer.in_proj.bias', 'model.backbone.layers.1.mixer.short_filter.weight', 'model.backbone.layers.1.mixer.short_filter.bias', 'model.backbone.layers.1.mixer.filter_fn.bias', 'model.backbone.layers.1.mixer.filter_fn.pos_emb.z', 'model.backbone.layers.1.mixer.filter_fn.pos_emb.t', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'model.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', 'model.backbone.layers.1.mixer.filter_fn.modulation.deltas', 'model.backbone.layers.1.norm1.weight', 'model.backbone.layers.1.norm1.bias', 'model.backbone.layers.1.mlp.fc1.weight', 'model.backbone.layers.1.mlp.fc1.bias', 'model.backbone.layers.1.mlp.fc2.weight', 'model.backbone.layers.1.mlp.fc2.bias', 'model.backbone.layers.1.norm2.weight', 'model.backbone.layers.1.norm2.bias', 'model.backbone.ln_f.weight', 'model.backbone.ln_f.bias', 'model.lm_head.weight', 'train_torchmetrics.num_tokens.count', 'val_torchmetrics.num_tokens.count', 'test_torchmetrics.num_tokens.count'])\n",
      "model.backbone.embeddings.word_embeddings.weight torch.Size([16, 128])\n",
      "model.backbone.layers.0.mixer.out_proj.weight torch.Size([128, 128])\n",
      "model.backbone.layers.0.mixer.out_proj.bias torch.Size([128])\n",
      "model.backbone.layers.0.mixer.in_proj.weight torch.Size([384, 128])\n",
      "model.backbone.layers.0.mixer.in_proj.bias torch.Size([384])\n",
      "model.backbone.layers.0.mixer.short_filter.weight torch.Size([384, 1, 3])\n",
      "model.backbone.layers.0.mixer.short_filter.bias torch.Size([384])\n",
      "model.backbone.layers.0.mixer.filter_fn.bias torch.Size([128])\n",
      "model.backbone.layers.0.mixer.filter_fn.pos_emb.z torch.Size([1, 1026, 5])\n",
      "model.backbone.layers.0.mixer.filter_fn.pos_emb.t torch.Size([1, 1026, 1])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight torch.Size([64, 5])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias torch.Size([64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq torch.Size([1, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight torch.Size([64, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias torch.Size([64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq torch.Size([1, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight torch.Size([64, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias torch.Size([64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq torch.Size([1, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight torch.Size([128, 64])\n",
      "model.backbone.layers.0.mixer.filter_fn.modulation.deltas torch.Size([1, 1, 128])\n",
      "model.backbone.layers.0.norm1.weight torch.Size([128])\n",
      "model.backbone.layers.0.norm1.bias torch.Size([128])\n",
      "model.backbone.layers.0.mlp.fc1.weight torch.Size([512, 128])\n",
      "model.backbone.layers.0.mlp.fc1.bias torch.Size([512])\n",
      "model.backbone.layers.0.mlp.fc2.weight torch.Size([128, 512])\n",
      "model.backbone.layers.0.mlp.fc2.bias torch.Size([128])\n",
      "model.backbone.layers.0.norm2.weight torch.Size([128])\n",
      "model.backbone.layers.0.norm2.bias torch.Size([128])\n",
      "model.backbone.layers.1.mixer.out_proj.weight torch.Size([128, 128])\n",
      "model.backbone.layers.1.mixer.out_proj.bias torch.Size([128])\n",
      "model.backbone.layers.1.mixer.in_proj.weight torch.Size([384, 128])\n",
      "model.backbone.layers.1.mixer.in_proj.bias torch.Size([384])\n",
      "model.backbone.layers.1.mixer.short_filter.weight torch.Size([384, 1, 3])\n",
      "model.backbone.layers.1.mixer.short_filter.bias torch.Size([384])\n",
      "model.backbone.layers.1.mixer.filter_fn.bias torch.Size([128])\n",
      "model.backbone.layers.1.mixer.filter_fn.pos_emb.z torch.Size([1, 1026, 5])\n",
      "model.backbone.layers.1.mixer.filter_fn.pos_emb.t torch.Size([1, 1026, 1])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight torch.Size([64, 5])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias torch.Size([64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq torch.Size([1, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight torch.Size([64, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias torch.Size([64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq torch.Size([1, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight torch.Size([64, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias torch.Size([64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq torch.Size([1, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight torch.Size([128, 64])\n",
      "model.backbone.layers.1.mixer.filter_fn.modulation.deltas torch.Size([1, 1, 128])\n",
      "model.backbone.layers.1.norm1.weight torch.Size([128])\n",
      "model.backbone.layers.1.norm1.bias torch.Size([128])\n",
      "model.backbone.layers.1.mlp.fc1.weight torch.Size([512, 128])\n",
      "model.backbone.layers.1.mlp.fc1.bias torch.Size([512])\n",
      "model.backbone.layers.1.mlp.fc2.weight torch.Size([128, 512])\n",
      "model.backbone.layers.1.mlp.fc2.bias torch.Size([128])\n",
      "model.backbone.layers.1.norm2.weight torch.Size([128])\n",
      "model.backbone.layers.1.norm2.bias torch.Size([128])\n",
      "model.backbone.ln_f.weight torch.Size([128])\n",
      "model.backbone.ln_f.bias torch.Size([128])\n",
      "model.lm_head.weight torch.Size([16, 128])\n",
      "train_torchmetrics.num_tokens.count torch.Size([])\n",
      "val_torchmetrics.num_tokens.count torch.Size([])\n",
      "test_torchmetrics.num_tokens.count torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "#comparing this model to what we are loading\n",
    "\n",
    "#let's first load the model directly\n",
    "\n",
    "checkpoint = torch.load('/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', map_location=torch.device('cpu'))\n",
    "print(checkpoint['state_dict'].keys())\n",
    "for key in checkpoint['state_dict'].keys():\n",
    "    print(key, checkpoint['state_dict'][key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'global_step': 3345, 'pytorch-lightning_version': '2.1.3', 'state_dict': OrderedDict([('model.backbone.embeddings.word_embeddings.weight', tensor([[ 0.1124, -0.0071,  0.0401,  ..., -0.1609,  0.0100, -0.1181],\n",
      "        [ 0.0767, -0.1335,  0.1313,  ..., -0.1644,  0.0841,  0.0322],\n",
      "        [ 0.0983, -0.0116,  0.0418,  ..., -0.1830, -0.0067, -0.0839],\n",
      "        ...,\n",
      "        [ 0.1026, -0.0136,  0.0398,  ..., -0.1349,  0.0138, -0.1103],\n",
      "        [ 0.1071,  0.0046,  0.0365,  ..., -0.1672,  0.0050, -0.1017],\n",
      "        [ 0.0828,  0.0049,  0.0414,  ..., -0.1595,  0.0023, -0.1049]])), ('model.backbone.layers.0.mixer.out_proj.weight', tensor([[-0.0010,  0.0016, -0.0155,  ...,  0.0719, -0.0398, -0.0678],\n",
      "        [ 0.0311, -0.0296, -0.0372,  ..., -0.0094,  0.0124, -0.0407],\n",
      "        [-0.0006, -0.0083,  0.0080,  ..., -0.0402, -0.0444, -0.0476],\n",
      "        ...,\n",
      "        [ 0.0210,  0.0123, -0.0017,  ..., -0.0256, -0.0057, -0.0838],\n",
      "        [-0.0668,  0.0517, -0.0097,  ..., -0.0397,  0.0123, -0.0297],\n",
      "        [-0.0229, -0.0031,  0.0744,  ..., -0.0526, -0.0601, -0.0102]])), ('model.backbone.layers.0.mixer.out_proj.bias', tensor([-0.0048,  0.0502, -0.0269, -0.0118,  0.0164, -0.0015,  0.0134, -0.0255,\n",
      "         0.0036,  0.0290,  0.0408, -0.0231, -0.0029, -0.0203, -0.0067,  0.0013,\n",
      "         0.0273,  0.0281, -0.0097,  0.0018,  0.0050, -0.0036, -0.0029, -0.0106,\n",
      "         0.0051,  0.0209, -0.0205, -0.0075,  0.0157, -0.0329, -0.0075, -0.0091,\n",
      "         0.0250, -0.0010,  0.0039,  0.0281, -0.0203, -0.0112, -0.0175,  0.0237,\n",
      "        -0.0125, -0.0016, -0.0102, -0.0135,  0.0170, -0.0040, -0.0045, -0.0251,\n",
      "         0.0437, -0.0081,  0.0153,  0.0118, -0.0119,  0.0036, -0.0298, -0.0009,\n",
      "         0.0064,  0.0314,  0.0082,  0.0220,  0.0247, -0.0089,  0.0125,  0.0320,\n",
      "         0.0118,  0.0075, -0.0103, -0.0285, -0.0394, -0.0102,  0.0091,  0.0165,\n",
      "         0.0231, -0.0145, -0.0084,  0.0217,  0.0082,  0.0129,  0.0269,  0.0234,\n",
      "        -0.0259, -0.0065, -0.0088,  0.0191, -0.0057, -0.0101,  0.0009, -0.0387,\n",
      "         0.0385,  0.0135,  0.0003,  0.0335,  0.0187,  0.0039, -0.0184, -0.0286,\n",
      "        -0.0186, -0.0147,  0.0217, -0.0029,  0.0017, -0.0018,  0.0061, -0.0251,\n",
      "        -0.0303, -0.0044, -0.0128,  0.0177, -0.0071, -0.0084,  0.0150, -0.0028,\n",
      "        -0.0163, -0.0092,  0.0035,  0.0116, -0.0090, -0.0276, -0.0272, -0.0278,\n",
      "         0.0072, -0.0060, -0.0016,  0.0103, -0.0066,  0.0081, -0.0057, -0.0157])), ('model.backbone.layers.0.mixer.in_proj.weight', tensor([[ 0.0448,  0.0447,  0.0403,  ...,  0.0468,  0.0639, -0.0092],\n",
      "        [-0.0181,  0.0274,  0.0111,  ...,  0.0554, -0.0035,  0.0323],\n",
      "        [ 0.0179,  0.0157,  0.0120,  ..., -0.0387,  0.0098, -0.0305],\n",
      "        ...,\n",
      "        [-0.0017,  0.0344,  0.0182,  ...,  0.0321,  0.0176,  0.0244],\n",
      "        [ 0.0266,  0.0157,  0.0427,  ..., -0.0629,  0.0264, -0.0823],\n",
      "        [-0.0651, -0.1226, -0.1223,  ..., -0.1108, -0.1007, -0.0873]])), ('model.backbone.layers.0.mixer.in_proj.bias', tensor([-0.0043,  0.0274,  0.0119, -0.0885, -0.0329,  0.1288,  0.0030, -0.0158,\n",
      "         0.0188, -0.0798, -0.0198,  0.0848,  0.0326,  0.0111, -0.0005,  0.0432,\n",
      "         0.0055, -0.0576, -0.0142,  0.0211,  0.0575,  0.0645,  0.0296, -0.0351,\n",
      "         0.0508,  0.0119,  0.0235,  0.0393, -0.0248, -0.0267, -0.0308,  0.0331,\n",
      "         0.0778,  0.1286, -0.0364, -0.0076,  0.0305,  0.0336, -0.0810, -0.0858,\n",
      "         0.0636,  0.1229, -0.0374,  0.0629, -0.0088,  0.1105,  0.0080, -0.0539,\n",
      "         0.0492, -0.0408, -0.0833, -0.0445,  0.0236,  0.0179,  0.0662,  0.0658,\n",
      "        -0.0747, -0.0281, -0.0078, -0.0389,  0.0263, -0.0059, -0.0406,  0.1116,\n",
      "         0.1416,  0.0465,  0.0663, -0.0667, -0.0699,  0.0109, -0.0994, -0.0646,\n",
      "        -0.1416,  0.1164,  0.1050, -0.0809, -0.0041, -0.0413,  0.0301, -0.0315,\n",
      "         0.1638, -0.0114, -0.0068,  0.0637, -0.0559,  0.0880,  0.0392, -0.0275,\n",
      "         0.0699, -0.1911, -0.0418, -0.1112, -0.0830, -0.1141,  0.0037, -0.1615,\n",
      "        -0.0908, -0.1320, -0.0866, -0.0093, -0.0970, -0.0238,  0.0144, -0.0619,\n",
      "         0.0057, -0.0553, -0.0227, -0.0708, -0.0099, -0.0743, -0.1019,  0.0445,\n",
      "        -0.1100, -0.0279, -0.0373,  0.1098,  0.0027,  0.0864,  0.0417,  0.0337,\n",
      "        -0.0178, -0.0087,  0.0172, -0.0576, -0.0507, -0.2312,  0.0368, -0.0736,\n",
      "        -0.0304, -0.0257, -0.0046,  0.0079,  0.0805,  0.0206, -0.0295,  0.0373,\n",
      "         0.0158,  0.0377,  0.0121,  0.0312,  0.0888, -0.0572, -0.0065,  0.0866,\n",
      "        -0.0120,  0.0458,  0.0400,  0.0064,  0.0370, -0.0232,  0.1241,  0.0445,\n",
      "         0.0771,  0.0119, -0.0638, -0.0179, -0.0041, -0.0282,  0.0464,  0.0104,\n",
      "         0.0398, -0.0076, -0.0158,  0.0137, -0.1923,  0.0021, -0.0940, -0.0280,\n",
      "         0.0035, -0.0077,  0.0232, -0.0005,  0.1165,  0.0217, -0.0102, -0.0498,\n",
      "         0.0196,  0.0447, -0.0122,  0.0453, -0.0165,  0.0095,  0.0362,  0.0635,\n",
      "         0.0351, -0.0403,  0.0701,  0.0738,  0.0423, -0.0093, -0.0189, -0.0013,\n",
      "        -0.0606,  0.0455,  0.0483,  0.0775,  0.0126, -0.0633, -0.1089, -0.0283,\n",
      "         0.0298,  0.0026,  0.1911, -0.0306, -0.0312, -0.0325,  0.0620,  0.0086,\n",
      "        -0.0214,  0.1159,  0.0035,  0.0032,  0.0429,  0.0802,  0.0438,  0.0172,\n",
      "         0.0070,  0.0260, -0.0089,  0.0490, -0.0241, -0.0686,  0.0155,  0.0873,\n",
      "        -0.1493,  0.0116,  0.1149, -0.0561,  0.1169,  0.0364,  0.0315,  0.0330,\n",
      "        -0.0323,  0.0418,  0.0336, -0.0228, -0.0379, -0.0084,  0.0644, -0.0256,\n",
      "         0.0335,  0.0825,  0.0258,  0.0715, -0.0648, -0.0152, -0.0585,  0.1261,\n",
      "        -0.0189, -0.0079, -0.0704, -0.0248,  0.0197,  0.1451, -0.0061, -0.1466,\n",
      "         0.0131,  0.0220, -0.0043, -0.0130, -0.0331, -0.0056,  0.0261,  0.0073,\n",
      "         0.0008,  0.0083,  0.0275, -0.0375,  0.0403, -0.0071, -0.0176, -0.0664,\n",
      "         0.0128, -0.0238, -0.0474, -0.0350,  0.0054, -0.0156,  0.0157,  0.0277,\n",
      "         0.0231,  0.0488, -0.0093, -0.0019, -0.0746, -0.0140,  0.0459, -0.0416,\n",
      "        -0.0773,  0.0069,  0.0211,  0.0187, -0.0032, -0.0020,  0.0207,  0.0786,\n",
      "         0.0021, -0.0494,  0.0330,  0.0106,  0.0101,  0.1408,  0.0087, -0.0244,\n",
      "         0.0445,  0.0107, -0.0524,  0.1224,  0.0183,  0.0308, -0.0095, -0.0207,\n",
      "         0.0363, -0.0162, -0.0159, -0.0080,  0.0332,  0.0125,  0.0576,  0.0199,\n",
      "         0.0394,  0.0184, -0.0436,  0.0579,  0.0267,  0.0499,  0.0395, -0.0264,\n",
      "        -0.1486,  0.0024,  0.0663,  0.0145,  0.0156, -0.0014,  0.0192,  0.0194,\n",
      "        -0.0166, -0.0331,  0.0398, -0.0211,  0.0012,  0.0696,  0.0120, -0.1163,\n",
      "        -0.0055,  0.0829,  0.0626,  0.0231,  0.0469,  0.0339,  0.0026,  0.0564,\n",
      "         0.0023,  0.0515, -0.0027, -0.0225, -0.1373, -0.0846,  0.1871,  0.0542,\n",
      "        -0.0251, -0.0228, -0.0538,  0.0077, -0.0319, -0.0470,  0.0490,  0.0243,\n",
      "        -0.0867, -0.0131, -0.0163,  0.0117,  0.0056, -0.0024,  0.0130,  0.0394,\n",
      "        -0.0264,  0.0614, -0.2390,  0.0189, -0.0154,  0.1635,  0.0017,  0.0801])), ('model.backbone.layers.0.mixer.short_filter.weight', tensor([[[ 0.1863,  0.0042,  0.1267]],\n",
      "\n",
      "        [[ 0.1884,  0.1398,  0.1773]],\n",
      "\n",
      "        [[-0.0808, -0.0057,  0.0878]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2147, -0.0011,  0.1280]],\n",
      "\n",
      "        [[ 0.0067,  0.1915,  0.0324]],\n",
      "\n",
      "        [[-0.2240,  0.1139,  0.0640]]])), ('model.backbone.layers.0.mixer.short_filter.bias', tensor([-1.1735e-02,  2.2522e-01,  1.5866e-01,  1.7393e-01,  2.1571e-01,\n",
      "        -7.0519e-02,  2.3792e-01, -1.9029e-01, -2.6806e-01,  3.2887e-01,\n",
      "         2.6685e-01, -9.3379e-02, -3.0333e-01,  3.8129e-02, -2.2481e-01,\n",
      "        -5.5482e-02,  2.2609e-01, -6.1989e-02, -2.6230e-01, -1.4077e-01,\n",
      "        -2.1265e-01,  3.1136e-01,  5.0973e-02, -1.8768e-01, -2.7344e-01,\n",
      "         2.8166e-02, -2.8814e-01, -1.3260e-01, -1.1867e-01, -3.0212e-02,\n",
      "        -2.7231e-01, -1.6037e-02,  1.8056e-01,  3.6443e-01, -1.6235e-01,\n",
      "        -7.3135e-02, -2.5659e-02, -2.1912e-01,  1.9246e-01, -3.2892e-01,\n",
      "         1.3646e-01, -1.6860e-01, -2.2718e-01,  2.2039e-02, -5.1404e-03,\n",
      "        -2.0795e-01, -1.5691e-01,  2.0679e-01,  2.3786e-01,  3.8796e-02,\n",
      "        -3.0580e-01,  5.8169e-02,  1.1953e-02, -2.0736e-01,  2.5554e-01,\n",
      "         3.3282e-01,  3.2614e-01,  3.7221e-02,  1.0752e-02,  2.0670e-02,\n",
      "         2.5173e-01, -2.5241e-01,  2.4076e-01, -1.8193e-01,  1.4433e-01,\n",
      "        -8.3111e-02, -2.1253e-01, -1.6860e-01, -1.0484e-01,  1.8347e-01,\n",
      "         3.3760e-01, -7.5392e-02,  2.3108e-01, -1.7186e-01, -1.8148e-01,\n",
      "        -2.1072e-01, -1.9274e-01, -1.2055e-01,  1.4854e-01, -1.9966e-01,\n",
      "         3.8105e-01,  7.3425e-02, -1.2961e-01,  3.2635e-01,  2.2983e-01,\n",
      "         1.7331e-01, -2.5406e-01,  2.1755e-01, -2.2920e-01, -1.5684e-01,\n",
      "        -1.2014e-01,  1.3967e-01, -3.1823e-01, -3.3053e-01,  1.7004e-01,\n",
      "        -3.9175e-01, -3.2814e-01,  2.8800e-01, -9.3857e-02,  1.8108e-01,\n",
      "         3.3211e-01,  9.9750e-02,  4.0184e-02,  1.9459e-01, -1.7231e-02,\n",
      "        -3.1406e-01,  1.7843e-01,  2.6737e-01,  5.9893e-02, -2.5967e-01,\n",
      "        -3.4665e-01, -6.5357e-02, -2.5299e-01,  2.0302e-02,  2.6047e-01,\n",
      "         2.5176e-01, -1.8815e-01,  3.2559e-01, -2.1537e-02,  1.3962e-01,\n",
      "        -1.3861e-01, -7.7436e-03,  7.8353e-02, -2.9575e-01, -2.6436e-01,\n",
      "        -2.1399e-01,  6.5138e-03, -2.8726e-02,  9.8425e-02, -1.5270e-01,\n",
      "         2.5081e-01,  1.0320e-01,  1.4380e-01, -2.1547e-01,  2.4791e-01,\n",
      "         1.1288e-01, -1.0171e-01,  2.1387e-01, -2.4588e-01,  2.2781e-01,\n",
      "         2.8752e-03, -1.2833e-01,  8.9098e-02, -1.1792e-01,  1.3448e-02,\n",
      "         1.1725e-01,  1.9213e-01,  1.2896e-01,  6.6882e-02,  2.1114e-01,\n",
      "        -1.3196e-01,  1.5309e-01,  8.8264e-02, -2.3783e-01,  9.6533e-02,\n",
      "        -5.0838e-02, -7.8639e-04,  3.6099e-02, -7.6633e-02, -3.2102e-02,\n",
      "         2.5452e-01,  6.1289e-02, -1.6738e-01,  4.8497e-02,  2.6531e-01,\n",
      "         6.9181e-02,  2.9791e-01, -2.5960e-01,  2.2936e-01, -3.0742e-02,\n",
      "         1.7376e-03,  6.9459e-02,  2.1530e-01,  7.5766e-02,  1.2986e-02,\n",
      "        -1.0793e-01, -6.2266e-02,  2.7990e-01, -3.1835e-02, -1.7975e-01,\n",
      "        -1.4885e-01,  4.4841e-02,  1.9223e-01, -9.1011e-02,  1.0169e-01,\n",
      "         2.6641e-01,  2.7535e-01, -2.1290e-01,  7.4749e-02,  1.4484e-01,\n",
      "        -1.3888e-02, -1.8471e-01,  8.0655e-02, -2.4489e-01,  2.1775e-01,\n",
      "         7.8794e-02,  5.9048e-02, -1.9929e-02,  4.1471e-02, -1.8405e-01,\n",
      "        -7.0595e-02,  6.5572e-02, -4.8722e-02,  9.9419e-02,  4.9326e-02,\n",
      "         3.8144e-02,  1.8456e-01,  1.4490e-01,  2.5533e-01, -3.0656e-01,\n",
      "         1.2319e-01,  7.0692e-02, -2.5777e-01, -1.6868e-02,  1.0170e-01,\n",
      "         1.6285e-02,  1.7637e-01, -2.3029e-01, -2.8127e-02,  2.4252e-01,\n",
      "        -1.4106e-01, -3.3479e-01,  2.3004e-03,  3.0311e-01,  3.5589e-01,\n",
      "        -7.3260e-02,  8.7676e-02, -3.1169e-01,  3.2802e-01, -5.3254e-02,\n",
      "         2.4370e-02,  6.2391e-02,  1.7149e-01, -1.5703e-01,  1.2054e-01,\n",
      "         1.7683e-01, -1.6078e-01, -1.4497e-01,  2.1459e-01, -8.1899e-02,\n",
      "         1.0436e-01, -2.9511e-01, -1.6958e-02,  2.7171e-01,  1.8808e-01,\n",
      "         1.6102e-01,  1.3617e-01, -2.4596e-02,  2.0129e-01, -1.4104e-01,\n",
      "        -2.9067e-01,  2.3824e-01,  4.3151e-04, -2.2642e-01, -1.3920e-01,\n",
      "        -2.2616e-01, -1.3723e-01,  2.2399e-01,  1.3136e-01, -7.2411e-03,\n",
      "         6.5276e-04, -6.2108e-02,  7.6447e-02, -2.3170e-01,  1.9876e-01,\n",
      "         5.7758e-02,  9.6603e-02,  8.7977e-02, -1.5694e-01,  8.9913e-02,\n",
      "        -2.3748e-01,  2.2485e-01, -2.4044e-01, -1.3572e-01,  5.8780e-02,\n",
      "         1.6900e-01, -2.6904e-02, -1.6388e-03,  2.1376e-01,  2.1601e-01,\n",
      "        -1.3427e-01, -9.8379e-02, -8.3439e-02, -2.1127e-01,  9.9337e-02,\n",
      "         2.3008e-01, -2.8495e-02,  2.2636e-01,  8.5405e-02, -1.9364e-01,\n",
      "         2.0701e-01,  1.3908e-01,  4.3273e-02, -1.6671e-01,  5.3353e-03,\n",
      "        -5.2793e-02,  2.2496e-02, -1.5850e-01,  1.2952e-01,  1.1580e-01,\n",
      "         2.1428e-02, -3.0267e-01,  1.8903e-01, -2.4680e-01,  1.1081e-01,\n",
      "         1.6878e-04,  1.9712e-01,  3.1691e-01,  1.9485e-01,  3.0184e-01,\n",
      "        -8.1771e-02, -1.8236e-02,  2.7926e-01,  1.0972e-01, -1.3275e-01,\n",
      "         6.9492e-02, -1.1213e-01,  2.9328e-01,  2.3524e-01,  8.7883e-02,\n",
      "         5.6483e-02,  1.3341e-01,  5.8067e-03, -1.3614e-01,  9.7257e-02,\n",
      "        -9.5185e-02, -1.5156e-01, -1.0924e-01,  6.5147e-02,  2.0366e-01,\n",
      "        -2.2820e-01,  1.0174e-01,  2.0546e-01, -2.2843e-01,  9.6646e-02,\n",
      "        -1.9209e-01, -1.2153e-01, -3.2343e-02,  2.4163e-01, -2.3624e-01,\n",
      "        -4.1516e-02, -3.1702e-01,  2.3580e-01, -1.4418e-01,  1.0846e-01,\n",
      "        -9.7072e-02,  1.3609e-01, -7.6257e-02,  1.4790e-01, -4.9099e-02,\n",
      "         2.4638e-01, -7.6708e-02,  1.1710e-02, -3.2807e-01, -1.8060e-02,\n",
      "         1.4747e-01,  5.6397e-02,  1.8112e-01, -3.3820e-01,  1.8867e-01,\n",
      "        -2.3261e-01,  2.5339e-01,  2.2162e-01, -9.6016e-02,  2.0368e-01,\n",
      "         1.5674e-01, -2.9223e-01, -2.6114e-01, -1.8573e-01, -7.5934e-02,\n",
      "        -1.1901e-01, -1.2331e-02, -2.1372e-01, -3.8408e-02, -1.5568e-01,\n",
      "         1.2616e-01,  2.2361e-01, -2.7164e-01, -6.1387e-03, -3.4632e-02,\n",
      "         1.8919e-01, -1.7399e-01, -1.6607e-01,  6.6359e-02])), ('model.backbone.layers.0.mixer.filter_fn.bias', tensor([ 0.6192,  0.9703,  1.1758,  0.9357,  0.2511, -1.1248, -0.2519, -0.5032,\n",
      "        -0.4193, -1.2573, -0.8777,  0.2092,  0.8023,  0.0806, -0.8349, -1.0966,\n",
      "         0.3529,  0.1638, -0.5010, -0.3770, -0.5703,  0.4046, -1.2791, -0.1156,\n",
      "        -0.0108, -0.2078, -0.2785,  0.3345, -0.0189,  0.8437, -0.2433,  0.2991,\n",
      "        -0.3260, -0.8080, -0.3235, -0.3226, -1.3470,  0.1701,  0.7054, -1.0809,\n",
      "        -0.4097, -0.3595,  0.5019,  0.4861, -0.8842, -1.1334, -0.2673, -0.5623,\n",
      "         0.0790,  0.0554,  0.7637, -1.3414,  0.0724,  0.9335,  0.1143,  0.9992,\n",
      "         0.9923, -0.5947,  1.0749, -1.3057, -0.5605,  1.1084, -0.5901, -0.3806,\n",
      "         0.8018, -0.7247,  0.6033, -0.0273,  0.6448,  0.0028, -1.1872,  0.5831,\n",
      "         0.9940,  1.0927,  0.9913,  0.3819, -0.6608, -0.4194, -0.5624, -0.5304,\n",
      "        -0.8653, -0.9123,  0.1774,  1.0479, -0.3649,  0.4907,  0.8536, -0.4278,\n",
      "        -0.3815, -0.9928,  0.7599,  0.1380,  0.0166,  0.5739,  0.2714,  1.0111,\n",
      "         0.7804,  0.9225,  0.8170, -0.2475,  0.9546,  0.9876,  0.6655, -0.6935,\n",
      "         0.1755, -0.2657, -1.2608, -0.5626, -0.1357, -0.9336,  1.0664,  0.5452,\n",
      "         0.7947, -0.0854, -0.3552, -0.1087,  0.4888,  1.1022,  0.5666,  0.6734,\n",
      "         1.0269,  0.5059, -0.9232,  0.1832,  0.1873,  0.4663, -0.0901,  0.8689])), ('model.backbone.layers.0.mixer.filter_fn.pos_emb.z', tensor([[[ 0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 9.7561e-04,  1.0000e+00,  9.9998e-01, -6.1240e-07, -6.1239e-03],\n",
      "         [ 1.9512e-03,  1.0000e+00,  9.9993e-01, -1.2248e-06, -1.2248e-02],\n",
      "         ...,\n",
      "         [ 9.9805e-01,  1.0000e+00,  9.9983e-01, -6.2648e-04,  1.8371e-02],\n",
      "         [ 9.9902e-01,  1.0000e+00,  9.9993e-01, -6.2709e-04,  1.2248e-02],\n",
      "         [ 1.0000e+00,  1.0000e+00,  9.9998e-01, -6.2771e-04,  6.1238e-03]]])), ('model.backbone.layers.0.mixer.filter_fn.pos_emb.t', tensor([[[0.0000e+00],\n",
      "         [9.7561e-04],\n",
      "         [1.9512e-03],\n",
      "         ...,\n",
      "         [9.9805e-01],\n",
      "         [9.9902e-01],\n",
      "         [1.0000e+00]]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight', tensor([[ 3.1701e-02, -3.3456e-03,  6.0990e-03,  6.6863e-03,  1.6032e-03],\n",
      "        [ 1.8091e-02, -2.3023e-02,  2.5674e-02, -3.8720e-02, -1.0477e-01],\n",
      "        [ 3.6265e-03, -3.7024e-02,  3.9053e-02,  1.5394e-02, -4.9685e-02],\n",
      "        [ 2.7913e-02,  6.3520e-03,  8.1363e-03, -3.6188e-02,  1.1889e-03],\n",
      "        [ 4.0268e-03,  3.5304e-03,  2.2782e-03,  4.4466e-03, -7.0884e-04],\n",
      "        [ 2.1585e-02,  1.0113e-02, -8.4437e-04,  2.0729e-03,  3.4171e-03],\n",
      "        [ 6.3824e-02,  2.3926e-02, -2.1410e-02, -1.1543e-02, -1.7927e-02],\n",
      "        [ 6.3406e-02, -2.5730e-02,  1.3176e-02, -2.5151e-02, -1.3530e-01],\n",
      "        [-2.5085e-02,  1.7497e-03, -8.1624e-03,  9.2874e-03, -1.8656e-03],\n",
      "        [-8.1513e-03, -5.3989e-04,  1.8022e-02,  2.5108e-02, -1.2138e-02],\n",
      "        [ 5.8527e-03,  3.3198e-02, -1.2511e-02, -1.0403e-02,  6.9913e-03],\n",
      "        [-3.6600e-02,  1.0587e-02,  1.0946e-02,  1.1126e-02,  1.0301e-01],\n",
      "        [ 8.2679e-03,  1.5537e-02, -2.0153e-02,  1.5018e-02,  1.6947e-02],\n",
      "        [-1.9304e-02,  1.1012e-02, -4.6363e-03, -2.5959e-02, -2.9252e-03],\n",
      "        [ 2.1295e-02,  1.1324e-02,  4.9875e-03, -3.6792e-02,  6.5279e-03],\n",
      "        [ 4.9355e-03, -1.8004e-02,  6.0417e-03,  6.1076e-02, -2.5990e-03],\n",
      "        [ 5.1639e-02,  1.4248e-02, -1.5655e-02, -6.2312e-02,  1.8267e-02],\n",
      "        [-2.7240e-02,  1.7271e-02, -7.5183e-03, -2.0435e-02, -4.9113e-03],\n",
      "        [-3.9270e-02,  1.2753e-02, -2.4717e-03,  3.4317e-02,  1.2161e-01],\n",
      "        [-1.9205e-02,  2.0019e-02, -8.0211e-03,  9.4670e-03, -2.1718e-03],\n",
      "        [ 6.1047e-02,  1.3337e-02,  4.6881e-04, -4.8286e-02, -7.6797e-02],\n",
      "        [-3.8915e-02,  2.2727e-02, -1.5729e-02,  7.9686e-02,  1.2082e-01],\n",
      "        [ 1.9833e-02, -2.2653e-02,  2.5459e-03,  3.0863e-03,  3.9322e-03],\n",
      "        [-1.1602e-01, -6.1580e-04,  9.4517e-03,  1.0723e-01,  1.5718e-01],\n",
      "        [-4.6638e-02, -3.8213e-02,  3.3085e-02,  5.7808e-02, -3.1772e-02],\n",
      "        [ 5.9561e-02, -9.1542e-03,  7.3174e-03, -8.8720e-02, -1.2235e-01],\n",
      "        [-2.5360e-02, -5.9684e-03, -3.9774e-03,  2.1935e-03,  1.0315e-01],\n",
      "        [ 4.9846e-02,  3.3366e-03, -2.7884e-02, -9.1722e-02, -9.9135e-02],\n",
      "        [ 2.4980e-03, -1.3120e-02,  2.5252e-02, -3.9329e-02, -5.9233e-02],\n",
      "        [-4.0908e-02, -3.4655e-03,  1.1148e-02,  2.2155e-02,  6.0561e-03],\n",
      "        [-1.0081e-02, -3.4891e-04, -3.1227e-04,  2.4851e-02, -3.4205e-03],\n",
      "        [-1.1659e-02, -3.2018e-02,  2.4911e-02, -6.1389e-03, -1.1828e-02],\n",
      "        [ 5.7251e-02, -2.2865e-03, -1.1200e-02, -1.2557e-02,  2.1849e-02],\n",
      "        [-1.4573e-04, -8.3602e-04, -1.2777e-02, -5.9783e-02,  5.7015e-03],\n",
      "        [ 1.0983e-02, -2.0652e-02,  2.4945e-02,  2.0022e-02, -3.0956e-02],\n",
      "        [ 7.6274e-02,  4.9564e-03, -1.6813e-02, -4.8689e-02, -1.0593e-01],\n",
      "        [-1.4250e-02, -1.3299e-02, -2.2270e-03,  2.2704e-02, -5.2076e-03],\n",
      "        [ 4.7311e-02,  6.3606e-03, -1.0566e-02, -2.8076e-02,  1.8258e-02],\n",
      "        [-8.4635e-03,  1.3191e-02, -9.1983e-04,  1.6544e-02, -3.2170e-04],\n",
      "        [-3.4759e-02, -2.8919e-03, -8.5807e-03,  1.2961e-02,  1.6666e-03],\n",
      "        [-3.6080e-02,  7.1659e-03,  1.2282e-02,  4.9000e-02,  6.0994e-02],\n",
      "        [ 1.2105e-02,  1.1249e-02, -4.3698e-03, -2.2868e-02,  2.5304e-03],\n",
      "        [ 2.1834e-02,  1.6503e-02, -3.3215e-02, -4.2011e-02,  2.1518e-02],\n",
      "        [-3.9170e-02, -5.9996e-04,  9.4629e-03,  1.2851e-02, -2.0026e-02],\n",
      "        [ 1.4500e-02,  2.3194e-02, -3.3777e-02, -8.6119e-03,  2.2182e-02],\n",
      "        [ 2.2876e-02, -2.0693e-02,  6.6364e-03, -2.6893e-02,  4.6343e-03],\n",
      "        [-1.2735e-02,  2.4733e-02, -8.5504e-03,  1.3098e-02, -1.2764e-03],\n",
      "        [ 4.5670e-02,  2.7052e-02, -2.4074e-02, -1.1585e-02,  1.7942e-02],\n",
      "        [ 2.9560e-03, -1.3727e-02, -3.3605e-04,  2.4559e-02, -9.4516e-03],\n",
      "        [ 1.2213e-02, -1.9388e-02,  9.7025e-03,  7.2094e-04, -1.5667e-03],\n",
      "        [ 5.4630e-02, -6.3349e-03,  9.3916e-04, -4.8748e-02, -6.7744e-02],\n",
      "        [-2.9539e-02, -2.3782e-03,  1.7834e-04,  4.1756e-03, -3.9413e-03],\n",
      "        [ 7.2121e-03, -1.4908e-02,  1.2338e-03, -1.7842e-02,  1.2014e-03],\n",
      "        [-1.9328e-02,  3.3224e-03, -3.2775e-03,  4.8821e-02,  8.0033e-02],\n",
      "        [ 1.1347e-02,  1.9497e-02, -8.7924e-03, -3.4731e-02,  2.8227e-03],\n",
      "        [ 1.3747e-02,  1.0902e-02, -2.3481e-02, -3.5324e-02,  1.4447e-02],\n",
      "        [ 1.0219e-02, -1.2964e-02, -5.6922e-04, -1.9944e-02,  1.4311e-03],\n",
      "        [ 1.2579e-02, -1.8593e-02,  5.7680e-03,  3.6309e-02,  2.3396e-03],\n",
      "        [ 5.4822e-02, -2.9578e-03,  5.8289e-03, -3.6499e-02, -5.5833e-02],\n",
      "        [-4.6727e-02,  1.2088e-04, -1.8396e-02,  4.8242e-03,  1.1792e-01],\n",
      "        [ 5.5482e-02,  9.7431e-03, -4.9069e-03,  5.9548e-02,  1.1666e-02],\n",
      "        [-4.1068e-02, -3.5296e-02,  4.3102e-02,  3.8685e-02, -2.0067e-03],\n",
      "        [ 1.5734e-02,  2.6518e-03, -3.1314e-04, -1.1238e-02,  4.4664e-03],\n",
      "        [-5.5235e-02,  1.6185e-02, -1.2609e-02,  6.0145e-02, -6.4714e-03]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias', tensor([ 4.4853e-03, -5.2234e-03, -9.0496e-03, -1.2335e-02, -1.4364e-02,\n",
      "        -1.5927e-02, -7.8546e-03,  1.9225e-03,  3.7716e-03, -1.6265e-02,\n",
      "        -1.8963e-02, -3.3765e-03,  2.0683e-04, -2.2325e-04, -2.5289e-02,\n",
      "         2.1190e-02,  8.8454e-03, -7.4198e-03, -6.3112e-03, -1.1192e-02,\n",
      "        -9.0050e-03,  1.6587e-03,  2.2390e-02, -7.0594e-03, -2.3771e-03,\n",
      "        -1.0825e-04,  1.3246e-02,  5.1550e-03, -7.3096e-03, -1.4224e-02,\n",
      "        -2.3623e-03,  5.6298e-03,  1.1407e-02,  1.3350e-02, -8.5327e-03,\n",
      "         4.5212e-03,  1.7458e-02,  5.6496e-03, -1.0044e-02,  1.1938e-02,\n",
      "        -6.8165e-03, -1.1163e-02,  2.0707e-02, -1.0436e-02,  1.2348e-02,\n",
      "         1.1608e-02, -1.7943e-02, -8.2502e-05,  6.9993e-03,  9.7663e-03,\n",
      "        -3.7099e-03, -4.5536e-04,  1.4357e-02,  1.8360e-03, -7.5013e-03,\n",
      "         1.2476e-02,  5.7587e-03,  5.7099e-03, -2.8861e-03,  1.1543e-02,\n",
      "        -5.6580e-03, -6.2575e-03, -8.0362e-03, -4.5305e-04])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', tensor([[ 9.8741,  9.9779,  9.9477,  9.9237,  9.8951,  9.9624,  9.8981, 10.0159,\n",
      "          9.8702,  9.9576,  9.8119,  9.9446,  9.9956,  9.9308,  9.9159,  9.9346,\n",
      "         10.0516,  9.5969,  9.9887,  9.8698,  9.9421, 10.0112,  9.9146, 10.0494,\n",
      "          9.8992, 10.0198,  9.9650,  9.9534,  9.9083,  9.9923,  9.9448,  9.9514,\n",
      "          9.9197,  9.9036,  9.9010,  9.9741,  9.9241,  9.6049,  9.9184,  9.9264,\n",
      "          9.9058,  9.8549,  9.9296,  9.8227, 10.0770,  9.9791,  9.3892,  9.8565,\n",
      "          9.8586,  9.9699,  9.8643,  9.9069,  9.7694,  9.9238,  9.9515, 10.0256,\n",
      "          9.7750,  9.7241,  9.9042,  9.9762,  9.7498,  9.9487,  9.8086,  9.9197]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight', tensor([[ 1.2559e-02, -1.4839e-02,  2.8054e-02,  ...,  1.7167e-02,\n",
      "          2.0825e-02, -1.5788e-02],\n",
      "        [-6.1920e-03, -1.4238e-02,  2.3940e-02,  ..., -2.1819e-02,\n",
      "          8.4913e-05, -1.1755e-03],\n",
      "        [ 1.9514e-02,  1.9727e-02, -2.5319e-02,  ...,  3.0060e-03,\n",
      "          4.3487e-03,  1.0748e-02],\n",
      "        ...,\n",
      "        [-1.2098e-02,  5.4411e-02,  4.1819e-02,  ...,  4.4544e-02,\n",
      "         -1.9125e-02,  1.3861e-02],\n",
      "        [ 4.6428e-03,  2.5708e-03, -4.9814e-03,  ...,  4.6002e-03,\n",
      "         -1.0061e-02,  3.9070e-02],\n",
      "        [-8.8240e-03,  8.2802e-04,  1.3351e-02,  ...,  3.9341e-02,\n",
      "         -1.7478e-02, -4.2219e-03]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias', tensor([ 0.0019,  0.0074,  0.0241,  0.0190, -0.0042, -0.0080, -0.0069,  0.0021,\n",
      "        -0.0051,  0.0040,  0.0035,  0.0209,  0.0299,  0.0222,  0.0035,  0.0131,\n",
      "        -0.0171,  0.0068,  0.0097, -0.0020,  0.0028, -0.0007, -0.0098,  0.0146,\n",
      "         0.0023,  0.0079,  0.0433,  0.0168, -0.0135,  0.0124,  0.0017,  0.0289,\n",
      "        -0.0035, -0.0099,  0.0015,  0.0050, -0.0008, -0.0006, -0.0067,  0.0056,\n",
      "        -0.0211,  0.0053, -0.0005,  0.0025,  0.0217,  0.0165,  0.0032, -0.0011,\n",
      "        -0.0046,  0.0027,  0.0021, -0.0078,  0.0090,  0.0040,  0.0084,  0.0083,\n",
      "         0.0069, -0.0009, -0.0035,  0.0034, -0.0042, -0.0056,  0.0196,  0.0011])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', tensor([[ 9.8741,  9.9779,  9.9477,  9.9237,  9.8951,  9.9624,  9.8981, 10.0159,\n",
      "          9.8702,  9.9576,  9.8119,  9.9446,  9.9956,  9.9308,  9.9159,  9.9346,\n",
      "         10.0516,  9.5969,  9.9887,  9.8698,  9.9421, 10.0112,  9.9146, 10.0494,\n",
      "          9.8992, 10.0198,  9.9650,  9.9534,  9.9083,  9.9923,  9.9448,  9.9514,\n",
      "          9.9197,  9.9036,  9.9010,  9.9741,  9.9241,  9.6049,  9.9184,  9.9264,\n",
      "          9.9058,  9.8549,  9.9296,  9.8227, 10.0770,  9.9791,  9.3892,  9.8565,\n",
      "          9.8586,  9.9699,  9.8643,  9.9069,  9.7694,  9.9238,  9.9515, 10.0256,\n",
      "          9.7750,  9.7241,  9.9042,  9.9762,  9.7498,  9.9487,  9.8086,  9.9197]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight', tensor([[-0.0046, -0.0088, -0.0087,  ...,  0.0104, -0.0123, -0.0080],\n",
      "        [ 0.0006, -0.0133,  0.0103,  ..., -0.0371,  0.0136,  0.0009],\n",
      "        [ 0.0190,  0.0059,  0.0606,  ...,  0.0248, -0.0091, -0.0058],\n",
      "        ...,\n",
      "        [-0.0249,  0.0052, -0.0479,  ..., -0.0570,  0.0225,  0.0129],\n",
      "        [ 0.0048,  0.0047,  0.0074,  ...,  0.0026, -0.0196, -0.0018],\n",
      "        [-0.0217,  0.0014, -0.0075,  ...,  0.0085, -0.0195,  0.0174]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias', tensor([-4.8299e-02, -1.1271e-02,  9.7852e-03, -2.3710e-02,  2.4628e-02,\n",
      "        -6.1949e-02, -1.3997e-02, -8.0121e-03,  1.7903e-02,  4.8195e-04,\n",
      "         1.2164e-02, -1.6559e-02,  6.0261e-03,  2.7543e-02,  7.6178e-03,\n",
      "         1.4414e-02,  2.9745e-02,  5.3405e-03, -2.9245e-02,  4.7318e-02,\n",
      "        -2.9835e-02,  5.5110e-02,  5.9673e-04, -1.1890e-02,  1.5941e-02,\n",
      "         2.4508e-02,  4.2162e-02, -3.2274e-02,  4.6408e-04, -2.1980e-02,\n",
      "        -4.9625e-02, -5.4127e-02, -1.6822e-02,  1.0251e-04,  2.1817e-02,\n",
      "         5.5895e-02,  3.9061e-02,  1.8907e-03,  2.3322e-02, -7.0286e-05,\n",
      "        -5.1524e-02,  3.4299e-02, -1.4858e-02, -4.1297e-02, -3.4672e-02,\n",
      "         1.9395e-02, -7.6044e-04, -4.2831e-02,  4.8759e-02, -1.4730e-02,\n",
      "         3.2076e-02, -3.1457e-02,  2.7999e-03,  2.5955e-02,  2.2356e-02,\n",
      "         1.2249e-02, -5.9656e-02, -3.9672e-03,  3.0609e-02, -5.6245e-03,\n",
      "        -5.2902e-02, -1.3027e-02,  2.1763e-03, -1.2531e-02])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq', tensor([[ 9.8741,  9.9779,  9.9477,  9.9237,  9.8951,  9.9624,  9.8981, 10.0159,\n",
      "          9.8702,  9.9576,  9.8119,  9.9446,  9.9956,  9.9308,  9.9159,  9.9346,\n",
      "         10.0516,  9.5969,  9.9887,  9.8698,  9.9421, 10.0112,  9.9146, 10.0494,\n",
      "          9.8992, 10.0198,  9.9650,  9.9534,  9.9083,  9.9923,  9.9448,  9.9514,\n",
      "          9.9197,  9.9036,  9.9010,  9.9741,  9.9241,  9.6049,  9.9184,  9.9264,\n",
      "          9.9058,  9.8549,  9.9296,  9.8227, 10.0770,  9.9791,  9.3892,  9.8565,\n",
      "          9.8586,  9.9699,  9.8643,  9.9069,  9.7694,  9.9238,  9.9515, 10.0256,\n",
      "          9.7750,  9.7241,  9.9042,  9.9762,  9.7498,  9.9487,  9.8086,  9.9197]])), ('model.backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight', tensor([[ 0.0069, -0.0112, -0.1594,  ...,  0.0198, -0.0159,  0.0181],\n",
      "        [-0.0291, -0.0312,  0.0553,  ..., -0.0340,  0.0236, -0.0092],\n",
      "        [-0.0707, -0.0394, -0.0246,  ...,  0.1056, -0.0023,  0.0636],\n",
      "        ...,\n",
      "        [ 0.1559,  0.1055, -0.0697,  ...,  0.0290,  0.0243,  0.0129],\n",
      "        [ 0.0702,  0.0053, -0.0446,  ..., -0.0040, -0.0023,  0.0047],\n",
      "        [-0.0060, -0.0028,  0.0094,  ...,  0.0119,  0.0035,  0.0006]])), ('model.backbone.layers.0.mixer.filter_fn.modulation.deltas', tensor([[[ -3.0701,  -3.1668,  -3.2635,  -3.3602,  -3.4569,  -3.5536,  -3.6503,\n",
      "           -3.7470,  -3.8437,  -3.9404,  -4.0371,  -4.1338,  -4.2305,  -4.3272,\n",
      "           -4.4239,  -4.5206,  -4.6173,  -4.7140,  -4.8106,  -4.9073,  -5.0040,\n",
      "           -5.1007,  -5.1974,  -5.2941,  -5.3908,  -5.4875,  -5.5842,  -5.6809,\n",
      "           -5.7776,  -5.8743,  -5.9710,  -6.0677,  -6.1644,  -6.2611,  -6.3578,\n",
      "           -6.4545,  -6.5512,  -6.6479,  -6.7446,  -6.8413,  -6.9380,  -7.0347,\n",
      "           -7.1314,  -7.2281,  -7.3248,  -7.4215,  -7.5182,  -7.6148,  -7.7115,\n",
      "           -7.8082,  -7.9049,  -8.0016,  -8.0983,  -8.1950,  -8.2917,  -8.3884,\n",
      "           -8.4851,  -8.5818,  -8.6785,  -8.7752,  -8.8719,  -8.9686,  -9.0653,\n",
      "           -9.1620,  -9.2587,  -9.3554,  -9.4521,  -9.5488,  -9.6455,  -9.7422,\n",
      "           -9.8389,  -9.9356, -10.0323, -10.1290, -10.2257, -10.3223, -10.4190,\n",
      "          -10.5157, -10.6124, -10.7091, -10.8058, -10.9025, -10.9992, -11.0959,\n",
      "          -11.1926, -11.2893, -11.3860, -11.4827, -11.5794, -11.6761, -11.7728,\n",
      "          -11.8695, -11.9662, -12.0629, -12.1596, -12.2563, -12.3530, -12.4497,\n",
      "          -12.5464, -12.6431, -12.7398, -12.8365, -12.9332, -13.0299, -13.1265,\n",
      "          -13.2232, -13.3199, -13.4166, -13.5133, -13.6100, -13.7067, -13.8034,\n",
      "          -13.9001, -13.9968, -14.0935, -14.1902, -14.2869, -14.3836, -14.4803,\n",
      "          -14.5770, -14.6737, -14.7704, -14.8671, -14.9638, -15.0605, -15.1572,\n",
      "          -15.2539, -15.3506]]])), ('model.backbone.layers.0.norm1.weight', tensor([0.5196, 0.5746, 0.5235, 0.3825, 0.5351, 0.4639, 0.4833, 0.5451, 0.5828,\n",
      "        0.5221, 0.5836, 0.5495, 0.5206, 0.5978, 0.5434, 0.5856, 0.5529, 0.5246,\n",
      "        0.5265, 0.5709, 0.5286, 0.5122, 0.5961, 0.5183, 0.4864, 0.5368, 0.4799,\n",
      "        0.5523, 0.5806, 0.4616, 0.5057, 0.6072, 0.5370, 0.4865, 0.5230, 0.6310,\n",
      "        0.5185, 0.5297, 0.7271, 0.6387, 0.5574, 0.5263, 0.5117, 0.5919, 0.5684,\n",
      "        0.5266, 0.5504, 0.4813, 0.4776, 0.5356, 0.5462, 0.5137, 0.5372, 0.5056,\n",
      "        0.5076, 0.5337, 0.4993, 0.5108, 0.5290, 0.5584, 0.5342, 0.5852, 0.5590,\n",
      "        0.5079, 0.5115, 0.5393, 0.5920, 0.4883, 0.5913, 0.5479, 0.5038, 0.5100,\n",
      "        0.5905, 0.5486, 0.5375, 0.4886, 0.5313, 0.5422, 0.5884, 0.5640, 0.4848,\n",
      "        0.5858, 0.5379, 0.5094, 0.5787, 0.4423, 0.5266, 0.5055, 0.5679, 0.5183,\n",
      "        0.6075, 0.5417, 0.6376, 0.5785, 0.5210, 0.6034, 0.5614, 0.5430, 0.5204,\n",
      "        0.5169, 0.5052, 0.5889, 0.5346, 0.5513, 0.5839, 0.5089, 0.6115, 0.5614,\n",
      "        0.5542, 0.5736, 0.5568, 0.5055, 0.4879, 0.5435, 0.5178, 0.5751, 0.5393,\n",
      "        0.4890, 0.5349, 0.5115, 0.5024, 0.5535, 0.5465, 0.5684, 0.5567, 0.5580,\n",
      "        0.5211, 0.5357])), ('model.backbone.layers.0.norm1.bias', tensor([-0.0072,  0.0523,  0.0039,  0.0537,  0.0729, -0.1094,  0.0008, -0.0526,\n",
      "        -0.0274,  0.0225,  0.0696, -0.0289, -0.0070, -0.0639, -0.0297,  0.0498,\n",
      "         0.0974,  0.0435, -0.0447, -0.0161,  0.0458,  0.0101, -0.0021,  0.0270,\n",
      "         0.0048,  0.0099, -0.0782, -0.0549,  0.0707, -0.1095, -0.0065,  0.0317,\n",
      "         0.0486,  0.0154,  0.0464,  0.0252, -0.0134,  0.0369,  0.0040,  0.0398,\n",
      "        -0.0831,  0.0026,  0.0064, -0.0706, -0.0254, -0.1046, -0.0209, -0.0800,\n",
      "         0.0102, -0.0288,  0.0326, -0.0310, -0.0304,  0.0330, -0.0066,  0.0672,\n",
      "        -0.0017,  0.0361,  0.0072,  0.0864,  0.0033,  0.0043,  0.0719,  0.0440,\n",
      "         0.0501, -0.0134,  0.0067,  0.0085, -0.0494, -0.0167,  0.0091,  0.0004,\n",
      "         0.0247,  0.0522, -0.0609,  0.0391, -0.0050,  0.0175,  0.0438,  0.0453,\n",
      "         0.0037, -0.1121,  0.0297,  0.0275,  0.0398, -0.0205,  0.0365, -0.1263,\n",
      "         0.0875,  0.0280, -0.0706,  0.0140,  0.0589,  0.0250, -0.0087, -0.0379,\n",
      "        -0.0334, -0.0382,  0.0069, -0.0030,  0.0351, -0.0507,  0.0160, -0.0409,\n",
      "        -0.0249, -0.0321,  0.0814,  0.0711, -0.0233,  0.0191,  0.0020, -0.0073,\n",
      "         0.0208, -0.0100, -0.0051, -0.0007, -0.0250, -0.0267, -0.0472, -0.0565,\n",
      "         0.0024, -0.0287, -0.0346,  0.0281, -0.0588,  0.0173, -0.0142,  0.0218])), ('model.backbone.layers.0.mlp.fc1.weight', tensor([[ 0.1231, -0.0400,  0.0881,  ...,  0.0310,  0.0148, -0.0034],\n",
      "        [-0.0108, -0.0490, -0.0234,  ..., -0.0363,  0.0003, -0.0498],\n",
      "        [ 0.0353, -0.0271, -0.0521,  ..., -0.0398, -0.0432,  0.0290],\n",
      "        ...,\n",
      "        [-0.0396,  0.0446, -0.0115,  ..., -0.0083, -0.0041, -0.0205],\n",
      "        [-0.0906, -0.0163,  0.0589,  ...,  0.0195,  0.0629, -0.0656],\n",
      "        [-0.0328, -0.0337, -0.0135,  ...,  0.0413, -0.0319,  0.0198]])), ('model.backbone.layers.0.mlp.fc1.bias', tensor([-0.0096, -0.0693, -0.0343, -0.0310, -0.0463, -0.0433, -0.0498, -0.0419,\n",
      "        -0.0460, -0.0197, -0.0390, -0.0426, -0.0146, -0.0349, -0.0224, -0.0224,\n",
      "        -0.0393, -0.0470, -0.0457, -0.0460, -0.0406, -0.0390, -0.0476, -0.0500,\n",
      "        -0.0433, -0.0338, -0.0526, -0.0523, -0.0456, -0.0352, -0.0282, -0.0448,\n",
      "        -0.0268, -0.0378, -0.0359, -0.0250, -0.0513, -0.0940, -0.0510, -0.0366,\n",
      "        -0.0286, -0.0459, -0.0445, -0.0424, -0.0379, -0.0467, -0.0705, -0.0497,\n",
      "        -0.0464, -0.0492, -0.0680, -0.0514, -0.0439, -0.0276, -0.0578, -0.0455,\n",
      "        -0.0632, -0.0418, -0.0380, -0.0594, -0.0440, -0.0394, -0.0264, -0.0358,\n",
      "        -0.0616, -0.0330, -0.0353, -0.0516, -0.0728, -0.0408, -0.0377, -0.0057,\n",
      "        -0.0389, -0.0431, -0.0219, -0.0433, -0.0399, -0.0409, -0.0498, -0.0401,\n",
      "        -0.0648, -0.0651, -0.0499, -0.0166, -0.0433, -0.0164, -0.0187, -0.0375,\n",
      "        -0.0263, -0.0457, -0.0440, -0.0458, -0.0236, -0.0176, -0.0547, -0.0315,\n",
      "        -0.0585, -0.0331, -0.0406, -0.0260, -0.0322, -0.0651, -0.0151, -0.0641,\n",
      "        -0.0457, -0.0458, -0.0435, -0.0565, -0.0393, -0.0046, -0.0157, -0.0425,\n",
      "        -0.0480, -0.0212,  0.0041, -0.0336, -0.0260, -0.0408, -0.0396, -0.0261,\n",
      "        -0.0389, -0.0381, -0.0451, -0.0497, -0.0442, -0.0363, -0.0298, -0.0569,\n",
      "        -0.0278, -0.0500, -0.0537, -0.0334, -0.0369, -0.0360, -0.0158, -0.0636,\n",
      "        -0.0571, -0.0215, -0.0224, -0.0446, -0.0306, -0.0254, -0.0278, -0.0309,\n",
      "        -0.0270, -0.0518, -0.0432, -0.0564, -0.0530, -0.0655, -0.0588, -0.0426,\n",
      "        -0.0274, -0.0353, -0.0258, -0.0570, -0.0409, -0.0496, -0.0407, -0.0200,\n",
      "        -0.0349, -0.0293, -0.0296, -0.0467, -0.0174, -0.0518, -0.0423, -0.0349,\n",
      "        -0.0363, -0.0215, -0.0562, -0.0292, -0.0348, -0.0393, -0.0512, -0.0594,\n",
      "        -0.0290, -0.0320, -0.0452, -0.0575, -0.0305, -0.0324, -0.0457, -0.0388,\n",
      "        -0.0629, -0.0225, -0.0640, -0.0439, -0.0229, -0.0634, -0.0333, -0.0333,\n",
      "        -0.0552, -0.0473, -0.0477, -0.0299, -0.0311, -0.0373, -0.0417, -0.0444,\n",
      "        -0.0517, -0.0283, -0.0491, -0.0467, -0.0466, -0.0307, -0.0352, -0.0252,\n",
      "        -0.0338, -0.0327, -0.0535, -0.0325, -0.0371, -0.0537, -0.0451, -0.0490,\n",
      "        -0.0736, -0.0368, -0.0386, -0.0545, -0.0449, -0.0252, -0.0508, -0.0380,\n",
      "        -0.0280, -0.0363, -0.0600, -0.0213, -0.0430, -0.0512, -0.0454, -0.0253,\n",
      "        -0.0598, -0.0357, -0.0308, -0.0356, -0.0468, -0.0482, -0.0645, -0.0257,\n",
      "        -0.0268, -0.0468, -0.0491, -0.0286, -0.0223, -0.0575, -0.0497, -0.0310,\n",
      "        -0.0376, -0.0383, -0.0622, -0.0339, -0.0517, -0.0134, -0.0233, -0.0425,\n",
      "        -0.0511, -0.0334, -0.0439, -0.0495, -0.0179, -0.0291, -0.0469, -0.0186,\n",
      "        -0.0598, -0.0505, -0.0512, -0.0285, -0.0466, -0.0570, -0.0511, -0.0308,\n",
      "        -0.0450, -0.0602, -0.0710, -0.0541, -0.0422, -0.0420, -0.0570, -0.0403,\n",
      "        -0.0532, -0.0268, -0.0708, -0.0302, -0.0527, -0.0409, -0.0555, -0.0602,\n",
      "        -0.0356, -0.0395, -0.0494, -0.0489, -0.0196, -0.0481, -0.0437, -0.0578,\n",
      "        -0.0280, -0.0497, -0.0327, -0.0284, -0.0431, -0.0546, -0.0478, -0.0402,\n",
      "        -0.0254, -0.0382, -0.0508, -0.0395, -0.0377, -0.0546, -0.0267, -0.0615,\n",
      "        -0.0338, -0.0474, -0.0603, -0.0408, -0.0302, -0.0573, -0.0497, -0.0661,\n",
      "        -0.0430, -0.0452, -0.0486, -0.0334, -0.0200, -0.0274, -0.0414, -0.0239,\n",
      "        -0.0581, -0.0201, -0.0311, -0.0259, -0.0422, -0.0404, -0.0401, -0.0494,\n",
      "        -0.0636, -0.0613, -0.0340, -0.0436, -0.0297, -0.0449, -0.0538, -0.0303,\n",
      "        -0.0829, -0.0377, -0.0305, -0.0235, -0.0433, -0.0296, -0.0381, -0.0604,\n",
      "        -0.0582, -0.0255, -0.0430, -0.0423, -0.0525, -0.0263, -0.0292, -0.0472,\n",
      "        -0.0569, -0.0565, -0.0426, -0.0347, -0.0391, -0.0697, -0.0326, -0.0424,\n",
      "        -0.0533, -0.0425, -0.0407, -0.0388, -0.0223, -0.0378, -0.0449, -0.0355,\n",
      "        -0.0323, -0.0684, -0.0515, -0.0245, -0.0218, -0.0317, -0.0454, -0.0418,\n",
      "        -0.0229, -0.0415, -0.0529, -0.0469, -0.0452, -0.0378, -0.0811, -0.0496,\n",
      "        -0.0430, -0.0574, -0.0493, -0.0695, -0.0539, -0.0421, -0.0463, -0.0386,\n",
      "        -0.0261, -0.0187, -0.0472, -0.0315, -0.0340, -0.0548, -0.0457, -0.0365,\n",
      "        -0.0370, -0.0263, -0.0538, -0.0430, -0.0248, -0.0333, -0.0625, -0.0565,\n",
      "        -0.0280, -0.0394, -0.0307, -0.0528, -0.0231, -0.0378, -0.0448, -0.0471,\n",
      "        -0.0431, -0.0442, -0.0354, -0.0547, -0.0414, -0.0517, -0.0583, -0.0323,\n",
      "        -0.0580, -0.0542, -0.0402, -0.0245, -0.0690, -0.0311, -0.0399, -0.0442,\n",
      "        -0.0196, -0.0504, -0.0623, -0.0392, -0.0286, -0.0565, -0.0254, -0.0541,\n",
      "        -0.0528, -0.0169, -0.0588, -0.0462, -0.0414, -0.0465, -0.0456, -0.0526,\n",
      "        -0.0523, -0.0624, -0.0486, -0.0400, -0.0498, -0.0269, -0.0495, -0.0112,\n",
      "        -0.0520, -0.0557, -0.0488, -0.0432, -0.0269, -0.0292, -0.0411, -0.0281,\n",
      "        -0.0309, -0.0211, -0.0466, -0.0744, -0.0385, -0.0395, -0.0339, -0.0377,\n",
      "        -0.0488, -0.0304, -0.0374, -0.0520, -0.0308, -0.0226, -0.0375, -0.0233,\n",
      "        -0.0095, -0.0489, -0.0300, -0.0453, -0.0612, -0.0386, -0.0509, -0.0418,\n",
      "        -0.0315, -0.0427, -0.0544, -0.0478, -0.0309, -0.0421, -0.0509, -0.0738,\n",
      "        -0.0396, -0.0132, -0.0432, -0.0270, -0.0428, -0.0467, -0.0369, -0.0477])), ('model.backbone.layers.0.mlp.fc2.weight', tensor([[ 0.0060, -0.0641,  0.0035,  ...,  0.0324, -0.0121,  0.0577],\n",
      "        [ 0.0101,  0.0219,  0.0155,  ..., -0.0361, -0.0038, -0.0290],\n",
      "        [-0.0892, -0.0253,  0.0104,  ..., -0.0458,  0.0256, -0.0400],\n",
      "        ...,\n",
      "        [-0.0452,  0.0004, -0.0289,  ..., -0.0093, -0.0620, -0.0403],\n",
      "        [ 0.0484,  0.0151,  0.0490,  ...,  0.0503,  0.0001,  0.0878],\n",
      "        [-0.0097,  0.0483,  0.0251,  ...,  0.0187, -0.0264, -0.0409]])), ('model.backbone.layers.0.mlp.fc2.bias', tensor([-8.6417e-03, -2.0885e-02,  1.4988e-03,  2.4808e-03,  5.3821e-03,\n",
      "         1.0411e-02, -5.7475e-03, -2.7891e-02, -1.0827e-02,  2.1970e-03,\n",
      "        -1.6564e-03, -1.8154e-02, -2.2645e-02, -6.1027e-03, -4.5617e-03,\n",
      "         1.2769e-02,  4.3798e-04,  1.4522e-02, -1.9263e-02, -9.7446e-03,\n",
      "         7.1433e-03, -9.1042e-03, -2.0350e-02,  8.8787e-03,  7.5354e-03,\n",
      "        -5.6736e-03,  1.3258e-02,  1.1543e-02, -2.5662e-03,  9.5675e-03,\n",
      "         9.3443e-03,  6.5433e-05,  1.1823e-02,  4.3005e-03,  4.0256e-03,\n",
      "         8.2032e-04, -4.6052e-04,  2.3355e-02, -1.0421e-02, -6.3766e-03,\n",
      "         1.6690e-03, -3.6621e-02,  1.4835e-02, -9.3099e-03, -6.4884e-03,\n",
      "         1.5592e-02,  8.4891e-03, -1.5312e-02, -1.3542e-02,  4.2026e-03,\n",
      "         3.9695e-03, -7.1309e-03, -9.7554e-03, -1.8984e-02,  3.4332e-03,\n",
      "         4.1563e-03, -7.2125e-03,  1.3506e-02,  7.8407e-04,  8.1574e-04,\n",
      "         1.0798e-02, -1.4299e-02, -4.8130e-03, -5.4864e-03, -1.6007e-02,\n",
      "        -8.5428e-03,  9.7243e-03, -1.5282e-02,  3.4756e-02, -1.3193e-02,\n",
      "        -9.3249e-03, -5.9710e-03, -1.1929e-02,  1.4836e-02,  1.0446e-02,\n",
      "        -1.2173e-02,  2.3040e-02,  8.4181e-04, -1.8724e-02, -1.8157e-02,\n",
      "         9.4590e-04, -1.4557e-03, -9.4542e-04,  2.3407e-02, -8.4405e-03,\n",
      "         1.0102e-02,  1.2458e-02,  9.2631e-03,  1.0881e-02, -3.7919e-03,\n",
      "        -8.6449e-03,  7.2171e-03,  3.1907e-04, -2.0076e-02,  1.3071e-02,\n",
      "         6.8992e-04, -5.2203e-03,  3.4170e-03, -1.0776e-02,  6.8979e-03,\n",
      "         1.0573e-02,  4.3769e-03,  1.9688e-02,  1.5002e-02,  8.6367e-04,\n",
      "         1.7132e-02,  3.6246e-03,  2.9556e-03, -1.8524e-02,  1.1482e-02,\n",
      "         1.1867e-02,  1.2043e-02,  1.7130e-02, -2.3583e-03, -1.3377e-02,\n",
      "        -1.9195e-02,  5.8371e-04,  4.0722e-03, -4.5287e-03, -2.0790e-03,\n",
      "         9.7934e-03,  5.3823e-03, -8.9401e-04,  2.9176e-03, -1.1862e-03,\n",
      "         7.1889e-03, -8.6665e-03,  1.4445e-02])), ('model.backbone.layers.0.norm2.weight', tensor([0.9053, 1.0156, 0.8537, 0.8003, 0.8914, 0.9254, 0.7563, 0.9705, 0.9898,\n",
      "        1.0017, 1.0152, 0.9461, 0.9087, 1.0341, 0.9257, 0.9187, 0.9689, 0.9312,\n",
      "        0.9260, 0.9192, 0.8867, 0.8648, 0.9705, 0.9138, 0.9469, 0.9111, 0.8854,\n",
      "        0.9885, 1.0126, 0.8846, 0.8892, 0.9884, 0.8952, 0.8497, 0.8314, 0.9884,\n",
      "        0.9713, 0.8522, 0.7666, 1.0085, 0.9844, 0.8734, 1.0064, 0.9611, 0.8445,\n",
      "        0.9034, 0.9043, 0.9658, 0.9239, 0.8343, 0.8719, 0.8675, 0.8770, 0.8023,\n",
      "        0.9427, 0.8918, 0.8555, 0.9506, 0.8511, 0.9334, 0.9732, 0.7369, 0.9160,\n",
      "        0.9277, 0.8985, 0.8467, 0.9493, 0.8433, 0.9099, 0.9636, 0.8744, 0.9014,\n",
      "        0.9912, 0.8714, 0.9309, 0.8846, 0.7899, 0.8638, 0.9711, 0.9610, 0.8930,\n",
      "        0.9640, 0.9326, 0.9316, 0.9565, 0.8571, 0.8615, 1.0145, 0.9688, 0.9471,\n",
      "        0.9572, 0.9827, 1.0403, 0.7838, 0.8443, 0.9728, 1.0489, 0.9333, 0.8998,\n",
      "        0.8906, 0.8777, 0.9407, 0.7946, 0.9321, 0.9921, 0.8388, 1.0000, 0.9576,\n",
      "        0.9570, 0.8788, 0.9319, 0.8851, 0.8832, 0.7449, 0.8746, 0.9112, 0.9375,\n",
      "        0.9549, 0.9392, 0.8710, 0.8733, 0.7400, 1.0390, 0.8503, 0.9538, 0.8253,\n",
      "        0.9141, 0.9361])), ('model.backbone.layers.0.norm2.bias', tensor([-0.0214,  0.1507, -0.1619,  0.0923,  0.1487, -0.0705,  0.0409, -0.1040,\n",
      "         0.0491,  0.1243,  0.2063, -0.1347, -0.0153, -0.0129, -0.1336,  0.0465,\n",
      "         0.0431,  0.0501, -0.0564, -0.0386, -0.0478, -0.0005,  0.1062,  0.0484,\n",
      "        -0.0554,  0.1269, -0.0877, -0.1057,  0.0918, -0.1587,  0.0231, -0.0454,\n",
      "         0.0356, -0.0521,  0.0635,  0.1294, -0.0675,  0.0237, -0.0675,  0.1270,\n",
      "        -0.0149,  0.0930, -0.1125, -0.1525, -0.0170, -0.0514, -0.0817, -0.0291,\n",
      "         0.1151, -0.0558,  0.1368,  0.0721, -0.0636,  0.1264, -0.2423, -0.0056,\n",
      "         0.0145,  0.0812,  0.1310,  0.0569,  0.1771, -0.0723,  0.0278,  0.1323,\n",
      "         0.1092,  0.0485, -0.0337, -0.1216, -0.1730, -0.0581,  0.0606, -0.0079,\n",
      "         0.0770, -0.0201, -0.0411,  0.0500,  0.1191,  0.0238,  0.1750,  0.1720,\n",
      "        -0.0179, -0.0166, -0.0646,  0.0931, -0.0063, -0.1332,  0.1101, -0.1671,\n",
      "         0.1371,  0.1188, -0.1522,  0.1594,  0.1404,  0.1266, -0.1106, -0.1079,\n",
      "        -0.1326, -0.0624,  0.0974, -0.0565,  0.0225, -0.1151,  0.1362, -0.1491,\n",
      "        -0.0962, -0.0732, -0.0607,  0.1356, -0.0532, -0.0421,  0.0312,  0.0143,\n",
      "        -0.0608, -0.0411,  0.2068,  0.0661, -0.1153, -0.1514, -0.1132, -0.1238,\n",
      "        -0.0107, -0.0831, -0.0371,  0.1103, -0.0369,  0.1321, -0.0563, -0.0371])), ('model.backbone.layers.1.mixer.out_proj.weight', tensor([[ 0.0088, -0.0706,  0.0032,  ...,  0.0084, -0.0521,  0.0479],\n",
      "        [ 0.0397,  0.0231,  0.0173,  ..., -0.0137, -0.0826,  0.0275],\n",
      "        [ 0.0149,  0.0114,  0.0347,  ..., -0.0328,  0.0594, -0.0905],\n",
      "        ...,\n",
      "        [-0.0247,  0.0203, -0.0009,  ..., -0.0007, -0.0300, -0.0006],\n",
      "        [ 0.0122, -0.0488, -0.0129,  ..., -0.0186, -0.0021, -0.0131],\n",
      "        [-0.0548,  0.0086, -0.0482,  ...,  0.0064,  0.0476,  0.0269]])), ('model.backbone.layers.1.mixer.out_proj.bias', tensor([-0.0191,  0.0342, -0.0416,  0.0443,  0.0244, -0.0285,  0.0266, -0.0436,\n",
      "         0.0261,  0.0243,  0.0577, -0.0570, -0.0242, -0.0089, -0.0418,  0.0084,\n",
      "         0.0186,  0.0360, -0.0418, -0.0419,  0.0365, -0.0204,  0.0051,  0.0121,\n",
      "        -0.0407,  0.0888,  0.0060, -0.0351,  0.0339, -0.0345,  0.0461,  0.0282,\n",
      "         0.0209, -0.0353,  0.0080,  0.0411,  0.0452,  0.0296, -0.0089,  0.0595,\n",
      "        -0.0006, -0.0320, -0.0473, -0.0369, -0.0361,  0.0017, -0.0262, -0.0934,\n",
      "        -0.0129, -0.0389,  0.0442,  0.0032, -0.0037,  0.0286, -0.0466,  0.0119,\n",
      "        -0.0451,  0.0426,  0.0322,  0.0605,  0.0887, -0.0100,  0.0288,  0.0077,\n",
      "        -0.0019,  0.0072,  0.0337, -0.0016, -0.0256, -0.0311, -0.0235,  0.0422,\n",
      "        -0.0370,  0.0377, -0.0523, -0.0248,  0.0403,  0.0164,  0.0471,  0.0466,\n",
      "        -0.0353, -0.0791,  0.0289,  0.0298,  0.0011, -0.0320,  0.0427, -0.0371,\n",
      "         0.0253,  0.0360, -0.0380,  0.0628,  0.0075,  0.0556, -0.0170, -0.0559,\n",
      "        -0.0293, -0.0600,  0.0214,  0.0322,  0.0067, -0.0398,  0.0324, -0.0064,\n",
      "        -0.0355, -0.0356,  0.0268,  0.0459, -0.0555,  0.0100,  0.0443,  0.0147,\n",
      "         0.0351,  0.0155,  0.0436, -0.0192, -0.0635, -0.0530, -0.0170, -0.0517,\n",
      "        -0.0660, -0.0366, -0.0727,  0.0265, -0.0222,  0.0472, -0.0302,  0.0050])), ('model.backbone.layers.1.mixer.in_proj.weight', tensor([[ 0.0705,  0.0259, -0.0009,  ..., -0.0335, -0.0015, -0.1571],\n",
      "        [-0.0275,  0.0003, -0.0308,  ..., -0.0290,  0.0384,  0.0073],\n",
      "        [-0.0209, -0.0161, -0.0520,  ...,  0.0322, -0.0027,  0.0722],\n",
      "        ...,\n",
      "        [-0.0139, -0.0368,  0.0418,  ..., -0.0573,  0.0113, -0.0266],\n",
      "        [ 0.0278,  0.0720, -0.0019,  ...,  0.0173, -0.0263,  0.0103],\n",
      "        [-0.0355,  0.0103,  0.0251,  ..., -0.0162,  0.0236, -0.0818]])), ('model.backbone.layers.1.mixer.in_proj.bias', tensor([-5.6142e-02, -1.1273e-01, -2.1185e-02,  4.6680e-02, -9.8636e-02,\n",
      "        -7.3239e-02, -6.0205e-02, -4.7188e-02, -2.7132e-02,  2.7012e-02,\n",
      "        -1.3397e-02,  2.7678e-02,  2.8252e-02, -5.6364e-02,  4.3104e-02,\n",
      "         7.4160e-02,  6.0997e-02, -3.2928e-02,  2.9538e-02,  6.0711e-02,\n",
      "        -3.5615e-02, -5.8105e-02, -1.6625e-02,  6.6548e-06, -8.7377e-03,\n",
      "        -2.3600e-02, -1.0878e-03, -4.3052e-03,  3.3129e-02,  1.1107e-01,\n",
      "        -6.3528e-02, -6.7655e-03,  2.2447e-02,  9.2211e-03, -8.3854e-02,\n",
      "         1.0766e-01, -8.7877e-02, -7.4626e-02,  5.1593e-02, -2.3030e-02,\n",
      "         3.0437e-02,  5.1262e-02,  2.7687e-02,  6.8705e-02, -8.9618e-03,\n",
      "        -1.1021e-01,  1.4713e-01,  4.7715e-02,  7.1707e-02,  3.7584e-02,\n",
      "         2.1546e-02,  5.1006e-03,  3.9361e-02,  1.1850e-02, -2.8196e-02,\n",
      "         2.5354e-02, -2.6756e-02, -9.7002e-03,  3.5194e-02,  1.1495e-02,\n",
      "        -4.7510e-02,  3.2056e-02, -2.0418e-02, -4.4847e-02,  1.2675e-01,\n",
      "        -9.9369e-02,  4.4821e-03,  2.8606e-02, -8.2384e-03, -4.5496e-02,\n",
      "        -2.8916e-02,  1.0607e-02,  3.2000e-02,  5.8030e-02,  1.1228e-02,\n",
      "        -3.5679e-02,  3.3271e-02,  7.0982e-02, -7.6476e-02,  7.1702e-02,\n",
      "         2.2421e-02, -5.3439e-02, -2.7761e-02, -7.1975e-02,  3.1721e-02,\n",
      "        -4.4622e-02,  9.7090e-02, -6.2067e-02,  2.3921e-02,  5.6868e-03,\n",
      "        -4.0259e-02, -1.7922e-02,  5.7875e-03, -4.8411e-02, -6.8221e-02,\n",
      "         1.0801e-01,  3.3279e-02, -1.2002e-02, -4.6097e-02, -3.3712e-02,\n",
      "         1.9928e-02,  4.2593e-02,  8.2050e-02, -1.1398e-02, -4.3145e-02,\n",
      "        -2.7030e-03,  6.4273e-03,  2.4707e-02, -1.7355e-03, -2.3902e-02,\n",
      "        -1.1114e-02, -3.4246e-03,  2.1308e-02,  3.3846e-02, -3.8781e-02,\n",
      "        -1.8973e-02,  3.5298e-03, -3.5032e-02, -9.6246e-02, -9.9238e-03,\n",
      "        -3.3588e-03,  6.1213e-02, -1.5382e-02, -3.7000e-02,  8.7013e-03,\n",
      "        -2.1863e-03, -1.6843e-02,  4.2222e-02,  3.3593e-02,  9.9056e-03,\n",
      "        -9.7123e-04,  7.3367e-02, -6.1003e-02, -4.7594e-02, -1.6329e-02,\n",
      "        -7.6929e-03, -8.0950e-04, -8.3576e-04, -1.8789e-02,  1.8801e-02,\n",
      "        -1.6873e-02, -1.6071e-02,  4.6345e-02,  1.8969e-02,  3.7362e-02,\n",
      "        -3.5722e-02, -3.1481e-02,  1.2378e-03,  1.6130e-02,  1.1523e-01,\n",
      "        -3.6888e-02,  2.7201e-02, -1.1023e-02,  2.1024e-02,  3.2888e-02,\n",
      "         1.4549e-02,  2.6688e-02, -4.3451e-02, -1.9935e-02,  8.9784e-03,\n",
      "        -3.1956e-02,  4.5601e-03,  1.4281e-02, -9.7064e-03, -5.0712e-02,\n",
      "         3.9687e-03,  8.7571e-03, -9.7648e-02, -7.1219e-03, -3.9890e-02,\n",
      "        -1.6929e-02, -3.6711e-03, -3.0745e-02,  6.6870e-03, -9.0039e-02,\n",
      "         3.1578e-02, -2.2315e-02, -6.4801e-03, -5.7278e-02, -6.4234e-03,\n",
      "         4.6026e-02,  2.9842e-03,  1.2639e-02, -3.3052e-04,  8.5635e-03,\n",
      "        -1.4016e-03,  1.8408e-02,  4.6995e-02,  2.7119e-02, -1.1161e-03,\n",
      "        -3.7215e-02, -4.8635e-02, -1.8057e-02,  1.4400e-02, -2.4850e-02,\n",
      "        -2.9795e-02, -6.9122e-02, -5.4548e-02,  1.7659e-02, -1.4256e-02,\n",
      "        -3.3164e-02, -1.4504e-02, -3.4737e-02,  2.5517e-02,  8.1449e-03,\n",
      "        -1.0558e-01, -2.9515e-03,  7.5807e-02, -4.8821e-02, -4.7891e-03,\n",
      "         1.0083e-02,  3.7914e-02,  1.1608e-02,  6.6637e-03, -1.2655e-02,\n",
      "         5.3324e-02,  5.1184e-02,  2.2968e-02,  3.6321e-02,  1.6550e-02,\n",
      "         5.5325e-02, -2.1137e-02,  4.6764e-02,  4.2217e-03,  4.9063e-02,\n",
      "         1.7095e-02,  1.1315e-01, -3.7274e-02, -7.5445e-03,  1.8754e-02,\n",
      "        -1.3863e-02,  4.2108e-03, -1.4954e-02, -2.1429e-02, -3.1287e-02,\n",
      "         4.1939e-02, -5.4481e-02, -2.8477e-02, -6.9186e-02, -3.5072e-02,\n",
      "        -1.7573e-02,  9.6527e-02, -2.0526e-02,  2.4417e-02,  4.4046e-02,\n",
      "        -6.1998e-02, -1.0454e-01, -1.2822e-02, -2.5413e-02, -5.5920e-03,\n",
      "         1.2951e-02, -2.4730e-03, -2.2439e-02,  1.2953e-02, -2.2701e-02,\n",
      "         1.0256e-02, -4.2514e-02,  4.5911e-03, -1.4237e-02, -1.0236e-02,\n",
      "         4.4848e-02,  2.7751e-03,  9.2473e-03, -1.7257e-04,  7.4751e-04,\n",
      "        -4.2567e-02, -3.4988e-02, -1.6991e-02, -4.3046e-02,  1.6147e-02,\n",
      "         7.5703e-03, -1.0886e-01,  7.4487e-03,  2.7123e-02, -3.2778e-03,\n",
      "        -9.1243e-03, -1.7387e-02, -3.0741e-02,  2.1025e-02,  1.5750e-02,\n",
      "         4.5811e-02, -5.8290e-02,  1.9703e-02, -1.4660e-02,  5.1655e-02,\n",
      "        -9.4442e-03, -3.5464e-02, -3.3961e-02, -6.3747e-03, -4.5613e-03,\n",
      "        -2.4865e-02,  2.8988e-03,  3.4529e-02, -3.5308e-02, -3.2409e-02,\n",
      "         1.9074e-02, -2.3273e-02, -2.9543e-02,  9.7587e-03,  2.5221e-02,\n",
      "        -1.6171e-02, -1.0288e-02,  3.0045e-02, -3.4387e-02,  2.1945e-02,\n",
      "        -4.2518e-02, -1.2837e-02,  2.9837e-02, -3.7076e-02, -2.1960e-02,\n",
      "         2.9608e-02,  3.9080e-02, -9.7186e-02, -1.4818e-02, -7.9182e-03,\n",
      "         1.0990e-02,  3.2182e-02, -5.9773e-03, -2.0509e-03, -3.5902e-02,\n",
      "         8.5455e-02,  3.8783e-03, -2.0426e-02, -3.1412e-02,  1.2367e-02,\n",
      "         9.9887e-02, -2.9679e-02,  1.5131e-02, -5.0697e-04,  1.3473e-02,\n",
      "         2.9210e-03,  8.3790e-03,  1.2310e-02,  1.2286e-02,  3.7260e-03,\n",
      "         5.5367e-02, -4.0604e-02, -1.1467e-02,  1.7050e-02, -5.5692e-02,\n",
      "        -1.8828e-02, -3.9228e-03, -6.4216e-03,  6.2841e-02, -4.1294e-02,\n",
      "         9.1165e-03, -1.8245e-02, -4.7111e-03, -2.6751e-02,  3.3034e-02,\n",
      "        -1.8780e-02,  1.3988e-02, -3.9664e-02,  4.5858e-02, -3.0551e-02,\n",
      "         2.3726e-02,  1.8359e-02,  1.8057e-02, -5.9954e-02, -1.4667e-02,\n",
      "        -1.0037e-02,  1.9713e-02, -4.0341e-02,  1.2228e-02, -3.3976e-02,\n",
      "        -8.7651e-02,  2.3035e-02, -2.1313e-03,  7.4362e-02,  6.6406e-03,\n",
      "         4.8265e-02,  2.6847e-03,  1.8844e-02, -1.8895e-02, -9.9656e-02,\n",
      "        -6.4158e-02,  2.6507e-02,  8.0238e-03, -1.0490e-02, -1.2882e-02,\n",
      "         2.5059e-02,  1.1035e-02, -1.3250e-02,  5.0306e-03])), ('model.backbone.layers.1.mixer.short_filter.weight', tensor([[[ 0.0914,  0.2278,  0.3032]],\n",
      "\n",
      "        [[ 0.1523,  0.1180,  0.3066]],\n",
      "\n",
      "        [[-0.0851, -0.0033, -0.2106]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1904,  0.1379, -0.1831]],\n",
      "\n",
      "        [[-0.1699, -0.1407,  0.1140]],\n",
      "\n",
      "        [[ 0.2898,  0.2857,  0.2024]]])), ('model.backbone.layers.1.mixer.short_filter.bias', tensor([-0.3132, -0.2943,  0.2329,  0.2303,  0.0840, -0.2820,  0.0782, -0.1021,\n",
      "         0.1274,  0.0030,  0.0170,  0.1998, -0.2158,  0.0355, -0.1048,  0.1851,\n",
      "        -0.1312, -0.0626,  0.1656, -0.1706, -0.1072,  0.0140, -0.0215, -0.2677,\n",
      "        -0.1991, -0.0995,  0.0040,  0.2551, -0.2031,  0.3217,  0.3100,  0.0912,\n",
      "        -0.0054,  0.2169, -0.1400,  0.1044,  0.2548, -0.1735,  0.0123,  0.0042,\n",
      "         0.1565,  0.1869,  0.1124,  0.1336,  0.0418, -0.1065,  0.2412,  0.2181,\n",
      "         0.1844,  0.0945, -0.0081, -0.2546, -0.1236, -0.0386,  0.2012,  0.2857,\n",
      "        -0.1992,  0.0274,  0.2019,  0.1761,  0.2163, -0.0856, -0.0606,  0.1440,\n",
      "        -0.0826,  0.2124,  0.0484,  0.0576,  0.0955,  0.0428, -0.0310, -0.1109,\n",
      "        -0.2887, -0.1332, -0.0501,  0.1290, -0.0185,  0.0881, -0.0524, -0.2786,\n",
      "         0.0533,  0.0982,  0.0362,  0.1578, -0.2117,  0.1193, -0.2306, -0.2037,\n",
      "         0.0193,  0.0429,  0.2231, -0.0718, -0.0048,  0.0988, -0.1848, -0.1194,\n",
      "        -0.3006, -0.0044, -0.0017, -0.0027,  0.1004, -0.0084,  0.2515,  0.1085,\n",
      "         0.1033, -0.0882,  0.0198, -0.0203, -0.0786, -0.2060,  0.0312, -0.0473,\n",
      "         0.2251, -0.1240,  0.0389, -0.0102,  0.0123,  0.1000,  0.3613,  0.0318,\n",
      "         0.0228,  0.1776,  0.2532, -0.1525, -0.0259,  0.1332,  0.1710, -0.1323,\n",
      "         0.2036,  0.0027, -0.1267, -0.0940, -0.1031, -0.0489, -0.0269,  0.2643,\n",
      "        -0.1745, -0.1151,  0.1806,  0.2248,  0.0386,  0.1999, -0.0588, -0.2007,\n",
      "         0.0782, -0.1664, -0.0405, -0.0574,  0.1361,  0.3689, -0.1284,  0.2115,\n",
      "         0.0123, -0.0120, -0.0875,  0.0534,  0.2395, -0.0658, -0.0313, -0.0409,\n",
      "         0.0308,  0.1623,  0.0495,  0.2236,  0.0535, -0.0770,  0.0507, -0.3575,\n",
      "         0.0274, -0.0942,  0.1948,  0.1173, -0.0860,  0.1416, -0.1375,  0.1272,\n",
      "         0.0479, -0.0076, -0.1440,  0.0636,  0.0783,  0.1157, -0.2673, -0.1002,\n",
      "         0.1671,  0.0691, -0.0490,  0.1494, -0.1477,  0.0645,  0.2768, -0.2325,\n",
      "         0.0200,  0.1017, -0.0802, -0.1301, -0.3132, -0.1564, -0.2307,  0.0985,\n",
      "        -0.1201, -0.1734, -0.2804,  0.0451,  0.1823,  0.3601,  0.0804, -0.1218,\n",
      "        -0.0432, -0.0326, -0.1526, -0.2894, -0.0399,  0.1020, -0.0224,  0.3047,\n",
      "        -0.1916,  0.1272, -0.2080,  0.1913,  0.0147, -0.2218,  0.1237,  0.2513,\n",
      "         0.0818,  0.2152, -0.1715, -0.2109,  0.1637, -0.2222, -0.1539, -0.1257,\n",
      "         0.2095, -0.1031, -0.1273, -0.0458,  0.0991,  0.1320, -0.1945,  0.1438,\n",
      "        -0.0400, -0.2154, -0.0148,  0.2097,  0.1486,  0.2779, -0.0696,  0.0704,\n",
      "         0.1028, -0.1671, -0.1902, -0.1615, -0.0536, -0.1513, -0.0433,  0.1391,\n",
      "         0.1685,  0.0082, -0.0862,  0.0083,  0.1105,  0.0098, -0.0935,  0.1491,\n",
      "         0.0720,  0.1213, -0.2426,  0.1664,  0.1744, -0.1984,  0.2496, -0.0463,\n",
      "        -0.2290,  0.1688, -0.0286,  0.0605, -0.1826, -0.0154, -0.0226, -0.0760,\n",
      "        -0.0649, -0.1618,  0.0559, -0.1128,  0.0800, -0.0694,  0.0292, -0.1047,\n",
      "        -0.1607,  0.1502, -0.1437, -0.0029, -0.1388,  0.0346, -0.1441,  0.0728,\n",
      "         0.1419, -0.0990, -0.1912, -0.1876,  0.2190, -0.0825, -0.2073, -0.1691,\n",
      "         0.0317,  0.1770, -0.0183, -0.2326,  0.2064, -0.0647, -0.0904, -0.2620,\n",
      "        -0.2236, -0.1658,  0.2303,  0.1717,  0.2428,  0.1947, -0.0370,  0.0979,\n",
      "         0.1852, -0.1667,  0.0942,  0.2279,  0.0714, -0.0804, -0.1331, -0.1006,\n",
      "        -0.0313, -0.2048,  0.0563, -0.2680, -0.0999,  0.1316,  0.0757,  0.1688,\n",
      "        -0.2106,  0.1968,  0.2291,  0.0230, -0.1526,  0.1766, -0.0287, -0.0358,\n",
      "         0.0832,  0.1639, -0.0958, -0.2244,  0.0934,  0.1445, -0.0330,  0.0203,\n",
      "         0.0500, -0.1541, -0.0771, -0.0695,  0.0487,  0.0465, -0.1763,  0.1149,\n",
      "        -0.0235,  0.2173, -0.0673,  0.0875,  0.0980, -0.2877, -0.2395,  0.1006,\n",
      "        -0.2134,  0.0280, -0.0455, -0.0445, -0.1020,  0.0592, -0.3023, -0.1296,\n",
      "        -0.1132,  0.0954,  0.1822, -0.0493,  0.1621, -0.2157, -0.0920,  0.1306])), ('model.backbone.layers.1.mixer.filter_fn.bias', tensor([-0.3735,  0.9368, -0.2697, -0.5410, -0.3832, -0.7016,  0.2809,  0.9846,\n",
      "        -0.8301, -0.6338,  0.1555, -0.3789,  0.9276, -1.4030,  1.4103,  1.4381,\n",
      "         1.7451, -0.1776, -0.4683, -0.3370,  2.0637, -1.9167, -1.5278,  0.0401,\n",
      "        -0.3797,  1.1846,  0.9539,  0.6285,  0.8443,  0.7005, -0.1236, -0.0660,\n",
      "         0.1251, -1.6959, -0.7732, -0.6009, -0.0749,  0.0719, -0.5945,  1.5105,\n",
      "         0.7058,  1.1699, -0.4618,  0.7977,  0.6816, -0.0686,  1.3587,  1.5863,\n",
      "         0.4695,  0.2133,  1.5876, -1.3115,  0.8720,  1.3173, -0.3029, -1.5444,\n",
      "         1.2349, -1.0142, -0.3604,  1.4133,  1.6063, -0.8643,  1.1291,  1.2474,\n",
      "         1.3327, -0.5251,  1.3786,  0.0511,  1.7583,  0.8165,  0.1149, -0.3670,\n",
      "         0.0817, -0.1431, -1.5847, -0.4489,  1.1554, -1.1875, -0.0287,  0.9926,\n",
      "        -0.8773,  0.9774,  0.7986,  1.3437, -1.4989,  0.1945, -0.0178,  1.2599,\n",
      "         0.4508, -0.7206, -1.3605,  0.0559,  0.4501, -0.5579, -1.8613,  1.4046,\n",
      "         0.2942,  0.7469, -0.1208, -0.1878,  1.0297,  0.7382, -1.6806,  1.2911,\n",
      "        -0.5213, -0.6996,  1.5435,  0.5530, -1.7181,  0.9307, -0.2132,  0.7183,\n",
      "        -0.6958,  1.6847, -0.6716,  0.8101,  0.5322,  0.8946, -1.1808, -1.2043,\n",
      "        -0.5285, -1.4687,  1.1839, -0.0113,  0.3497,  0.2166,  0.2018, -0.6954])), ('model.backbone.layers.1.mixer.filter_fn.pos_emb.z', tensor([[[ 0.0000e+00,  1.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 9.7561e-04,  1.0000e+00,  9.9998e-01, -6.1240e-07, -6.1239e-03],\n",
      "         [ 1.9512e-03,  1.0000e+00,  9.9993e-01, -1.2248e-06, -1.2248e-02],\n",
      "         ...,\n",
      "         [ 9.9805e-01,  1.0000e+00,  9.9983e-01, -6.2648e-04,  1.8371e-02],\n",
      "         [ 9.9902e-01,  1.0000e+00,  9.9993e-01, -6.2709e-04,  1.2248e-02],\n",
      "         [ 1.0000e+00,  1.0000e+00,  9.9998e-01, -6.2771e-04,  6.1238e-03]]])), ('model.backbone.layers.1.mixer.filter_fn.pos_emb.t', tensor([[[0.0000e+00],\n",
      "         [9.7561e-04],\n",
      "         [1.9512e-03],\n",
      "         ...,\n",
      "         [9.9805e-01],\n",
      "         [9.9902e-01],\n",
      "         [1.0000e+00]]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight', tensor([[-4.3045e-03,  8.9401e-03, -3.4622e-02,  2.7123e-02,  1.3002e-02],\n",
      "        [-1.4598e-02,  2.3011e-02, -3.7659e-02, -2.5086e-02,  3.8701e-02],\n",
      "        [ 3.0638e-02, -1.2190e-02,  2.5221e-02, -4.7175e-02, -3.6569e-02],\n",
      "        [-2.6211e-03, -5.6698e-03, -4.9379e-03,  2.0110e-02,  6.2460e-03],\n",
      "        [-2.5181e-02,  2.2482e-02, -4.6861e-02,  3.3359e-02,  7.5339e-02],\n",
      "        [-8.9768e-03,  2.5639e-02, -2.8379e-03,  9.4451e-03,  6.2021e-03],\n",
      "        [ 1.3695e-03,  2.5988e-02, -1.6780e-02,  5.0285e-03,  1.3959e-02],\n",
      "        [ 2.1899e-02, -1.1005e-02, -6.7917e-03, -3.5298e-03, -2.9219e-02],\n",
      "        [ 7.3628e-03,  2.3102e-03,  2.3123e-02, -1.7529e-02, -6.4923e-02],\n",
      "        [-2.0437e-02, -1.6455e-03,  3.1078e-02, -3.0240e-02,  1.1246e-02],\n",
      "        [-1.3216e-03,  1.8170e-02, -7.6784e-03, -3.4819e-03, -1.1522e-04],\n",
      "        [ 2.9299e-02,  1.5406e-02, -1.0460e-02, -7.2984e-03, -2.0057e-02],\n",
      "        [-1.5637e-02,  1.3955e-02, -1.9053e-02, -2.9215e-03,  8.7868e-02],\n",
      "        [-4.2286e-03,  1.9104e-02, -1.5716e-02,  1.5234e-02,  1.2226e-02],\n",
      "        [-2.0730e-02, -8.6675e-03,  1.4181e-02,  2.8053e-03,  3.1411e-02],\n",
      "        [-7.8043e-03,  2.1910e-02, -2.1676e-02,  7.6082e-04,  1.4740e-02],\n",
      "        [ 1.2482e-02, -1.1625e-02,  1.8695e-02,  3.0908e-02, -5.6005e-02],\n",
      "        [-1.7158e-03,  1.7897e-02, -1.8256e-02, -1.8031e-02,  1.0264e-03],\n",
      "        [ 3.0723e-02, -1.8044e-03,  2.3705e-03, -2.7893e-02, -1.2303e-01],\n",
      "        [-5.7745e-04,  8.3737e-03, -3.3610e-02, -1.6447e-02,  7.0820e-03],\n",
      "        [-1.5324e-02,  2.4581e-02, -1.5504e-02, -5.3057e-03,  2.5002e-02],\n",
      "        [-1.9166e-03, -2.0177e-03,  1.2391e-02,  6.7644e-03,  1.2923e-03],\n",
      "        [-1.6188e-02, -5.4489e-03,  4.9490e-03, -4.9958e-02, -9.4542e-04],\n",
      "        [ 3.6281e-03,  1.0813e-02, -7.0852e-03,  4.5628e-02, -6.1773e-03],\n",
      "        [-5.7831e-03, -1.3980e-02,  2.4276e-03,  1.5815e-02,  7.0546e-03],\n",
      "        [ 1.4875e-02, -2.5355e-03, -8.6851e-03, -3.8499e-02, -3.7316e-02],\n",
      "        [-2.4947e-02, -1.5205e-02,  1.3293e-02,  3.2577e-02,  3.4840e-02],\n",
      "        [ 1.1338e-02, -3.2227e-02,  3.3247e-02, -2.0423e-02, -2.9191e-02],\n",
      "        [ 6.1632e-04, -1.2045e-02,  5.1713e-02, -4.5394e-02, -2.3470e-02],\n",
      "        [-2.5459e-02,  9.9884e-03, -1.1862e-02,  6.7095e-03,  1.4289e-02],\n",
      "        [-4.4594e-03,  1.3817e-03,  7.4010e-03,  3.0331e-02,  1.9404e-03],\n",
      "        [ 7.9018e-03,  3.6042e-03,  3.8670e-04, -5.5466e-03, -5.0332e-03],\n",
      "        [ 2.2689e-03,  9.0981e-03, -3.9724e-03, -2.1202e-02, -2.1530e-03],\n",
      "        [-5.1564e-03,  2.6380e-03, -2.5547e-02, -2.7197e-02,  1.3138e-02],\n",
      "        [-4.7410e-04,  3.3875e-03,  1.7804e-02,  1.9286e-02, -1.4419e-04],\n",
      "        [ 3.9851e-02, -3.3548e-02,  1.4635e-02, -2.9634e-02, -5.8860e-02],\n",
      "        [-8.4732e-02, -8.6208e-04, -1.9016e-03,  2.5849e-02, -7.4296e-02],\n",
      "        [-2.8618e-03,  1.4177e-02,  1.7571e-03,  5.5938e-03,  7.9124e-02],\n",
      "        [ 1.6356e-02,  2.1600e-02, -2.4158e-02, -7.9566e-03,  8.3386e-02],\n",
      "        [-5.6291e-03,  3.8683e-02, -6.3251e-02,  4.5184e-02,  2.5098e-02],\n",
      "        [ 3.1098e-02, -3.2045e-02,  2.2347e-02, -9.8879e-04, -8.6894e-02],\n",
      "        [ 2.9469e-03, -3.9407e-02,  2.9544e-02, -5.2435e-03, -1.1502e-02],\n",
      "        [-3.3335e-03, -1.2401e-02,  3.7138e-02,  6.0875e-02,  1.0981e-02],\n",
      "        [-2.6408e-02,  2.0969e-02, -2.4360e-02,  7.7598e-03,  2.9372e-02],\n",
      "        [-5.3221e-03,  1.1884e-02, -8.0523e-03, -4.0986e-02,  8.0380e-03],\n",
      "        [-2.7997e-02, -2.0251e-03, -1.5598e-03,  6.4251e-03,  1.1509e-01],\n",
      "        [ 2.2719e-02,  1.0141e-02, -2.0304e-02,  2.2638e-02, -2.9311e-02],\n",
      "        [ 3.9942e-03, -2.0842e-02,  1.6078e-02,  2.8325e-02, -7.3312e-03],\n",
      "        [ 2.8959e-02, -1.6830e-02,  2.1871e-03,  3.8397e-03, -1.0339e-01],\n",
      "        [ 4.0212e-02, -3.3036e-02,  2.0451e-02, -3.9440e-03, -3.7135e-02],\n",
      "        [ 7.8501e-03, -2.2131e-02,  2.2225e-02, -2.4131e-02, -4.8570e-02],\n",
      "        [ 5.5617e-04, -1.6934e-02,  1.9304e-02,  3.7602e-03, -3.1404e-03],\n",
      "        [-9.9399e-03,  2.9226e-03,  1.5647e-02, -3.0935e-03,  9.0108e-03],\n",
      "        [-1.0727e-02,  3.6262e-02, -2.9485e-02,  3.1113e-02,  2.6578e-02],\n",
      "        [-4.4523e-02,  1.0938e-02, -9.9438e-03,  1.3242e-02,  1.0640e-01],\n",
      "        [-4.5063e-03,  5.1743e-03, -8.0024e-04,  2.1393e-03,  8.5281e-03],\n",
      "        [ 1.9117e-02,  9.2331e-04, -2.0857e-02, -3.7378e-02, -2.1669e-02],\n",
      "        [-1.4436e-03,  6.1669e-03, -9.0406e-03,  1.5599e-02,  2.3087e-03],\n",
      "        [-6.6182e-03, -8.6082e-03,  1.7999e-02,  3.1631e-02,  1.3501e-02],\n",
      "        [-1.1919e-02,  1.9787e-02, -9.2593e-03,  8.2500e-03,  2.0719e-02],\n",
      "        [-1.0239e-02,  8.0123e-03, -2.7966e-02,  2.5379e-02,  1.6931e-02],\n",
      "        [-1.3780e-02, -1.7841e-02,  2.3819e-02,  1.1656e-02, -1.0814e-01],\n",
      "        [ 2.1369e-02, -3.9007e-03, -1.2096e-02, -1.5869e-02, -3.6082e-02],\n",
      "        [-1.0955e-02,  6.9609e-03, -2.8928e-02, -4.5165e-03,  2.4737e-02]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias', tensor([ 2.4904e-02,  1.5806e-02, -1.7728e-02,  8.6362e-03,  1.5528e-02,\n",
      "        -1.7707e-02, -1.5270e-02,  1.1730e-02, -1.8699e-02, -1.8690e-02,\n",
      "        -9.2693e-03, -1.3070e-03,  6.5402e-03,  3.4355e-03, -5.2424e-03,\n",
      "         9.5443e-05, -9.7232e-03,  6.4421e-03, -3.2909e-03,  2.4736e-02,\n",
      "        -4.8575e-03, -7.7371e-03,  6.8260e-03, -5.4060e-03,  1.3126e-02,\n",
      "         9.6812e-04,  2.8085e-03, -3.5500e-03, -2.5629e-02,  1.7306e-02,\n",
      "        -1.5862e-02, -7.1991e-03,  4.7186e-03,  2.2080e-02, -1.9989e-02,\n",
      "        -5.8117e-03, -1.0922e-03,  3.0398e-03,  9.4381e-03,  1.1542e-02,\n",
      "        -6.6244e-03,  1.2198e-02, -1.6697e-02,  1.1157e-02, -6.1671e-03,\n",
      "         5.4993e-03,  8.5378e-03,  5.7770e-04, -1.9332e-03,  2.3117e-03,\n",
      "        -9.3537e-03, -6.1431e-03, -1.3796e-02, -4.6382e-03,  8.7136e-03,\n",
      "        -1.0724e-02,  1.2282e-02,  4.1694e-03, -3.3046e-04, -1.0611e-02,\n",
      "         2.1809e-02, -2.9916e-03,  4.3471e-03,  1.8331e-02])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', tensor([[ 9.8967,  9.7494,  9.9302,  9.8895, 10.0053,  9.8923,  9.8354,  9.8517,\n",
      "          9.8776,  9.8313,  9.9560,  9.8262, 10.0002,  9.8933,  9.8198,  9.7891,\n",
      "          9.8768,  9.8655, 10.0077,  9.7771,  9.8811,  9.9236,  9.9814,  9.9291,\n",
      "          9.8266,  9.8756,  9.9501,  9.7783,  9.8296,  9.9343,  9.7548,  9.7620,\n",
      "          9.9583,  9.9260,  9.6803,  9.9203,  9.9909,  9.9116,  9.9634,  9.9178,\n",
      "          9.9750,  9.8436,  9.7834,  9.9165,  9.8560, 10.0198,  9.9065, 10.0316,\n",
      "          9.9782,  9.9839,  9.9333,  9.8646, 10.0118,  9.8512, 10.0262,  9.8680,\n",
      "          9.8816,  9.9248,  9.7630,  9.8361,  9.9515, 10.0380,  9.8725, 10.0130]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight', tensor([[-0.0076, -0.0619,  0.0289,  ...,  0.0121, -0.0146, -0.0133],\n",
      "        [-0.0264, -0.0294,  0.0046,  ..., -0.0181, -0.0032, -0.0336],\n",
      "        [ 0.0147,  0.0258, -0.0214,  ...,  0.1015,  0.0103,  0.0005],\n",
      "        ...,\n",
      "        [ 0.0602,  0.0606, -0.0038,  ..., -0.0300, -0.0254,  0.0512],\n",
      "        [ 0.0006,  0.0219, -0.0248,  ..., -0.0362,  0.0147,  0.0330],\n",
      "        [ 0.0556,  0.0401, -0.0462,  ...,  0.0857,  0.0391,  0.0244]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias', tensor([ 0.0060,  0.0116, -0.0257,  0.0200,  0.0099, -0.0097, -0.0097, -0.0017,\n",
      "         0.0608, -0.0361,  0.0028, -0.0253,  0.0020, -0.0277,  0.0107,  0.0240,\n",
      "        -0.0576, -0.0059, -0.0124, -0.0106,  0.0219, -0.0170, -0.0327, -0.0358,\n",
      "        -0.0081,  0.0095, -0.0683, -0.0079, -0.0050, -0.0124, -0.0065,  0.0013,\n",
      "        -0.0327, -0.0064, -0.0130, -0.0048, -0.0222,  0.0071,  0.0076,  0.0004,\n",
      "        -0.0109,  0.0017, -0.0228, -0.0222, -0.0094,  0.0008,  0.0135, -0.0109,\n",
      "        -0.0299,  0.0625,  0.0382, -0.0158,  0.0402,  0.0082, -0.0142, -0.0021,\n",
      "        -0.0474, -0.0141, -0.0010,  0.0052,  0.0260, -0.0025,  0.0170, -0.0040])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq', tensor([[ 9.8967,  9.7494,  9.9302,  9.8895, 10.0053,  9.8923,  9.8354,  9.8517,\n",
      "          9.8776,  9.8313,  9.9560,  9.8262, 10.0002,  9.8933,  9.8198,  9.7891,\n",
      "          9.8768,  9.8655, 10.0077,  9.7771,  9.8811,  9.9236,  9.9814,  9.9291,\n",
      "          9.8266,  9.8756,  9.9501,  9.7783,  9.8296,  9.9343,  9.7548,  9.7620,\n",
      "          9.9583,  9.9260,  9.6803,  9.9203,  9.9909,  9.9116,  9.9634,  9.9178,\n",
      "          9.9750,  9.8436,  9.7834,  9.9165,  9.8560, 10.0198,  9.9065, 10.0316,\n",
      "          9.9782,  9.9839,  9.9333,  9.8646, 10.0118,  9.8512, 10.0262,  9.8680,\n",
      "          9.8816,  9.9248,  9.7630,  9.8361,  9.9515, 10.0380,  9.8725, 10.0130]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight', tensor([[ 0.0027,  0.0232, -0.0205,  ...,  0.0851,  0.0105,  0.0434],\n",
      "        [ 0.0092,  0.0109,  0.0227,  ...,  0.0202,  0.0459, -0.0337],\n",
      "        [-0.0186, -0.0062,  0.0101,  ..., -0.0239,  0.0052, -0.0325],\n",
      "        ...,\n",
      "        [-0.0014,  0.0449,  0.0344,  ...,  0.0152,  0.0233,  0.1283],\n",
      "        [ 0.0150,  0.0012, -0.0154,  ..., -0.0052, -0.0104, -0.0233],\n",
      "        [-0.0283, -0.0435,  0.0280,  ...,  0.0535, -0.0097, -0.0110]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias', tensor([ 0.0025,  0.0312,  0.0673,  0.0654, -0.0066, -0.0289,  0.0115, -0.0201,\n",
      "         0.0489, -0.0256,  0.0484,  0.0133, -0.0477, -0.0037,  0.0188, -0.0412,\n",
      "         0.0003, -0.0726, -0.0316, -0.0180,  0.0172,  0.0529,  0.0256, -0.0293,\n",
      "         0.0020,  0.0107, -0.0083,  0.0430,  0.0004, -0.0410,  0.0043, -0.0178,\n",
      "         0.0142, -0.0258, -0.0356,  0.0329, -0.0741,  0.0322,  0.0144,  0.0622,\n",
      "         0.0141, -0.0454, -0.0087,  0.0141, -0.0162,  0.0395,  0.0082,  0.0116,\n",
      "         0.0568, -0.0421, -0.0123, -0.0332,  0.0286, -0.0119, -0.0049,  0.0064,\n",
      "        -0.0027,  0.0115,  0.0037,  0.0048,  0.0368, -0.0200, -0.0361,  0.0429])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', tensor([[ 9.8967,  9.7494,  9.9302,  9.8895, 10.0053,  9.8923,  9.8354,  9.8517,\n",
      "          9.8776,  9.8313,  9.9560,  9.8262, 10.0002,  9.8933,  9.8198,  9.7891,\n",
      "          9.8768,  9.8655, 10.0077,  9.7771,  9.8811,  9.9236,  9.9814,  9.9291,\n",
      "          9.8266,  9.8756,  9.9501,  9.7783,  9.8296,  9.9343,  9.7548,  9.7620,\n",
      "          9.9583,  9.9260,  9.6803,  9.9203,  9.9909,  9.9116,  9.9634,  9.9178,\n",
      "          9.9750,  9.8436,  9.7834,  9.9165,  9.8560, 10.0198,  9.9065, 10.0316,\n",
      "          9.9782,  9.9839,  9.9333,  9.8646, 10.0118,  9.8512, 10.0262,  9.8680,\n",
      "          9.8816,  9.9248,  9.7630,  9.8361,  9.9515, 10.0380,  9.8725, 10.0130]])), ('model.backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight', tensor([[ 0.0431, -0.0680, -0.0593,  ..., -0.0478,  0.0032,  0.0014],\n",
      "        [-0.0843,  0.1339,  0.0624,  ..., -0.0442, -0.1239, -0.0505],\n",
      "        [ 0.0597, -0.1036, -0.0968,  ..., -0.0209,  0.1591,  0.0247],\n",
      "        ...,\n",
      "        [-0.0529,  0.0914,  0.0707,  ..., -0.0354, -0.0616, -0.1543],\n",
      "        [ 0.0833, -0.1292, -0.0869,  ...,  0.0961,  0.0824,  0.2321],\n",
      "        [-0.0352,  0.0791,  0.0195,  ..., -0.1027, -0.0014, -0.0909]])), ('model.backbone.layers.1.mixer.filter_fn.modulation.deltas', tensor([[[ -3.0701,  -3.1668,  -3.2635,  -3.3602,  -3.4569,  -3.5536,  -3.6503,\n",
      "           -3.7470,  -3.8437,  -3.9404,  -4.0371,  -4.1338,  -4.2305,  -4.3272,\n",
      "           -4.4239,  -4.5206,  -4.6173,  -4.7140,  -4.8106,  -4.9073,  -5.0040,\n",
      "           -5.1007,  -5.1974,  -5.2941,  -5.3908,  -5.4875,  -5.5842,  -5.6809,\n",
      "           -5.7776,  -5.8743,  -5.9710,  -6.0677,  -6.1644,  -6.2611,  -6.3578,\n",
      "           -6.4545,  -6.5512,  -6.6479,  -6.7446,  -6.8413,  -6.9380,  -7.0347,\n",
      "           -7.1314,  -7.2281,  -7.3248,  -7.4215,  -7.5182,  -7.6148,  -7.7115,\n",
      "           -7.8082,  -7.9049,  -8.0016,  -8.0983,  -8.1950,  -8.2917,  -8.3884,\n",
      "           -8.4851,  -8.5818,  -8.6785,  -8.7752,  -8.8719,  -8.9686,  -9.0653,\n",
      "           -9.1620,  -9.2587,  -9.3554,  -9.4521,  -9.5488,  -9.6455,  -9.7422,\n",
      "           -9.8389,  -9.9356, -10.0323, -10.1290, -10.2257, -10.3223, -10.4190,\n",
      "          -10.5157, -10.6124, -10.7091, -10.8058, -10.9025, -10.9992, -11.0959,\n",
      "          -11.1926, -11.2893, -11.3860, -11.4827, -11.5794, -11.6761, -11.7728,\n",
      "          -11.8695, -11.9662, -12.0629, -12.1596, -12.2563, -12.3530, -12.4497,\n",
      "          -12.5464, -12.6431, -12.7398, -12.8365, -12.9332, -13.0299, -13.1265,\n",
      "          -13.2232, -13.3199, -13.4166, -13.5133, -13.6100, -13.7067, -13.8034,\n",
      "          -13.9001, -13.9968, -14.0935, -14.1902, -14.2869, -14.3836, -14.4803,\n",
      "          -14.5770, -14.6737, -14.7704, -14.8671, -14.9638, -15.0605, -15.1572,\n",
      "          -15.2539, -15.3506]]])), ('model.backbone.layers.1.norm1.weight', tensor([0.7372, 0.7777, 0.6840, 0.6946, 0.7166, 0.7348, 0.7176, 0.7445, 0.7270,\n",
      "        0.7053, 0.7897, 0.7239, 0.6919, 0.7171, 0.7323, 0.7414, 0.7598, 0.7604,\n",
      "        0.6805, 0.6840, 0.6991, 0.7200, 0.7464, 0.6742, 0.7346, 0.7221, 0.7596,\n",
      "        0.7457, 0.7394, 0.7247, 0.7234, 0.7113, 0.7428, 0.7304, 0.7406, 0.7206,\n",
      "        0.7926, 0.7236, 0.6425, 0.7513, 0.6969, 0.7068, 0.7285, 0.7351, 0.6537,\n",
      "        0.7010, 0.7086, 0.7024, 0.6926, 0.7465, 0.7084, 0.7344, 0.7204, 0.7173,\n",
      "        0.7471, 0.7091, 0.6985, 0.7595, 0.7058, 0.7228, 0.6788, 0.6629, 0.7839,\n",
      "        0.7215, 0.7322, 0.7342, 0.7492, 0.7148, 0.7790, 0.7925, 0.6948, 0.7663,\n",
      "        0.6913, 0.7779, 0.7606, 0.7398, 0.6359, 0.6975, 0.7203, 0.7643, 0.7117,\n",
      "        0.7477, 0.7095, 0.7480, 0.7999, 0.7696, 0.6717, 0.7541, 0.7181, 0.7221,\n",
      "        0.6801, 0.7730, 0.7486, 0.7263, 0.7051, 0.7684, 0.6429, 0.7321, 0.7394,\n",
      "        0.6893, 0.7158, 0.7518, 0.6632, 0.7323, 0.7465, 0.6836, 0.7603, 0.7634,\n",
      "        0.8131, 0.6947, 0.6796, 0.7263, 0.7118, 0.7444, 0.7552, 0.7179, 0.6861,\n",
      "        0.7522, 0.7143, 0.7034, 0.7032, 0.6860, 0.7628, 0.6863, 0.7948, 0.7236,\n",
      "        0.7160, 0.7411])), ('model.backbone.layers.1.norm1.bias', tensor([ 0.0675,  0.0112,  0.0047, -0.0475, -0.0673,  0.0347,  0.0120, -0.0029,\n",
      "         0.0389,  0.0071,  0.0128,  0.0419,  0.0448,  0.0426,  0.0010, -0.0012,\n",
      "         0.0414, -0.0553,  0.0295,  0.0447, -0.0343,  0.0644, -0.0226, -0.0622,\n",
      "         0.0288,  0.0026, -0.0256, -0.0394, -0.0026, -0.0171, -0.0223,  0.0466,\n",
      "         0.0116, -0.0219, -0.0223, -0.0038,  0.0205, -0.0478, -0.0128,  0.0253,\n",
      "        -0.0312,  0.0120,  0.0306,  0.0555,  0.0390, -0.0076, -0.0352,  0.0288,\n",
      "        -0.0210, -0.0337,  0.0021,  0.0326,  0.0631,  0.0275, -0.0253, -0.0210,\n",
      "         0.0293,  0.0381, -0.0115, -0.0284, -0.0793,  0.0341,  0.0198,  0.0099,\n",
      "         0.0377,  0.0036, -0.0525,  0.0457,  0.0032, -0.0211, -0.0055,  0.0275,\n",
      "         0.0082, -0.0162, -0.0318,  0.0321, -0.0663,  0.0302,  0.0242,  0.0158,\n",
      "         0.0178,  0.0091, -0.0081, -0.0005, -0.0256, -0.0138, -0.0082, -0.0097,\n",
      "        -0.0198, -0.0150,  0.0614, -0.0356, -0.0275,  0.0351, -0.0179, -0.0359,\n",
      "         0.0588,  0.0609,  0.0046,  0.0531, -0.0273, -0.0200, -0.0614, -0.0377,\n",
      "        -0.0220, -0.0361,  0.0196, -0.0386,  0.0232, -0.0550,  0.0228,  0.0266,\n",
      "        -0.0717, -0.0109, -0.0421,  0.0402,  0.0550,  0.0405,  0.0277,  0.0137,\n",
      "        -0.0476, -0.0059,  0.0102, -0.0446, -0.0508, -0.0298,  0.0322, -0.0444])), ('model.backbone.layers.1.mlp.fc1.weight', tensor([[ 0.0822, -0.0473,  0.0236,  ..., -0.0475, -0.0023,  0.0623],\n",
      "        [ 0.0233,  0.0146,  0.0721,  ..., -0.0625,  0.0079,  0.0254],\n",
      "        [ 0.0012,  0.0018,  0.0338,  ...,  0.0205,  0.0431, -0.0851],\n",
      "        ...,\n",
      "        [-0.0013, -0.0495, -0.0154,  ..., -0.0253,  0.0769,  0.0128],\n",
      "        [-0.0338, -0.0892,  0.1252,  ..., -0.0409, -0.0742,  0.0317],\n",
      "        [ 0.0903, -0.0772,  0.0462,  ..., -0.0318,  0.0083,  0.0337]])), ('model.backbone.layers.1.mlp.fc1.bias', tensor([-1.4093e-02, -6.8611e-03, -1.0364e-02,  7.5724e-03,  1.9776e-04,\n",
      "         1.1704e-02, -3.7710e-02, -1.1218e-03,  1.0800e-02,  4.3490e-04,\n",
      "        -1.7824e-02, -3.6989e-02, -6.5768e-03,  2.3797e-02,  7.7999e-03,\n",
      "        -2.7753e-02, -3.9156e-02,  1.3254e-02,  1.4990e-02, -3.0411e-02,\n",
      "         5.6000e-03, -1.8239e-02, -7.5962e-02, -3.9615e-03, -1.6587e-03,\n",
      "        -5.5790e-02, -1.8043e-02,  1.5907e-02, -2.5041e-02, -2.5354e-02,\n",
      "         1.1244e-02,  9.5243e-03, -1.3540e-02, -1.1564e-02,  1.5549e-02,\n",
      "         2.8714e-03,  1.0005e-02,  1.3312e-02,  1.9651e-02, -6.5289e-02,\n",
      "        -8.9727e-03,  4.9566e-03, -1.1534e-02, -7.4994e-02, -2.1831e-02,\n",
      "         1.4806e-02, -1.6848e-02,  1.2194e-02, -6.9867e-03,  1.7335e-02,\n",
      "        -6.2347e-03,  2.1148e-02,  2.0146e-02, -1.5058e-02, -3.6554e-02,\n",
      "         2.5222e-02, -1.1355e-02,  2.6584e-02,  1.1525e-02, -2.4304e-02,\n",
      "         1.9081e-02, -1.0861e-02, -3.1430e-03, -4.3254e-02,  1.6521e-02,\n",
      "        -1.7113e-03,  2.5715e-02, -4.7475e-02, -2.5221e-02,  2.5789e-02,\n",
      "         1.0931e-02, -4.9824e-03,  4.2703e-04,  1.8306e-02,  2.0852e-03,\n",
      "         7.1907e-03, -3.3281e-03, -3.2015e-02,  2.7351e-02,  1.9530e-02,\n",
      "        -3.6583e-02, -4.8107e-02,  7.2592e-03,  3.7812e-02,  6.0603e-02,\n",
      "        -1.8675e-01,  2.5446e-02, -2.1193e-02, -1.6434e-02,  2.3600e-02,\n",
      "        -1.0136e-02,  2.0590e-02,  1.1367e-02, -8.0416e-03, -3.7060e-02,\n",
      "         5.0113e-03, -1.2192e-02,  1.5692e-02, -5.8603e-03, -1.3739e-02,\n",
      "        -7.1745e-02, -1.3225e-02,  4.3534e-02, -1.5337e-02,  1.6010e-02,\n",
      "         1.2550e-04,  1.2383e-02, -1.6006e-02,  1.9197e-02, -5.0859e-02,\n",
      "        -4.7516e-03,  1.7767e-02,  1.4757e-02, -4.8111e-02,  2.3071e-02,\n",
      "        -1.4632e-02,  1.2654e-02, -1.2346e-03,  2.2331e-02, -3.0676e-02,\n",
      "         7.3384e-03, -9.1311e-03,  8.1996e-03,  1.4159e-02,  2.5568e-02,\n",
      "         1.5346e-02, -1.5306e-02,  1.7572e-02,  9.7458e-03,  3.0296e-02,\n",
      "        -2.7219e-03,  2.3523e-02,  1.7028e-02,  1.0324e-02,  1.1602e-02,\n",
      "        -3.7291e-02, -3.2496e-02, -3.8563e-05,  1.4550e-02, -3.4956e-02,\n",
      "        -1.3052e-02, -1.6679e-02, -8.3191e-02, -2.8573e-03,  3.0764e-03,\n",
      "         3.1295e-02, -3.0081e-03,  2.6645e-02, -1.4612e-03, -8.8014e-04,\n",
      "        -2.9559e-02, -1.3008e-02, -1.0598e-02, -3.3315e-02, -6.2904e-03,\n",
      "         1.3986e-02,  2.6063e-02,  4.9909e-02, -3.5731e-03, -5.1820e-02,\n",
      "        -1.0285e-01, -1.4987e-02,  3.8946e-02,  4.8068e-03, -4.2500e-02,\n",
      "        -4.2399e-02,  6.0640e-02,  7.7401e-03,  2.8717e-02,  2.9102e-03,\n",
      "         1.5475e-02, -5.7878e-02, -4.0793e-03,  2.3327e-02, -7.3434e-03,\n",
      "        -2.7877e-02, -1.5558e-02, -4.2321e-02, -6.1934e-03, -4.6173e-02,\n",
      "        -1.6239e-02,  7.2162e-03,  1.5570e-02, -4.3520e-03,  2.8298e-02,\n",
      "         2.3004e-02,  8.4817e-03, -5.0822e-02, -2.2960e-03, -1.4906e-02,\n",
      "        -3.4264e-02,  8.8983e-03, -4.2628e-02, -2.8376e-02,  1.4235e-02,\n",
      "         9.4572e-03,  2.8790e-02, -1.5430e-02, -1.5858e-01, -1.4436e-02,\n",
      "        -6.6207e-03, -2.7587e-02,  2.3571e-02, -1.4250e-01,  1.0727e-05,\n",
      "         1.1253e-02, -1.2058e-01, -6.1534e-02, -3.9636e-02, -1.0422e-02,\n",
      "        -3.1832e-02, -9.8416e-03,  2.1965e-03,  3.2370e-02, -5.6713e-02,\n",
      "        -1.2879e-02,  1.0347e-02, -1.0743e-02,  3.3036e-03,  2.7635e-02,\n",
      "        -2.6587e-02,  3.5042e-02, -5.3677e-02,  3.6805e-03,  1.5537e-02,\n",
      "        -3.0191e-02, -2.0652e-04,  1.3138e-03, -6.4327e-02, -2.5195e-02,\n",
      "         6.4048e-03,  1.9385e-02, -9.2837e-02,  1.1841e-02, -7.1141e-02,\n",
      "        -9.0681e-03,  1.8736e-02,  3.4334e-02, -1.0023e-02, -1.8567e-03,\n",
      "        -3.2358e-02, -7.6887e-03,  4.4838e-03, -9.1943e-03,  1.8046e-02,\n",
      "        -5.5349e-02, -1.0796e-03, -1.1628e-02,  1.2927e-02, -8.6064e-04,\n",
      "        -1.8104e-02,  6.6223e-03, -1.1313e-01,  1.7618e-02, -2.2497e-03,\n",
      "         1.0823e-02, -3.4286e-02,  1.8819e-02,  3.1206e-02,  1.8744e-02,\n",
      "        -4.0805e-02, -2.8807e-02, -1.5054e-01,  3.6903e-02,  3.3465e-02,\n",
      "        -7.6541e-03, -3.4664e-03,  1.7712e-02,  3.7943e-02, -1.9105e-03,\n",
      "         1.8565e-02,  9.8064e-03, -5.8954e-02,  1.8438e-02, -7.7000e-03,\n",
      "        -2.4189e-02, -3.9107e-02,  2.2852e-02, -1.4238e-01, -1.9715e-02,\n",
      "        -3.0559e-02, -3.7796e-02,  5.3320e-03,  1.9054e-02, -2.0726e-02,\n",
      "        -5.5666e-02, -9.2855e-02,  2.1521e-02, -6.0074e-02,  1.7577e-03,\n",
      "        -3.5945e-02, -3.7938e-02,  3.0754e-02, -1.8842e-02, -6.5533e-02,\n",
      "        -1.0349e-01, -1.8903e-02,  2.2873e-02,  9.0135e-03, -4.4407e-02,\n",
      "        -4.8677e-02,  1.6677e-02,  6.9319e-03,  1.1062e-02, -1.4738e-03,\n",
      "         3.1567e-02, -2.8847e-02, -3.7590e-02,  2.1015e-02, -1.1696e-01,\n",
      "        -5.4100e-03, -1.3933e-01,  4.1309e-04, -2.2233e-02,  3.1528e-02,\n",
      "        -2.5959e-02, -1.5627e-02, -2.3888e-02, -5.6735e-03, -6.2146e-03,\n",
      "        -2.1107e-03,  1.0521e-03, -4.5581e-02, -2.0845e-02, -6.1788e-02,\n",
      "        -3.0756e-02, -2.7924e-02,  2.4283e-03, -5.7153e-03,  7.9128e-03,\n",
      "         2.1562e-02,  1.0801e-03,  4.0413e-02, -2.7759e-02, -1.6306e-01,\n",
      "         2.1689e-02,  1.6806e-02, -3.4512e-02,  2.3827e-02,  1.6690e-02,\n",
      "        -3.5627e-02,  1.9217e-02,  3.6355e-02, -2.2260e-02, -1.5328e-02,\n",
      "        -9.5238e-03, -2.7367e-02,  1.0313e-02, -2.1306e-03, -1.1509e-03,\n",
      "         2.4866e-03, -2.3473e-03, -5.0864e-02, -2.3284e-02, -2.1320e-02,\n",
      "         8.8449e-03, -8.8697e-02,  4.9605e-03,  6.4249e-03,  1.5805e-02,\n",
      "        -1.5638e-02,  1.4320e-02,  1.9658e-02, -8.4668e-03, -2.5383e-02,\n",
      "         2.8208e-02, -1.0304e-02, -1.3849e-01, -8.7004e-03, -3.6972e-02,\n",
      "         1.5687e-03,  1.5159e-02,  4.0288e-03,  2.0025e-02,  1.1868e-03,\n",
      "        -2.9875e-02,  4.6597e-02, -3.9151e-02,  1.4333e-02,  1.0813e-02,\n",
      "        -2.2486e-02,  1.9114e-02, -2.9479e-02,  1.0914e-02, -3.7423e-02,\n",
      "        -2.8106e-02, -3.0210e-02, -2.7444e-02,  4.2691e-03, -2.5885e-01,\n",
      "         2.5035e-02,  1.7216e-02,  1.6636e-02,  3.2829e-03,  1.6763e-02,\n",
      "        -4.2162e-03,  1.3158e-02,  8.9757e-03, -1.2459e-01,  4.8015e-03,\n",
      "         2.6828e-02, -8.7176e-03, -4.0602e-04, -5.4845e-02, -7.4629e-02,\n",
      "        -4.0315e-02, -3.8016e-02,  2.6906e-02,  2.1552e-02,  2.0958e-02,\n",
      "         2.8188e-03, -1.1386e-01,  1.2038e-02,  1.7129e-02, -3.3068e-02,\n",
      "        -2.9518e-02, -1.0658e-02, -3.4442e-02,  1.9949e-02, -6.1878e-02,\n",
      "         2.6903e-02, -4.3604e-02, -1.0629e-02, -1.1687e-02,  2.1085e-02,\n",
      "         1.8485e-02, -8.8297e-03, -1.4562e-03, -2.5708e-03,  4.6643e-03,\n",
      "         6.5843e-04, -3.0610e-02,  9.8734e-03, -3.5709e-02,  1.8294e-02,\n",
      "        -9.3054e-03, -3.7502e-02,  2.2413e-03,  1.7107e-02, -1.3173e-01,\n",
      "        -2.3809e-02, -3.9289e-02,  1.0198e-02, -1.2239e-02, -7.5527e-03,\n",
      "        -1.8417e-02, -6.9805e-02, -8.9742e-03,  7.8886e-02,  8.2257e-03,\n",
      "         2.8965e-02, -9.8451e-03, -1.2708e-02, -8.0647e-03, -5.3250e-02,\n",
      "        -7.6177e-02, -1.7777e-02,  3.1283e-02,  2.2168e-02,  4.0236e-03,\n",
      "         1.9525e-03,  1.8333e-02,  1.8274e-02, -2.6879e-02,  2.3190e-02,\n",
      "         2.5509e-02,  1.1621e-02, -3.4432e-02, -3.3853e-02, -1.0828e-02,\n",
      "         3.0713e-02, -9.0082e-02,  2.4117e-02,  3.5378e-02,  1.4296e-02,\n",
      "         2.7165e-02, -2.0997e-01, -5.5570e-03,  1.6348e-02, -3.6913e-02,\n",
      "        -8.0564e-03,  7.3398e-03,  2.6921e-03,  2.0712e-02, -1.4798e-02,\n",
      "        -1.5773e-01, -5.1481e-03, -5.4771e-02, -4.0571e-03, -9.0663e-03,\n",
      "         1.0051e-02,  9.0932e-03,  3.9040e-02, -5.3525e-02,  2.3137e-02,\n",
      "        -6.8798e-03, -2.2641e-02, -1.5558e-02, -8.2704e-03, -7.9897e-02,\n",
      "         2.6251e-02,  4.0552e-02,  3.5227e-03, -5.3135e-02, -6.4923e-02,\n",
      "        -1.0300e-03,  8.9636e-03,  1.3911e-02,  7.8494e-03,  2.0273e-03,\n",
      "         1.1796e-03,  1.9137e-03])), ('model.backbone.layers.1.mlp.fc2.weight', tensor([[-0.0137, -0.0030,  0.0170,  ..., -0.0306, -0.0010, -0.0387],\n",
      "        [-0.0051, -0.0018,  0.0237,  ...,  0.0034, -0.0040, -0.0105],\n",
      "        [-0.0039, -0.0172, -0.0374,  ..., -0.0144, -0.0004, -0.0448],\n",
      "        ...,\n",
      "        [ 0.0278,  0.0120, -0.0273,  ...,  0.0306,  0.0253, -0.0348],\n",
      "        [-0.0027,  0.0248, -0.0439,  ..., -0.0406,  0.0215, -0.0310],\n",
      "        [ 0.0265, -0.0026,  0.0685,  ...,  0.0103,  0.0154, -0.0218]])), ('model.backbone.layers.1.mlp.fc2.bias', tensor([-1.2216e-02, -7.5255e-03, -1.0752e-02,  1.2048e-01, -3.5451e-03,\n",
      "        -4.8529e-02,  3.4960e-03,  5.8171e-04, -1.2406e-02,  4.5096e-03,\n",
      "         8.4157e-03, -1.0697e-02, -1.2903e-03, -1.2470e-02, -1.2566e-03,\n",
      "         3.2116e-03,  2.8884e-03, -6.3707e-03, -1.1096e-02, -3.5536e-03,\n",
      "         4.2115e-03,  7.7893e-03, -8.5936e-04,  7.3880e-03, -7.7153e-03,\n",
      "         2.1226e-02, -2.7140e-02, -8.0297e-03,  3.5094e-03, -3.0520e-02,\n",
      "         8.4296e-03,  5.7589e-04,  1.9319e-02, -6.2047e-03,  3.0731e-02,\n",
      "        -1.5975e-02, -1.4702e-02,  8.9151e-03, -1.3838e-04,  1.3894e-02,\n",
      "        -3.5664e-02,  5.4357e-03, -6.8559e-03, -1.4417e-02, -7.4216e-03,\n",
      "        -1.0869e-01, -7.6386e-03, -1.0282e-01,  2.5896e-02, -6.7995e-03,\n",
      "        -1.1511e-03, -5.2165e-03, -1.1138e-02,  2.2518e-02, -1.1988e-02,\n",
      "         2.3833e-02, -6.4784e-03,  2.3632e-02,  2.0454e-04, -3.1858e-03,\n",
      "         9.0390e-03,  6.9046e-03, -7.6690e-03,  2.4452e-02,  2.8718e-03,\n",
      "         1.1000e-02,  3.1815e-03, -1.1576e-02,  2.7732e-03,  1.1595e-03,\n",
      "         1.1871e-02,  3.3466e-03,  1.4278e-02, -4.4161e-03,  7.3931e-03,\n",
      "         2.8163e-02,  8.3933e-03,  8.0029e-03, -3.1886e-03, -1.6123e-02,\n",
      "        -6.9301e-03, -3.1130e-02,  5.2026e-03, -8.9250e-03,  5.2696e-03,\n",
      "        -4.8338e-02,  2.4194e-03, -3.1714e-02, -6.5994e-03, -3.0795e-03,\n",
      "        -4.1357e-03,  2.8310e-03,  4.1976e-02,  8.5858e-03, -2.1179e-02,\n",
      "        -6.1174e-03,  4.5873e-04, -2.2611e-04,  3.6836e-03,  7.7696e-03,\n",
      "        -2.1147e-03,  8.4130e-04, -2.0360e-03, -1.4640e-02, -1.6242e-02,\n",
      "        -1.7143e-02, -1.3308e-03,  5.2052e-02, -1.1282e-02,  1.2549e-02,\n",
      "         7.1139e-05,  4.6343e-03,  1.4965e-02,  1.7027e-03,  9.1385e-03,\n",
      "        -6.1320e-03, -1.7275e-03, -2.1778e-02,  2.3055e-03,  5.0070e-04,\n",
      "        -5.4500e-03, -5.0887e-03, -4.4423e-03,  1.2723e-02,  7.4078e-03,\n",
      "         1.2539e-02, -7.4607e-03,  1.3372e-02])), ('model.backbone.layers.1.norm2.weight', tensor([0.8521, 0.8548, 0.8818, 0.7837, 0.9035, 0.8409, 0.8044, 0.9252, 0.8478,\n",
      "        0.7637, 0.9206, 0.8433, 0.7632, 0.9136, 0.7480, 0.9233, 0.8776, 0.8524,\n",
      "        0.8888, 0.9332, 0.8351, 0.9097, 0.9153, 0.8494, 0.8670, 0.8503, 0.7673,\n",
      "        0.8653, 0.9271, 0.8996, 0.8092, 0.8785, 0.8131, 0.8489, 0.8010, 0.9516,\n",
      "        0.8124, 0.7297, 0.8004, 0.8842, 0.8626, 0.8888, 0.7935, 0.8310, 0.7042,\n",
      "        0.8889, 0.8924, 0.8014, 0.8282, 0.8479, 0.8427, 0.8636, 0.9062, 0.8300,\n",
      "        0.8999, 0.8504, 0.9299, 0.8881, 0.8529, 0.9262, 0.9379, 0.8649, 0.8211,\n",
      "        0.7949, 0.8584, 0.8031, 0.8515, 0.8958, 0.8776, 0.9098, 0.9350, 0.9413,\n",
      "        0.8838, 0.8610, 0.8919, 0.8353, 0.8193, 0.8473, 0.8494, 0.9489, 0.8704,\n",
      "        0.7819, 0.8312, 0.8424, 0.8658, 0.8290, 0.7659, 0.8387, 0.8975, 0.8123,\n",
      "        0.8520, 0.9179, 0.7375, 0.8388, 0.8597, 0.9515, 0.8214, 0.8331, 0.8469,\n",
      "        0.9359, 0.8200, 0.8159, 0.8071, 0.8848, 0.9282, 0.9178, 0.8074, 0.9503,\n",
      "        0.8869, 0.7843, 0.8429, 0.8243, 0.8828, 0.7170, 0.8850, 0.8487, 0.8192,\n",
      "        0.8673, 0.9211, 0.8024, 0.8894, 0.8143, 0.8477, 0.8328, 0.8763, 0.7735,\n",
      "        0.8778, 0.7691])), ('model.backbone.layers.1.norm2.bias', tensor([-0.0406,  0.0463, -0.0903,  0.0334,  0.0299, -0.0506,  0.1201, -0.1326,\n",
      "        -0.0400,  0.1243,  0.0551, -0.0501, -0.0086, -0.0613, -0.0672,  0.0522,\n",
      "         0.0117,  0.1034, -0.1067, -0.1046, -0.0345,  0.0094,  0.0735,  0.1264,\n",
      "         0.0021,  0.0653,  0.0549, -0.1783,  0.0859,  0.0167,  0.1066, -0.0589,\n",
      "        -0.0229, -0.1134,  0.1185,  0.0092, -0.0270,  0.1355, -0.0680,  0.0815,\n",
      "        -0.1465, -0.0293, -0.0872, -0.1361, -0.0744, -0.0642, -0.1263, -0.1138,\n",
      "         0.0297, -0.0844,  0.0662, -0.0304, -0.0638, -0.0312, -0.0420,  0.1198,\n",
      "        -0.0910,  0.0222,  0.0024,  0.0945,  0.1509, -0.0814,  0.0026, -0.0429,\n",
      "        -0.0192, -0.0381,  0.0551,  0.0066,  0.0687,  0.0167, -0.0585,  0.0308,\n",
      "        -0.0377,  0.0842, -0.0696,  0.0675,  0.1364,  0.0946,  0.0247,  0.0194,\n",
      "        -0.1100, -0.0624,  0.0211,  0.0281,  0.0116,  0.0272,  0.0354, -0.0180,\n",
      "         0.0112,  0.1180, -0.1606,  0.1472,  0.1032,  0.1128, -0.1439, -0.0866,\n",
      "        -0.0823, -0.0268, -0.0214,  0.1152,  0.1552, -0.0323,  0.0523, -0.0910,\n",
      "        -0.1573, -0.1310,  0.0049,  0.1126, -0.1373,  0.1206,  0.0522,  0.0079,\n",
      "         0.0983,  0.0422,  0.0845, -0.0591, -0.1619, -0.0561, -0.0607, -0.0383,\n",
      "        -0.0836, -0.0643, -0.1458,  0.0132,  0.0158,  0.1270, -0.0623,  0.0912])), ('model.backbone.ln_f.weight', tensor([0.6531, 0.6318, 0.7373, 0.9865, 0.6031, 0.6416, 1.0595, 0.6340, 0.7001,\n",
      "        0.8133, 0.6147, 0.5917, 0.7037, 0.5746, 0.7010, 0.6667, 0.7321, 0.6469,\n",
      "        0.5869, 0.6626, 0.6528, 0.9484, 0.8154, 0.7100, 0.8575, 0.7072, 0.6621,\n",
      "        0.7041, 0.5898, 0.8133, 0.8939, 0.6586, 0.7275, 0.8652, 0.7369, 0.7100,\n",
      "        0.8086, 0.7788, 1.0449, 0.5401, 0.5632, 0.9193, 0.7814, 0.5112, 0.7985,\n",
      "        0.4770, 0.8167, 0.5657, 0.7737, 0.8323, 0.7859, 0.7468, 0.6779, 0.8022,\n",
      "        0.8351, 0.7066, 0.7511, 0.7207, 0.8109, 0.5897, 0.6391, 1.0895, 0.6092,\n",
      "        0.6716, 0.7548, 0.8729, 0.7153, 0.8914, 0.6663, 0.6491, 0.8809, 0.7629,\n",
      "        0.7270, 0.6882, 0.8064, 0.8543, 0.6917, 0.7562, 0.6466, 0.8351, 0.9609,\n",
      "        0.6101, 0.8148, 0.8767, 0.7271, 0.8883, 0.8737, 0.6109, 0.5408, 0.7436,\n",
      "        0.5571, 0.6342, 0.6061, 0.7825, 0.6843, 0.5894, 0.6446, 0.6080, 0.7437,\n",
      "        0.7408, 0.8171, 0.5996, 0.8469, 0.6946, 0.7821, 0.7434, 0.5950, 0.6075,\n",
      "        0.7154, 0.7231, 0.6415, 0.8244, 0.7601, 1.0836, 0.8382, 0.8823, 0.6894,\n",
      "        0.7088, 0.5917, 0.6612, 0.8427, 0.8903, 0.5655, 0.8299, 0.7231, 0.7218,\n",
      "        0.6823, 0.7836])), ('model.backbone.ln_f.bias', tensor([-1.1617e-01,  8.5529e-03, -4.5383e-02,  6.6757e-01,  9.6916e-02,\n",
      "        -1.9376e-01,  1.4596e-02, -1.0625e-02, -1.5878e-01,  1.7489e-02,\n",
      "        -5.0792e-02, -9.3871e-02, -8.3272e-02,  1.9965e-02, -1.4524e-01,\n",
      "         1.2577e-01,  1.8628e-02, -4.8779e-04, -9.7360e-02, -1.2182e-01,\n",
      "         6.2062e-02,  6.1260e-02, -1.4589e-01,  3.0829e-02, -1.2703e-01,\n",
      "         1.6150e-01, -1.2864e-01, -1.0480e-02,  9.2019e-02, -2.2199e-01,\n",
      "         1.1397e-01,  3.0183e-02,  1.0020e-01, -1.0863e-01,  1.9423e-01,\n",
      "         1.4201e-01, -1.2248e-01,  6.8383e-02, -8.3234e-02,  1.2963e-01,\n",
      "        -1.5854e-02,  2.5160e-02, -1.5827e-02, -7.4054e-02, -1.0223e-01,\n",
      "        -5.0501e-02, -1.2218e-01, -1.9997e-01,  5.9966e-02, -1.5409e-01,\n",
      "         1.0068e-01, -5.9807e-02, -5.0750e-02,  1.9021e-01, -1.7312e-01,\n",
      "         4.3223e-02, -2.0206e-02,  2.1524e-01,  3.0149e-02,  9.3607e-02,\n",
      "         7.4506e-04, -2.0891e-02,  1.3228e-01,  1.9709e-01,  1.1427e-01,\n",
      "         1.2388e-01,  1.6070e-01, -4.4918e-02, -1.1958e-01, -2.7967e-02,\n",
      "         1.2234e-01,  8.0833e-02,  1.9612e-01,  7.1445e-02, -2.7817e-02,\n",
      "         1.3761e-01,  7.3254e-02,  4.6132e-02,  1.1669e-01,  3.9675e-02,\n",
      "        -4.0091e-02, -1.7683e-01,  1.5622e-01,  4.9480e-03,  7.0894e-02,\n",
      "        -3.2209e-01,  1.4196e-01, -8.2373e-02,  3.0382e-02,  1.3697e-01,\n",
      "        -7.2935e-02,  1.0374e-01,  1.5498e-01,  7.6880e-02, -1.0155e-01,\n",
      "        -9.1817e-02, -4.0322e-02, -5.1852e-02,  6.0655e-02,  1.6003e-02,\n",
      "         2.6362e-03, -9.2919e-02,  1.1300e-01, -1.5962e-01,  2.2554e-02,\n",
      "        -1.4379e-01,  1.0283e-01,  6.0962e-02, -7.3669e-03,  6.9199e-04,\n",
      "         1.1445e-01,  1.3513e-01,  1.6183e-01,  3.5876e-02,  3.9224e-02,\n",
      "        -1.4236e-01, -1.3610e-02, -8.2723e-02, -1.2175e-01, -9.0052e-02,\n",
      "        -9.6382e-02, -9.4825e-02, -2.6437e-02,  1.1466e-01, -5.1659e-03,\n",
      "         9.1511e-02, -2.6806e-02,  1.3812e-01])), ('model.lm_head.weight', tensor([[ 0.1124, -0.0071,  0.0401,  ..., -0.1609,  0.0100, -0.1181],\n",
      "        [ 0.0767, -0.1335,  0.1313,  ..., -0.1644,  0.0841,  0.0322],\n",
      "        [ 0.0983, -0.0116,  0.0418,  ..., -0.1830, -0.0067, -0.0839],\n",
      "        ...,\n",
      "        [ 0.1026, -0.0136,  0.0398,  ..., -0.1349,  0.0138, -0.1103],\n",
      "        [ 0.1071,  0.0046,  0.0365,  ..., -0.1672,  0.0050, -0.1017],\n",
      "        [ 0.0828,  0.0049,  0.0414,  ..., -0.1595,  0.0023, -0.1049]])), ('train_torchmetrics.num_tokens.count', tensor(4359516148)), ('val_torchmetrics.num_tokens.count', tensor(333615978)), ('test_torchmetrics.num_tokens.count', tensor(305353578))]), 'loops': {'fit_loop': {'state_dict': {}, 'epoch_loop.state_dict': {'_batches_that_stepped': 3345}, 'epoch_loop.batch_progress': {'total': {'ready': 3345, 'completed': 3345, 'started': 3345, 'processed': 3345}, 'current': {'ready': 3345, 'completed': 3345, 'started': 3345, 'processed': 3345}, 'is_last_batch': True}, 'epoch_loop.scheduler_progress': {'total': {'ready': 3345, 'completed': 3345}, 'current': {'ready': 3345, 'completed': 3345}}, 'epoch_loop.automatic_optimization.state_dict': {}, 'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 3345, 'completed': 3345}, 'current': {'ready': 3345, 'completed': 3345}}, 'zero_grad': {'total': {'ready': 3345, 'completed': 3345, 'started': 3345}, 'current': {'ready': 3345, 'completed': 3345, 'started': 3345}}}}, 'epoch_loop.manual_optimization.state_dict': {}, 'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0, 'completed': 0}, 'current': {'ready': 0, 'completed': 0}}, 'epoch_loop.val_loop.state_dict': {}, 'epoch_loop.val_loop.batch_progress': {'total': {'ready': 402, 'completed': 402, 'started': 402, 'processed': 402}, 'current': {'ready': 402, 'completed': 402, 'started': 402, 'processed': 402}, 'is_last_batch': True}, 'epoch_progress': {'total': {'ready': 1, 'completed': 0, 'started': 1, 'processed': 1}, 'current': {'ready': 1, 'completed': 0, 'started': 1, 'processed': 1}}}, 'validate_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'is_last_batch': False}}, 'test_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'is_last_batch': False}}, 'predict_loop': {'state_dict': {}, 'batch_progress': {'total': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}, 'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}}, 'callbacks': {\"ModelCheckpoint{'monitor': 'test/loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': 'test/loss', 'best_model_score': tensor(1.2228), 'best_model_path': '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/test/loss.ckpt', 'current_score': tensor(1.2228), 'dirpath': '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints', 'best_k_models': {'/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/test/loss.ckpt': tensor(1.2228)}, 'kth_best_model_path': '/lila/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/test/loss.ckpt', 'kth_value': tensor(1.2228), 'last_model_path': ''}}, 'optimizer_states': [{'state': {0: {'step': tensor(3341.), 'exp_avg': tensor([[ 3.1829e-07, -7.3768e-07,  4.9298e-07,  ..., -4.8114e-07,\n",
      "          9.4663e-07,  1.5424e-06],\n",
      "        [ 1.3984e-04, -6.0857e-05,  7.3343e-05,  ..., -1.2026e-04,\n",
      "          6.9908e-05, -8.2070e-05],\n",
      "        [ 1.9593e-07, -6.1003e-07,  3.7392e-07,  ..., -3.2496e-07,\n",
      "          7.8636e-07,  1.4014e-06],\n",
      "        ...,\n",
      "        [ 1.4791e-07, -6.0999e-07,  3.7970e-07,  ..., -2.7091e-07,\n",
      "          8.2975e-07,  1.5516e-06],\n",
      "        [ 2.1026e-07, -6.2624e-07,  3.9716e-07,  ..., -3.3605e-07,\n",
      "          8.2422e-07,  1.4428e-06],\n",
      "        [ 2.7983e-07, -6.8730e-07,  4.5629e-07,  ..., -4.2587e-07,\n",
      "          8.9698e-07,  1.4960e-06]]), 'exp_avg_sq': tensor([[9.6507e-10, 2.2074e-08, 2.9891e-08,  ..., 1.7427e-08, 8.8334e-09,\n",
      "         2.6286e-08],\n",
      "        [8.0802e-09, 1.3132e-09, 1.5937e-09,  ..., 3.4484e-09, 1.0291e-09,\n",
      "         4.7415e-08],\n",
      "        [1.0417e-09, 1.6141e-08, 2.5061e-08,  ..., 1.6577e-08, 6.0017e-09,\n",
      "         1.8940e-08],\n",
      "        ...,\n",
      "        [8.7007e-10, 2.1168e-08, 2.8119e-08,  ..., 1.6152e-08, 8.5397e-09,\n",
      "         2.5329e-08],\n",
      "        [8.9875e-10, 1.8048e-08, 2.5135e-08,  ..., 1.5350e-08, 7.1652e-09,\n",
      "         2.1400e-08],\n",
      "        [8.8678e-10, 1.8941e-08, 2.5137e-08,  ..., 1.5330e-08, 7.6304e-09,\n",
      "         2.2537e-08]])}, 1: {'step': tensor(3341.), 'exp_avg': tensor([[ 7.0691e-05,  7.4855e-05,  2.4272e-05,  ...,  1.7419e-04,\n",
      "         -9.6516e-06,  9.8327e-05],\n",
      "        [-2.1537e-05, -8.6779e-06,  1.5405e-04,  ...,  1.7473e-04,\n",
      "          8.4921e-05,  1.0287e-05],\n",
      "        [ 4.9106e-05,  5.1915e-05, -2.4910e-05,  ..., -1.4927e-04,\n",
      "          2.6526e-05,  6.4956e-05],\n",
      "        ...,\n",
      "        [-4.0690e-05,  6.3747e-05,  1.3914e-04,  ...,  7.2190e-05,\n",
      "          1.2640e-05, -1.4349e-04],\n",
      "        [ 4.2090e-05, -3.8023e-05,  9.8969e-05,  ..., -1.7625e-04,\n",
      "         -1.9572e-05, -6.6006e-05],\n",
      "        [ 2.2699e-05,  8.3153e-05, -4.0796e-04,  ..., -1.9421e-04,\n",
      "         -1.5036e-07, -9.8690e-05]]), 'exp_avg_sq': tensor([[3.7150e-09, 1.2379e-08, 3.2906e-07,  ..., 3.6227e-08, 5.2734e-09,\n",
      "         1.3582e-08],\n",
      "        [4.1659e-09, 6.5855e-09, 2.6119e-07,  ..., 3.3082e-08, 7.4132e-09,\n",
      "         2.1984e-08],\n",
      "        [3.8225e-09, 4.5347e-09, 2.0884e-07,  ..., 2.1428e-08, 4.9127e-09,\n",
      "         1.3680e-08],\n",
      "        ...,\n",
      "        [2.2439e-09, 3.2266e-09, 7.2291e-08,  ..., 1.7137e-08, 4.4328e-09,\n",
      "         1.2261e-08],\n",
      "        [2.5173e-09, 3.3552e-09, 8.9801e-08,  ..., 2.2067e-08, 5.0376e-09,\n",
      "         9.9510e-09],\n",
      "        [5.6453e-09, 3.1143e-08, 1.1929e-06,  ..., 6.3878e-08, 9.6646e-09,\n",
      "         1.7396e-08]])}, 2: {'step': tensor(3341.), 'exp_avg': tensor([ 1.8085e-04, -2.6436e-05, -1.1556e-04, -1.1973e-04, -5.1842e-04,\n",
      "        -1.2253e-04,  2.0398e-04, -3.1305e-04,  2.8544e-04,  1.9780e-04,\n",
      "         4.2278e-04, -2.6386e-04,  3.1153e-05,  7.5109e-04, -1.7838e-04,\n",
      "         4.1597e-04,  5.1738e-04, -9.1992e-05, -7.1574e-05, -2.1197e-04,\n",
      "         3.3265e-04, -1.9883e-04, -7.5117e-05, -1.7776e-04, -5.4214e-04,\n",
      "         2.5676e-05,  2.2898e-04, -6.3952e-04,  5.9298e-04, -4.0361e-04,\n",
      "         3.8216e-04,  5.1005e-04, -3.1583e-04, -2.3649e-04, -4.7525e-04,\n",
      "         8.8287e-04,  6.2824e-04, -4.7358e-04, -1.3431e-04,  3.6099e-04,\n",
      "         1.2231e-05,  3.6509e-04, -3.1357e-04, -4.9894e-04,  1.1342e-04,\n",
      "         8.8269e-05, -9.1940e-04, -1.5567e-04, -6.6433e-05, -8.4805e-04,\n",
      "         3.6844e-04,  3.7225e-04,  1.7947e-04,  9.9491e-04, -7.0028e-04,\n",
      "        -2.5320e-04,  9.4885e-06,  5.8409e-04,  1.1329e-04,  1.2078e-05,\n",
      "         4.3358e-05,  2.9581e-04,  9.1043e-04, -1.1443e-04, -2.1535e-05,\n",
      "         6.8363e-04, -6.7302e-04,  1.2320e-04, -2.6826e-04, -1.7100e-04,\n",
      "         6.7127e-05,  5.9562e-04,  1.9630e-04,  6.9327e-05, -5.5246e-04,\n",
      "         5.2516e-05, -4.2553e-04,  3.7166e-04,  2.2094e-04,  8.2256e-04,\n",
      "        -2.0803e-04, -2.7235e-05,  3.0235e-04, -1.5570e-04, -7.5845e-04,\n",
      "        -3.3677e-04, -2.2188e-05, -7.7415e-04, -5.5223e-04,  8.3871e-05,\n",
      "         1.7741e-04, -9.5853e-05, -3.6955e-04,  6.5053e-04,  1.4764e-04,\n",
      "        -3.3924e-04, -8.2316e-06,  1.4340e-04,  1.1740e-03,  1.2784e-04,\n",
      "        -1.0547e-04, -7.2559e-04,  3.2548e-04, -8.3123e-04,  2.6301e-04,\n",
      "        -1.6508e-04,  1.9754e-04,  5.9635e-04,  1.4352e-04, -4.5276e-04,\n",
      "         1.3940e-06,  5.9129e-04, -1.4541e-04,  5.8977e-05,  5.5496e-04,\n",
      "         8.0624e-04, -1.3947e-04, -4.2539e-04, -9.6003e-05, -6.5603e-04,\n",
      "         3.2950e-04, -2.3262e-04, -5.0062e-04,  1.1974e-04, -7.5927e-04,\n",
      "         1.9415e-04,  1.7997e-04, -1.2150e-03]), 'exp_avg_sq': tensor([2.7440e-06, 1.7455e-06, 1.9600e-06, 6.2640e-07, 1.0148e-06, 1.9573e-06,\n",
      "        3.1840e-07, 1.5690e-06, 3.2396e-06, 8.8039e-07, 2.5350e-06, 1.6075e-06,\n",
      "        1.1074e-06, 1.8684e-06, 8.2974e-07, 1.2471e-06, 8.1649e-07, 8.8981e-07,\n",
      "        1.4401e-06, 4.4034e-07, 2.1708e-06, 9.4672e-07, 6.0892e-07, 1.7611e-06,\n",
      "        9.8087e-07, 8.1168e-07, 1.5120e-06, 1.9146e-06, 9.4112e-07, 2.0462e-06,\n",
      "        3.0198e-07, 2.5307e-06, 6.3491e-07, 1.1913e-06, 1.6877e-06, 7.3975e-07,\n",
      "        1.6872e-06, 1.4124e-06, 5.2090e-06, 1.8247e-06, 4.2186e-07, 2.1720e-06,\n",
      "        2.3758e-06, 1.8545e-06, 3.5899e-07, 1.0453e-06, 3.2151e-06, 7.2658e-07,\n",
      "        9.4890e-07, 2.1480e-06, 7.1456e-07, 1.9422e-06, 1.0012e-06, 2.7150e-06,\n",
      "        1.5377e-06, 1.0353e-06, 5.9021e-07, 1.2268e-06, 2.8762e-06, 1.8726e-06,\n",
      "        8.6610e-07, 9.2366e-07, 4.2134e-06, 1.0503e-06, 1.2905e-06, 8.7051e-07,\n",
      "        1.3054e-06, 1.5033e-06, 1.3435e-06, 8.4315e-07, 5.6148e-07, 3.2391e-06,\n",
      "        4.4768e-07, 6.0531e-07, 2.5274e-06, 7.9802e-07, 2.0681e-06, 8.9483e-07,\n",
      "        1.9467e-06, 4.3734e-06, 1.0241e-06, 3.9739e-06, 6.8001e-07, 1.7873e-06,\n",
      "        2.4031e-06, 2.2975e-06, 2.0355e-06, 1.6999e-06, 9.1478e-07, 1.1682e-06,\n",
      "        6.2206e-06, 7.3156e-07, 3.2012e-06, 2.0942e-06, 1.4152e-06, 8.5389e-07,\n",
      "        1.7239e-06, 1.5909e-06, 2.7678e-06, 4.8299e-06, 3.1780e-06, 1.0676e-06,\n",
      "        4.8242e-06, 2.0742e-06, 5.3116e-07, 3.1414e-06, 1.8256e-06, 1.1595e-06,\n",
      "        1.2892e-06, 2.6318e-06, 3.7371e-07, 8.6843e-07, 9.8132e-07, 1.3670e-06,\n",
      "        2.0486e-06, 5.8513e-06, 1.2625e-06, 2.0017e-06, 1.3613e-06, 1.3662e-06,\n",
      "        5.8595e-07, 4.7156e-07, 6.1080e-07, 1.7441e-06, 2.3430e-06, 6.3665e-07,\n",
      "        1.0163e-06, 7.9037e-06])}, 3: {'step': tensor(3341.), 'exp_avg': tensor([[-8.1595e-06, -5.5229e-07, -9.7392e-06,  ...,  3.7522e-06,\n",
      "         -6.8940e-06,  1.0326e-05],\n",
      "        [-1.6129e-06,  1.3331e-06,  7.0025e-08,  ..., -9.0358e-06,\n",
      "          1.5367e-06, -2.7856e-06],\n",
      "        [ 5.5255e-06,  6.5271e-06,  2.1559e-05,  ...,  7.3222e-06,\n",
      "          1.0333e-05, -1.7366e-06],\n",
      "        ...,\n",
      "        [ 2.9125e-06, -1.5336e-05,  3.4588e-06,  ..., -5.8845e-05,\n",
      "         -1.6209e-05, -4.4593e-05],\n",
      "        [-6.9556e-06, -9.8250e-06, -2.2195e-05,  ..., -1.0123e-05,\n",
      "         -1.5147e-05,  8.2878e-07],\n",
      "        [-8.3863e-07, -2.2375e-06, -4.4975e-06,  ..., -3.7133e-06,\n",
      "         -1.5787e-06,  7.4363e-06]]), 'exp_avg_sq': tensor([[4.2651e-11, 3.2709e-11, 1.6757e-10,  ..., 1.3638e-10, 7.5765e-11,\n",
      "         4.5622e-11],\n",
      "        [7.5350e-11, 1.7086e-11, 1.0775e-10,  ..., 1.5803e-10, 4.9014e-11,\n",
      "         2.1045e-11],\n",
      "        [1.8248e-10, 2.0522e-10, 7.0321e-10,  ..., 7.5467e-10, 3.1419e-10,\n",
      "         1.3675e-10],\n",
      "        ...,\n",
      "        [9.9009e-10, 2.0041e-09, 1.2061e-08,  ..., 4.7682e-09, 4.7211e-09,\n",
      "         4.6119e-09],\n",
      "        [1.4366e-10, 4.4560e-10, 1.1595e-09,  ..., 5.9198e-10, 4.5625e-10,\n",
      "         4.0332e-11],\n",
      "        [3.2280e-11, 5.7325e-11, 2.9583e-10,  ..., 8.0831e-11, 1.1573e-10,\n",
      "         6.4502e-11]])}, 4: {'step': tensor(3341.), 'exp_avg': tensor([ 1.8459e-05,  1.8790e-05,  3.3018e-06, -2.6282e-05, -1.8528e-05,\n",
      "         2.9192e-05, -1.9467e-05,  3.9045e-06, -3.0297e-07,  5.7027e-05,\n",
      "        -6.8352e-07,  8.6201e-06,  7.6107e-06, -6.9188e-06, -2.0397e-05,\n",
      "        -9.4978e-07,  8.3834e-06,  8.5375e-06, -5.0420e-06, -6.7757e-06,\n",
      "         6.2757e-07, -3.3283e-06,  7.8171e-06,  4.4239e-06,  4.6517e-06,\n",
      "        -5.5456e-06, -5.8018e-06, -4.6809e-07,  5.4544e-06, -3.1395e-05,\n",
      "         2.7069e-07,  4.2940e-06,  5.4366e-05,  6.0537e-07,  2.6410e-05,\n",
      "        -1.7382e-05, -1.7236e-06,  2.1025e-05,  3.4696e-05,  1.4544e-05,\n",
      "        -7.1934e-06,  4.5808e-05,  1.3220e-05, -8.5182e-05,  6.7496e-05,\n",
      "         3.0588e-06,  5.0959e-07,  5.9749e-07, -9.1216e-06,  3.9097e-05,\n",
      "         1.7492e-05,  3.9055e-06,  7.8470e-05, -7.8051e-06, -8.2149e-06,\n",
      "         2.4548e-06,  6.5990e-06, -7.5274e-07, -1.3472e-05, -3.6332e-05,\n",
      "        -1.2450e-05,  9.3324e-05, -7.1365e-07,  9.9344e-06,  1.1014e-05,\n",
      "         2.6992e-08,  3.9270e-06, -6.0584e-06, -1.5414e-05,  5.4827e-07,\n",
      "         6.7020e-06,  1.8780e-06, -2.8900e-06,  1.6648e-05, -1.6341e-05,\n",
      "        -1.0863e-04, -4.1621e-05,  1.6287e-05, -2.0036e-04, -2.0889e-06,\n",
      "         1.3244e-05, -9.1687e-06,  3.9807e-05,  4.1188e-06, -5.3528e-05,\n",
      "         1.9224e-05, -4.3718e-06, -2.8777e-07, -1.4465e-05,  4.6534e-05,\n",
      "         1.1817e-05, -1.2981e-05,  2.3334e-05,  9.8871e-06, -1.6310e-05,\n",
      "        -1.3722e-05, -8.3463e-06, -2.0482e-05,  3.2752e-05,  2.5351e-04,\n",
      "         3.7945e-05,  3.9430e-05, -4.7650e-07, -1.7462e-07,  5.6763e-05,\n",
      "         7.1357e-06,  1.7132e-04,  2.0038e-06,  6.6001e-05,  4.1843e-05,\n",
      "         7.2718e-06,  8.8293e-06,  6.7909e-06,  1.6220e-05, -4.2598e-05,\n",
      "        -6.0748e-05, -5.5028e-09,  1.9641e-05,  2.1976e-06,  1.2726e-05,\n",
      "         1.0085e-04,  2.8377e-06,  1.9925e-05,  1.2120e-04,  6.0283e-06,\n",
      "         3.8934e-05,  8.4216e-06, -2.8677e-08, -3.1797e-07, -9.1355e-06,\n",
      "         2.4967e-06, -6.1148e-06, -4.7095e-05, -3.9731e-05, -1.4824e-05,\n",
      "        -1.7083e-06,  9.4016e-05, -1.8121e-05,  8.8318e-06, -2.6952e-06,\n",
      "        -1.9163e-05, -3.6086e-06, -3.2379e-05, -1.1733e-04,  2.1508e-04,\n",
      "         2.4986e-05, -2.0806e-06,  1.3810e-05, -8.6083e-06, -1.4686e-05,\n",
      "        -4.4628e-05, -1.1102e-05,  1.1526e-05, -2.6901e-05,  2.4974e-06,\n",
      "        -1.4960e-05,  1.7268e-05, -4.0403e-05, -9.9948e-06,  1.2484e-05,\n",
      "         3.7707e-05, -5.0626e-06, -5.4064e-07, -9.6131e-05, -6.7196e-07,\n",
      "         1.3877e-04,  9.6163e-06,  6.0041e-06, -2.0487e-05,  1.4580e-04,\n",
      "         3.9425e-06, -1.9567e-04,  4.5908e-05,  1.4439e-05, -8.9806e-05,\n",
      "         5.1579e-05,  2.0629e-05,  9.0838e-06, -1.8370e-05, -3.5493e-05,\n",
      "         1.0309e-04, -1.5446e-05, -2.5143e-06,  1.6645e-05, -1.1069e-05,\n",
      "        -8.3524e-06,  1.7479e-06, -4.0522e-06, -4.3644e-06,  7.4954e-05,\n",
      "         3.2639e-05,  4.5898e-05, -7.2311e-06,  9.4015e-06,  1.9507e-05,\n",
      "        -4.0160e-06,  2.6467e-06, -1.4352e-06, -1.8041e-05, -3.0481e-05,\n",
      "        -2.4905e-06, -1.3990e-04,  2.8447e-05, -1.4054e-05,  3.5223e-05,\n",
      "         8.8951e-05,  1.5856e-05, -3.1231e-06,  6.0032e-06,  4.0790e-06,\n",
      "        -9.3394e-05,  1.4419e-05,  7.0104e-05, -2.7313e-06, -6.7630e-05,\n",
      "         1.3081e-04,  2.7910e-05, -7.6761e-06,  9.4253e-05,  2.4932e-07,\n",
      "         2.5036e-05, -4.7764e-06,  6.4244e-05, -1.9231e-05, -1.3690e-05,\n",
      "         5.3323e-06, -2.2292e-05,  5.3494e-06,  2.9555e-06,  5.2184e-05,\n",
      "        -4.6788e-06, -1.2055e-05, -3.2368e-05,  2.8236e-05,  1.1317e-04,\n",
      "         3.9311e-06, -6.6366e-06, -2.7465e-06, -3.1253e-06, -2.2509e-05,\n",
      "        -3.1307e-05, -5.3998e-06,  5.9139e-05, -6.0141e-05,  2.8489e-05,\n",
      "        -9.7083e-06, -5.0538e-05, -2.5591e-05, -4.3278e-06,  1.0760e-06,\n",
      "         3.7148e-06, -2.0350e-05, -1.7883e-05,  1.0896e-05,  1.5149e-05,\n",
      "         2.1179e-05, -8.9551e-06,  6.6222e-06, -2.0277e-05,  8.1796e-06,\n",
      "        -6.6318e-05,  1.7546e-04,  2.5283e-05,  3.7065e-06, -7.6195e-06,\n",
      "         5.3710e-05,  2.8264e-05, -2.7804e-05, -4.1588e-05, -2.7937e-06,\n",
      "        -2.0688e-05,  1.5825e-05, -4.9400e-05, -1.0216e-05, -2.8392e-06,\n",
      "         5.9919e-05,  2.3193e-05,  2.4171e-04,  4.7079e-06,  2.2762e-05,\n",
      "        -2.2866e-06,  4.5752e-05, -3.3863e-06,  1.0783e-05, -1.5672e-05,\n",
      "        -4.4600e-05,  9.6897e-06,  3.7344e-06, -5.0649e-05,  1.7391e-05,\n",
      "        -7.2708e-06, -1.0606e-04, -5.4473e-06,  1.6912e-05, -5.1477e-06,\n",
      "         3.5418e-06,  1.9609e-04, -9.8602e-06,  3.0978e-06, -9.3230e-05,\n",
      "         1.7224e-04,  1.1032e-05, -9.7849e-06,  1.0462e-05,  1.2063e-05,\n",
      "        -2.8120e-05,  9.9011e-06,  1.8968e-05,  9.0074e-05,  2.2516e-05,\n",
      "         8.5686e-06,  2.5081e-05, -1.8800e-05, -5.2984e-06,  1.3746e-05,\n",
      "        -1.3489e-05, -1.9343e-05, -2.6981e-05,  6.3246e-05,  8.7692e-05,\n",
      "         9.1531e-05, -1.1692e-05,  6.3713e-05,  6.3027e-07, -4.9449e-06,\n",
      "        -3.9031e-06,  1.4981e-06, -1.7612e-05, -8.0036e-06,  9.8765e-05,\n",
      "         1.5918e-06, -3.4255e-05, -1.9110e-05,  1.0443e-05,  1.2750e-04,\n",
      "         7.2762e-07,  1.8950e-06, -5.8835e-06,  1.4739e-06,  1.1105e-05,\n",
      "        -1.0494e-04,  1.0054e-05,  2.9278e-06, -1.1809e-07,  1.6209e-05,\n",
      "         4.5330e-05, -1.7499e-06, -1.6266e-05,  7.0638e-07, -1.3161e-06,\n",
      "         3.1996e-05, -1.1092e-06, -1.0669e-05,  1.2481e-05, -5.9984e-05,\n",
      "         4.3429e-06, -2.0051e-06,  4.5383e-05, -1.3182e-05, -1.1253e-05,\n",
      "        -1.0516e-05,  4.0781e-06,  3.0067e-05, -1.2537e-05,  2.4718e-08,\n",
      "         2.6583e-05, -9.7672e-06,  3.2034e-05, -2.7616e-05,  1.9877e-05,\n",
      "        -4.2030e-05,  3.5818e-05,  7.8053e-06,  3.7054e-05,  5.8109e-06,\n",
      "         2.2538e-05, -6.7586e-06, -2.0712e-06,  6.0449e-06,  1.1474e-05,\n",
      "         5.7788e-06,  3.8461e-05, -4.3954e-06,  5.0086e-06]), 'exp_avg_sq': tensor([1.0110e-09, 6.9225e-10, 3.5061e-09, 6.1849e-10, 1.4532e-10, 2.9221e-09,\n",
      "        4.2833e-10, 5.5168e-10, 2.2200e-12, 1.2478e-09, 3.7501e-10, 1.6901e-09,\n",
      "        2.3241e-10, 5.3270e-10, 4.5697e-10, 1.2907e-12, 9.6097e-10, 9.3223e-10,\n",
      "        3.8401e-11, 2.7850e-10, 4.1191e-10, 9.7051e-11, 9.1686e-11, 8.9095e-11,\n",
      "        9.1506e-11, 1.5232e-10, 3.6302e-10, 2.9902e-10, 3.6685e-11, 1.5453e-09,\n",
      "        1.3593e-12, 5.2960e-11, 2.7639e-09, 4.5985e-10, 8.8022e-10, 1.6687e-08,\n",
      "        5.9994e-10, 1.0638e-09, 7.4166e-10, 2.0122e-10, 1.0567e-10, 1.9739e-09,\n",
      "        2.2648e-10, 6.8540e-09, 1.0700e-08, 4.5937e-11, 2.1409e-09, 5.5747e-11,\n",
      "        2.2887e-10, 5.1650e-10, 2.8531e-10, 1.8920e-11, 1.0736e-08, 4.2840e-10,\n",
      "        2.4163e-10, 7.0205e-12, 1.2799e-10, 1.1950e-11, 9.9947e-09, 1.5960e-09,\n",
      "        3.7872e-10, 5.3505e-07, 2.1533e-10, 4.0073e-09, 3.2090e-09, 5.5066e-09,\n",
      "        8.3813e-10, 7.9001e-10, 4.9475e-09, 1.1681e-12, 5.1861e-10, 7.5243e-10,\n",
      "        3.2898e-09, 1.1004e-08, 8.3136e-09, 1.1537e-08, 1.4706e-09, 4.0637e-08,\n",
      "        7.0802e-08, 2.3922e-10, 2.5828e-10, 4.7530e-10, 3.6713e-09, 3.9314e-10,\n",
      "        7.0579e-09, 5.6121e-10, 4.5424e-10, 7.0015e-12, 2.7654e-09, 1.8602e-09,\n",
      "        3.5669e-10, 5.3188e-10, 5.1246e-09, 7.6712e-10, 1.6831e-09, 4.9083e-10,\n",
      "        8.3721e-11, 4.1926e-10, 2.7693e-09, 1.7557e-07, 2.1066e-09, 1.2491e-09,\n",
      "        4.5726e-12, 3.4086e-10, 1.5520e-08, 3.8666e-10, 1.2278e-08, 8.3581e-11,\n",
      "        1.2703e-07, 1.9367e-09, 5.6265e-10, 1.2325e-09, 8.5070e-09, 3.3243e-08,\n",
      "        1.0672e-08, 6.9310e-09, 2.8581e-13, 4.1049e-09, 5.1896e-09, 3.1286e-10,\n",
      "        7.8681e-08, 7.5751e-09, 4.6449e-10, 4.9925e-09, 4.5482e-10, 1.6251e-09,\n",
      "        5.4450e-10, 1.1211e-13, 9.5884e-10, 5.2985e-10, 3.7160e-10, 5.0069e-09,\n",
      "        1.6410e-09, 2.2235e-09, 4.5996e-10, 3.3287e-08, 8.0144e-07, 1.3583e-09,\n",
      "        1.0044e-09, 3.3803e-10, 1.7463e-09, 2.1760e-10, 7.0584e-09, 3.2625e-09,\n",
      "        5.0478e-07, 1.2823e-09, 1.9915e-10, 1.6897e-08, 2.2511e-10, 2.2743e-10,\n",
      "        2.4682e-10, 1.3362e-10, 2.1574e-10, 2.5131e-08, 2.3211e-10, 6.2504e-09,\n",
      "        6.9306e-08, 3.5706e-08, 2.9949e-10, 8.1796e-09, 1.0372e-09, 3.4615e-09,\n",
      "        4.6314e-10, 5.4549e-09, 3.8955e-09, 3.6308e-07, 1.6702e-10, 5.9936e-11,\n",
      "        6.0857e-10, 8.5020e-08, 9.7357e-11, 2.5287e-08, 3.3501e-09, 1.2254e-09,\n",
      "        9.6312e-08, 3.3751e-09, 7.3406e-10, 9.8664e-10, 5.7800e-09, 1.7410e-09,\n",
      "        6.4227e-09, 8.8680e-10, 4.3808e-10, 1.2403e-09, 1.3547e-08, 1.2213e-09,\n",
      "        3.6778e-12, 1.0836e-09, 2.4309e-09, 1.2257e-07, 7.6096e-10, 1.7362e-09,\n",
      "        6.9902e-09, 9.3307e-10, 9.4859e-10, 7.0358e-10, 5.8154e-08, 3.3687e-10,\n",
      "        1.3033e-09, 3.9236e-09, 3.2938e-10, 1.0060e-06, 3.9771e-09, 3.5885e-09,\n",
      "        5.6285e-09, 3.0947e-08, 2.6074e-10, 4.1825e-11, 1.5295e-10, 3.6083e-09,\n",
      "        2.4158e-08, 3.2336e-08, 2.4814e-08, 2.2669e-10, 1.2567e-08, 4.7100e-07,\n",
      "        2.0835e-07, 7.6809e-09, 1.1403e-08, 4.4388e-12, 4.6801e-08, 1.0415e-10,\n",
      "        7.4439e-08, 2.4598e-09, 3.0721e-10, 7.5570e-11, 7.4447e-09, 2.6565e-09,\n",
      "        3.6930e-11, 3.8316e-09, 3.7787e-10, 7.8149e-10, 7.9096e-09, 3.0829e-08,\n",
      "        1.6668e-08, 5.8708e-10, 3.2438e-09, 4.9712e-11, 3.4528e-09, 1.0549e-09,\n",
      "        6.1198e-08, 4.8671e-09, 6.5393e-08, 5.1890e-09, 8.5465e-10, 2.0276e-08,\n",
      "        1.4785e-08, 8.2025e-10, 8.5397e-10, 8.0168e-10, 1.2977e-10, 9.0617e-09,\n",
      "        1.2848e-08, 2.3140e-09, 2.9181e-10, 1.9356e-09, 2.0899e-10, 7.3428e-11,\n",
      "        2.8011e-08, 1.0496e-08, 6.2397e-09, 4.0905e-08, 2.5175e-08, 6.8672e-10,\n",
      "        9.1542e-10, 4.3487e-09, 4.4309e-08, 1.2546e-08, 9.3340e-09, 7.6934e-11,\n",
      "        2.0328e-09, 3.6817e-10, 3.9397e-08, 3.0645e-10, 5.3776e-10, 8.6136e-08,\n",
      "        3.9663e-09, 1.6284e-07, 3.9514e-11, 6.3283e-10, 2.5533e-11, 4.1650e-09,\n",
      "        2.9931e-11, 7.9713e-10, 4.8642e-09, 8.8052e-09, 2.8221e-10, 6.9271e-10,\n",
      "        5.7092e-10, 6.0263e-10, 1.3748e-09, 6.6740e-09, 1.7298e-09, 1.3241e-08,\n",
      "        4.6174e-10, 2.2420e-10, 3.6706e-08, 1.0650e-09, 1.5663e-11, 4.2457e-09,\n",
      "        8.7528e-08, 2.7774e-10, 1.7766e-10, 1.0012e-09, 5.8241e-10, 1.0946e-08,\n",
      "        6.0946e-10, 8.5130e-10, 3.3020e-09, 5.1922e-10, 6.5875e-09, 4.4451e-09,\n",
      "        4.8641e-10, 5.7267e-08, 6.0725e-09, 5.7381e-10, 2.5944e-09, 4.7049e-08,\n",
      "        1.4348e-09, 8.8268e-09, 6.8485e-08, 2.1740e-09, 1.3110e-09, 1.2515e-11,\n",
      "        3.2967e-08, 2.3556e-10, 5.9172e-11, 1.9318e-09, 1.2917e-10, 3.7759e-07,\n",
      "        7.8435e-11, 1.6837e-08, 8.7239e-10, 1.0965e-09, 2.7470e-08, 4.8029e-12,\n",
      "        3.5542e-10, 1.4696e-10, 1.7359e-10, 1.0199e-09, 4.1594e-07, 1.7537e-10,\n",
      "        1.0778e-09, 1.5033e-08, 5.1558e-07, 7.1974e-09, 1.4356e-09, 4.5479e-08,\n",
      "        2.5960e-10, 1.6388e-11, 3.1388e-09, 3.4660e-09, 5.6956e-10, 1.9464e-10,\n",
      "        1.1326e-08, 4.9608e-09, 1.6447e-10, 7.7781e-10, 2.1782e-09, 3.0533e-10,\n",
      "        5.0864e-10, 1.7721e-10, 1.5143e-09, 6.3171e-08, 1.7655e-12, 5.4173e-10,\n",
      "        2.9057e-09, 1.5651e-09, 9.3406e-09, 1.1343e-08, 4.7095e-08, 1.9173e-08,\n",
      "        1.0025e-10, 1.2320e-06, 3.0065e-10, 1.3963e-10, 1.8837e-09, 2.1506e-08,\n",
      "        2.5952e-10, 8.1709e-08, 4.0866e-10, 4.4080e-09, 1.0898e-09, 1.7326e-11])}, 5: {'step': tensor(3341.), 'exp_avg': tensor([[[-1.8151e-05, -1.3036e-05, -1.9229e-05]],\n",
      "\n",
      "        [[-3.0750e-06,  2.1481e-05, -3.2833e-05]],\n",
      "\n",
      "        [[ 3.8924e-05,  3.9280e-05,  1.2542e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.9239e-05, -5.1476e-05, -8.0396e-04]],\n",
      "\n",
      "        [[ 1.3082e-04,  1.1337e-06,  1.4291e-04]],\n",
      "\n",
      "        [[-5.4176e-05, -1.4543e-04, -7.8056e-05]]]), 'exp_avg_sq': tensor([[[6.5054e-09, 5.7556e-09, 5.3656e-09]],\n",
      "\n",
      "        [[1.5336e-09, 1.2281e-09, 1.4272e-09]],\n",
      "\n",
      "        [[2.0502e-08, 2.2377e-08, 2.5262e-08]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.3650e-07, 1.0180e-07, 7.6729e-07]],\n",
      "\n",
      "        [[1.4918e-08, 1.7813e-08, 1.4005e-08]],\n",
      "\n",
      "        [[1.2390e-07, 2.1908e-07, 2.5488e-07]]])}, 6: {'step': tensor(3341.), 'exp_avg': tensor([ 5.8308e-05,  3.7142e-05, -1.2303e-03,  2.3313e-04,  2.5084e-04,\n",
      "        -4.5468e-05, -1.1318e-04,  2.2307e-05, -2.0943e-05, -1.9829e-04,\n",
      "         4.0925e-06,  8.2459e-05, -3.3923e-05, -1.7181e-05, -9.4397e-05,\n",
      "         1.5835e-04,  6.4906e-05, -1.5683e-05, -6.2821e-05,  4.2207e-04,\n",
      "        -5.1431e-06, -7.1984e-05,  5.9891e-05,  4.5228e-05, -2.7391e-05,\n",
      "        -9.5565e-05,  1.3013e-05,  3.6732e-06, -1.1843e-04, -1.4623e-04,\n",
      "         2.6732e-05, -3.8174e-05,  1.7383e-04,  1.5900e-06, -7.0247e-05,\n",
      "         9.1598e-05, -7.1357e-06, -1.7077e-04, -1.2756e-04,  6.6584e-05,\n",
      "         6.9186e-05, -1.6489e-04,  4.6937e-05,  1.4861e-04,  2.1573e-04,\n",
      "        -7.2144e-05,  1.3640e-06, -1.2769e-05, -8.1630e-05,  2.4010e-04,\n",
      "         1.6205e-04, -9.9023e-05, -1.6327e-04, -1.9109e-05, -6.3249e-05,\n",
      "         1.5912e-04, -4.4911e-05, -5.7404e-05,  6.0751e-05, -1.2780e-04,\n",
      "        -1.6026e-04,  1.2762e-03,  2.8390e-06, -7.5131e-05,  4.7649e-05,\n",
      "         5.9234e-09, -1.0611e-05, -1.0839e-05, -5.0184e-05,  3.4176e-05,\n",
      "        -1.8213e-05, -1.2677e-05,  5.2496e-06, -7.0853e-05,  3.2437e-05,\n",
      "        -2.6426e-04,  1.0266e-04, -2.8480e-05,  3.2001e-04, -1.2766e-05,\n",
      "         8.0179e-05, -3.0467e-05,  9.0699e-05,  2.4320e-05,  2.5495e-04,\n",
      "         3.2274e-05,  4.4949e-05, -2.6453e-05,  8.3691e-05,  1.4131e-04,\n",
      "         1.2002e-04,  7.5610e-05,  3.3920e-05,  5.2124e-05, -3.4115e-05,\n",
      "        -6.3465e-05, -4.4090e-05,  6.0987e-05,  1.6277e-04,  8.1275e-04,\n",
      "        -8.4217e-05,  6.4624e-05,  1.9574e-05,  3.5196e-07,  3.6118e-04,\n",
      "         3.4867e-05, -3.2233e-04, -4.9108e-05,  2.4187e-04,  1.4302e-04,\n",
      "         3.8439e-05,  1.2210e-04,  1.6226e-05,  4.4764e-05,  1.0437e-04,\n",
      "        -2.5717e-04, -1.9083e-06,  1.0158e-04, -3.4359e-05,  7.0374e-05,\n",
      "        -3.4824e-04,  2.0013e-05, -6.0921e-05,  2.7341e-04,  5.4668e-05,\n",
      "         1.2777e-04, -2.9125e-05,  5.2092e-05, -1.0665e-06,  4.6829e-05,\n",
      "        -7.8049e-04,  1.9525e-05,  2.2358e-04,  5.5104e-04, -7.5025e-05,\n",
      "         7.8582e-06,  2.1960e-04, -4.9690e-05, -3.2219e-05,  4.9352e-05,\n",
      "        -2.6098e-04,  1.0983e-05, -2.1136e-04, -2.5071e-04,  4.3577e-04,\n",
      "        -6.3826e-05,  6.4114e-06, -3.9711e-04,  1.1652e-04, -3.1620e-04,\n",
      "        -1.0160e-04, -1.1824e-04, -3.1464e-05, -5.0700e-05,  8.7534e-06,\n",
      "         3.4610e-05,  5.1265e-05, -1.1237e-04,  5.2711e-05, -1.2948e-04,\n",
      "         2.9843e-04,  2.1604e-05,  1.9491e-06, -5.8445e-04,  1.3825e-06,\n",
      "        -2.8570e-04, -2.9082e-05,  5.4833e-05,  1.4247e-04, -4.4392e-04,\n",
      "        -1.1278e-04, -4.6611e-04,  2.4326e-04, -9.2992e-05, -1.7160e-04,\n",
      "        -1.1977e-04,  8.4504e-05,  4.3047e-05,  4.6098e-05, -2.1731e-04,\n",
      "        -4.5440e-04, -7.3844e-05,  3.3218e-05, -3.9318e-05,  2.7697e-05,\n",
      "         3.2029e-05, -3.8385e-05,  4.0156e-05,  2.3177e-05,  1.3791e-04,\n",
      "        -1.6209e-04,  2.4773e-04,  6.5442e-05, -6.6536e-05,  5.3347e-05,\n",
      "         7.7983e-06,  1.3427e-05,  4.7075e-06,  8.5792e-05, -1.5045e-04,\n",
      "         1.2295e-05,  6.7675e-04, -4.6339e-05, -1.1354e-04,  1.2479e-04,\n",
      "        -2.8566e-04,  4.3872e-04, -2.2682e-05,  6.1071e-05, -8.9937e-06,\n",
      "         2.8398e-04, -3.6506e-05, -1.9195e-04,  2.8742e-05,  1.3389e-04,\n",
      "        -2.7129e-04, -1.2431e-04, -1.8415e-05,  2.4564e-04,  1.8906e-05,\n",
      "        -5.2324e-05, -6.3029e-05, -2.0179e-04, -2.8812e-05,  2.6577e-05,\n",
      "         5.8090e-05, -4.6675e-05,  8.0515e-05,  2.5676e-05,  1.9764e-04,\n",
      "         2.3753e-05,  3.6257e-05,  9.3037e-05,  6.7432e-05, -1.6444e-04,\n",
      "        -5.5369e-05, -2.1447e-05, -3.3589e-05, -1.0177e-05, -1.8590e-04,\n",
      "         7.8532e-05,  1.0215e-05, -3.7748e-04, -2.9893e-04,  7.6217e-05,\n",
      "         5.8849e-05,  3.1515e-04,  1.0513e-04,  3.5148e-05,  5.7232e-05,\n",
      "         8.2525e-05, -9.9085e-05, -3.7145e-05, -9.2179e-05, -1.3557e-04,\n",
      "         6.5051e-05,  5.3892e-05,  7.5838e-05, -1.5333e-03, -1.9705e-05,\n",
      "         2.1465e-04,  1.4569e-03, -5.2793e-05,  1.3580e-05,  4.1593e-05,\n",
      "        -1.6961e-04, -6.4713e-05,  1.4906e-04, -6.6936e-05,  4.6336e-05,\n",
      "         1.1005e-04, -9.0655e-05,  1.0691e-04, -8.1782e-05, -1.0209e-05,\n",
      "        -4.3008e-04, -8.7829e-05,  7.2899e-04, -4.2198e-05, -9.1733e-05,\n",
      "        -3.1608e-05, -3.1251e-04,  1.0556e-04,  2.7531e-05,  4.4530e-05,\n",
      "         1.2448e-04,  6.4894e-05, -1.3186e-05, -7.2372e-05,  8.5041e-05,\n",
      "         1.8647e-05, -1.8889e-04, -5.7298e-05,  8.9690e-05,  7.2974e-05,\n",
      "         1.7607e-05, -9.1446e-04, -7.6820e-05,  1.5236e-05, -3.3323e-04,\n",
      "         4.7135e-04, -5.3069e-05, -1.3029e-04, -3.3421e-05, -3.2234e-05,\n",
      "        -1.0783e-04, -6.1335e-05,  6.7801e-05,  2.6196e-04,  6.1421e-05,\n",
      "        -6.6266e-05,  1.9276e-04, -6.0109e-05, -1.2740e-05, -3.3268e-05,\n",
      "        -1.7666e-05, -4.5888e-05, -3.0567e-04,  9.1296e-05, -2.6413e-04,\n",
      "        -2.1798e-04,  7.6258e-05, -1.1990e-04, -4.5615e-05, -1.5858e-05,\n",
      "        -1.6314e-05, -2.7214e-05, -2.0476e-04, -3.6706e-05,  2.5396e-04,\n",
      "        -1.1792e-04,  6.3556e-05, -5.9737e-05,  1.0348e-04,  6.5632e-04,\n",
      "         1.5753e-05,  2.2045e-05,  1.5109e-05,  3.4278e-05, -6.9126e-05,\n",
      "        -2.5975e-04, -2.7202e-05,  2.3497e-05,  4.4943e-08,  5.0356e-05,\n",
      "        -1.2080e-04, -5.5518e-06, -3.3915e-05,  2.0851e-05, -9.5456e-05,\n",
      "         1.2037e-04,  8.0108e-06,  8.2710e-05, -5.1578e-05, -2.1014e-04,\n",
      "        -1.9447e-04, -1.3577e-05, -1.1625e-04,  3.4638e-05,  3.8591e-05,\n",
      "        -1.1906e-04, -4.6545e-05, -1.8350e-04, -3.0424e-05,  2.4168e-05,\n",
      "         7.5809e-05,  2.2016e-05, -6.8448e-05, -8.9917e-05,  1.4100e-04,\n",
      "        -1.3910e-04,  7.7656e-05,  5.0850e-05,  9.9590e-05, -3.3060e-04,\n",
      "         2.2428e-04,  2.9704e-05,  5.2844e-06, -2.8895e-05, -4.1843e-05,\n",
      "        -4.9398e-05, -4.3747e-04, -1.9481e-05, -1.0810e-04]), 'exp_avg_sq': tensor([9.5453e-09, 2.5976e-09, 4.4853e-05, 4.6401e-08, 3.5880e-08, 6.9740e-09,\n",
      "        1.3580e-08, 1.4617e-08, 2.0058e-08, 1.4733e-08, 1.2053e-08, 1.5450e-07,\n",
      "        4.8612e-09, 3.1419e-09, 9.3266e-09, 8.1148e-09, 5.2654e-08, 2.9940e-09,\n",
      "        5.2091e-09, 8.7911e-06, 3.2444e-08, 3.2192e-08, 4.8026e-09, 8.0755e-09,\n",
      "        2.9965e-09, 4.9276e-08, 1.7330e-09, 1.5843e-08, 1.6222e-08, 3.2430e-08,\n",
      "        1.0380e-08, 4.3777e-09, 2.7083e-08, 3.5929e-09, 5.9088e-09, 4.7101e-07,\n",
      "        9.7275e-09, 6.7740e-08, 9.6467e-09, 3.8958e-09, 9.0946e-09, 2.4365e-08,\n",
      "        2.7151e-09, 2.0216e-08, 1.0992e-07, 2.7992e-08, 1.9047e-08, 1.9252e-08,\n",
      "        1.6810e-08, 1.9286e-08, 2.2367e-08, 1.0695e-08, 4.4110e-08, 2.4093e-09,\n",
      "        1.7575e-08, 2.5037e-08, 5.0922e-09, 1.0914e-07, 1.9992e-07, 1.9084e-08,\n",
      "        5.4090e-08, 6.9937e-05, 3.4037e-09, 2.1411e-07, 5.5156e-08, 9.1238e-09,\n",
      "        5.7689e-09, 2.5192e-09, 5.1806e-08, 6.3654e-09, 3.7487e-09, 3.3368e-08,\n",
      "        1.0861e-08, 1.9686e-07, 3.0793e-08, 6.4947e-08, 8.5259e-09, 1.1740e-07,\n",
      "        1.7464e-07, 8.5490e-09, 8.6869e-09, 5.3355e-09, 1.8169e-08, 1.1543e-08,\n",
      "        1.5074e-07, 1.6100e-09, 4.7393e-08, 7.7096e-08, 9.2331e-08, 1.6420e-08,\n",
      "        3.1248e-08, 1.8345e-08, 1.0240e-08, 2.0581e-08, 7.0517e-09, 9.2534e-09,\n",
      "        2.3366e-09, 3.7613e-09, 6.6377e-08, 1.7527e-06, 1.0160e-08, 3.2229e-09,\n",
      "        8.7669e-09, 1.9692e-09, 5.6562e-07, 7.6829e-09, 4.2080e-08, 3.1506e-08,\n",
      "        1.6531e-06, 2.1705e-08, 1.4916e-08, 2.0309e-07, 4.4465e-08, 2.4062e-07,\n",
      "        6.1831e-08, 1.1666e-07, 4.0454e-09, 1.0277e-07, 1.1551e-06, 9.3626e-09,\n",
      "        9.0687e-07, 3.8988e-07, 4.0498e-09, 2.4174e-08, 2.5901e-08, 1.7493e-08,\n",
      "        6.4494e-09, 5.7066e-09, 2.1060e-08, 1.1912e-08, 1.8046e-05, 4.2935e-08,\n",
      "        3.3254e-08, 3.9303e-07, 1.1223e-08, 3.3209e-07, 4.2403e-06, 9.9707e-09,\n",
      "        1.2659e-08, 1.2423e-07, 3.3395e-07, 2.3384e-09, 2.6600e-07, 1.4786e-08,\n",
      "        2.0356e-06, 8.7795e-09, 1.5846e-09, 1.2342e-05, 4.5163e-08, 1.2996e-07,\n",
      "        1.2598e-09, 1.5427e-08, 1.6139e-09, 8.4512e-08, 3.1848e-09, 3.1185e-08,\n",
      "        6.0146e-07, 2.7148e-07, 7.4631e-09, 7.0717e-07, 6.1781e-08, 5.4765e-08,\n",
      "        6.1844e-09, 2.0213e-07, 1.8011e-08, 1.4786e-06, 1.5174e-09, 4.8897e-09,\n",
      "        2.7892e-08, 7.6573e-07, 6.8646e-08, 1.3851e-07, 9.1341e-08, 4.8167e-08,\n",
      "        3.4293e-07, 1.7434e-08, 1.2116e-08, 2.0749e-08, 3.7639e-08, 6.0777e-08,\n",
      "        1.3171e-07, 1.8450e-08, 6.7874e-08, 6.5510e-09, 8.2935e-08, 1.6332e-08,\n",
      "        2.7185e-09, 7.5522e-08, 6.1191e-08, 1.4596e-04, 1.8346e-08, 4.9981e-08,\n",
      "        5.9721e-07, 4.9911e-08, 6.7948e-09, 2.3133e-09, 1.5105e-06, 2.7426e-09,\n",
      "        3.1065e-08, 9.0111e-08, 7.0668e-09, 2.3372e-05, 1.0009e-08, 2.2573e-07,\n",
      "        6.8471e-08, 3.1079e-07, 1.9973e-07, 2.5948e-09, 1.3870e-08, 1.7953e-08,\n",
      "        2.1367e-07, 2.0047e-07, 1.7507e-07, 1.6916e-08, 5.2608e-08, 2.0157e-06,\n",
      "        4.1859e-06, 4.2333e-08, 7.1652e-08, 2.6227e-08, 1.8982e-07, 1.9980e-08,\n",
      "        7.3624e-07, 5.2939e-09, 1.1434e-09, 8.4251e-09, 3.1774e-08, 4.8888e-07,\n",
      "        2.6126e-09, 5.1707e-08, 1.0597e-08, 6.7058e-09, 6.3873e-08, 1.8104e-07,\n",
      "        3.3485e-08, 1.0038e-07, 2.9014e-08, 5.1483e-09, 3.0514e-08, 6.4822e-08,\n",
      "        3.6868e-07, 1.5462e-08, 2.5832e-06, 1.1432e-07, 6.2451e-09, 7.7937e-07,\n",
      "        5.7641e-07, 1.2727e-08, 4.5005e-08, 1.5964e-06, 6.3095e-08, 2.1177e-07,\n",
      "        5.4935e-08, 1.8707e-07, 2.3062e-08, 1.8363e-08, 6.2779e-09, 8.4937e-09,\n",
      "        6.7770e-05, 5.5992e-08, 6.6201e-08, 2.7190e-06, 1.0452e-07, 9.3890e-09,\n",
      "        3.0352e-08, 4.0765e-08, 2.3162e-07, 3.3590e-07, 2.3145e-08, 1.8902e-08,\n",
      "        5.0939e-08, 1.1269e-08, 1.7581e-07, 1.7161e-08, 1.1696e-08, 3.8410e-06,\n",
      "        5.2488e-08, 1.4760e-06, 2.9062e-09, 8.1958e-09, 5.2218e-09, 1.7928e-07,\n",
      "        1.6566e-08, 4.8068e-09, 3.6857e-08, 6.4963e-08, 1.3232e-08, 9.3084e-09,\n",
      "        1.1522e-09, 1.3953e-08, 8.8433e-09, 2.0196e-08, 1.6193e-07, 3.7614e-07,\n",
      "        8.2118e-08, 7.2510e-09, 7.9166e-07, 5.7398e-08, 3.7693e-10, 5.3528e-08,\n",
      "        6.2328e-07, 6.1078e-09, 1.9707e-08, 9.0833e-09, 4.0998e-09, 1.5772e-07,\n",
      "        2.0073e-08, 1.0401e-08, 2.8297e-08, 3.7204e-09, 4.2213e-07, 2.4481e-07,\n",
      "        4.7897e-09, 3.5571e-07, 3.2017e-08, 9.7273e-10, 1.3819e-08, 3.7964e-05,\n",
      "        2.8637e-09, 7.8522e-08, 3.9157e-07, 9.4466e-08, 4.5666e-09, 3.6294e-08,\n",
      "        3.2108e-07, 3.9548e-09, 2.1096e-08, 2.5683e-07, 2.5532e-09, 2.4306e-06,\n",
      "        8.5920e-08, 5.4953e-08, 8.0424e-09, 1.0262e-07, 7.3040e-07, 1.2300e-09,\n",
      "        4.5646e-08, 9.5755e-10, 4.2335e-08, 4.0984e-08, 2.4544e-06, 1.2913e-09,\n",
      "        7.7486e-08, 2.3486e-08, 5.0399e-06, 4.9788e-08, 1.3339e-08, 2.0097e-07,\n",
      "        4.1082e-07, 1.5800e-07, 4.3488e-08, 1.5014e-07, 3.1074e-08, 3.4306e-09,\n",
      "        1.3808e-07, 1.4617e-06, 8.6606e-09, 4.9874e-09, 1.4839e-08, 3.4609e-09,\n",
      "        7.5679e-08, 2.5447e-08, 5.5106e-08, 3.6184e-07, 8.8045e-09, 4.1963e-09,\n",
      "        1.3854e-08, 7.0053e-09, 9.5620e-08, 5.2985e-07, 4.9328e-07, 8.7431e-08,\n",
      "        6.2296e-09, 8.7233e-06, 5.8496e-07, 1.4877e-08, 3.0070e-08, 1.2764e-07,\n",
      "        5.4177e-09, 1.1464e-06, 3.5310e-08, 4.8868e-07, 2.0998e-08, 6.9966e-09])}, 7: {'step': tensor(3341.), 'exp_avg': tensor([ 1.4900e-06, -6.7061e-06, -4.2387e-06,  1.6307e-06, -8.8996e-06,\n",
      "         3.2180e-06, -2.1848e-06,  3.9847e-07,  2.6839e-06,  4.5282e-05,\n",
      "        -2.5279e-06, -9.1056e-07,  4.8732e-06,  2.4982e-07,  4.7557e-06,\n",
      "        -1.2524e-05,  1.2400e-06,  2.6080e-07, -1.5365e-06,  2.0821e-06,\n",
      "         3.9226e-06,  4.7587e-06,  2.9796e-06, -4.0804e-06,  1.8465e-06,\n",
      "        -4.6584e-06, -4.6487e-06, -9.1379e-08, -2.7690e-06, -2.8667e-06,\n",
      "         2.2218e-06, -5.3934e-06,  4.7717e-06,  1.2106e-05, -3.5142e-06,\n",
      "         2.7168e-06, -5.3124e-05,  2.0091e-06, -2.3291e-05,  3.8816e-05,\n",
      "        -2.0079e-06, -4.0168e-06, -2.4123e-06,  2.6986e-06,  1.7907e-05,\n",
      "        -1.8611e-05,  8.2788e-07,  1.4195e-06, -3.7764e-06, -4.2133e-06,\n",
      "        -4.4275e-06,  1.6643e-06, -1.0296e-06,  1.4154e-05,  4.1021e-06,\n",
      "        -5.9834e-06,  5.2149e-06,  2.0905e-06, -9.6521e-08,  1.4714e-05,\n",
      "         3.3000e-06,  7.9719e-06,  1.2118e-05, -2.2766e-06,  1.3585e-06,\n",
      "        -2.9424e-06, -2.1965e-06, -4.7708e-06,  3.1861e-06, -5.5185e-06,\n",
      "         5.7960e-06, -1.8277e-07, -1.1338e-05,  5.0466e-06, -9.0148e-06,\n",
      "         1.7767e-05, -6.4113e-06,  3.9000e-06, -2.6572e-07, -5.4347e-06,\n",
      "        -5.7988e-06, -7.2851e-07, -2.0706e-06,  5.1302e-06, -4.3729e-06,\n",
      "         4.3137e-05, -4.1163e-06, -9.7199e-06, -1.0660e-05, -6.4360e-05,\n",
      "        -8.3561e-06,  3.5182e-06, -1.4075e-05, -3.4047e-05,  2.4552e-06,\n",
      "        -4.6026e-05,  7.7588e-05,  6.3643e-05,  3.6045e-05,  1.2821e-06,\n",
      "         2.5257e-05, -5.0472e-05,  4.2448e-05,  9.6302e-07, -2.0161e-06,\n",
      "        -7.7351e-06, -4.5137e-06, -3.8437e-06,  5.6584e-07,  2.2991e-05,\n",
      "         2.2189e-05, -4.0537e-06, -3.3612e-05,  2.5467e-07, -1.9381e-06,\n",
      "        -8.1577e-06,  1.3186e-05,  6.0009e-06,  1.3960e-06, -1.8838e-05,\n",
      "        -2.3958e-06, -1.5443e-06, -5.1999e-07, -6.6749e-06, -5.0044e-07,\n",
      "         6.0360e-06, -6.0644e-06, -3.0164e-05]), 'exp_avg_sq': tensor([2.0073e-11, 3.5991e-11, 2.0027e-10, 2.0831e-11, 5.8090e-11, 1.4978e-10,\n",
      "        1.5687e-11, 1.5633e-11, 1.0148e-11, 1.4505e-09, 8.6741e-12, 8.3562e-12,\n",
      "        8.1020e-11, 1.0115e-11, 2.3707e-11, 3.0857e-10, 6.8045e-12, 7.6349e-11,\n",
      "        2.1951e-11, 9.8096e-11, 3.7230e-11, 4.0500e-11, 2.0109e-11, 2.3678e-11,\n",
      "        1.1449e-10, 1.1107e-11, 9.1719e-12, 3.2348e-12, 3.1610e-11, 6.8123e-11,\n",
      "        2.9349e-11, 8.1048e-11, 2.2916e-10, 3.4803e-10, 2.3981e-11, 5.9723e-12,\n",
      "        5.2844e-09, 8.9475e-12, 3.0066e-10, 1.2305e-09, 2.6233e-11, 1.5389e-11,\n",
      "        2.4188e-11, 4.8166e-11, 2.1220e-10, 2.2458e-09, 7.9795e-12, 2.1193e-11,\n",
      "        4.9141e-11, 2.9101e-11, 2.2469e-10, 4.5099e-10, 7.4345e-12, 2.0952e-10,\n",
      "        3.7217e-11, 8.6596e-10, 7.4536e-11, 2.1881e-11, 2.6829e-11, 5.0851e-10,\n",
      "        1.2191e-10, 3.7878e-09, 1.3449e-10, 9.8331e-11, 3.1025e-11, 4.1055e-10,\n",
      "        1.3031e-10, 1.5040e-10, 2.1143e-10, 3.3672e-11, 8.3140e-10, 6.2075e-12,\n",
      "        6.6827e-10, 4.9566e-11, 1.6236e-09, 2.3153e-10, 3.8026e-11, 6.1944e-11,\n",
      "        2.8733e-11, 2.7513e-11, 2.4299e-09, 1.0101e-08, 3.7400e-11, 2.0914e-10,\n",
      "        5.2528e-11, 7.3568e-10, 5.3086e-11, 7.3198e-11, 6.3810e-11, 1.3232e-08,\n",
      "        4.8878e-11, 2.3142e-11, 2.8903e-10, 2.1989e-09, 1.5107e-10, 4.7786e-09,\n",
      "        5.4921e-09, 2.8182e-09, 8.9059e-10, 6.9689e-11, 1.5930e-09, 9.5686e-10,\n",
      "        2.0134e-08, 1.0215e-10, 1.9630e-11, 1.0264e-10, 9.1486e-11, 4.6041e-11,\n",
      "        6.2466e-12, 3.6434e-10, 1.1934e-09, 1.6914e-10, 1.9764e-09, 1.0631e-10,\n",
      "        4.2521e-11, 1.1304e-10, 6.4602e-11, 5.3776e-11, 1.7076e-09, 4.9740e-09,\n",
      "        2.6784e-11, 4.7210e-11, 2.7460e-09, 8.7660e-11, 1.6515e-10, 1.1604e-10,\n",
      "        2.8311e-10, 8.8963e-09])}, 8: {'step': tensor(3341.), 'exp_avg': tensor([-1.8742e-05,  9.5415e-07, -2.1121e-05, -1.1939e-04,  9.1652e-06,\n",
      "        -2.7654e-05, -6.0943e-06, -7.6034e-06, -5.9295e-06, -1.2454e-05,\n",
      "        -1.4106e-05, -4.2783e-06,  1.3397e-05, -2.9684e-06,  1.3671e-06,\n",
      "         3.9345e-07, -1.1172e-05, -1.0413e-05,  8.6339e-06, -7.2931e-06,\n",
      "         7.5743e-06,  2.6556e-05,  7.1374e-06, -1.3061e-05, -1.8561e-05,\n",
      "        -8.6237e-06, -8.5947e-06, -7.4862e-06,  2.6758e-05, -3.3855e-05,\n",
      "         3.3296e-06, -3.4303e-06, -1.0896e-05, -1.5395e-05, -2.2435e-05,\n",
      "        -1.4714e-05, -1.2809e-05, -3.2251e-05,  1.0240e-04, -2.6808e-06,\n",
      "        -5.5945e-06,  2.3696e-05, -2.5550e-05, -1.4797e-05,  2.4251e-05,\n",
      "        -1.7692e-05, -1.1992e-05, -1.4522e-05, -2.4123e-05, -9.8132e-06,\n",
      "         8.0779e-06, -1.4546e-05,  8.2430e-06,  6.5478e-06, -5.6494e-05,\n",
      "        -7.9441e-06,  5.8159e-06, -1.6297e-05, -5.4351e-06,  1.1704e-05,\n",
      "        -4.2698e-06,  4.7104e-05, -1.5130e-05,  7.7755e-06, -3.7738e-07,\n",
      "        -5.4904e-05,  2.2992e-06, -5.1765e-05, -3.1445e-06, -1.2444e-05,\n",
      "        -1.0444e-05,  1.4431e-05, -1.6654e-05, -4.3133e-09, -1.5808e-05,\n",
      "        -1.4929e-05, -7.2541e-06,  3.2794e-06,  8.7777e-06,  7.0470e-06,\n",
      "        -1.1966e-05,  6.6369e-06,  1.1457e-05, -9.9033e-06,  5.1916e-06,\n",
      "        -1.0533e-04,  8.9603e-06, -2.1347e-05,  9.3275e-06,  1.6603e-05,\n",
      "        -1.1272e-05,  5.3305e-06, -6.1842e-06,  1.7519e-05,  1.2426e-05,\n",
      "        -6.2066e-06, -1.8121e-05,  5.1101e-07, -1.5612e-05, -1.4784e-05,\n",
      "        -1.0310e-05, -1.8426e-06,  9.1168e-06, -9.7196e-06, -2.2694e-05,\n",
      "        -2.9804e-05,  1.8120e-06, -1.6424e-05, -5.2080e-07,  1.9430e-05,\n",
      "        -1.7376e-06, -1.6599e-05, -7.3700e-06,  1.6684e-05, -4.1082e-05,\n",
      "        -2.3963e-06,  6.5162e-06, -3.7510e-05, -9.3604e-06, -1.6733e-05,\n",
      "        -9.4207e-06, -8.6014e-06, -9.0043e-06, -4.2816e-05, -1.0835e-05,\n",
      "        -1.0810e-05, -2.7677e-05,  2.4777e-05]), 'exp_avg_sq': tensor([5.6988e-09, 1.8082e-09, 3.4623e-08, 6.8406e-08, 4.1377e-09, 5.2651e-09,\n",
      "        2.7615e-08, 9.5325e-09, 7.5141e-10, 9.7484e-09, 2.2868e-09, 2.3608e-09,\n",
      "        3.5566e-09, 3.8565e-09, 6.4552e-09, 3.3751e-09, 5.9160e-09, 4.9735e-09,\n",
      "        2.7104e-09, 2.8597e-09, 7.5130e-09, 2.6521e-08, 3.3973e-09, 9.5872e-09,\n",
      "        6.3712e-09, 3.3803e-09, 3.9341e-09, 4.0829e-09, 4.1113e-09, 6.5299e-09,\n",
      "        6.0917e-09, 1.4470e-09, 4.7240e-09, 4.2910e-09, 2.4876e-08, 1.1923e-09,\n",
      "        4.0064e-09, 4.2652e-08, 8.4028e-07, 9.1276e-10, 6.1372e-09, 9.5884e-09,\n",
      "        7.0108e-09, 1.5811e-09, 9.3411e-08, 1.9958e-09, 1.1910e-09, 4.0539e-09,\n",
      "        7.0889e-09, 2.5373e-08, 2.6210e-08, 8.8474e-09, 9.7762e-09, 7.1505e-09,\n",
      "        2.3241e-08, 5.5794e-09, 7.8513e-09, 1.1054e-08, 2.8094e-08, 2.0437e-09,\n",
      "        2.2551e-09, 1.1742e-07, 1.9864e-09, 1.4914e-09, 8.3846e-09, 7.6473e-08,\n",
      "        1.6822e-09, 3.8737e-08, 2.9463e-09, 4.3634e-09, 8.9206e-09, 1.7053e-08,\n",
      "        1.4289e-09, 9.8321e-09, 1.5432e-09, 1.1158e-08, 2.3975e-08, 1.9437e-08,\n",
      "        1.7499e-09, 2.5222e-09, 1.4132e-08, 3.5744e-09, 8.4387e-09, 2.4871e-09,\n",
      "        3.8380e-09, 5.1980e-08, 1.4969e-08, 5.2528e-09, 1.8722e-09, 5.1571e-09,\n",
      "        5.5434e-09, 3.6648e-09, 5.3987e-10, 9.3300e-08, 5.3910e-09, 6.8332e-09,\n",
      "        2.5049e-09, 2.8422e-09, 5.8303e-09, 1.3881e-08, 7.7079e-09, 6.1183e-09,\n",
      "        1.1331e-08, 1.6105e-09, 2.8992e-09, 2.3684e-08, 1.7498e-09, 2.0069e-09,\n",
      "        1.0321e-08, 1.0245e-08, 2.5206e-09, 2.0796e-08, 1.3538e-09, 2.0789e-08,\n",
      "        5.3404e-08, 5.2171e-09, 1.9270e-08, 8.5715e-09, 4.2903e-09, 1.7355e-08,\n",
      "        1.1455e-08, 1.1441e-07, 2.6116e-09, 1.4800e-08, 5.5319e-09, 2.0517e-08,\n",
      "        1.3933e-08, 6.5813e-09])}, 9: {'step': tensor(3341.), 'exp_avg': tensor([ 7.3611e-05,  3.6689e-06,  5.9711e-05, -1.8270e-05, -6.3521e-05,\n",
      "         5.4623e-07,  2.3651e-05, -3.0278e-05,  5.1027e-05,  4.2408e-05,\n",
      "         2.7907e-05,  4.1046e-05, -6.9611e-05,  7.5177e-07, -3.3091e-05,\n",
      "        -1.7134e-05,  2.2415e-05, -3.6474e-05,  6.4789e-05,  7.3694e-06,\n",
      "        -5.4837e-05,  2.2970e-05,  3.1053e-05, -5.9066e-05, -2.4230e-05,\n",
      "        -8.3960e-05,  4.4671e-05, -3.3230e-06, -4.3152e-05,  2.9012e-05,\n",
      "        -9.3338e-05,  7.9801e-05,  5.0372e-05,  9.5233e-05,  6.8797e-06,\n",
      "        -4.5787e-05, -4.8260e-05, -5.6236e-05,  5.3019e-05, -7.5631e-05,\n",
      "        -1.9144e-05,  3.1342e-05,  5.2490e-05,  9.2639e-06, -4.3919e-05,\n",
      "         2.7942e-05, -7.6172e-05,  4.8071e-05, -1.1298e-05, -6.9449e-06,\n",
      "         3.3172e-05,  3.4296e-05, -8.6813e-06, -6.7246e-05,  9.1279e-05,\n",
      "         1.3144e-05,  4.3170e-06,  5.0027e-05, -7.9993e-05, -6.5623e-05,\n",
      "        -7.4809e-05,  5.7494e-05, -3.1261e-05, -6.9411e-05, -4.9701e-05,\n",
      "        -7.1567e-05, -4.7424e-05,  5.5423e-05, -1.7189e-05,  5.3208e-05,\n",
      "         8.6168e-05, -2.9975e-05, -7.3286e-05,  2.4221e-05,  1.0806e-05,\n",
      "        -3.9780e-05, -9.4519e-05,  2.4158e-05, -1.7877e-05, -7.1527e-05,\n",
      "        -2.8946e-05,  1.8405e-05,  8.9934e-07,  4.1127e-06, -5.5753e-05,\n",
      "         6.8210e-05, -7.1729e-05, -1.2781e-05, -2.7195e-05, -5.3060e-05,\n",
      "         2.1352e-05, -7.4889e-05, -6.7613e-05,  3.2074e-05,  6.0225e-05,\n",
      "        -2.3594e-05,  6.5408e-05,  6.0104e-05, -3.4304e-05, -5.5354e-05,\n",
      "        -4.4370e-05, -2.8813e-06, -4.3703e-05, -2.7976e-05, -4.4857e-05,\n",
      "         6.5348e-05,  1.8508e-05,  2.2948e-05, -3.2798e-05,  2.0609e-05,\n",
      "        -9.9129e-05, -5.0094e-05, -2.4184e-05, -7.1067e-05, -5.9730e-05,\n",
      "         6.3438e-05, -3.3626e-05,  5.3904e-05,  1.1101e-05,  5.6584e-05,\n",
      "         6.8917e-05,  6.3646e-05,  5.8289e-05,  3.4572e-05, -1.1627e-05,\n",
      "         8.1351e-06,  6.0927e-05, -5.9075e-05]), 'exp_avg_sq': tensor([1.9641e-07, 9.1174e-08, 6.2441e-08, 9.5321e-09, 4.5503e-08, 2.8534e-08,\n",
      "        3.4995e-08, 1.2529e-07, 1.3148e-07, 1.8615e-07, 6.0022e-08, 1.7572e-08,\n",
      "        8.3690e-08, 1.0735e-07, 6.1031e-08, 2.7364e-08, 2.2899e-08, 2.5261e-08,\n",
      "        6.5539e-08, 3.3463e-08, 5.7435e-08, 4.1424e-08, 1.3402e-07, 1.2614e-07,\n",
      "        3.3620e-08, 1.9959e-07, 3.7432e-08, 2.8215e-08, 2.4171e-08, 1.2544e-08,\n",
      "        2.6065e-07, 2.7149e-07, 1.5849e-07, 2.1538e-07, 5.7631e-08, 9.1338e-08,\n",
      "        6.9041e-08, 7.3903e-08, 1.7822e-07, 2.4566e-07, 1.5488e-08, 3.1391e-08,\n",
      "        3.0220e-08, 8.4300e-08, 1.5988e-07, 3.5792e-08, 2.5461e-07, 5.6484e-08,\n",
      "        1.4215e-08, 5.1481e-08, 8.6836e-08, 3.0674e-08, 1.0738e-07, 4.2789e-08,\n",
      "        1.0495e-07, 3.6580e-08, 2.1540e-08, 1.7540e-07, 1.3257e-07, 4.2731e-08,\n",
      "        1.1643e-07, 1.2857e-07, 1.8972e-08, 1.1216e-07, 3.4102e-08, 1.8208e-07,\n",
      "        2.0184e-07, 6.3947e-08, 1.1353e-07, 1.3621e-07, 1.9223e-07, 8.7578e-08,\n",
      "        1.6703e-07, 5.2222e-08, 1.9417e-08, 1.6197e-08, 2.0455e-07, 4.1347e-08,\n",
      "        5.3164e-08, 8.6105e-08, 2.8024e-08, 1.6527e-08, 1.8157e-08, 9.4998e-08,\n",
      "        5.8796e-08, 9.1057e-08, 8.4785e-08, 3.2726e-08, 2.3254e-08, 8.3271e-08,\n",
      "        6.7354e-08, 1.3906e-07, 1.4840e-07, 1.3267e-07, 6.5175e-08, 1.6909e-07,\n",
      "        1.0821e-07, 5.7193e-08, 4.9615e-08, 1.5389e-07, 3.2853e-08, 7.8934e-08,\n",
      "        3.6705e-08, 1.4440e-07, 1.4090e-07, 8.2668e-08, 1.7042e-08, 3.4032e-08,\n",
      "        6.5545e-08, 7.3251e-08, 3.4653e-07, 8.6007e-08, 2.7005e-08, 2.2503e-07,\n",
      "        1.0085e-07, 1.2570e-07, 1.1811e-07, 2.6829e-08, 3.5393e-08, 4.2205e-08,\n",
      "        1.7926e-07, 5.7345e-08, 5.6945e-08, 1.9224e-07, 2.3374e-08, 6.9555e-08,\n",
      "        7.5590e-08, 1.4258e-07])}, 10: {'step': tensor(3341.), 'exp_avg': tensor([[ 1.0119e-05, -2.9136e-05,  3.1452e-05,  ..., -1.6483e-05,\n",
      "          1.6238e-06,  2.8287e-05],\n",
      "        [-4.5886e-05,  4.2431e-05,  2.2942e-05,  ...,  1.3560e-05,\n",
      "          1.2684e-05,  1.0356e-05],\n",
      "        [ 4.3372e-05,  5.7620e-06,  7.4533e-05,  ..., -9.5388e-06,\n",
      "          2.1361e-05, -7.1469e-05],\n",
      "        ...,\n",
      "        [ 3.5561e-05, -1.2105e-05, -5.1153e-05,  ...,  1.7053e-05,\n",
      "         -1.1238e-04,  2.2702e-05],\n",
      "        [-2.7198e-06,  1.9647e-05, -1.2439e-05,  ...,  1.0191e-05,\n",
      "          1.4762e-05, -1.8332e-05],\n",
      "        [-4.0439e-05, -6.3857e-05, -3.4868e-05,  ...,  1.7132e-05,\n",
      "          9.3936e-06,  1.9819e-05]]), 'exp_avg_sq': tensor([[2.4884e-09, 2.3707e-09, 1.8079e-09,  ..., 4.3027e-09, 1.2715e-09,\n",
      "         4.1426e-09],\n",
      "        [3.1555e-09, 3.2628e-09, 2.5893e-09,  ..., 1.9723e-09, 2.2857e-09,\n",
      "         2.6433e-09],\n",
      "        [2.4485e-09, 2.7753e-09, 5.9694e-09,  ..., 2.3684e-09, 3.5102e-09,\n",
      "         5.7408e-09],\n",
      "        ...,\n",
      "        [4.0873e-09, 3.9127e-09, 2.8726e-09,  ..., 4.6510e-09, 3.4046e-09,\n",
      "         3.5591e-09],\n",
      "        [6.8434e-09, 2.6785e-09, 1.4571e-09,  ..., 3.8999e-09, 1.6044e-09,\n",
      "         2.1166e-09],\n",
      "        [4.1749e-09, 2.5753e-09, 2.5124e-09,  ..., 4.0902e-09, 1.2258e-09,\n",
      "         2.5134e-09]])}, 11: {'step': tensor(3341.), 'exp_avg': tensor([-2.5106e-05,  2.9054e-05, -1.0717e-04, -2.1067e-05,  6.9410e-05,\n",
      "        -3.8653e-05, -1.1585e-05, -4.8420e-05,  5.2717e-05,  3.8516e-05,\n",
      "         2.5818e-06,  2.5932e-05,  4.7852e-06, -6.4818e-06, -3.4485e-05,\n",
      "        -2.9337e-05,  9.9590e-05, -5.3984e-05,  1.5858e-06, -4.0891e-06,\n",
      "        -3.8535e-05, -1.0322e-06, -9.6368e-06,  3.2328e-06,  2.1046e-05,\n",
      "         2.6215e-05,  4.8802e-05,  1.1511e-05,  1.5002e-05,  5.5698e-05,\n",
      "         3.0257e-05, -7.2848e-06,  6.3155e-06, -1.8021e-05,  2.6485e-05,\n",
      "         1.8948e-06,  1.2091e-05,  1.8968e-05,  1.4993e-05, -4.2648e-05,\n",
      "        -3.8170e-05,  5.1989e-05, -4.0235e-05, -9.9164e-05, -2.1117e-05,\n",
      "         9.8219e-06,  1.1179e-05,  4.0132e-05, -9.0875e-05,  4.9070e-05,\n",
      "        -3.6541e-05,  2.2194e-05, -3.9365e-05,  5.0058e-06, -8.3706e-05,\n",
      "         1.8836e-05,  3.3120e-05, -4.8287e-05, -9.3734e-06, -2.1787e-05,\n",
      "        -1.3606e-06, -5.1774e-05,  3.2588e-05,  1.7722e-05, -6.7246e-06,\n",
      "        -3.0472e-06, -8.4411e-07, -2.9632e-05,  5.7230e-06, -1.6501e-05,\n",
      "         2.1788e-05,  6.5906e-06, -2.9029e-05,  6.7590e-05,  4.9709e-05,\n",
      "         3.7659e-06, -3.4272e-05, -2.9855e-05,  2.2280e-05, -3.5993e-05,\n",
      "         2.0463e-05, -2.1048e-05, -1.2091e-04, -4.8066e-05, -6.5744e-08,\n",
      "         1.3351e-05,  2.8173e-05, -4.3363e-05, -1.6222e-05, -1.0161e-05,\n",
      "        -6.1401e-07, -4.8395e-05, -2.2596e-05,  2.4547e-05, -1.3356e-04,\n",
      "        -8.0401e-06,  4.1208e-05, -3.9455e-05,  1.2482e-05,  1.6275e-05,\n",
      "        -5.5967e-05, -1.0401e-05, -2.3179e-05,  1.2277e-05, -1.1215e-05,\n",
      "         1.5223e-05, -5.7723e-05, -3.7421e-07, -4.5666e-06,  1.7131e-05,\n",
      "         2.3951e-06, -2.4776e-05, -2.1731e-05,  2.0770e-05, -1.7908e-05,\n",
      "        -2.9868e-05,  6.9743e-05, -1.9447e-05, -2.7267e-05,  2.2015e-05,\n",
      "        -3.9930e-06,  4.0127e-05,  4.9736e-05,  7.7597e-05, -1.3297e-05,\n",
      "        -2.0224e-05, -6.5755e-05, -3.8922e-05, -1.8412e-06,  1.6193e-05,\n",
      "         1.2415e-05, -3.0408e-05, -2.8198e-05,  5.2178e-05,  3.2020e-05,\n",
      "        -6.1266e-06, -1.1242e-05, -6.7650e-05, -1.7149e-05, -5.0551e-05,\n",
      "        -4.9387e-05, -4.1547e-05,  2.4797e-05, -2.5602e-05, -2.7580e-05,\n",
      "         1.3521e-05,  2.3075e-05,  1.4471e-06, -5.6814e-05,  2.1860e-05,\n",
      "         6.6543e-06, -2.7202e-05, -6.7305e-05, -6.5250e-05,  3.4598e-05,\n",
      "        -6.4930e-05, -6.9685e-07, -7.8502e-06,  6.8481e-05, -1.1976e-04,\n",
      "         1.6060e-05,  5.2465e-05, -3.5172e-05,  1.3459e-04,  3.4913e-05,\n",
      "        -1.7306e-05, -3.7948e-06, -8.6763e-05,  3.0212e-05, -5.3267e-05,\n",
      "         4.5444e-05,  7.4152e-05, -5.4371e-05,  1.0073e-05, -1.9257e-05,\n",
      "         2.8152e-05,  1.2412e-05,  1.5656e-05, -1.6163e-05, -3.8910e-05,\n",
      "        -8.6204e-05,  3.4650e-05, -1.3204e-06, -5.1584e-06, -4.5334e-05,\n",
      "         6.3940e-05,  6.9323e-05,  1.0133e-04, -2.3086e-05, -4.4789e-05,\n",
      "         4.8137e-05,  1.0335e-05,  4.3758e-05,  8.3875e-05, -1.1583e-05,\n",
      "        -3.4769e-05,  4.5766e-06, -1.5344e-05, -4.7461e-05, -7.1294e-05,\n",
      "        -9.4926e-05, -2.0995e-05, -4.7117e-05, -6.3394e-05,  2.1681e-05,\n",
      "         7.3578e-05,  1.6214e-05, -6.9961e-05, -4.2026e-06, -8.2137e-05,\n",
      "        -5.8565e-05,  3.7304e-05, -5.6400e-05, -5.0140e-05, -2.8500e-05,\n",
      "        -3.1509e-06,  3.8963e-06, -4.8851e-06,  1.0933e-04, -6.3155e-05,\n",
      "        -1.1169e-04,  2.7861e-05, -1.8558e-05, -3.4241e-05, -5.4795e-06,\n",
      "        -2.7645e-05, -1.0206e-05,  4.4055e-05,  5.0348e-06,  1.3223e-05,\n",
      "        -7.1364e-05, -2.2293e-05, -6.6335e-06, -2.2613e-05, -4.6190e-05,\n",
      "        -3.1784e-05, -5.1321e-05, -8.4360e-05, -2.2701e-06, -1.1944e-04,\n",
      "        -6.5345e-05, -5.1844e-05, -4.9788e-05, -6.3007e-06, -1.7961e-05,\n",
      "        -4.2605e-05, -3.1407e-05,  1.5080e-05,  1.3845e-05,  1.1932e-05,\n",
      "        -1.5923e-05,  1.4507e-05, -1.8774e-05, -4.3393e-05, -9.1783e-05,\n",
      "        -3.3886e-05, -2.2790e-05, -4.2666e-05, -1.7813e-06, -8.4937e-05,\n",
      "         1.1064e-06,  2.8628e-05,  1.2352e-05, -2.5087e-05, -3.1906e-05,\n",
      "         5.4144e-05,  5.8614e-05, -3.3635e-05,  4.0424e-05,  5.0836e-05,\n",
      "        -5.3497e-05, -1.7266e-05,  7.1043e-05, -6.6860e-05, -7.3392e-06,\n",
      "        -1.9087e-05, -1.4812e-05,  4.2764e-06,  6.6592e-05, -3.1544e-05,\n",
      "        -3.7394e-06,  2.0863e-05,  2.1508e-05, -2.6915e-05, -2.2836e-05,\n",
      "         1.0341e-05,  8.0316e-05, -5.7737e-05,  2.6076e-06,  2.4788e-05,\n",
      "        -3.5698e-05, -4.9478e-05,  4.5777e-05,  4.4152e-05,  3.5771e-05,\n",
      "         2.1040e-05,  3.2507e-05,  6.9265e-05,  6.7927e-05, -1.9951e-05,\n",
      "         5.8280e-05,  2.4905e-05,  3.3541e-05,  1.1099e-04,  7.9711e-07,\n",
      "         4.4116e-05, -4.3124e-05,  3.0212e-05, -5.1256e-05,  2.1202e-05,\n",
      "         4.0637e-05, -4.3590e-05, -4.1746e-05, -4.2830e-06,  2.7312e-05,\n",
      "         5.3489e-05,  6.4890e-05, -3.1235e-05,  1.7769e-05,  4.5695e-05,\n",
      "        -4.0689e-05,  4.5046e-05, -1.4552e-05,  8.5776e-05, -2.8610e-05,\n",
      "        -3.1923e-05, -3.8637e-06, -1.5846e-06, -3.7282e-07,  3.2883e-06,\n",
      "        -2.9111e-06, -5.4471e-05, -5.5215e-05,  1.0696e-04, -5.1474e-05,\n",
      "        -6.0106e-05,  2.5918e-05, -1.0499e-04, -9.7435e-06,  2.9967e-05,\n",
      "        -5.4507e-05,  8.6924e-06,  1.9708e-06,  4.9194e-06, -1.3274e-05,\n",
      "        -1.0083e-04,  6.2586e-05, -7.8083e-06,  4.0711e-05,  5.2041e-05,\n",
      "         8.6167e-06,  3.9870e-05, -5.4746e-06, -3.9778e-05,  1.2110e-05,\n",
      "         9.6722e-05,  4.7845e-05,  8.2992e-06,  1.6103e-05, -4.6562e-05,\n",
      "        -8.1734e-06, -4.5932e-05, -3.9261e-05, -4.9969e-05,  1.3625e-05,\n",
      "         2.5211e-06, -4.9780e-05,  5.8848e-06,  6.6218e-06, -1.2928e-05,\n",
      "         5.5716e-06, -2.7345e-05, -3.9118e-05,  5.6669e-07,  4.2109e-05,\n",
      "         2.2428e-05,  4.6101e-05, -5.6593e-05,  1.7310e-05, -5.0956e-05,\n",
      "        -3.9934e-05, -6.4965e-05, -5.7100e-05, -1.5750e-05, -4.2008e-05,\n",
      "        -6.7959e-06, -5.1242e-05, -1.0159e-04, -2.1674e-05,  1.2696e-05,\n",
      "         8.4953e-05,  2.0485e-06,  3.2577e-05, -4.8107e-05,  4.5936e-05,\n",
      "        -5.4251e-05,  3.7120e-05,  3.4308e-05,  2.9965e-05,  8.1365e-05,\n",
      "         2.8653e-05, -1.0867e-05,  2.5592e-05,  1.6675e-05,  5.3945e-05,\n",
      "        -1.9040e-05, -5.0457e-05, -1.5950e-05, -3.7279e-05,  4.9061e-05,\n",
      "         1.4352e-05,  4.7013e-05, -5.2280e-05, -4.2081e-05, -1.0770e-05,\n",
      "         1.6667e-05,  8.5159e-07,  5.2474e-05, -5.2975e-06, -7.6459e-06,\n",
      "         5.5043e-05, -4.5027e-06, -4.1084e-05, -7.7343e-05,  3.7434e-05,\n",
      "        -8.9074e-06,  7.0210e-05,  5.0394e-05, -5.7219e-05,  4.8900e-05,\n",
      "        -1.1152e-05, -3.1273e-05, -1.1502e-04,  7.3351e-05, -7.4535e-05,\n",
      "        -2.4851e-05, -4.2244e-05, -1.5219e-05, -1.5274e-05, -4.8118e-05,\n",
      "         7.9716e-05, -1.3027e-05,  3.9901e-05, -2.8483e-07, -4.6454e-06,\n",
      "        -6.7992e-06,  4.1776e-05, -4.5467e-05, -1.0048e-04,  3.6548e-05,\n",
      "        -5.8637e-05,  1.8999e-05,  4.4585e-05,  5.6142e-05, -1.5066e-05,\n",
      "        -2.1955e-05,  2.7247e-05, -3.2557e-05, -5.3551e-05, -1.8884e-05,\n",
      "         1.9632e-05, -2.7419e-05, -1.1207e-04,  5.1254e-05, -3.9214e-05,\n",
      "         8.6901e-06, -3.1691e-05,  1.1983e-05,  2.9236e-05,  2.2319e-05,\n",
      "         1.2997e-05,  3.9423e-05,  5.3573e-05, -6.1722e-05, -1.2178e-05,\n",
      "         1.7380e-06, -3.9418e-05,  5.8733e-05, -7.6728e-05, -6.4817e-05,\n",
      "        -3.0791e-05,  5.6665e-05, -1.9328e-06, -2.1360e-06, -6.1084e-05,\n",
      "        -1.1310e-05, -3.6314e-05, -3.0920e-05,  4.9049e-05,  9.4834e-06,\n",
      "         8.9299e-06,  1.0947e-05, -6.3804e-05, -5.4423e-05, -2.0766e-05,\n",
      "        -2.1169e-05, -2.9989e-05,  2.1969e-05, -7.2610e-05,  2.2192e-05,\n",
      "         3.5817e-05,  1.1945e-06,  1.1455e-05, -2.3750e-05,  2.5058e-05,\n",
      "        -1.9221e-05, -4.0590e-06,  3.1469e-05, -3.4743e-05, -2.2650e-05,\n",
      "        -3.6381e-05,  2.1837e-05]), 'exp_avg_sq': tensor([9.2352e-09, 6.7030e-09, 2.4723e-08, 1.1101e-08, 7.2224e-09, 2.9849e-08,\n",
      "        3.9548e-09, 8.5664e-09, 5.3121e-09, 7.1845e-09, 5.6713e-09, 5.5409e-09,\n",
      "        2.3028e-08, 6.7871e-09, 1.2865e-08, 2.3969e-08, 8.3055e-09, 8.9914e-09,\n",
      "        7.2912e-09, 1.8114e-08, 8.0233e-09, 9.0392e-09, 1.6616e-08, 1.3832e-08,\n",
      "        3.5293e-08, 9.1130e-09, 6.4770e-09, 5.1999e-09, 1.3160e-08, 7.3035e-09,\n",
      "        7.0383e-09, 1.2945e-08, 1.2970e-08, 8.1429e-09, 5.9860e-09, 4.1622e-09,\n",
      "        7.1702e-09, 1.5941e-08, 5.6768e-09, 8.7802e-09, 6.9571e-09, 1.1533e-08,\n",
      "        1.0782e-08, 7.9823e-09, 5.6244e-09, 9.5934e-09, 6.4686e-09, 3.4013e-08,\n",
      "        1.0599e-08, 4.2766e-09, 2.5518e-09, 7.8443e-09, 7.5802e-09, 5.4972e-09,\n",
      "        1.0900e-08, 3.9085e-08, 1.4549e-08, 2.7685e-09, 2.9207e-08, 1.1030e-08,\n",
      "        1.2672e-08, 9.0636e-09, 5.7160e-09, 4.7309e-09, 5.1522e-09, 9.4533e-09,\n",
      "        3.4302e-09, 8.6163e-09, 1.1072e-08, 2.1333e-08, 2.7550e-08, 3.8533e-08,\n",
      "        1.3190e-08, 8.3041e-09, 1.1700e-08, 1.3411e-08, 7.9309e-09, 5.4436e-09,\n",
      "        7.7771e-09, 1.0911e-08, 6.7890e-09, 9.7048e-09, 1.1825e-08, 9.3326e-09,\n",
      "        9.2761e-09, 9.9917e-09, 8.0570e-09, 9.7580e-09, 2.9791e-08, 1.2127e-08,\n",
      "        6.7241e-09, 1.5121e-08, 1.1792e-08, 9.0037e-09, 8.4844e-09, 4.6255e-09,\n",
      "        1.4348e-08, 8.6274e-09, 1.2996e-08, 8.7003e-09, 9.4010e-09, 9.5868e-09,\n",
      "        3.9267e-08, 9.2315e-09, 5.6062e-09, 9.9981e-09, 5.8341e-09, 6.7286e-09,\n",
      "        5.4672e-09, 5.0358e-09, 2.1769e-08, 1.1365e-08, 1.1750e-08, 1.3293e-08,\n",
      "        4.7062e-09, 1.1805e-08, 8.1496e-09, 8.1481e-09, 6.4449e-09, 1.7817e-08,\n",
      "        8.8578e-09, 1.0281e-08, 5.0921e-09, 7.0026e-09, 1.6831e-08, 1.1734e-08,\n",
      "        1.6316e-08, 1.5570e-08, 5.0316e-09, 5.6007e-09, 5.5050e-09, 6.3310e-09,\n",
      "        1.2818e-08, 6.8259e-09, 6.5863e-09, 9.0339e-09, 8.5380e-09, 7.4559e-09,\n",
      "        1.3397e-08, 6.0948e-09, 6.2174e-09, 1.6636e-08, 2.8614e-09, 9.6172e-09,\n",
      "        1.1418e-08, 1.1447e-08, 8.8399e-09, 1.2526e-08, 8.4392e-09, 2.9826e-09,\n",
      "        1.1039e-08, 4.2526e-09, 9.4001e-09, 7.0746e-09, 3.4852e-08, 5.7837e-09,\n",
      "        6.9124e-09, 4.0270e-09, 5.1097e-09, 1.4663e-08, 7.4413e-09, 9.3205e-09,\n",
      "        1.8090e-08, 1.0560e-08, 2.8462e-08, 1.4314e-08, 9.2152e-09, 7.7769e-09,\n",
      "        2.3981e-09, 9.1783e-09, 9.3168e-09, 5.1373e-09, 1.2185e-08, 9.5840e-09,\n",
      "        1.0823e-08, 2.3312e-08, 2.5876e-08, 6.5082e-09, 6.2060e-09, 6.3616e-09,\n",
      "        9.0009e-09, 8.7277e-09, 2.2348e-08, 1.5449e-08, 1.9761e-08, 9.6947e-09,\n",
      "        1.1509e-08, 1.4306e-08, 9.8040e-09, 3.8660e-09, 7.5634e-09, 3.3234e-09,\n",
      "        1.3938e-08, 8.8574e-08, 4.5396e-08, 5.0562e-09, 6.1915e-09, 7.1379e-09,\n",
      "        1.0224e-08, 1.0011e-08, 6.4340e-09, 8.2680e-09, 1.4999e-08, 8.6619e-09,\n",
      "        1.7445e-08, 1.1437e-08, 4.9493e-09, 7.7179e-09, 1.3546e-08, 2.3878e-08,\n",
      "        1.6675e-08, 1.7810e-08, 2.1496e-08, 6.8752e-09, 6.5144e-09, 2.9376e-08,\n",
      "        1.4033e-08, 5.4836e-09, 3.4167e-08, 9.9045e-09, 9.8690e-09, 7.3840e-09,\n",
      "        2.8708e-09, 2.6050e-08, 2.0653e-08, 1.1410e-08, 2.0673e-08, 1.4603e-08,\n",
      "        8.4563e-09, 8.4603e-09, 1.0052e-08, 1.0106e-08, 9.9798e-09, 1.7566e-08,\n",
      "        6.6196e-09, 8.7086e-09, 1.8395e-08, 1.0936e-08, 2.7233e-09, 2.8523e-08,\n",
      "        2.9885e-08, 1.3066e-08, 1.7604e-08, 2.6332e-08, 8.5297e-09, 1.1508e-08,\n",
      "        6.3933e-09, 1.0800e-08, 8.5260e-09, 1.1338e-08, 5.7296e-09, 1.9953e-08,\n",
      "        7.5370e-09, 5.4531e-09, 1.7169e-08, 2.0602e-08, 6.6110e-09, 1.7723e-08,\n",
      "        8.1885e-09, 1.0797e-08, 1.0268e-08, 3.3137e-08, 3.3509e-09, 1.2380e-08,\n",
      "        5.9981e-09, 6.6324e-09, 5.5301e-09, 4.4060e-09, 1.1903e-08, 1.0029e-08,\n",
      "        8.5779e-09, 1.4323e-08, 2.9890e-08, 8.1649e-09, 5.1731e-09, 1.1337e-08,\n",
      "        1.1021e-08, 1.2789e-08, 7.5462e-09, 5.3232e-09, 4.4309e-09, 9.6003e-09,\n",
      "        5.9482e-09, 2.1229e-08, 6.7766e-09, 1.9035e-08, 1.0152e-08, 7.7473e-09,\n",
      "        5.6103e-09, 3.9525e-09, 4.8695e-09, 2.0676e-08, 2.2756e-08, 4.5682e-09,\n",
      "        8.7211e-09, 9.3948e-09, 1.1713e-08, 2.0309e-08, 5.6518e-09, 8.5524e-09,\n",
      "        5.6363e-09, 7.0842e-09, 9.2536e-09, 1.1944e-08, 1.1368e-08, 1.4831e-08,\n",
      "        1.4791e-08, 6.0643e-09, 1.6373e-08, 2.6454e-08, 1.1859e-08, 8.0988e-09,\n",
      "        1.8303e-08, 1.1455e-08, 2.2024e-08, 3.5275e-09, 2.7995e-08, 1.0163e-08,\n",
      "        1.6449e-08, 1.3377e-08, 1.1323e-08, 2.3485e-08, 1.0470e-08, 1.6724e-08,\n",
      "        2.4493e-08, 7.5373e-09, 2.8366e-08, 5.4994e-09, 1.5770e-08, 1.9497e-08,\n",
      "        5.3753e-09, 9.4751e-09, 8.6854e-09, 1.6300e-08, 4.8659e-09, 1.1619e-08,\n",
      "        6.0973e-08, 6.8607e-09, 4.6166e-09, 6.9049e-09, 1.6166e-08, 7.9547e-09,\n",
      "        9.8079e-09, 1.2547e-08, 1.1613e-08, 2.2459e-08, 9.9213e-09, 6.5223e-09,\n",
      "        5.0932e-08, 5.4270e-08, 7.7978e-09, 4.3328e-09, 8.5952e-09, 1.3316e-08,\n",
      "        9.5174e-09, 1.3134e-08, 4.0386e-09, 1.0860e-08, 2.0578e-08, 1.4908e-08,\n",
      "        4.8015e-09, 6.7772e-09, 1.1060e-08, 6.5172e-09, 1.0852e-08, 4.3466e-09,\n",
      "        1.2061e-08, 1.6394e-08, 1.2046e-08, 1.3091e-08, 7.8843e-09, 5.0816e-09,\n",
      "        5.9589e-09, 8.4294e-09, 3.2465e-08, 1.8422e-08, 2.3818e-08, 7.6335e-09,\n",
      "        5.7557e-09, 3.3179e-08, 8.5722e-09, 7.9492e-09, 1.0056e-08, 5.0122e-09,\n",
      "        1.3729e-08, 8.2206e-09, 6.2039e-09, 9.3716e-09, 1.0160e-08, 2.1532e-08,\n",
      "        2.4830e-08, 2.9457e-08, 7.3799e-09, 1.7754e-08, 1.7958e-08, 4.7848e-09,\n",
      "        9.2598e-09, 8.9406e-09, 5.8966e-09, 1.3520e-08, 5.9339e-09, 1.3260e-08,\n",
      "        1.2386e-08, 1.1511e-08, 1.5105e-08, 1.3551e-08, 1.5351e-08, 7.7331e-09,\n",
      "        1.9246e-08, 3.8009e-08, 1.2986e-08, 6.5143e-09, 2.6133e-08, 2.0090e-08,\n",
      "        3.6309e-09, 6.4109e-09, 1.1077e-08, 6.9948e-09, 1.0617e-08, 5.3690e-09,\n",
      "        2.1069e-08, 7.0956e-09, 6.0129e-09, 7.0508e-09, 6.1347e-09, 7.3398e-09,\n",
      "        1.0897e-08, 9.4331e-09, 6.7126e-09, 7.2318e-09, 7.9410e-09, 8.2979e-09,\n",
      "        1.7167e-08, 6.2906e-09, 2.2213e-08, 7.5322e-09, 8.4733e-09, 6.9141e-09,\n",
      "        8.3358e-09, 1.1326e-08, 1.4668e-08, 3.9763e-09, 4.2152e-09, 2.1896e-08,\n",
      "        1.0237e-08, 8.8474e-09, 1.7771e-08, 1.8164e-08, 8.1556e-09, 5.0188e-09,\n",
      "        6.4707e-09, 6.4396e-09, 6.2989e-09, 6.9009e-09, 7.4637e-09, 4.9646e-09,\n",
      "        6.1720e-09, 3.8883e-09, 1.2264e-08, 4.6099e-09, 4.2382e-09, 1.7123e-08,\n",
      "        1.0183e-08, 1.7091e-08, 1.3674e-08, 6.7096e-09, 1.0222e-08, 1.7934e-08,\n",
      "        1.4046e-08, 8.8775e-09, 2.3721e-08, 6.7254e-09, 1.7969e-08, 2.2839e-08,\n",
      "        9.9956e-09, 5.3194e-09, 9.2785e-09, 5.8616e-09, 1.0626e-08, 1.8437e-08,\n",
      "        7.9257e-09, 1.6851e-08, 5.2867e-09, 4.9973e-09, 1.1092e-08, 1.2600e-08,\n",
      "        2.3416e-08, 1.6243e-08, 9.8130e-09, 1.0284e-08, 2.8975e-08, 4.0658e-09,\n",
      "        5.2628e-09, 6.0514e-09, 2.4031e-09, 1.2444e-08, 9.9787e-09, 4.9399e-09,\n",
      "        1.2977e-08, 1.6861e-08, 1.5831e-08, 8.2531e-09, 4.8482e-09, 3.4755e-09,\n",
      "        1.5303e-08, 1.2037e-08, 6.4788e-09, 1.0664e-08, 7.3450e-09, 1.1671e-08,\n",
      "        9.9252e-09, 8.0685e-09])}, 12: {'step': tensor(3341.), 'exp_avg': tensor([[-1.9111e-05, -1.3547e-05,  1.2467e-06,  ..., -9.6189e-06,\n",
      "         -8.7701e-05,  2.7140e-05],\n",
      "        [-3.8139e-05,  1.3562e-05,  3.8035e-05,  ...,  6.8173e-06,\n",
      "          3.8000e-06, -2.0527e-05],\n",
      "        [ 3.3182e-05, -5.9231e-05,  6.2174e-05,  ..., -1.3686e-05,\n",
      "          1.8503e-05, -2.6923e-05],\n",
      "        ...,\n",
      "        [-2.8078e-07, -3.7968e-06, -5.7683e-05,  ...,  1.1101e-05,\n",
      "         -7.0121e-05, -6.5363e-05],\n",
      "        [ 1.8776e-05, -2.8361e-05,  4.1678e-05,  ...,  3.1428e-05,\n",
      "         -2.2651e-05, -4.4535e-05],\n",
      "        [ 4.6861e-05, -5.1968e-05, -6.7647e-05,  ..., -2.7444e-05,\n",
      "         -3.9791e-05, -3.5527e-05]]), 'exp_avg_sq': tensor([[1.6226e-08, 1.2586e-08, 5.4591e-09,  ..., 5.5884e-09, 6.5684e-09,\n",
      "         2.2536e-08],\n",
      "        [8.5344e-08, 4.3617e-08, 6.7577e-09,  ..., 8.5407e-09, 5.9559e-09,\n",
      "         1.0307e-07],\n",
      "        [7.6878e-09, 3.7856e-09, 3.8794e-09,  ..., 3.0055e-09, 3.7161e-09,\n",
      "         5.7922e-09],\n",
      "        ...,\n",
      "        [2.7050e-08, 1.4638e-08, 1.7784e-09,  ..., 2.9983e-09, 2.7402e-09,\n",
      "         3.4404e-08],\n",
      "        [4.9277e-09, 3.7453e-09, 4.0436e-09,  ..., 2.8113e-09, 3.2201e-09,\n",
      "         4.6078e-09],\n",
      "        [3.8287e-08, 2.1363e-08, 1.4136e-08,  ..., 1.2074e-08, 1.4119e-08,\n",
      "         3.9071e-08]])}, 13: {'step': tensor(3341.), 'exp_avg': tensor([ 8.8883e-04,  5.0948e-04, -8.6672e-05,  4.4593e-04, -7.7296e-04,\n",
      "        -2.9894e-04,  8.7919e-05,  3.6433e-04,  1.0557e-04, -4.7504e-04,\n",
      "         2.7230e-04,  3.0583e-04,  8.6003e-05,  5.3694e-04,  2.3215e-04,\n",
      "        -3.7940e-04,  2.0943e-05, -3.9591e-04,  2.5928e-04, -2.4070e-04,\n",
      "         4.6994e-04, -1.3031e-05, -1.2422e-04, -1.1053e-04, -9.0107e-04,\n",
      "         7.2579e-05,  1.6153e-04, -3.3228e-04, -1.8451e-04, -2.2083e-04,\n",
      "        -1.4958e-05,  1.2031e-04, -1.2962e-04, -4.5407e-05, -8.6106e-04,\n",
      "         6.2526e-04,  4.4719e-04, -7.0556e-04, -5.9377e-05,  5.3914e-04,\n",
      "         2.0898e-04,  6.5736e-04,  2.5457e-04, -3.7865e-04,  2.1569e-04,\n",
      "         4.3197e-04, -3.4153e-04,  3.9076e-05, -1.3064e-04, -1.3902e-03,\n",
      "        -5.4719e-05,  5.2771e-05,  3.0903e-04,  1.8531e-04, -5.4180e-04,\n",
      "        -4.2445e-04,  1.8717e-04,  2.7818e-04,  8.4230e-06,  6.6048e-05,\n",
      "        -3.7892e-04,  5.3008e-04,  5.4563e-04,  4.4223e-05,  1.6454e-04,\n",
      "        -1.7249e-05, -1.5499e-04,  2.4300e-04, -7.9545e-05,  2.9712e-04,\n",
      "         1.7794e-05,  8.0099e-04,  4.1002e-05, -1.7609e-04, -8.5912e-04,\n",
      "        -7.7900e-05, -7.0187e-04,  3.5678e-04,  3.3347e-04,  6.0128e-04,\n",
      "        -1.7128e-04,  1.4271e-04,  4.5163e-04, -3.6164e-04, -8.2718e-04,\n",
      "        -4.3295e-04, -5.8003e-05, -2.6517e-04, -4.9714e-04, -1.6359e-04,\n",
      "         7.3911e-04, -4.5979e-04, -5.8052e-04,  3.7367e-04,  5.4914e-04,\n",
      "         1.9211e-04,  4.6415e-04, -4.5816e-05,  9.9778e-04,  7.4734e-04,\n",
      "        -4.5155e-04, -7.0727e-04, -1.7228e-04,  4.0605e-05,  2.5946e-04,\n",
      "        -2.3203e-04,  6.6449e-04, -7.8387e-05,  1.0056e-03, -8.7301e-04,\n",
      "        -3.4311e-04,  6.4476e-05, -1.2469e-04,  3.2232e-04,  2.0226e-04,\n",
      "         1.1469e-03,  3.5715e-04, -4.1501e-04, -1.8455e-05, -6.6522e-04,\n",
      "         5.3878e-05, -2.1521e-04,  7.2825e-05, -4.3268e-04, -6.6056e-04,\n",
      "         2.0001e-04, -2.2996e-04, -9.3072e-04]), 'exp_avg_sq': tensor([1.9328e-06, 3.0268e-06, 1.3611e-06, 1.0498e-06, 5.9081e-07, 1.1771e-06,\n",
      "        8.0900e-07, 6.3951e-07, 1.0214e-06, 7.4146e-07, 1.8427e-06, 8.3303e-07,\n",
      "        7.0319e-07, 1.6823e-06, 5.1294e-07, 1.4926e-06, 5.1770e-07, 7.7113e-07,\n",
      "        1.1080e-06, 1.4143e-06, 1.4177e-06, 1.2550e-06, 5.1097e-07, 1.6293e-06,\n",
      "        9.7321e-07, 4.6678e-07, 1.2488e-06, 2.2978e-06, 3.4576e-07, 1.2749e-06,\n",
      "        3.7070e-07, 1.4317e-06, 5.9117e-07, 8.7938e-07, 2.1377e-06, 6.8492e-07,\n",
      "        1.6144e-06, 2.6311e-06, 5.5482e-06, 2.5413e-06, 3.4900e-07, 2.5015e-06,\n",
      "        1.8251e-06, 6.7891e-07, 4.8033e-07, 3.5344e-07, 1.7998e-06, 5.3472e-07,\n",
      "        1.0221e-06, 3.3086e-06, 6.1832e-07, 2.1581e-06, 1.1713e-06, 2.9390e-06,\n",
      "        1.3724e-06, 7.5660e-07, 5.2071e-07, 1.4909e-06, 2.8748e-06, 9.5746e-07,\n",
      "        3.4678e-07, 1.0868e-06, 2.6568e-06, 8.3525e-07, 5.1639e-07, 9.0697e-07,\n",
      "        1.1672e-06, 9.5511e-07, 1.4278e-06, 9.9920e-07, 4.9105e-07, 2.6954e-06,\n",
      "        4.9947e-07, 7.6432e-07, 3.8497e-06, 8.6103e-07, 1.4651e-06, 8.9758e-07,\n",
      "        1.2471e-06, 4.8545e-06, 9.7090e-07, 4.0361e-06, 9.0645e-07, 1.2744e-06,\n",
      "        2.6072e-06, 3.5003e-06, 1.8254e-06, 1.3480e-06, 8.3462e-07, 9.8587e-07,\n",
      "        3.8669e-06, 7.0688e-07, 2.6847e-06, 1.9966e-06, 1.3921e-06, 5.0456e-07,\n",
      "        2.7431e-06, 7.1212e-07, 2.5596e-06, 4.2491e-06, 2.7514e-06, 9.1936e-07,\n",
      "        3.9028e-06, 1.2411e-06, 1.3058e-06, 2.0627e-06, 1.3911e-06, 4.9377e-07,\n",
      "        1.1938e-06, 2.3967e-06, 1.1601e-06, 4.8018e-07, 1.3434e-06, 1.0200e-06,\n",
      "        1.2409e-06, 5.1521e-06, 5.2927e-07, 8.9476e-07, 7.2195e-07, 1.3271e-06,\n",
      "        8.2557e-07, 7.8894e-07, 7.1055e-07, 3.1303e-06, 2.4956e-06, 2.4610e-07,\n",
      "        3.1978e-07, 7.7461e-06])}, 14: {'step': tensor(3341.), 'exp_avg': tensor([-5.2865e-05, -8.4295e-05, -6.8536e-05, -5.1321e-05, -1.5860e-05,\n",
      "         2.8688e-05, -2.3620e-05,  3.9132e-05, -7.2718e-05,  1.8109e-07,\n",
      "         3.7811e-05,  1.6230e-05, -3.8949e-06, -1.0456e-04, -9.9893e-05,\n",
      "        -9.7213e-06, -4.4791e-05, -1.2863e-06, -5.6282e-05, -5.9058e-05,\n",
      "        -3.0940e-05, -3.7885e-05,  1.5790e-05, -3.6810e-05, -1.8876e-05,\n",
      "        -2.2069e-05, -7.3314e-05, -1.7687e-06,  5.0987e-05,  6.9051e-06,\n",
      "        -3.4328e-05,  4.6770e-06, -2.1481e-05, -6.2553e-06, -6.1899e-06,\n",
      "         4.7590e-05, -4.1465e-05,  5.5946e-05, -6.1350e-05, -2.3760e-05,\n",
      "        -8.1803e-05,  1.6657e-05, -5.3816e-05, -1.2643e-05, -3.0048e-05,\n",
      "        -4.2089e-05, -1.5047e-04,  4.1974e-05, -6.3186e-05, -1.4592e-05,\n",
      "        -2.9057e-05,  5.4777e-07,  1.4951e-06,  5.8594e-05, -3.5745e-05,\n",
      "        -2.1833e-05, -5.1437e-05, -2.7986e-05,  1.5777e-05, -9.0160e-05,\n",
      "        -1.6036e-05, -1.0178e-04, -8.6974e-06, -1.7425e-05, -1.0487e-04,\n",
      "        -2.7562e-05,  4.2895e-05, -6.5837e-05, -3.4982e-05, -4.5379e-05,\n",
      "        -3.1167e-05, -1.0049e-04, -3.9537e-05, -5.6257e-05, -9.8803e-05,\n",
      "        -1.5036e-05,  2.2158e-05, -1.0272e-05, -4.3505e-05, -1.3599e-04,\n",
      "         1.3510e-05, -1.4332e-04, -2.2595e-05, -1.9439e-05, -3.1864e-05,\n",
      "        -2.3223e-05, -4.0336e-05, -7.4315e-05, -5.6638e-05,  3.4507e-05,\n",
      "         5.4086e-06,  5.6885e-05,  7.3548e-05,  3.2639e-05,  6.0967e-06,\n",
      "         1.3095e-05, -7.2841e-05, -9.3105e-05, -5.8216e-05,  3.7918e-05,\n",
      "        -1.6546e-05, -1.1366e-04,  4.4984e-05,  1.4124e-05, -2.6596e-05,\n",
      "        -6.5685e-05, -6.3336e-05, -3.3443e-05,  2.1941e-06, -1.2522e-05,\n",
      "        -8.9533e-06,  4.3377e-06, -3.6796e-05, -9.0287e-05,  3.5508e-06,\n",
      "        -2.0233e-05,  3.1188e-05, -6.4935e-05, -6.9584e-06,  8.0728e-06,\n",
      "        -3.8044e-05,  8.4227e-06, -3.2968e-05,  3.9375e-05, -6.4122e-05,\n",
      "        -2.4691e-05,  5.5085e-06, -3.7901e-05]), 'exp_avg_sq': tensor([1.9654e-08, 4.1281e-09, 5.0181e-09, 3.0791e-07, 1.3695e-09, 5.8615e-09,\n",
      "        4.6662e-09, 5.5672e-09, 5.6846e-09, 2.1383e-09, 3.3880e-09, 3.6913e-09,\n",
      "        2.1457e-09, 4.7580e-09, 3.8294e-09, 2.4079e-09, 3.0806e-09, 4.8594e-09,\n",
      "        2.9477e-09, 2.9335e-09, 3.4296e-09, 3.3298e-09, 3.1209e-09, 3.3501e-09,\n",
      "        3.8263e-09, 2.7435e-09, 7.1412e-09, 3.3831e-09, 2.9660e-09, 2.0153e-08,\n",
      "        3.2417e-09, 6.0055e-09, 2.2940e-09, 4.6142e-09, 4.2140e-08, 3.3893e-09,\n",
      "        2.5637e-09, 5.4595e-08, 9.1708e-09, 3.1788e-09, 3.7358e-09, 2.6770e-09,\n",
      "        3.2213e-09, 3.4998e-09, 3.7631e-09, 6.1230e-09, 6.9012e-09, 2.3493e-09,\n",
      "        7.4967e-09, 1.7787e-08, 3.3981e-09, 9.1856e-09, 1.5997e-08, 2.3628e-08,\n",
      "        2.0434e-08, 5.0673e-09, 3.0112e-09, 4.7210e-09, 3.2702e-09, 3.6950e-09,\n",
      "        2.1805e-09, 4.2652e-09, 4.5725e-09, 3.5821e-09, 4.4094e-09, 5.8636e-09,\n",
      "        6.6595e-09, 6.3131e-09, 5.1043e-09, 4.8962e-09, 2.2113e-09, 4.9925e-09,\n",
      "        2.1653e-09, 6.0535e-09, 4.3357e-09, 2.0353e-08, 2.3828e-09, 2.7826e-09,\n",
      "        2.1993e-09, 6.0193e-09, 3.6921e-09, 7.2754e-09, 4.9462e-09, 4.8873e-09,\n",
      "        6.6727e-09, 7.9649e-07, 2.1270e-09, 5.3326e-09, 4.5123e-09, 2.9625e-09,\n",
      "        3.3951e-09, 3.7018e-09, 4.1441e-09, 3.0017e-09, 3.3501e-09, 2.7325e-09,\n",
      "        6.0993e-09, 4.1152e-09, 4.1554e-09, 3.6147e-09, 3.8752e-09, 3.4966e-09,\n",
      "        4.9157e-09, 3.1286e-09, 5.0182e-09, 5.1470e-09, 2.5421e-09, 3.5162e-09,\n",
      "        3.6666e-09, 2.9814e-09, 3.0695e-09, 4.6540e-09, 3.8106e-09, 4.1329e-09,\n",
      "        5.4531e-09, 4.2094e-09, 6.7569e-09, 5.8246e-09, 1.9011e-09, 1.9472e-09,\n",
      "        4.4535e-09, 3.2424e-09, 3.4007e-09, 1.1862e-08, 3.0382e-09, 3.7507e-09,\n",
      "        2.6278e-09, 5.4995e-09])}, 15: {'step': tensor(3341.), 'exp_avg': tensor([-1.0906e-04, -6.5541e-05, -7.4406e-06, -9.8305e-05,  5.6282e-05,\n",
      "         2.1530e-05,  2.1012e-05, -1.0789e-04,  3.2136e-05,  1.0299e-04,\n",
      "         4.2814e-05, -9.5366e-05, -1.0625e-05,  3.7312e-05, -9.0557e-05,\n",
      "         1.2457e-04,  7.2498e-05,  4.6117e-05, -6.4814e-05,  5.5861e-06,\n",
      "        -2.3711e-05, -2.4775e-05,  1.0799e-05, -9.5281e-06,  4.1771e-05,\n",
      "         1.1212e-05, -9.9423e-06, -6.9747e-05,  1.3011e-04, -4.2526e-05,\n",
      "         5.7074e-05,  6.7687e-05, -2.0345e-05, -4.0715e-05,  6.5447e-05,\n",
      "         4.9761e-05,  2.1346e-05,  2.9842e-05, -1.8781e-05, -1.1521e-05,\n",
      "        -1.8202e-05, -4.3596e-05, -1.0477e-04, -2.1105e-05, -2.4700e-05,\n",
      "        -7.0571e-05, -1.1515e-04, -2.6624e-05,  1.6291e-05,  8.6173e-05,\n",
      "         8.7823e-05,  6.8348e-05, -2.3616e-05,  1.6296e-04, -3.3358e-05,\n",
      "         3.4210e-05, -3.9863e-05,  6.4614e-05,  2.9620e-05, -1.5724e-05,\n",
      "         6.5655e-05, -4.7942e-05,  8.2407e-05, -1.2931e-05, -1.6088e-05,\n",
      "         1.2650e-04, -7.9856e-05, -1.9845e-05, -5.0022e-05, -7.2745e-05,\n",
      "         1.0926e-05, -2.1790e-05,  2.3075e-05,  4.6602e-05,  4.0013e-05,\n",
      "         1.9275e-05,  5.1378e-05,  1.0460e-05,  9.8084e-06,  4.3559e-05,\n",
      "         2.4481e-06, -5.1525e-05, -2.7299e-05,  3.3698e-05,  4.1125e-06,\n",
      "        -2.2895e-06,  1.1168e-05, -9.0054e-05,  1.2442e-06,  4.0682e-05,\n",
      "        -8.0451e-05,  6.8739e-05,  3.7928e-05,  8.2026e-05, -6.2232e-05,\n",
      "        -9.8320e-05, -6.9951e-05,  2.5416e-05,  4.4326e-05, -8.0336e-05,\n",
      "         5.4811e-05, -2.1789e-05,  8.7246e-05, -1.5258e-04, -9.9894e-07,\n",
      "        -3.6885e-06, -6.6110e-05,  1.1784e-04, -1.3235e-04,  4.8226e-05,\n",
      "         6.0128e-05,  8.7800e-05, -2.0145e-05, -5.6142e-05,  6.2203e-05,\n",
      "        -4.8286e-05, -8.1287e-05, -1.6077e-05, -1.6865e-05, -4.7224e-07,\n",
      "         5.3029e-05, -2.2129e-05, -7.4929e-05,  1.0103e-04, -2.1900e-05,\n",
      "         9.5855e-06,  5.4328e-05, -6.3169e-05]), 'exp_avg_sq': tensor([4.0160e-08, 2.4152e-08, 1.2533e-08, 2.0697e-08, 1.9733e-08, 1.1841e-08,\n",
      "        1.9763e-08, 3.0195e-08, 3.4049e-08, 1.4355e-08, 1.5573e-08, 2.7691e-08,\n",
      "        1.5733e-08, 1.6674e-08, 1.6942e-08, 2.5391e-08, 9.9214e-09, 1.3650e-08,\n",
      "        1.4042e-08, 2.7580e-08, 1.2686e-08, 1.0935e-08, 2.3959e-08, 1.5372e-08,\n",
      "        1.0774e-08, 2.3365e-08, 1.7957e-08, 1.5929e-08, 2.4497e-08, 1.4338e-08,\n",
      "        9.8052e-09, 2.4042e-08, 8.5480e-09, 8.9008e-09, 3.0528e-08, 1.1492e-08,\n",
      "        1.1915e-08, 4.9349e-08, 1.8319e-08, 1.4157e-08, 5.9317e-09, 1.3357e-08,\n",
      "        1.7326e-08, 2.3051e-08, 1.3082e-08, 2.2252e-08, 3.5090e-08, 1.5689e-08,\n",
      "        2.5485e-08, 3.0340e-08, 1.7838e-08, 3.0221e-08, 2.3389e-08, 1.8703e-08,\n",
      "        1.8331e-08, 1.4827e-08, 7.2386e-09, 1.3150e-08, 3.2582e-08, 2.3092e-08,\n",
      "        1.0793e-08, 1.5997e-08, 3.7304e-08, 1.7057e-08, 2.0772e-08, 1.4675e-08,\n",
      "        1.9831e-08, 1.7960e-08, 1.3745e-08, 1.3008e-08, 6.5322e-09, 1.3724e-08,\n",
      "        8.7839e-09, 9.6584e-09, 2.0767e-08, 2.2257e-08, 1.3695e-08, 8.8653e-09,\n",
      "        1.3828e-08, 1.8218e-08, 1.3323e-08, 1.4712e-08, 2.3289e-08, 3.2355e-08,\n",
      "        1.7986e-08, 4.5283e-08, 1.5403e-08, 2.6846e-08, 1.8762e-08, 2.1288e-08,\n",
      "        1.8212e-08, 1.3308e-08, 1.5293e-08, 1.8943e-08, 2.8931e-08, 4.5710e-08,\n",
      "        4.1487e-08, 1.4393e-08, 2.0031e-08, 9.3420e-09, 2.2819e-08, 2.9325e-08,\n",
      "        1.7876e-08, 2.8375e-08, 2.0433e-08, 1.9851e-08, 3.2325e-08, 1.8158e-08,\n",
      "        1.4008e-08, 9.4593e-09, 2.4177e-08, 1.7690e-08, 1.4766e-08, 1.6274e-08,\n",
      "        1.4137e-08, 9.3741e-09, 3.7609e-08, 2.1570e-08, 1.5971e-08, 1.8677e-08,\n",
      "        1.2497e-08, 1.2995e-08, 1.1639e-08, 2.5192e-08, 7.3534e-09, 1.4509e-08,\n",
      "        2.5554e-08, 1.1195e-08])}, 16: {'step': tensor(3341.), 'exp_avg': tensor([[ 1.0507e-05,  2.7845e-05, -3.3509e-06,  ...,  1.4130e-05,\n",
      "         -1.3330e-05, -5.9527e-06],\n",
      "        [ 2.4762e-04,  5.6700e-06,  9.1894e-05,  ...,  1.7938e-04,\n",
      "          6.9808e-05,  2.2970e-05],\n",
      "        [ 2.7860e-04,  3.8003e-05,  7.9284e-05,  ...,  1.2491e-04,\n",
      "          1.8993e-05,  7.8018e-06],\n",
      "        ...,\n",
      "        [ 3.2799e-04, -7.8658e-05,  2.8291e-05,  ...,  3.7873e-05,\n",
      "          4.2101e-05, -2.0525e-06],\n",
      "        [ 2.3395e-04,  2.6219e-05, -5.4719e-06,  ..., -5.3652e-05,\n",
      "         -4.6163e-06,  1.2476e-06],\n",
      "        [-2.0127e-04,  5.7453e-05, -1.7961e-05,  ..., -2.2104e-04,\n",
      "         -2.2488e-05,  6.4635e-06]]), 'exp_avg_sq': tensor([[2.5542e-07, 1.7367e-08, 1.2658e-08,  ..., 1.8326e-08, 4.3466e-09,\n",
      "         3.7868e-10],\n",
      "        [5.8626e-07, 5.1520e-08, 4.5996e-08,  ..., 1.4144e-07, 1.8650e-08,\n",
      "         1.0284e-09],\n",
      "        [1.3911e-06, 3.9148e-08, 4.8430e-08,  ..., 1.0551e-07, 1.5851e-08,\n",
      "         6.7896e-10],\n",
      "        ...,\n",
      "        [4.0296e-07, 2.9455e-08, 1.4556e-08,  ..., 2.2195e-08, 4.8870e-09,\n",
      "         4.6073e-10],\n",
      "        [7.9567e-07, 2.8179e-08, 5.2013e-08,  ..., 6.4769e-08, 5.6344e-09,\n",
      "         4.2139e-10],\n",
      "        [1.1824e-06, 4.7735e-08, 5.2739e-08,  ..., 1.7038e-07, 1.5009e-08,\n",
      "         7.5812e-10]])}, 17: {'step': tensor(3341.), 'exp_avg': tensor([ 1.1165e-04,  3.8213e-04,  3.5226e-05,  2.0810e-04, -7.0430e-05,\n",
      "        -4.2546e-04, -5.9434e-05,  1.7424e-04,  8.9893e-05, -4.7130e-05,\n",
      "         2.9063e-04,  2.4655e-04, -1.6496e-04,  2.6252e-05, -1.5839e-04,\n",
      "         1.1965e-04,  2.6360e-04, -8.3847e-05, -1.6374e-04, -1.6990e-04,\n",
      "         4.3533e-04,  1.8350e-04, -2.5877e-04, -1.6016e-04, -2.1136e-04,\n",
      "        -1.4289e-04, -9.6628e-05,  2.1453e-04, -1.0985e-04, -7.8151e-04,\n",
      "         5.8011e-05, -9.2642e-05,  4.1507e-04, -1.4058e-04, -2.2668e-04,\n",
      "         7.6176e-04,  1.8752e-04, -2.9509e-04,  2.9556e-05,  1.7387e-05,\n",
      "         8.7896e-05,  8.7784e-05, -8.7484e-05,  9.4103e-05, -1.6529e-04,\n",
      "         1.9769e-04,  1.8925e-04,  3.9338e-05, -1.4612e-04, -3.3104e-04,\n",
      "         3.1012e-04,  1.4772e-04,  2.7633e-04,  5.5644e-04, -1.1334e-03,\n",
      "        -3.5725e-04, -9.7426e-06,  8.7728e-04,  3.0466e-04, -5.6608e-05,\n",
      "        -2.2529e-04, -7.5008e-05,  5.2819e-04,  2.6096e-04,  3.8396e-05,\n",
      "         3.0600e-04,  2.5086e-05, -3.9733e-05, -8.9275e-05, -8.0803e-05,\n",
      "        -1.8299e-04,  5.0906e-04,  2.6626e-04, -9.7848e-05, -3.7074e-04,\n",
      "        -2.5727e-04, -3.3769e-04, -1.8221e-04,  4.4548e-04,  7.3843e-04,\n",
      "        -2.5994e-04, -2.1036e-05,  9.1060e-05,  4.7386e-04, -2.5461e-04,\n",
      "        -9.4760e-04,  1.6337e-04, -5.9018e-04,  6.8140e-04,  6.2100e-05,\n",
      "         2.0921e-04, -3.9020e-04, -1.0455e-04, -2.2424e-04,  6.6457e-05,\n",
      "        -1.1018e-04,  3.8135e-05, -6.6937e-05,  7.1264e-04, -3.0583e-05,\n",
      "        -5.1405e-04, -6.5739e-04,  2.3164e-04,  2.9173e-04,  3.1919e-05,\n",
      "         1.6347e-04,  2.2825e-04, -1.5153e-04,  1.8773e-04, -4.3429e-04,\n",
      "        -2.6133e-05,  4.6432e-04, -1.4829e-04, -4.6621e-05,  3.7287e-05,\n",
      "         2.7280e-04, -3.2123e-05, -9.5651e-05, -4.1815e-04, -2.6280e-04,\n",
      "        -1.4617e-04, -3.6768e-04,  2.4411e-04,  2.0745e-04, -5.6121e-04,\n",
      "        -6.6149e-05,  8.6161e-05, -4.6862e-04]), 'exp_avg_sq': tensor([1.4443e-07, 1.1661e-06, 7.6994e-07, 4.6125e-07, 2.9896e-07, 1.1818e-06,\n",
      "        5.1076e-07, 1.2161e-07, 2.2248e-07, 1.9648e-07, 2.0484e-06, 3.4113e-07,\n",
      "        5.3079e-07, 8.3096e-08, 5.7027e-07, 5.5915e-07, 6.0507e-07, 3.4921e-07,\n",
      "        3.6435e-07, 5.0889e-07, 1.7472e-06, 3.4513e-07, 3.2950e-07, 8.7135e-07,\n",
      "        5.8922e-07, 1.6122e-07, 6.0541e-07, 3.6297e-07, 1.3503e-07, 3.1529e-06,\n",
      "        2.6130e-07, 5.1342e-07, 1.4669e-06, 6.1125e-07, 3.5857e-07, 2.3025e-06,\n",
      "        2.7142e-07, 6.2108e-07, 2.8671e-06, 3.8165e-07, 1.7480e-07, 4.7685e-07,\n",
      "        3.6466e-07, 3.8635e-07, 7.5808e-07, 4.1183e-07, 1.6829e-07, 3.8509e-07,\n",
      "        3.2199e-07, 7.7681e-07, 9.4797e-07, 2.7745e-07, 2.7221e-07, 2.7419e-06,\n",
      "        4.6742e-06, 2.4122e-07, 3.6035e-07, 3.8587e-06, 1.5346e-06, 2.6413e-07,\n",
      "        6.5747e-08, 7.7518e-07, 1.9780e-06, 8.9983e-07, 2.5755e-07, 7.1679e-07,\n",
      "        1.6258e-07, 1.7751e-07, 8.0841e-07, 2.3005e-07, 4.8222e-07, 9.2906e-07,\n",
      "        2.7340e-07, 3.7220e-07, 6.5456e-07, 7.0725e-07, 4.1360e-07, 5.1987e-07,\n",
      "        1.2398e-06, 3.2849e-06, 3.9853e-07, 6.0232e-07, 1.1596e-07, 9.5828e-07,\n",
      "        3.6658e-07, 4.3031e-06, 1.2369e-06, 2.2401e-06, 3.2857e-06, 2.7378e-07,\n",
      "        2.1423e-07, 2.7899e-07, 5.1319e-07, 1.5905e-07, 3.8140e-07, 2.7941e-07,\n",
      "        1.0798e-06, 3.1934e-07, 2.2086e-06, 7.4589e-07, 7.9087e-07, 3.4954e-06,\n",
      "        9.5331e-07, 2.8819e-07, 1.3704e-07, 5.4444e-07, 6.1244e-07, 2.0106e-07,\n",
      "        1.0881e-07, 5.5797e-07, 2.1390e-07, 5.8970e-07, 1.9034e-07, 5.2826e-07,\n",
      "        2.7786e-07, 1.0702e-06, 1.6477e-07, 3.2558e-07, 6.8200e-07, 6.7601e-07,\n",
      "        2.9067e-07, 1.3158e-06, 1.2304e-07, 6.8745e-07, 1.0945e-06, 2.8141e-07,\n",
      "        3.6248e-07, 1.0254e-06])}, 18: {'step': tensor(3341.), 'exp_avg': tensor([[ 8.5111e-05,  2.4248e-05, -1.0909e-05,  ..., -2.2065e-05,\n",
      "          3.6121e-05, -2.6741e-05],\n",
      "        [ 2.0124e-06, -1.6013e-05, -1.5220e-05,  ...,  1.8615e-06,\n",
      "         -1.3080e-05, -6.3788e-06],\n",
      "        [-8.6286e-06, -7.8974e-08, -3.0265e-05,  ..., -1.3810e-05,\n",
      "         -1.8395e-05,  3.9293e-05],\n",
      "        ...,\n",
      "        [ 1.8086e-06,  5.0454e-06, -4.8626e-06,  ..., -5.8354e-06,\n",
      "          3.1436e-06,  7.9779e-06],\n",
      "        [-4.5055e-05, -7.0279e-05, -8.3526e-05,  ..., -4.4306e-06,\n",
      "         -1.5176e-05, -8.9485e-06],\n",
      "        [ 2.8665e-06,  7.7134e-06,  7.1290e-06,  ...,  7.6284e-06,\n",
      "         -5.7326e-06,  1.8188e-06]]), 'exp_avg_sq': tensor([[2.5397e-08, 1.1370e-08, 7.2316e-09,  ..., 1.5734e-08, 1.6353e-08,\n",
      "         3.3342e-08],\n",
      "        [1.2547e-09, 1.5425e-09, 1.3756e-09,  ..., 1.6948e-09, 1.2112e-09,\n",
      "         1.8675e-09],\n",
      "        [3.9364e-09, 4.5818e-09, 2.5147e-09,  ..., 2.6210e-09, 1.6477e-09,\n",
      "         7.8895e-09],\n",
      "        ...,\n",
      "        [1.2069e-09, 1.2959e-09, 5.0699e-10,  ..., 4.3136e-10, 2.8362e-09,\n",
      "         3.3304e-10],\n",
      "        [9.0648e-10, 1.5061e-09, 4.0960e-09,  ..., 9.1664e-10, 7.2343e-10,\n",
      "         1.9014e-09],\n",
      "        [5.5143e-10, 8.2656e-10, 1.1725e-09,  ..., 6.2075e-10, 9.4990e-10,\n",
      "         1.2787e-09]])}, 19: {'step': tensor(3341.), 'exp_avg': tensor([-3.1155e-04,  6.2782e-05, -1.0619e-04,  1.5490e-04, -3.6720e-05,\n",
      "        -7.6266e-05, -1.1989e-07, -5.2305e-05,  1.2044e-04, -1.6063e-06,\n",
      "        -2.1156e-05,  1.2104e-03, -5.4753e-05, -5.5259e-05, -5.1480e-05,\n",
      "        -4.0073e-05,  4.3483e-06,  3.9163e-05,  1.9669e-05,  6.3104e-05,\n",
      "         7.4304e-05,  2.5566e-05,  1.0060e-05,  9.3124e-06, -3.1825e-05,\n",
      "         1.0017e-05,  2.6384e-05,  3.3941e-05, -2.0717e-05, -5.7436e-05,\n",
      "         5.5928e-06,  6.4132e-06, -1.2428e-05, -1.2738e-05,  1.1673e-05,\n",
      "         9.7475e-06,  9.6436e-07, -4.3263e-06, -3.9051e-06, -1.5205e-05,\n",
      "        -1.0643e-04, -4.8474e-05,  6.6883e-06, -4.2287e-06, -7.9334e-06,\n",
      "        -1.9247e-05,  4.8079e-06, -5.9494e-06, -1.6416e-05, -3.9076e-05,\n",
      "        -1.0961e-05, -2.0747e-05,  1.7654e-05,  7.2319e-05, -4.0586e-05,\n",
      "        -5.7044e-06, -2.0696e-05, -2.8119e-05,  4.9887e-06, -2.4157e-05,\n",
      "        -7.4808e-05, -4.4974e-05, -2.8085e-05,  1.6561e-05,  5.1592e-06,\n",
      "         2.4803e-04, -7.3177e-05,  4.8883e-05, -2.5659e-05, -6.5847e-05,\n",
      "         4.6970e-05,  1.0992e-05,  1.8838e-05,  2.8745e-06,  1.7630e-05,\n",
      "        -3.0700e-05,  3.7009e-06, -1.9469e-05, -4.9475e-05, -2.0431e-05,\n",
      "         1.3106e-05,  5.8740e-06,  1.1671e-05,  1.1638e-05,  2.3735e-05,\n",
      "        -1.9219e-05, -1.2760e-05, -2.9062e-05, -2.1302e-06, -8.8015e-06,\n",
      "        -1.0988e-05, -2.1962e-05,  1.3946e-06,  1.6115e-04,  3.9086e-05,\n",
      "         9.9144e-06,  1.1620e-05, -6.4482e-07, -6.0839e-06, -1.4259e-05,\n",
      "        -3.1875e-05, -1.9993e-04,  4.0358e-05, -2.1718e-05,  3.6442e-06,\n",
      "        -2.9721e-05,  1.5009e-06, -2.3076e-06, -2.6362e-05, -1.0068e-04,\n",
      "        -1.3774e-05,  3.0813e-05,  2.0987e-06, -2.1299e-05, -4.7755e-06,\n",
      "        -3.9301e-06,  2.2524e-05,  1.4728e-05, -2.3081e-06, -7.8802e-06,\n",
      "        -6.3845e-05,  5.5644e-05, -9.1798e-05, -5.0086e-06, -1.2220e-04,\n",
      "         2.3667e-06,  3.0602e-05,  8.5752e-05,  8.3222e-04,  8.8721e-05,\n",
      "         8.6161e-05, -1.6438e-04,  3.8623e-05, -5.3933e-05,  4.2018e-05,\n",
      "        -1.6115e-05, -1.1706e-04,  9.3786e-05,  2.8225e-05, -3.5951e-04,\n",
      "        -5.1029e-05, -1.7307e-05, -1.9782e-05, -1.8388e-05, -3.3727e-05,\n",
      "        -3.8990e-05,  8.9363e-07, -1.6807e-05,  1.5641e-05,  1.8282e-05,\n",
      "        -6.5327e-05, -4.5133e-06, -1.3297e-04, -1.3193e-04,  2.9666e-05,\n",
      "        -7.6261e-05, -3.9422e-06, -5.4455e-06, -3.4127e-05,  1.4608e-04,\n",
      "         2.7487e-05,  1.3831e-05, -1.0145e-04,  5.1459e-05,  2.5583e-05,\n",
      "         4.9449e-06,  6.6842e-06,  1.1260e-05, -2.4576e-04,  4.5939e-06,\n",
      "         6.9687e-06, -5.6628e-05,  5.9902e-05,  3.7623e-05, -8.8187e-06,\n",
      "         8.3339e-05, -1.7003e-06,  2.7247e-05,  3.4041e-06, -9.6273e-06,\n",
      "        -1.4022e-04, -2.0170e-05, -2.7546e-06,  2.4032e-05, -6.6364e-06,\n",
      "        -6.9240e-05,  3.0811e-05, -1.4455e-05,  3.8140e-05, -4.4337e-05,\n",
      "         9.3202e-06,  1.1038e-05,  3.3218e-05, -6.1274e-05,  1.3317e-05,\n",
      "         8.1951e-05,  2.3637e-05, -1.1951e-04,  1.8916e-05,  1.9537e-05,\n",
      "         7.7694e-05, -9.3155e-05,  3.4963e-06, -2.7973e-04,  1.0284e-05,\n",
      "         7.6975e-05, -5.7801e-05, -9.3466e-06,  2.1899e-05,  3.1823e-05,\n",
      "         5.4267e-06, -8.8746e-06,  5.1973e-05,  1.0916e-04, -2.2606e-07,\n",
      "         4.9587e-06, -3.0913e-06, -3.5022e-06,  7.7344e-05,  1.6721e-05,\n",
      "        -4.8712e-06, -5.6951e-05, -8.4644e-06, -9.8636e-05, -7.4233e-05,\n",
      "         2.7675e-05, -3.7841e-07,  1.8677e-05,  1.3922e-05,  1.1716e-05,\n",
      "         8.2458e-05, -1.0612e-04, -1.0914e-05,  1.4297e-04,  6.9716e-05,\n",
      "        -1.9117e-05,  9.7586e-05,  1.3304e-05, -7.0828e-06,  2.3583e-05,\n",
      "        -3.9933e-05, -3.4881e-06,  8.9919e-05, -8.1762e-06, -7.6917e-06,\n",
      "         1.0224e-05, -5.4810e-06, -6.3052e-05, -4.4680e-05,  2.4720e-05,\n",
      "         2.7879e-05, -1.6806e-05,  4.5127e-05, -1.2084e-05,  4.4263e-04,\n",
      "        -4.1911e-05, -1.9205e-03, -9.6139e-05, -2.5128e-04, -1.6691e-04,\n",
      "        -2.6442e-05,  1.9449e-04, -4.7912e-05, -1.6836e-05, -1.5026e-04,\n",
      "         1.6687e-04,  3.5022e-06,  4.4386e-04, -5.6565e-05,  1.6690e-05,\n",
      "         1.1952e-05,  3.0491e-05,  4.1508e-06, -1.2822e-05, -2.9162e-04,\n",
      "        -2.0645e-04, -1.1290e-05, -3.5629e-05,  1.4001e-04,  1.7877e-04,\n",
      "        -4.2584e-05, -7.8042e-05, -4.9537e-05, -3.5047e-05, -1.9423e-07,\n",
      "        -8.8486e-05,  1.4607e-04, -8.4996e-05, -6.9336e-06, -8.9769e-06,\n",
      "         7.4978e-05,  1.4532e-06,  2.4951e-04, -1.0267e-04,  1.8020e-06,\n",
      "         5.5638e-07, -1.2151e-04, -1.5569e-04,  4.2211e-05, -2.3285e-05,\n",
      "        -1.2150e-05,  5.9615e-05,  1.6157e-05,  1.4459e-04, -4.7999e-06,\n",
      "        -6.2989e-06, -2.7977e-05, -3.5282e-06, -1.9410e-05, -8.0010e-06,\n",
      "        -1.5151e-05,  6.8999e-06, -9.1458e-07, -9.5961e-05, -6.1250e-05,\n",
      "         2.3084e-08, -5.4516e-06, -3.3652e-05, -5.5844e-05,  1.3150e-05,\n",
      "        -6.3334e-06,  1.3394e-04,  1.0178e-05,  2.8561e-05, -1.3483e-06,\n",
      "        -1.7783e-05, -1.3272e-05,  8.0468e-06,  1.4017e-04,  1.6441e-04,\n",
      "         2.9728e-05, -6.3676e-05, -9.6902e-05,  4.7303e-05,  9.2680e-06,\n",
      "         3.1357e-05,  2.2758e-05, -5.8580e-06,  1.1147e-05, -5.0294e-05,\n",
      "        -4.8055e-06, -1.4485e-04,  1.5560e-05, -1.2279e-06,  4.7328e-05,\n",
      "         1.1503e-04, -4.3496e-05,  3.5315e-05, -2.1038e-05,  1.5328e-04,\n",
      "        -2.3637e-05,  1.0291e-04,  1.2557e-04,  2.6697e-05, -2.6155e-05,\n",
      "         1.9350e-04,  7.3747e-06, -1.5554e-04, -4.0011e-05,  1.0945e-04,\n",
      "         4.1949e-05,  1.7161e-06,  5.4733e-05, -2.7452e-05,  4.3220e-05,\n",
      "         1.4737e-05,  5.1711e-05, -6.6797e-05,  3.5724e-06, -1.5922e-06,\n",
      "        -4.3708e-05, -8.8648e-05, -2.9772e-05, -2.4119e-05, -2.0434e-05,\n",
      "        -9.4442e-06,  9.0348e-05,  1.6571e-04, -3.3401e-06,  4.1199e-05,\n",
      "         5.2101e-05, -1.6077e-05,  9.1649e-05, -2.1227e-05]), 'exp_avg_sq': tensor([6.4899e-07, 2.8682e-08, 4.0797e-08, 1.3178e-08, 1.4437e-09, 2.5755e-08,\n",
      "        1.9664e-12, 1.7988e-07, 1.7760e-07, 1.9630e-11, 2.0638e-09, 7.3038e-06,\n",
      "        1.5099e-09, 3.0433e-09, 2.3970e-09, 2.3523e-09, 2.8114e-09, 1.3493e-08,\n",
      "        1.7743e-09, 1.6337e-08, 6.9855e-09, 1.2791e-09, 3.6348e-10, 1.7428e-09,\n",
      "        1.9141e-09, 2.0259e-08, 7.4584e-10, 1.0100e-09, 1.7944e-09, 1.2788e-08,\n",
      "        1.4242e-08, 1.2453e-09, 4.6695e-09, 1.1836e-10, 1.7577e-09, 1.2854e-09,\n",
      "        3.7832e-09, 1.5831e-10, 3.6977e-12, 4.6276e-09, 1.6992e-08, 2.3654e-08,\n",
      "        2.5551e-08, 5.4911e-09, 1.3094e-08, 4.2510e-10, 1.6041e-10, 3.9089e-09,\n",
      "        7.2929e-10, 4.9048e-09, 8.7579e-11, 1.1784e-09, 1.4019e-09, 7.1144e-09,\n",
      "        2.7674e-08, 5.9110e-09, 1.2188e-09, 4.2090e-10, 8.6779e-09, 2.6634e-09,\n",
      "        1.3631e-08, 5.3993e-09, 3.6789e-08, 1.5755e-10, 1.7267e-10, 7.1845e-08,\n",
      "        3.7940e-09, 5.1585e-09, 2.6623e-10, 2.9993e-09, 2.1269e-09, 2.8793e-09,\n",
      "        2.3985e-09, 2.2368e-09, 3.2515e-09, 4.2223e-08, 1.0071e-09, 1.9597e-09,\n",
      "        1.7603e-09, 1.3547e-08, 2.1191e-10, 2.0609e-09, 2.7023e-09, 1.1098e-08,\n",
      "        4.5286e-10, 5.1212e-08, 4.5154e-09, 1.6674e-09, 1.2703e-09, 3.6600e-08,\n",
      "        3.7054e-09, 9.8765e-09, 2.2999e-10, 2.8454e-08, 2.6229e-08, 8.3455e-11,\n",
      "        7.4292e-09, 1.2122e-10, 8.4094e-11, 1.3680e-07, 4.5155e-09, 3.4226e-08,\n",
      "        2.3203e-09, 3.0706e-09, 8.1227e-11, 8.7537e-09, 1.1542e-11, 8.6240e-09,\n",
      "        7.5288e-09, 3.0883e-09, 3.5757e-10, 3.4760e-09, 3.4237e-11, 3.6499e-09,\n",
      "        1.8556e-10, 6.5868e-10, 7.7986e-10, 2.5305e-09, 5.4192e-10, 1.2184e-09,\n",
      "        2.8771e-08, 8.5074e-09, 5.1081e-09, 2.0938e-08, 2.0199e-08, 3.2615e-09,\n",
      "        6.1718e-09, 1.1195e-08, 5.1199e-06, 4.6064e-08, 1.9853e-07, 4.1804e-08,\n",
      "        1.1801e-09, 5.8616e-08, 4.2048e-09, 7.7025e-09, 4.1465e-08, 4.0603e-09,\n",
      "        5.9173e-09, 3.0467e-07, 5.7618e-08, 4.5962e-09, 6.0354e-09, 3.9031e-10,\n",
      "        2.7761e-07, 1.0021e-08, 1.1261e-08, 1.5911e-07, 3.8075e-09, 4.0259e-10,\n",
      "        8.1782e-09, 4.3146e-09, 4.2543e-08, 1.1288e-08, 3.8265e-09, 4.3800e-08,\n",
      "        3.3607e-10, 1.3210e-09, 7.3549e-08, 3.5815e-09, 3.3438e-09, 1.9977e-09,\n",
      "        8.5213e-09, 8.4807e-09, 6.5566e-09, 3.2658e-10, 1.4953e-09, 7.7595e-10,\n",
      "        6.0382e-07, 9.8608e-09, 4.1406e-07, 3.4814e-08, 1.1776e-08, 4.9909e-09,\n",
      "        3.3610e-10, 5.0041e-08, 8.9030e-10, 2.7531e-09, 2.2213e-10, 7.8503e-09,\n",
      "        5.5839e-09, 7.0840e-10, 4.8618e-08, 6.0957e-10, 2.7073e-10, 5.4608e-09,\n",
      "        1.6379e-06, 3.8686e-10, 7.6468e-09, 7.5536e-09, 2.1327e-08, 2.3664e-09,\n",
      "        3.6897e-10, 2.3407e-08, 5.9874e-09, 1.6831e-08, 2.6019e-09, 1.2565e-08,\n",
      "        2.3354e-09, 1.5586e-09, 1.9819e-08, 7.6100e-09, 1.0683e-09, 7.9145e-08,\n",
      "        5.9049e-11, 2.6210e-09, 7.5723e-09, 1.0897e-08, 1.9200e-09, 6.0849e-09,\n",
      "        6.7466e-09, 1.8164e-09, 2.2804e-08, 4.2147e-09, 9.2773e-09, 2.7710e-09,\n",
      "        3.4726e-09, 3.0384e-08, 1.7024e-09, 7.6364e-10, 2.1996e-11, 6.3320e-09,\n",
      "        4.4334e-08, 3.2473e-09, 6.5190e-08, 1.7061e-09, 1.3236e-12, 8.3239e-08,\n",
      "        3.7611e-09, 6.1070e-09, 2.9360e-09, 1.1029e-08, 1.1318e-09, 1.2332e-07,\n",
      "        4.3584e-09, 9.8139e-09, 4.3240e-09, 6.6657e-10, 9.0277e-10, 2.6901e-10,\n",
      "        6.6653e-09, 1.1011e-11, 5.1480e-09, 1.1179e-09, 1.0601e-09, 5.2365e-10,\n",
      "        7.8938e-10, 1.2143e-09, 3.0536e-09, 4.4574e-09, 4.1394e-09, 5.4155e-09,\n",
      "        1.8011e-09, 5.2128e-09, 1.8231e-07, 4.8676e-09, 7.8809e-06, 2.3405e-08,\n",
      "        3.8093e-06, 2.9743e-07, 2.3434e-09, 9.4607e-08, 2.8025e-09, 1.1898e-08,\n",
      "        5.5317e-08, 1.4311e-08, 9.1764e-10, 5.0279e-07, 7.2649e-09, 5.6921e-09,\n",
      "        1.9121e-09, 8.0770e-10, 1.0301e-08, 7.1884e-09, 2.1791e-07, 5.2125e-07,\n",
      "        1.4885e-09, 5.7630e-09, 2.1703e-08, 2.2053e-08, 3.0474e-09, 4.5464e-09,\n",
      "        7.4216e-09, 2.6713e-09, 2.8948e-10, 1.1573e-07, 5.1953e-08, 2.0290e-09,\n",
      "        3.0330e-10, 2.6586e-09, 3.9118e-09, 3.2545e-08, 9.5286e-08, 2.7225e-09,\n",
      "        1.3911e-09, 2.3077e-12, 2.5145e-07, 4.2578e-08, 6.7721e-08, 8.1012e-09,\n",
      "        1.7478e-09, 1.1426e-08, 4.9704e-10, 1.5083e-08, 6.3341e-09, 2.9534e-10,\n",
      "        1.1273e-09, 3.0239e-10, 1.5079e-09, 4.1797e-09, 1.5313e-08, 7.9126e-10,\n",
      "        3.6813e-10, 4.4079e-09, 6.6517e-08, 1.4425e-12, 3.6698e-09, 2.0748e-09,\n",
      "        5.6770e-08, 2.4629e-09, 8.4997e-11, 1.1337e-07, 1.2664e-09, 6.8871e-09,\n",
      "        2.8625e-11, 4.2253e-10, 2.9565e-09, 1.5880e-09, 1.9700e-08, 1.4073e-08,\n",
      "        5.4756e-09, 1.0128e-08, 4.7154e-09, 1.1230e-09, 2.4601e-09, 1.9468e-08,\n",
      "        3.5976e-10, 5.7882e-10, 4.7168e-09, 2.4960e-09, 1.0498e-09, 9.8344e-09,\n",
      "        3.7832e-08, 8.3663e-10, 1.0052e-08, 1.3903e-08, 3.1301e-09, 2.3750e-09,\n",
      "        7.1166e-10, 2.2831e-08, 1.6050e-08, 8.9081e-09, 8.7903e-08, 4.4424e-09,\n",
      "        8.6044e-10, 9.7245e-07, 1.8470e-09, 3.8547e-07, 1.8780e-09, 1.1533e-08,\n",
      "        1.1959e-08, 1.4511e-08, 5.8351e-09, 3.8177e-09, 1.5063e-09, 3.1578e-10,\n",
      "        2.1772e-09, 4.2773e-09, 1.3451e-09, 2.7708e-09, 2.5154e-09, 6.0617e-09,\n",
      "        6.1560e-09, 4.7582e-09, 3.9183e-10, 1.9521e-09, 1.1401e-08, 2.6001e-07,\n",
      "        3.9351e-11, 2.4939e-08, 2.4477e-09, 1.3643e-08, 8.5581e-09, 5.5301e-09])}, 20: {'step': tensor(3341.), 'exp_avg': tensor([[[ 3.7870e-04,  3.7429e-04,  4.0269e-04]],\n",
      "\n",
      "        [[-2.8802e-05, -2.5791e-05, -4.2063e-05]],\n",
      "\n",
      "        [[-3.7150e-05, -1.2758e-04, -3.3056e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.4848e-05,  2.9249e-06, -6.8109e-06]],\n",
      "\n",
      "        [[ 1.1400e-04,  3.0743e-04,  2.7400e-04]],\n",
      "\n",
      "        [[-3.3721e-05, -2.7080e-05, -1.9259e-05]]]), 'exp_avg_sq': tensor([[[1.0807e-06, 1.0799e-06, 1.0845e-06]],\n",
      "\n",
      "        [[2.3980e-08, 2.4528e-08, 2.3893e-08]],\n",
      "\n",
      "        [[2.8682e-08, 2.9176e-08, 3.0881e-08]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.0266e-08, 6.4808e-09, 8.9015e-09]],\n",
      "\n",
      "        [[9.2268e-09, 4.6259e-08, 4.9547e-08]],\n",
      "\n",
      "        [[1.4024e-09, 1.1965e-09, 1.1778e-09]]])}, 21: {'step': tensor(3341.), 'exp_avg': tensor([-4.8902e-04,  1.0901e-04,  3.5496e-04,  2.0433e-04, -1.1034e-04,\n",
      "        -1.3729e-04,  6.5264e-06,  1.4966e-04,  2.4982e-04, -2.9757e-04,\n",
      "         2.1002e-04, -3.2841e-03,  2.6861e-04, -3.4242e-04, -7.4071e-05,\n",
      "        -3.4019e-05, -2.4520e-05, -1.1049e-04,  7.6753e-05, -1.0515e-04,\n",
      "        -1.3819e-04,  6.1058e-05, -1.2560e-04,  1.3372e-05, -4.4480e-05,\n",
      "        -1.4649e-05,  2.5548e-04,  6.1353e-05, -4.8654e-05, -1.2236e-04,\n",
      "        -1.0256e-05,  1.2583e-05,  2.2333e-05, -5.2315e-05, -4.6902e-05,\n",
      "        -7.2976e-05, -1.3116e-06,  1.5435e-05,  1.4536e-04,  2.1131e-05,\n",
      "        -1.8493e-04, -7.1960e-05,  1.9020e-05,  2.3640e-05,  1.1618e-05,\n",
      "         9.9154e-05,  2.1978e-05,  8.2339e-06,  5.3862e-05, -6.8895e-05,\n",
      "        -1.1251e-04,  6.4447e-05,  4.0202e-05,  1.3595e-04, -7.2894e-05,\n",
      "        -2.0320e-05, -2.0283e-05, -2.0765e-04,  5.4962e-05,  1.9705e-05,\n",
      "         6.8019e-05, -1.0764e-04, -9.2136e-05, -1.6000e-04,  1.8384e-05,\n",
      "        -7.9490e-04,  1.2123e-04,  1.2249e-04,  1.3821e-04, -1.4001e-04,\n",
      "        -1.6588e-04,  1.8310e-05, -2.3955e-05,  1.7193e-05,  6.8439e-05,\n",
      "        -4.4825e-05,  3.4145e-05,  6.8352e-05,  2.5604e-04,  2.2781e-05,\n",
      "        -7.6632e-05,  1.5415e-05,  7.9931e-05,  1.7322e-05,  7.5758e-05,\n",
      "        -4.2475e-05,  2.8308e-05, -4.0934e-05,  4.8528e-05,  1.8846e-05,\n",
      "        -2.7108e-05,  5.0292e-05,  1.3531e-05,  4.4726e-04, -4.7719e-05,\n",
      "         9.3252e-05, -2.1923e-05, -9.8327e-06,  7.2773e-05, -6.6479e-05,\n",
      "         4.4921e-05,  2.5527e-04,  3.5951e-05, -1.1432e-04,  4.5697e-05,\n",
      "         1.3589e-04,  1.6910e-04, -7.8703e-06,  6.8589e-05,  1.3307e-04,\n",
      "         1.5848e-04,  5.3239e-05, -2.7579e-05,  2.3007e-05, -5.5445e-05,\n",
      "         4.7141e-05,  3.3626e-04, -5.9666e-05,  5.1119e-06,  2.4550e-05,\n",
      "        -1.1633e-04,  1.0779e-04,  8.3074e-05,  1.0905e-05, -2.1190e-04,\n",
      "         4.3351e-05, -4.8023e-05,  1.1802e-04,  2.2065e-03, -3.1969e-04,\n",
      "        -2.8907e-04, -4.7371e-04, -1.3055e-04,  1.8468e-04,  2.7393e-04,\n",
      "         3.5501e-05, -2.8143e-04,  2.6121e-04,  4.4856e-05, -2.3289e-03,\n",
      "        -1.3277e-04, -2.9740e-05, -9.0772e-05, -4.6039e-05,  9.5757e-05,\n",
      "         4.9072e-05,  3.6223e-06,  7.9742e-05, -2.0634e-05,  3.5135e-05,\n",
      "        -9.3665e-05, -6.9719e-06, -2.7626e-04,  3.2573e-04, -3.5307e-05,\n",
      "         1.2782e-04, -1.0192e-05,  4.1752e-04,  6.8635e-05,  2.6293e-04,\n",
      "         1.0736e-04,  1.3252e-05,  3.0554e-04,  7.0080e-05,  1.7111e-04,\n",
      "         2.7357e-05,  1.2907e-05,  1.1888e-05, -5.3060e-04, -1.2022e-05,\n",
      "         1.2709e-05,  1.0916e-04, -9.9623e-05,  6.1077e-05,  2.4245e-05,\n",
      "        -1.0576e-04, -1.5893e-05, -2.6252e-04,  7.6061e-06, -5.4079e-05,\n",
      "         2.9308e-04, -6.9698e-05,  1.0208e-05, -1.2124e-04,  1.8690e-05,\n",
      "        -1.1401e-04,  7.9128e-05, -6.9608e-05,  6.4093e-05,  6.3643e-05,\n",
      "        -1.1459e-05,  1.2223e-05, -4.3891e-04, -1.2018e-03, -2.3982e-05,\n",
      "        -1.2589e-04,  1.8193e-05,  1.3137e-04,  2.8827e-05, -5.1443e-05,\n",
      "        -9.4253e-05, -2.4258e-04,  5.3404e-06, -4.5141e-04,  1.5957e-04,\n",
      "        -1.0586e-04, -9.6308e-05, -2.6998e-05,  4.8595e-05,  8.8133e-05,\n",
      "        -1.1422e-05,  9.3679e-06, -6.3651e-05,  6.7965e-04, -1.4311e-06,\n",
      "         4.2114e-06,  9.0169e-06, -8.4605e-06,  1.6802e-04, -9.4820e-05,\n",
      "         2.1908e-04,  1.1104e-04, -1.1282e-05,  1.2555e-04,  1.0992e-04,\n",
      "         4.0347e-05,  3.8394e-05,  4.8206e-05, -2.1487e-05,  2.9581e-05,\n",
      "        -2.6202e-04,  1.9699e-04,  1.9597e-05, -2.5007e-04,  1.4439e-04,\n",
      "         5.5075e-05, -3.9212e-04,  2.5102e-05,  2.3110e-05,  1.4998e-04,\n",
      "         7.9019e-05,  9.0412e-05,  1.5758e-04,  1.8305e-05, -4.6419e-05,\n",
      "        -3.3966e-05,  2.3938e-05, -3.3472e-04,  2.1281e-04, -1.2445e-04,\n",
      "         2.5036e-05,  3.2055e-05, -4.0857e-04,  1.1398e-04, -6.3461e-04,\n",
      "        -6.5529e-05,  3.5935e-03, -2.9197e-04,  5.5903e-04, -5.5750e-04,\n",
      "         4.1868e-05, -5.2145e-04,  1.1146e-04,  3.4218e-05,  5.9038e-04,\n",
      "         1.8614e-04, -1.7192e-05, -3.0738e-03, -1.1971e-04,  2.7738e-05,\n",
      "        -2.0003e-05, -8.7118e-05,  1.0157e-05,  2.5237e-05,  8.7519e-04,\n",
      "        -3.9607e-04,  1.9573e-05,  1.3555e-04, -2.8549e-04,  3.2121e-04,\n",
      "        -1.7543e-04,  1.9002e-04, -9.1310e-05, -9.4996e-05, -2.4533e-05,\n",
      "         6.2213e-04,  7.0361e-04,  2.1721e-04, -2.7690e-05, -8.9117e-06,\n",
      "        -1.6741e-04, -6.3078e-06,  3.3983e-04, -1.8455e-04, -3.2041e-06,\n",
      "        -3.2291e-05, -1.6129e-04,  2.8678e-04,  1.1976e-04, -9.5002e-05,\n",
      "        -2.4354e-05, -7.4668e-05, -3.6977e-05, -2.0445e-04,  5.9447e-06,\n",
      "        -1.9419e-05,  2.3866e-04,  3.0787e-05, -4.0590e-05,  2.1833e-05,\n",
      "        -1.1664e-04, -1.1388e-05, -2.2247e-06, -1.3128e-04,  1.6808e-04,\n",
      "         3.3448e-05, -1.4936e-05, -5.3542e-05, -3.9467e-04,  1.8538e-05,\n",
      "        -1.7163e-05,  7.8347e-04, -4.4141e-05,  4.8479e-05,  1.9499e-05,\n",
      "        -1.6657e-04,  2.0123e-05,  1.2930e-05, -6.4299e-04, -3.4018e-04,\n",
      "        -4.3314e-05,  1.0471e-04, -2.1528e-04, -4.5885e-04,  3.5376e-05,\n",
      "        -3.9590e-05, -7.8736e-05,  1.6492e-05,  2.4671e-05, -5.3567e-05,\n",
      "        -1.4449e-05,  3.1331e-04, -2.6275e-05, -1.0153e-05, -1.3141e-04,\n",
      "        -2.1825e-04,  3.9327e-05,  9.2484e-05,  4.5228e-05, -2.6707e-04,\n",
      "        -4.1554e-05,  1.8335e-04,  2.2544e-04,  2.9745e-05,  6.7127e-05,\n",
      "        -3.6088e-04,  3.0805e-05, -2.0197e-04, -1.2160e-04, -3.1988e-04,\n",
      "        -7.6801e-05, -2.3019e-06,  1.7375e-04, -1.2167e-04, -3.3126e-04,\n",
      "         3.6361e-05,  5.7566e-05,  1.2243e-04, -7.6603e-06,  1.5470e-06,\n",
      "         1.4445e-04, -2.4571e-04,  8.8295e-05, -8.0711e-05, -6.8547e-05,\n",
      "         1.1078e-05, -1.7860e-04, -2.6438e-04,  4.3829e-05, -1.3461e-04,\n",
      "         1.1594e-04,  6.8607e-05, -4.7362e-04, -2.7213e-05]), 'exp_avg_sq': tensor([1.5199e-06, 8.3384e-08, 4.4602e-07, 2.2538e-08, 1.2133e-08, 8.3696e-08,\n",
      "        1.3663e-08, 1.3621e-06, 7.7121e-07, 2.4823e-07, 1.9886e-07, 5.2607e-05,\n",
      "        3.7487e-08, 1.0647e-07, 4.9869e-09, 1.6891e-09, 9.1174e-08, 1.0772e-07,\n",
      "        2.6418e-08, 4.1733e-08, 2.3882e-08, 7.1318e-09, 4.6123e-08, 3.3942e-09,\n",
      "        3.6722e-09, 4.0371e-08, 9.1597e-08, 3.2090e-09, 9.6897e-09, 5.9352e-08,\n",
      "        4.5649e-08, 4.7249e-09, 1.4442e-08, 1.9723e-09, 2.5069e-08, 9.2547e-08,\n",
      "        7.1294e-09, 1.9924e-09, 9.6977e-09, 8.8574e-09, 4.5872e-08, 4.8316e-08,\n",
      "        2.1400e-07, 1.6894e-07, 2.6605e-08, 9.9727e-09, 3.3560e-09, 7.3023e-09,\n",
      "        7.1127e-09, 1.5394e-08, 8.0147e-09, 1.2766e-08, 7.3268e-09, 2.4322e-08,\n",
      "        9.2648e-08, 6.4056e-08, 1.1526e-09, 2.3845e-08, 9.3387e-07, 1.7649e-09,\n",
      "        1.0587e-08, 2.8685e-08, 4.3268e-07, 1.1479e-08, 2.0480e-09, 7.3168e-07,\n",
      "        1.0211e-08, 2.9585e-08, 8.2243e-09, 1.2357e-08, 2.4454e-08, 8.0789e-09,\n",
      "        3.7767e-09, 7.4443e-08, 4.4761e-08, 9.0094e-08, 9.1237e-08, 2.3648e-08,\n",
      "        4.6651e-08, 1.5505e-08, 7.3489e-09, 1.3564e-08, 1.4427e-07, 2.2951e-08,\n",
      "        4.6157e-09, 2.4546e-07, 2.1326e-08, 3.3512e-09, 8.3966e-07, 1.5028e-07,\n",
      "        1.9002e-08, 5.1333e-08, 2.0018e-08, 2.1106e-07, 3.5521e-08, 6.4689e-09,\n",
      "        2.5449e-08, 3.2372e-08, 1.1696e-08, 2.9436e-06, 9.3408e-09, 5.4383e-08,\n",
      "        1.8266e-09, 8.3143e-08, 1.0329e-08, 1.8516e-07, 1.3600e-07, 9.5762e-08,\n",
      "        5.2086e-08, 5.3441e-09, 5.0452e-08, 9.5674e-09, 5.6174e-09, 4.5146e-09,\n",
      "        3.0258e-08, 1.0196e-07, 1.7667e-07, 4.1279e-08, 2.7402e-09, 1.1326e-08,\n",
      "        9.3736e-08, 3.1513e-08, 3.8604e-09, 1.0717e-07, 5.8889e-08, 8.9483e-07,\n",
      "        1.4706e-08, 2.0628e-08, 3.3545e-05, 5.8060e-07, 1.9602e-06, 3.2572e-07,\n",
      "        1.3749e-08, 6.7023e-07, 1.7230e-07, 3.1566e-08, 2.2625e-07, 3.1435e-08,\n",
      "        1.4563e-08, 1.2652e-05, 3.6961e-07, 1.3491e-08, 1.2676e-07, 2.2701e-09,\n",
      "        2.1850e-06, 1.5695e-08, 1.2187e-07, 3.8418e-06, 6.4540e-09, 1.5181e-09,\n",
      "        1.5790e-08, 1.0499e-08, 1.7756e-07, 6.8925e-08, 5.3976e-09, 1.2088e-07,\n",
      "        2.1448e-08, 4.8831e-06, 2.7526e-07, 1.1631e-08, 4.9087e-08, 1.7895e-09,\n",
      "        7.2896e-08, 1.7585e-08, 3.5825e-07, 1.0029e-08, 5.1562e-09, 8.7747e-10,\n",
      "        2.3505e-06, 6.5009e-08, 1.3185e-06, 1.1541e-07, 3.1188e-08, 1.2624e-08,\n",
      "        2.5355e-09, 7.9295e-08, 7.7945e-08, 2.6791e-07, 1.0899e-09, 2.9090e-07,\n",
      "        2.4666e-08, 9.4965e-09, 1.6985e-07, 1.4278e-08, 2.0301e-09, 1.4761e-08,\n",
      "        1.0537e-05, 9.5860e-09, 2.2358e-08, 1.5744e-08, 2.9320e-08, 2.9308e-09,\n",
      "        5.0424e-08, 9.5426e-06, 1.8747e-08, 3.8296e-08, 1.5284e-09, 1.4735e-08,\n",
      "        5.0386e-09, 1.0270e-08, 2.8597e-08, 4.8129e-08, 2.3632e-09, 1.9885e-07,\n",
      "        1.0352e-08, 4.9047e-09, 2.0225e-08, 8.1706e-08, 9.0822e-09, 5.4139e-08,\n",
      "        3.0204e-08, 2.1408e-09, 3.3992e-08, 1.7261e-07, 1.4582e-06, 2.0625e-09,\n",
      "        2.8260e-08, 2.7065e-07, 7.5691e-09, 2.5329e-08, 8.4727e-08, 2.2693e-08,\n",
      "        7.3356e-08, 5.1298e-09, 1.3511e-07, 3.6816e-09, 2.7502e-09, 5.0738e-07,\n",
      "        9.9238e-09, 3.7336e-08, 2.8926e-08, 3.6384e-08, 3.5006e-09, 3.7544e-07,\n",
      "        1.8014e-08, 8.0007e-08, 6.7643e-08, 2.2002e-09, 9.4624e-09, 8.3018e-09,\n",
      "        2.7731e-08, 4.8083e-09, 1.5972e-08, 5.6717e-09, 3.5531e-08, 5.5188e-09,\n",
      "        1.4770e-08, 3.8379e-08, 7.8543e-08, 1.0805e-07, 3.6221e-09, 2.3499e-08,\n",
      "        1.3747e-07, 6.3743e-07, 3.6174e-07, 1.2359e-08, 2.5979e-05, 2.0451e-07,\n",
      "        1.8304e-05, 3.2276e-06, 5.8665e-09, 6.8694e-07, 1.5176e-08, 4.7176e-08,\n",
      "        7.8242e-07, 1.7102e-08, 2.1692e-08, 2.2395e-05, 3.3327e-08, 1.5740e-08,\n",
      "        5.2823e-09, 6.5785e-09, 6.9746e-08, 3.1079e-08, 1.8787e-06, 1.8456e-06,\n",
      "        4.2003e-09, 8.4008e-08, 9.0120e-08, 7.1605e-08, 4.6663e-08, 2.6953e-08,\n",
      "        2.5079e-08, 1.9528e-08, 4.4669e-08, 5.7214e-06, 1.1422e-06, 1.3533e-08,\n",
      "        4.9216e-09, 2.5963e-09, 1.9671e-08, 5.0438e-07, 1.7243e-07, 8.9765e-09,\n",
      "        4.1015e-09, 1.2028e-08, 3.8319e-07, 1.4497e-07, 5.3994e-07, 1.1650e-07,\n",
      "        6.6213e-09, 1.7362e-08, 2.6059e-09, 2.9734e-08, 9.3908e-09, 2.9009e-09,\n",
      "        6.2727e-08, 1.7673e-08, 6.2994e-09, 3.2270e-08, 8.8200e-06, 2.2742e-09,\n",
      "        2.3044e-09, 8.0674e-09, 4.4036e-07, 7.0872e-09, 3.1516e-08, 5.3759e-09,\n",
      "        2.8919e-06, 4.7948e-09, 6.5956e-10, 3.7647e-06, 2.2852e-08, 1.9193e-08,\n",
      "        6.6921e-09, 3.5597e-08, 6.4080e-09, 3.7744e-09, 4.0048e-07, 5.9118e-08,\n",
      "        1.1718e-08, 2.5404e-08, 2.3868e-08, 8.7155e-08, 3.6414e-08, 2.8205e-08,\n",
      "        4.3676e-09, 3.9385e-09, 1.9790e-08, 2.8262e-09, 9.9309e-09, 4.5435e-08,\n",
      "        1.0416e-07, 4.3220e-08, 7.5698e-08, 4.6081e-08, 2.4961e-09, 1.6733e-08,\n",
      "        3.0962e-09, 6.9002e-08, 5.0084e-08, 2.8496e-08, 2.7050e-07, 5.5312e-09,\n",
      "        5.5533e-09, 3.2966e-06, 3.2031e-08, 7.4269e-07, 1.6310e-08, 8.9655e-08,\n",
      "        4.1285e-08, 2.1781e-08, 5.4837e-08, 7.8236e-08, 8.6051e-08, 1.8282e-09,\n",
      "        2.6363e-09, 1.3787e-08, 6.7376e-09, 2.6338e-09, 2.8317e-08, 4.6568e-08,\n",
      "        5.4589e-08, 4.9875e-08, 4.7327e-09, 2.5974e-09, 4.3633e-08, 6.5381e-07,\n",
      "        1.9776e-08, 2.5374e-07, 1.2303e-08, 2.2337e-07, 2.2267e-07, 8.9701e-09])}, 22: {'step': tensor(3341.), 'exp_avg': tensor([-7.6160e-06,  1.1788e-06, -2.0571e-07,  1.2410e-06,  7.3304e-07,\n",
      "        -2.3972e-08, -7.3216e-07, -8.9438e-07, -4.6421e-07,  6.1821e-07,\n",
      "         1.0439e-06, -4.5288e-06, -6.2151e-07,  1.7434e-07, -2.7967e-07,\n",
      "         2.2918e-06,  2.0352e-07,  3.0028e-07,  2.9959e-07, -2.8124e-07,\n",
      "        -1.1760e-06, -8.3445e-06,  6.4767e-07,  1.3014e-06,  2.4108e-07,\n",
      "         1.1400e-06, -6.4898e-07,  9.9155e-07, -1.9195e-06, -7.1531e-07,\n",
      "        -7.6349e-07,  1.3307e-06,  9.6460e-07,  2.6449e-05,  5.5647e-07,\n",
      "         6.3873e-07, -1.3347e-06,  1.2224e-06,  5.0466e-07, -1.4980e-05,\n",
      "        -3.5412e-07, -9.8628e-07, -9.2167e-08, -2.6842e-07, -1.6446e-07,\n",
      "         6.7435e-07,  5.7550e-06, -2.1463e-07, -4.2346e-07,  8.5439e-07,\n",
      "        -3.6174e-06, -1.3595e-06, -1.1364e-06, -6.0188e-07,  2.2954e-05,\n",
      "         3.0286e-06, -1.6062e-05,  1.5633e-06, -8.6097e-07, -1.3476e-05,\n",
      "         2.1451e-06,  3.0778e-07, -3.0585e-07,  4.2425e-07, -3.5983e-06,\n",
      "         4.3477e-06, -1.0066e-06,  3.6431e-07, -6.6283e-06, -1.1749e-06,\n",
      "        -2.0051e-08,  2.4824e-06,  7.0844e-07, -1.7672e-06,  2.0966e-06,\n",
      "         5.2823e-07, -2.6126e-07,  6.5524e-06,  6.4936e-07, -2.4345e-05,\n",
      "        -2.9253e-06, -2.0785e-07, -1.2922e-07, -9.3046e-06, -3.3535e-07,\n",
      "        -5.6164e-07,  1.0573e-06,  1.4091e-05,  1.8489e-07, -1.4667e-07,\n",
      "         1.2424e-06,  1.0634e-06, -2.8639e-06, -1.3968e-07,  1.3911e-06,\n",
      "         8.5915e-07,  4.9760e-07, -5.4822e-08, -3.7057e-06,  3.3267e-06,\n",
      "         1.4396e-06, -3.5195e-07, -1.0674e-05,  8.9125e-07,  1.0754e-06,\n",
      "         1.0773e-07, -2.1808e-07,  3.2592e-06, -1.4558e-06, -2.1526e-05,\n",
      "         3.3124e-07,  9.5163e-07, -2.5208e-06, -7.6116e-07,  2.3524e-07,\n",
      "        -4.4844e-07,  2.5999e-08, -2.6129e-06,  1.9375e-07, -3.8648e-06,\n",
      "         7.2718e-07, -3.5672e-07,  2.3117e-05,  3.8739e-07, -8.8545e-07,\n",
      "        -1.3675e-07,  4.4850e-07, -2.8787e-07]), 'exp_avg_sq': tensor([2.5070e-09, 8.0975e-12, 2.5698e-12, 9.4492e-13, 1.6574e-12, 1.9315e-12,\n",
      "        1.3577e-12, 4.6774e-13, 5.0416e-13, 2.7399e-13, 1.3906e-12, 1.2673e-10,\n",
      "        4.9330e-13, 1.8281e-13, 2.9677e-12, 3.1846e-10, 5.8524e-13, 3.8153e-12,\n",
      "        1.8059e-13, 1.3529e-12, 8.7988e-12, 2.9386e-11, 3.8116e-13, 9.5209e-13,\n",
      "        1.6023e-13, 6.8994e-13, 6.4146e-13, 2.9121e-13, 4.9712e-12, 2.9294e-12,\n",
      "        3.8352e-12, 6.4473e-13, 2.6328e-12, 2.9162e-10, 6.1535e-13, 2.8258e-12,\n",
      "        8.8971e-13, 4.6339e-13, 3.1983e-13, 1.9104e-10, 8.6599e-13, 1.6589e-12,\n",
      "        6.7165e-13, 2.3121e-12, 7.8659e-13, 1.7710e-12, 1.9714e-10, 6.3073e-13,\n",
      "        2.6564e-12, 6.8083e-13, 2.7098e-12, 2.3012e-12, 6.5425e-13, 3.8117e-13,\n",
      "        9.2324e-08, 5.5591e-11, 1.7987e-10, 5.8100e-13, 3.5299e-12, 1.3712e-10,\n",
      "        1.7055e-10, 2.2174e-13, 4.4291e-13, 1.0154e-11, 1.5297e-11, 6.3827e-12,\n",
      "        1.1158e-12, 1.7871e-13, 2.5931e-10, 1.0757e-12, 3.7683e-13, 3.3597e-12,\n",
      "        1.4482e-12, 1.3215e-12, 5.6164e-12, 8.0961e-13, 6.6766e-13, 1.1801e-10,\n",
      "        2.8188e-13, 1.3670e-08, 6.8368e-12, 8.4594e-13, 4.8779e-13, 3.7463e-11,\n",
      "        6.0889e-12, 8.4323e-13, 1.0912e-12, 5.1605e-10, 1.8027e-13, 9.0071e-12,\n",
      "        1.9355e-11, 1.0338e-12, 5.2712e-12, 9.0456e-13, 5.9983e-11, 9.5032e-13,\n",
      "        2.4022e-12, 4.8540e-13, 2.7196e-12, 1.3559e-11, 2.7708e-11, 7.8854e-12,\n",
      "        1.1637e-10, 1.2063e-12, 1.6110e-12, 1.0394e-12, 5.7255e-13, 1.6279e-12,\n",
      "        7.6544e-13, 1.0649e-09, 2.4317e-12, 5.4314e-13, 1.9028e-11, 2.8086e-10,\n",
      "        5.5456e-13, 2.8665e-13, 3.0429e-13, 8.0393e-12, 6.8547e-10, 9.7237e-12,\n",
      "        3.7743e-13, 4.9905e-12, 8.8987e-10, 4.2920e-12, 1.2600e-12, 5.2431e-12,\n",
      "        1.3802e-12, 1.4604e-12])}, 23: {'step': tensor(3341.), 'exp_avg': tensor([-6.2268e-05,  3.7233e-05,  6.6684e-05,  1.4781e-05,  1.0621e-05,\n",
      "        -1.1185e-07,  4.8775e-05,  1.4978e-05,  1.2085e-05,  9.3766e-05,\n",
      "         4.1490e-05, -1.0876e-04, -9.6574e-05,  1.0712e-05, -8.4634e-06,\n",
      "        -7.5223e-05,  4.9587e-05, -4.7421e-05,  1.8849e-05,  7.4360e-05,\n",
      "         1.7942e-04,  7.3179e-05, -1.8952e-05, -2.3921e-05,  3.3961e-05,\n",
      "        -2.2485e-05, -1.6886e-05, -5.9989e-05, -4.3999e-05,  2.2752e-06,\n",
      "         8.8744e-06, -7.4742e-05,  4.7540e-06, -9.8551e-05,  3.3480e-05,\n",
      "         3.7919e-05, -2.6109e-05, -3.8505e-05, -5.5303e-05, -2.5150e-05,\n",
      "        -2.9697e-05, -9.2741e-05,  6.8467e-06,  1.3381e-05,  3.5115e-05,\n",
      "         3.6275e-05,  3.9176e-05, -2.6390e-05,  2.7896e-05,  5.7506e-05,\n",
      "        -1.1243e-05,  1.2136e-05, -2.5911e-05,  8.1619e-05, -2.5186e-05,\n",
      "         4.7044e-05,  1.5787e-05,  1.1164e-05,  5.3475e-06,  3.9833e-05,\n",
      "        -5.5096e-05,  2.9152e-05, -2.4786e-05, -2.6640e-05, -4.7809e-05,\n",
      "         2.7628e-05, -1.4958e-04, -3.1562e-05,  1.0115e-05,  6.9057e-05,\n",
      "        -3.8806e-05,  2.4922e-05,  2.0601e-05, -6.1960e-05, -3.8436e-05,\n",
      "        -1.3785e-05, -6.4653e-05, -9.6402e-06, -8.7845e-06,  1.5414e-04,\n",
      "        -7.7149e-06,  5.6620e-05,  5.9844e-06,  5.1271e-05,  1.2871e-05,\n",
      "        -5.3754e-05, -2.0643e-05,  6.8276e-05, -5.4551e-05,  7.8798e-05,\n",
      "        -9.1565e-05,  1.6151e-05,  5.9915e-06, -5.3438e-05,  7.2107e-05,\n",
      "         1.3860e-05,  1.0219e-05, -2.2719e-05,  1.3388e-05,  2.6846e-05,\n",
      "        -5.0205e-05, -1.2228e-05, -8.3410e-06, -8.8640e-06, -2.1583e-05,\n",
      "        -9.9225e-05,  5.3654e-05, -5.1087e-05, -4.0680e-05,  6.3281e-05,\n",
      "        -9.4872e-06,  5.0839e-05,  5.5817e-05, -1.2998e-05, -4.7198e-05,\n",
      "        -4.5882e-06, -8.1650e-06, -1.0383e-05, -6.3623e-05,  6.9969e-05,\n",
      "         3.0109e-05, -5.4667e-05, -1.1749e-06, -1.1174e-05,  2.8659e-05,\n",
      "         3.5638e-05,  1.3757e-05,  3.9416e-05]), 'exp_avg_sq': tensor([1.2306e-08, 7.7486e-09, 1.4360e-08, 4.5984e-09, 5.6477e-09, 5.5476e-09,\n",
      "        8.0097e-09, 5.9405e-09, 1.2745e-08, 5.9504e-09, 4.2832e-09, 2.7070e-08,\n",
      "        3.7017e-08, 2.1360e-08, 5.2371e-09, 4.8159e-09, 6.1476e-09, 5.2157e-09,\n",
      "        2.4963e-09, 1.9083e-08, 3.9584e-08, 2.1213e-08, 4.3592e-09, 1.2179e-08,\n",
      "        5.3629e-09, 2.2860e-09, 8.9678e-09, 1.2517e-08, 8.5021e-09, 4.9020e-09,\n",
      "        7.8954e-09, 1.4358e-08, 4.7234e-09, 4.5979e-09, 2.6630e-08, 6.2751e-09,\n",
      "        5.0506e-09, 2.2983e-08, 9.0253e-09, 4.1338e-09, 3.9872e-08, 3.2479e-08,\n",
      "        7.3044e-09, 1.7008e-08, 8.4268e-09, 5.7758e-09, 8.4476e-09, 1.0244e-08,\n",
      "        8.5215e-09, 2.0851e-08, 1.7160e-08, 8.9283e-09, 7.9459e-09, 1.0410e-08,\n",
      "        8.7843e-09, 7.0120e-09, 3.4892e-08, 1.6765e-08, 8.2378e-09, 5.3523e-09,\n",
      "        1.4623e-08, 1.0444e-08, 9.0996e-09, 2.5885e-09, 7.5526e-09, 6.9954e-09,\n",
      "        7.2049e-09, 1.2077e-08, 1.6447e-08, 5.4479e-09, 4.1255e-09, 1.1385e-08,\n",
      "        2.9600e-08, 3.6511e-09, 1.8318e-08, 3.6652e-09, 5.2526e-08, 1.7862e-08,\n",
      "        7.1263e-09, 3.0426e-08, 8.2349e-09, 7.7816e-09, 8.6746e-09, 3.0527e-08,\n",
      "        3.2696e-09, 9.1283e-09, 6.8037e-09, 4.4738e-09, 8.3359e-09, 7.6843e-09,\n",
      "        1.1348e-07, 5.7303e-09, 1.9013e-08, 1.3466e-08, 8.5111e-09, 5.8594e-09,\n",
      "        1.3174e-08, 6.1915e-09, 1.4034e-08, 1.4587e-08, 7.9998e-09, 1.0923e-08,\n",
      "        8.0554e-08, 4.2040e-09, 6.7765e-09, 1.3542e-08, 5.0860e-09, 5.1233e-09,\n",
      "        6.9045e-09, 1.8729e-08, 1.8448e-08, 4.1546e-09, 1.8767e-08, 3.2622e-09,\n",
      "        6.0976e-09, 3.6044e-08, 2.2942e-08, 4.4093e-09, 3.6293e-09, 5.7378e-09,\n",
      "        1.2931e-08, 3.7997e-09, 3.1382e-09, 2.8704e-08, 1.1449e-08, 3.2037e-09,\n",
      "        2.5612e-09, 3.4459e-08])}, 24: {'step': tensor(3341.), 'exp_avg': tensor([ 1.8512e-04,  3.7675e-05, -3.4449e-05,  4.9767e-05, -1.6155e-04,\n",
      "         5.1023e-05,  4.4428e-05,  3.8406e-05, -1.5309e-05, -9.0580e-05,\n",
      "        -2.7594e-06,  1.8736e-05,  5.5894e-05,  1.2348e-04,  1.0236e-04,\n",
      "        -1.1652e-04, -5.5018e-05, -7.0274e-05,  1.1115e-04, -1.2427e-05,\n",
      "         5.2066e-06, -4.3571e-05,  3.8883e-05,  2.2268e-05, -1.4132e-04,\n",
      "         4.2011e-05,  7.1718e-05, -1.1911e-04, -1.6610e-05,  1.3847e-04,\n",
      "        -1.0656e-05,  4.2044e-05, -1.4067e-04,  2.3270e-05, -1.3591e-04,\n",
      "        -3.4218e-05,  6.9796e-05, -9.0947e-05, -3.5288e-05,  1.2406e-04,\n",
      "         2.3062e-05,  1.4685e-04,  9.0828e-05, -1.1732e-04,  1.0473e-04,\n",
      "         5.9824e-05, -1.2763e-04,  1.5808e-05,  2.1376e-05, -2.4716e-04,\n",
      "        -7.9214e-05, -2.0686e-05,  9.0475e-06, -8.7741e-05,  1.4081e-04,\n",
      "        -1.2749e-05,  5.5220e-05, -1.3672e-04, -7.6822e-05,  2.9382e-05,\n",
      "        -4.2822e-05,  1.5528e-04, -4.3861e-06, -5.1149e-05,  1.6494e-05,\n",
      "        -7.8004e-05, -3.9303e-05,  7.1280e-05,  1.6223e-05,  8.9111e-05,\n",
      "         4.8008e-05,  5.0623e-05, -4.4716e-05, -1.5128e-05, -1.0055e-04,\n",
      "         5.3440e-05, -9.1794e-05,  1.4240e-04, -3.4351e-05, -2.4791e-05,\n",
      "         2.8455e-05,  5.9693e-05,  8.7263e-05, -1.8299e-04, -1.1982e-04,\n",
      "         1.3087e-04, -5.8931e-05,  7.7631e-05, -2.7783e-04, -4.6258e-05,\n",
      "         1.3325e-04, -1.0704e-05, -9.9169e-05,  1.4763e-04,  1.0122e-04,\n",
      "         7.0568e-05,  1.2512e-04,  1.4728e-05,  6.4359e-05,  2.0221e-04,\n",
      "         2.7465e-05,  8.3394e-06, -8.5366e-05, -5.5328e-05,  5.4127e-05,\n",
      "        -8.8283e-05,  9.2914e-05,  3.1820e-05,  1.7329e-04, -9.9593e-05,\n",
      "        -7.5275e-05, -7.8894e-05,  1.1685e-05,  9.2920e-05,  3.2626e-05,\n",
      "         2.0445e-04,  1.0032e-04, -6.9228e-05,  9.6346e-05, -8.1497e-05,\n",
      "         5.4117e-05,  5.0255e-05, -2.9120e-05, -1.6551e-04, -1.9500e-05,\n",
      "         6.5529e-05, -6.3315e-05, -1.1440e-04]), 'exp_avg_sq': tensor([8.5170e-08, 1.2361e-07, 2.8630e-08, 5.6439e-08, 2.0825e-08, 4.3435e-08,\n",
      "        2.3831e-08, 3.4332e-08, 5.4637e-08, 3.6059e-08, 9.0360e-08, 5.1900e-08,\n",
      "        5.2064e-08, 8.4580e-08, 3.4845e-08, 3.8193e-08, 4.6560e-08, 1.3874e-08,\n",
      "        2.3970e-08, 4.2648e-08, 9.7332e-08, 6.6065e-08, 2.7059e-08, 4.4184e-08,\n",
      "        4.2613e-08, 2.3633e-08, 6.1841e-08, 9.3170e-08, 2.5925e-08, 1.8512e-07,\n",
      "        3.1768e-08, 7.4735e-08, 7.9439e-08, 1.8366e-08, 9.8899e-08, 1.2456e-07,\n",
      "        5.9619e-08, 8.9312e-08, 4.4384e-08, 7.9580e-08, 2.6046e-08, 9.7611e-08,\n",
      "        8.1921e-08, 2.8595e-08, 5.2327e-08, 2.9304e-08, 7.9008e-08, 2.6119e-08,\n",
      "        4.3607e-08, 1.1447e-07, 7.8480e-08, 1.2444e-07, 3.3919e-08, 2.2750e-07,\n",
      "        2.5459e-07, 3.9591e-08, 3.3434e-08, 1.9297e-07, 6.6870e-08, 4.3665e-08,\n",
      "        2.3270e-08, 5.5759e-08, 1.4417e-07, 4.7536e-08, 2.1546e-08, 6.2463e-08,\n",
      "        5.3066e-08, 4.2038e-08, 8.2237e-08, 5.2216e-08, 2.2521e-08, 1.3329e-07,\n",
      "        2.6436e-08, 3.0443e-08, 1.4285e-07, 3.4049e-08, 5.6001e-08, 4.5045e-08,\n",
      "        1.2230e-07, 3.2526e-07, 5.4679e-08, 1.3403e-07, 4.1186e-08, 9.9568e-08,\n",
      "        8.1838e-08, 2.7922e-07, 3.3513e-08, 1.0535e-07, 1.6388e-07, 3.9000e-08,\n",
      "        2.2002e-07, 2.9170e-08, 1.2057e-07, 1.1038e-07, 2.6914e-08, 4.0328e-08,\n",
      "        5.7409e-08, 1.9491e-08, 9.0307e-08, 1.3812e-07, 8.2421e-08, 2.3498e-07,\n",
      "        1.4638e-07, 5.3828e-08, 5.0984e-08, 9.3925e-08, 4.2963e-08, 1.1346e-08,\n",
      "        4.4656e-08, 1.3336e-07, 3.9392e-08, 3.1230e-08, 7.3506e-08, 1.6321e-08,\n",
      "        3.3635e-08, 1.5902e-07, 4.9141e-08, 3.7654e-08, 5.1559e-08, 4.2914e-08,\n",
      "        7.4374e-08, 7.6484e-08, 2.2231e-08, 1.1419e-07, 1.0234e-07, 1.0033e-08,\n",
      "        2.0484e-08, 2.8825e-07])}, 25: {'step': tensor(3341.), 'exp_avg': tensor([[-2.9813e-05,  3.8453e-06, -1.2975e-05,  ...,  2.1746e-05,\n",
      "          5.4047e-06,  1.3642e-05],\n",
      "        [ 2.1234e-06, -7.2653e-06,  1.2777e-06,  ...,  2.2858e-05,\n",
      "         -1.8754e-05,  1.5772e-05],\n",
      "        [-7.8631e-06,  2.8104e-06, -1.3965e-06,  ...,  8.1350e-06,\n",
      "         -8.7281e-06,  1.3571e-06],\n",
      "        ...,\n",
      "        [ 1.3193e-05, -5.1916e-06, -2.4622e-06,  ...,  1.4435e-05,\n",
      "          8.1014e-06,  1.4728e-05],\n",
      "        [-3.4205e-06,  3.2534e-06,  5.9745e-06,  ..., -1.6119e-05,\n",
      "         -1.6422e-07,  3.8082e-07],\n",
      "        [-4.1161e-06, -1.1143e-06,  1.7766e-05,  ..., -1.8679e-05,\n",
      "          9.5074e-06,  7.5024e-06]]), 'exp_avg_sq': tensor([[5.1220e-09, 5.6472e-09, 1.2488e-08,  ..., 9.8178e-09, 2.0251e-08,\n",
      "         2.3418e-09],\n",
      "        [5.2626e-10, 1.4981e-09, 2.6079e-10,  ..., 1.3778e-09, 5.9779e-10,\n",
      "         1.8480e-10],\n",
      "        [3.5627e-10, 4.2657e-10, 4.7136e-10,  ..., 6.5604e-10, 9.3616e-10,\n",
      "         9.1324e-11],\n",
      "        ...,\n",
      "        [1.7571e-09, 2.0382e-09, 1.6928e-09,  ..., 4.2819e-09, 1.0462e-09,\n",
      "         7.4327e-10],\n",
      "        [1.9141e-09, 5.1770e-10, 3.1762e-10,  ..., 2.4063e-09, 2.0367e-09,\n",
      "         7.2873e-10],\n",
      "        [3.9658e-09, 3.2694e-09, 4.8820e-09,  ..., 6.0998e-09, 7.3372e-09,\n",
      "         1.6731e-09]])}, 26: {'step': tensor(3341.), 'exp_avg': tensor([ 3.5878e-05,  1.5959e-05,  1.5193e-05,  3.3630e-05, -9.4009e-06,\n",
      "         1.0493e-05, -9.9024e-06,  4.1310e-06,  1.9401e-05, -1.4460e-05,\n",
      "         1.6032e-05,  2.7980e-05, -3.0842e-05, -1.3812e-05, -3.5786e-05,\n",
      "         4.9745e-05, -3.2888e-05, -8.9773e-06, -7.8906e-06, -8.1446e-06,\n",
      "         2.3720e-05,  1.1842e-05,  3.6932e-05, -4.6022e-05, -7.9360e-06,\n",
      "         4.8131e-06, -7.4178e-06, -5.4360e-06, -4.8836e-05, -4.4732e-05,\n",
      "        -7.8471e-06, -1.3527e-05, -1.4586e-05,  2.4433e-05,  4.6250e-06,\n",
      "        -3.1095e-06,  8.6311e-07,  2.3367e-06,  2.8462e-05,  6.7382e-05,\n",
      "         2.4440e-05, -7.2929e-05,  1.9792e-05, -2.0900e-06,  3.9908e-05,\n",
      "         1.1624e-05,  1.1754e-05, -4.6257e-06, -9.0774e-06,  2.2938e-06,\n",
      "        -2.4747e-05, -1.3548e-06, -6.7228e-06,  1.4136e-05,  4.7654e-06,\n",
      "         8.3556e-06,  7.5583e-05, -1.4889e-05, -4.8045e-06, -1.7412e-05,\n",
      "         4.8674e-06, -6.1642e-05, -1.0421e-05, -2.6413e-05, -1.4063e-05,\n",
      "        -2.5723e-05, -2.2603e-05, -2.9720e-06, -2.3103e-06, -7.0595e-06,\n",
      "        -3.0428e-05, -1.4815e-05,  1.6761e-05, -6.7434e-06,  1.7533e-05,\n",
      "        -4.7965e-05, -2.7166e-07,  1.8538e-05,  1.6612e-05, -1.3310e-06,\n",
      "        -9.5287e-06,  1.4820e-05, -8.0131e-06,  7.6038e-06, -9.6325e-05,\n",
      "        -4.3488e-05, -1.3502e-05, -1.3221e-05, -7.7691e-06, -5.4732e-05,\n",
      "        -2.5532e-05, -1.9641e-05,  2.5197e-06, -9.1517e-06, -4.2594e-05,\n",
      "         1.3424e-05, -8.2705e-06, -1.1222e-05, -1.2723e-05, -3.2723e-05,\n",
      "        -5.8174e-05, -1.5612e-05, -1.2897e-05,  5.6468e-06, -7.7493e-06,\n",
      "         2.0600e-05, -2.4731e-05, -4.4247e-05, -2.8256e-05, -5.8779e-05,\n",
      "        -4.2018e-05,  1.4227e-05, -2.9850e-05,  2.5115e-05, -1.0169e-05,\n",
      "         5.6830e-05, -6.1148e-06, -2.1626e-05,  1.3243e-05, -2.1213e-05,\n",
      "        -2.4495e-06,  6.0330e-06,  1.0540e-05, -1.0236e-05, -8.2878e-06,\n",
      "        -4.9114e-08,  5.0159e-06, -2.6338e-05,  4.4488e-05, -3.8062e-07,\n",
      "        -5.3032e-07,  3.7893e-06, -6.0435e-06, -2.4459e-05,  3.0576e-06,\n",
      "         2.8315e-05,  2.3602e-05, -1.3826e-05,  2.3720e-06,  7.0868e-06,\n",
      "         3.6071e-05,  2.8816e-05, -1.0307e-05,  1.5440e-06, -3.2764e-05,\n",
      "        -9.9771e-06, -2.4228e-05,  5.0687e-06, -3.6808e-06,  1.3431e-05,\n",
      "         1.1273e-05, -1.8235e-07,  2.1805e-05,  1.5728e-05, -1.3019e-05,\n",
      "        -8.8380e-06, -9.4466e-06, -1.4649e-04,  1.5157e-05, -1.0441e-04,\n",
      "        -8.4813e-06,  1.4327e-05, -3.6042e-05,  2.3035e-05, -4.1714e-05,\n",
      "         7.4292e-05, -1.0568e-04, -5.8317e-05,  2.9895e-06, -3.9718e-06,\n",
      "         2.5622e-06,  4.2234e-05,  1.3202e-05,  8.6970e-06, -3.6582e-05,\n",
      "        -5.8406e-06, -1.9902e-05,  2.5737e-05, -5.3328e-07, -8.9346e-06,\n",
      "        -1.0744e-05, -3.1758e-05,  1.0081e-05,  1.4938e-05, -1.1179e-04,\n",
      "         5.4539e-06, -8.5773e-06, -8.2131e-06,  9.7791e-06,  4.2182e-06,\n",
      "         9.0412e-06, -4.3327e-05, -1.2747e-05, -1.8761e-05, -2.1159e-05,\n",
      "        -1.6055e-05,  1.9335e-05,  1.5607e-05,  6.1500e-07,  2.2358e-05,\n",
      "        -2.0930e-06,  3.4033e-06,  4.1715e-06,  5.9148e-06,  1.5917e-05,\n",
      "        -3.8050e-06,  6.6843e-06, -1.6147e-05,  5.0692e-05, -5.8026e-06,\n",
      "         3.7911e-06, -5.2195e-05,  1.1641e-06,  8.5875e-06,  8.2432e-05,\n",
      "         1.2637e-05, -2.1328e-05,  2.8489e-06, -2.5879e-05,  5.0295e-06,\n",
      "        -1.1631e-05,  1.5369e-05,  2.3131e-05, -7.8633e-07, -2.3863e-05,\n",
      "        -1.3332e-05,  1.2334e-06,  1.0774e-07, -1.0425e-05,  6.7298e-06,\n",
      "        -3.1508e-05, -1.2160e-05,  9.6789e-05, -3.0235e-05,  1.7185e-05,\n",
      "        -3.0519e-05, -2.9141e-05, -4.8278e-06,  1.0515e-05,  1.3750e-05,\n",
      "        -1.6673e-05,  9.5782e-06, -7.7073e-06,  4.1563e-05,  1.3728e-06,\n",
      "        -6.2566e-05, -1.9029e-05,  8.0704e-06, -1.2285e-05, -2.4171e-05,\n",
      "         1.5971e-05, -3.5104e-05, -6.6112e-05, -6.3643e-06, -1.0278e-05,\n",
      "        -1.1660e-05, -1.1821e-05, -1.6166e-05, -1.1748e-05, -1.2984e-05,\n",
      "        -9.0332e-05, -2.0249e-05,  5.6404e-05, -2.1305e-06,  8.8042e-07,\n",
      "        -1.4527e-06,  1.2789e-05,  1.1464e-05, -1.4518e-05,  3.8092e-06,\n",
      "         1.2444e-05,  2.1792e-05,  3.0906e-05,  1.1076e-05,  3.1479e-05,\n",
      "        -8.8759e-06,  2.5784e-05, -1.8398e-05, -8.2469e-06, -1.9996e-05,\n",
      "         4.3555e-05, -2.4043e-06,  9.7888e-06, -1.9595e-05,  5.5007e-05,\n",
      "        -1.3040e-06, -2.3879e-05,  2.9228e-05, -1.0302e-06,  1.7762e-05,\n",
      "        -5.2306e-05, -3.6686e-06, -5.5685e-06,  4.7484e-05,  3.6073e-05,\n",
      "         3.4535e-05,  4.7638e-05, -3.4041e-06, -4.6145e-05, -6.6936e-05,\n",
      "         3.4820e-05,  2.6984e-07,  4.6460e-06, -3.7385e-06,  1.2219e-05,\n",
      "        -9.0296e-06, -4.4348e-05,  1.6566e-05, -1.4854e-04,  6.0071e-05,\n",
      "        -3.0286e-05, -1.3007e-05, -3.1557e-05,  4.6920e-06, -9.5950e-06,\n",
      "        -1.8263e-05, -2.3180e-05,  1.2480e-05, -1.0129e-05, -8.7478e-07,\n",
      "         3.6285e-05,  4.1328e-05,  4.6738e-05,  4.3838e-06, -1.2562e-07,\n",
      "        -9.9912e-05,  7.7224e-05,  1.0450e-05,  5.4055e-06, -1.9737e-05,\n",
      "        -1.5129e-05,  1.0514e-05, -7.9332e-05,  2.3887e-05, -2.4421e-05,\n",
      "        -3.2990e-05,  2.6054e-06,  1.3560e-05, -1.3946e-05,  1.0195e-05,\n",
      "        -1.2130e-05,  6.2657e-06, -2.4003e-05,  1.9715e-05, -1.7534e-05,\n",
      "        -9.3144e-07,  1.6526e-05,  1.9289e-08, -4.6482e-05, -3.4366e-06,\n",
      "        -4.6720e-06, -1.8488e-05, -1.0326e-05, -2.2967e-05,  3.1710e-05,\n",
      "        -1.8477e-05, -4.4370e-05, -1.6928e-05, -2.1850e-06,  1.2304e-06,\n",
      "        -2.6827e-05, -1.2130e-05, -1.9015e-06, -1.4303e-05,  1.2271e-05,\n",
      "        -8.1763e-06,  1.5903e-05,  1.1419e-05,  1.5108e-05,  1.1213e-05,\n",
      "         4.5504e-06,  6.9736e-06, -1.9067e-05,  2.9929e-06,  2.4372e-06,\n",
      "        -1.1566e-04,  2.6412e-07,  4.3448e-05, -7.7600e-06, -2.8484e-06,\n",
      "        -2.0170e-05,  2.8384e-06,  1.1007e-05,  1.0875e-05,  1.4644e-05,\n",
      "         1.3392e-06,  9.8203e-06,  8.7438e-05,  3.5281e-06,  4.6616e-05,\n",
      "        -1.8260e-05, -5.8523e-06,  4.1440e-06, -2.4605e-06, -1.1338e-05,\n",
      "         2.5217e-05, -1.4567e-07,  1.0566e-05,  2.5627e-06, -7.1361e-06,\n",
      "        -1.2011e-05,  4.1437e-06, -2.4256e-05,  1.0960e-05, -6.4445e-06,\n",
      "        -2.0502e-05, -1.2780e-05,  4.8931e-07,  2.6156e-05,  5.6114e-05,\n",
      "         1.9447e-06,  3.7950e-05, -1.0295e-05,  4.0136e-06, -5.2455e-05,\n",
      "        -1.4443e-05,  1.5400e-05,  1.3482e-06,  1.1808e-05,  1.9280e-06,\n",
      "        -2.0093e-05,  1.1512e-05, -6.1538e-06,  7.9083e-06, -2.1903e-05,\n",
      "        -1.2768e-05, -7.3845e-06, -2.2127e-05,  2.6267e-05,  1.5438e-05,\n",
      "        -2.2005e-05, -4.4988e-05, -1.0624e-05,  2.2664e-05, -8.4929e-06,\n",
      "        -4.3158e-06, -1.6824e-05, -5.5016e-06,  8.5100e-06, -8.2694e-05,\n",
      "        -5.7117e-06,  4.0191e-05,  1.8286e-05, -7.4095e-06,  1.0056e-05,\n",
      "        -3.4762e-05, -8.7941e-06, -1.3403e-05,  5.9931e-05, -7.8486e-06,\n",
      "         2.3884e-06,  1.4529e-05,  4.4041e-05, -1.0978e-05,  4.3260e-05,\n",
      "        -3.1424e-06,  2.2413e-05,  1.3767e-05,  1.4609e-05,  1.7666e-05,\n",
      "        -1.5141e-05, -4.1485e-05, -1.1624e-05,  1.5484e-07, -1.0220e-06,\n",
      "        -4.4145e-06, -2.0442e-06, -2.8550e-05, -1.6882e-05,  1.2868e-05,\n",
      "        -7.1654e-07,  5.7910e-05, -6.1161e-05, -3.2175e-06,  2.1642e-06,\n",
      "         4.0458e-06,  8.7343e-06,  4.3647e-05, -1.9589e-05, -6.1858e-06,\n",
      "         1.6667e-05,  1.9967e-07, -1.5624e-05,  1.5841e-05, -3.9839e-05,\n",
      "         1.7519e-05,  3.4203e-05, -1.3761e-05,  2.5340e-05,  3.1034e-05,\n",
      "        -6.1586e-06, -2.5108e-05, -7.8723e-06, -1.6461e-07, -1.6500e-05,\n",
      "        -9.0634e-06, -6.1599e-06,  4.7437e-05, -1.1066e-05,  2.6757e-05,\n",
      "        -2.1162e-05, -3.1544e-05,  2.6541e-06,  3.1677e-06,  3.7610e-05,\n",
      "        -4.5623e-06, -1.0014e-05, -3.3841e-05,  1.4902e-05, -2.2028e-06,\n",
      "         2.3549e-06,  3.1943e-07]), 'exp_avg_sq': tensor([4.9807e-08, 3.0695e-09, 2.5343e-09, 5.1493e-09, 2.7410e-09, 9.1055e-09,\n",
      "        2.3390e-08, 1.3410e-08, 6.3242e-09, 1.8178e-09, 8.7091e-09, 2.4719e-09,\n",
      "        2.8536e-09, 1.0295e-09, 8.2977e-09, 7.5704e-09, 6.2022e-09, 7.6880e-09,\n",
      "        5.2021e-10, 7.3888e-10, 2.3027e-09, 3.3944e-08, 6.0866e-09, 1.2622e-08,\n",
      "        1.4645e-08, 2.1523e-08, 1.3344e-08, 4.1618e-09, 6.7527e-09, 4.3854e-08,\n",
      "        1.5428e-09, 1.6086e-08, 3.8856e-09, 8.4569e-09, 5.0096e-10, 8.9424e-09,\n",
      "        4.3434e-10, 1.4122e-08, 1.2797e-08, 1.0729e-08, 5.3146e-09, 3.8343e-08,\n",
      "        2.4356e-08, 5.4007e-09, 6.0133e-09, 9.1669e-10, 1.1152e-08, 3.9050e-09,\n",
      "        2.2474e-08, 5.6179e-10, 2.8007e-09, 2.6210e-10, 6.9247e-09, 7.6315e-09,\n",
      "        1.9737e-08, 4.0729e-10, 1.0654e-08, 2.2476e-09, 4.9915e-09, 6.7646e-09,\n",
      "        1.1722e-09, 2.7029e-08, 6.4276e-09, 2.1064e-09, 1.0702e-09, 2.6037e-09,\n",
      "        1.3093e-09, 8.8942e-10, 8.7026e-09, 7.2834e-09, 1.1565e-09, 1.1395e-08,\n",
      "        6.5765e-09, 2.8733e-10, 1.8205e-08, 1.0899e-08, 1.0437e-08, 9.6695e-09,\n",
      "        1.1695e-09, 1.1190e-08, 5.0267e-09, 6.8609e-09, 2.3666e-09, 5.2115e-10,\n",
      "        1.4819e-08, 1.2293e-08, 2.2697e-09, 5.3381e-09, 3.1908e-09, 7.9917e-09,\n",
      "        3.3439e-09, 3.1497e-09, 7.3076e-09, 2.1348e-09, 4.2430e-09, 1.5261e-09,\n",
      "        2.2263e-09, 8.8050e-09, 3.6276e-09, 5.0317e-09, 1.7868e-08, 1.4460e-09,\n",
      "        1.3279e-08, 2.6771e-09, 5.4249e-10, 2.2574e-08, 1.6764e-08, 1.0731e-08,\n",
      "        7.0523e-09, 2.1718e-09, 3.0142e-09, 6.5657e-10, 4.0300e-09, 1.0874e-08,\n",
      "        3.5184e-08, 3.5994e-08, 4.3295e-09, 4.2428e-09, 4.4551e-10, 2.6760e-09,\n",
      "        2.5056e-08, 2.8512e-09, 4.2995e-09, 5.1058e-10, 6.0825e-09, 1.5174e-08,\n",
      "        4.0458e-09, 7.4497e-09, 1.3573e-08, 7.3092e-10, 5.9801e-09, 2.8220e-09,\n",
      "        1.9809e-09, 8.6060e-09, 5.2493e-09, 9.5212e-09, 9.6071e-10, 1.2662e-08,\n",
      "        9.2304e-09, 1.1908e-08, 3.4471e-08, 4.4166e-09, 1.3701e-08, 5.3008e-09,\n",
      "        4.2313e-08, 8.4577e-10, 1.3057e-09, 2.1992e-09, 6.6858e-09, 1.1737e-08,\n",
      "        2.7289e-08, 1.4984e-09, 8.9262e-09, 6.5733e-09, 3.3876e-09, 3.4355e-09,\n",
      "        3.9510e-10, 9.7439e-08, 1.1055e-08, 6.0862e-08, 1.2400e-08, 1.8247e-08,\n",
      "        4.0384e-09, 8.4939e-09, 2.3767e-09, 7.7692e-09, 2.5474e-08, 1.7812e-08,\n",
      "        4.1640e-10, 4.3054e-09, 5.9777e-10, 4.4723e-09, 4.6540e-09, 1.1398e-09,\n",
      "        3.5438e-09, 8.5187e-09, 1.5433e-09, 1.1037e-09, 2.2850e-09, 9.2038e-09,\n",
      "        4.8166e-09, 5.4074e-09, 1.0107e-09, 6.0225e-09, 6.4522e-08, 1.8926e-10,\n",
      "        4.7875e-09, 1.8598e-08, 2.1559e-08, 4.1393e-09, 1.0617e-09, 2.9420e-08,\n",
      "        1.3919e-08, 1.2773e-08, 4.6644e-09, 1.4487e-09, 1.1831e-09, 1.8827e-08,\n",
      "        7.8119e-09, 6.1395e-09, 6.2313e-09, 6.5483e-09, 5.7616e-10, 2.2594e-08,\n",
      "        5.5763e-09, 5.7093e-10, 1.0354e-08, 3.1535e-08, 1.0888e-08, 5.1225e-09,\n",
      "        4.0992e-08, 2.2543e-08, 1.0085e-08, 4.2910e-10, 2.3789e-07, 1.5196e-08,\n",
      "        4.9380e-09, 1.6316e-08, 2.1932e-08, 2.8490e-09, 1.5879e-09, 4.8286e-10,\n",
      "        3.1424e-08, 1.1062e-08, 5.2931e-09, 9.3378e-09, 1.6984e-09, 3.2522e-09,\n",
      "        4.2583e-09, 2.3802e-09, 6.8900e-09, 7.0218e-10, 1.1333e-08, 1.7803e-08,\n",
      "        7.1468e-09, 1.2257e-07, 2.9514e-09, 2.9923e-10, 9.6776e-08, 4.4537e-09,\n",
      "        1.4898e-08, 1.5459e-08, 4.0028e-09, 1.3375e-08, 9.7060e-10, 1.2277e-07,\n",
      "        4.4679e-09, 2.3584e-09, 2.2325e-09, 2.9663e-09, 9.7614e-09, 2.9267e-08,\n",
      "        1.8932e-08, 3.2847e-09, 1.2176e-09, 2.3404e-10, 2.1739e-09, 4.5352e-10,\n",
      "        1.1155e-08, 1.3922e-09, 4.7561e-08, 3.3030e-09, 2.7828e-08, 2.9079e-10,\n",
      "        3.3735e-10, 6.4861e-09, 2.7597e-08, 8.7444e-10, 2.6361e-09, 9.4755e-09,\n",
      "        7.3820e-10, 1.0086e-08, 6.5709e-09, 2.0113e-10, 7.0209e-09, 1.9439e-08,\n",
      "        6.4727e-09, 5.7292e-09, 2.4460e-08, 1.8340e-08, 7.5932e-09, 5.9381e-09,\n",
      "        1.2983e-08, 2.8398e-09, 3.4488e-08, 2.8611e-09, 4.5472e-09, 3.5945e-09,\n",
      "        2.5703e-09, 9.1048e-10, 7.0201e-09, 1.4914e-09, 1.3000e-09, 2.4986e-08,\n",
      "        9.7719e-09, 8.7179e-09, 1.1630e-08, 2.4573e-10, 1.3061e-08, 9.0331e-09,\n",
      "        1.2426e-08, 6.6330e-10, 9.5771e-10, 1.7701e-08, 4.3933e-09, 9.0371e-10,\n",
      "        2.9976e-09, 1.7058e-08, 9.7653e-08, 9.9466e-08, 3.9815e-09, 2.1176e-08,\n",
      "        7.9724e-09, 2.9234e-08, 4.1450e-10, 7.5059e-09, 4.4045e-09, 3.9550e-09,\n",
      "        5.6863e-09, 2.4490e-09, 3.1669e-08, 5.0546e-09, 7.4551e-09, 4.3034e-09,\n",
      "        5.3378e-09, 7.9869e-09, 5.4052e-08, 9.1486e-09, 3.1347e-10, 8.4476e-09,\n",
      "        2.0369e-08, 1.6882e-09, 4.1306e-08, 3.2649e-09, 2.7851e-08, 2.3836e-08,\n",
      "        3.0585e-09, 1.5991e-09, 1.0405e-08, 1.9108e-10, 1.1985e-09, 3.4570e-09,\n",
      "        1.2935e-09, 7.8056e-09, 1.3257e-08, 7.4342e-09, 1.1316e-08, 4.1738e-10,\n",
      "        2.5569e-08, 7.2891e-09, 1.2305e-08, 1.9584e-09, 2.2118e-09, 3.3349e-09,\n",
      "        2.0998e-08, 1.0512e-09, 6.6577e-09, 2.7259e-09, 2.1227e-08, 8.9686e-09,\n",
      "        2.8696e-08, 6.0469e-09, 6.2175e-10, 3.1697e-09, 1.3091e-08, 7.8264e-09,\n",
      "        9.1534e-10, 2.0166e-08, 5.5671e-09, 1.2336e-08, 3.7986e-09, 2.9338e-08,\n",
      "        1.4707e-08, 1.8664e-09, 8.4122e-09, 1.7005e-08, 2.8045e-09, 4.7485e-09,\n",
      "        1.0289e-09, 1.1057e-09, 1.2788e-08, 5.9422e-10, 4.0237e-09, 2.3598e-08,\n",
      "        1.6701e-08, 4.2709e-09, 2.2157e-08, 6.5198e-08, 2.1350e-08, 6.8871e-09,\n",
      "        4.0029e-09, 1.1008e-09, 4.9849e-09, 3.8622e-09, 1.1639e-09, 2.5582e-08,\n",
      "        4.5049e-09, 5.6769e-09, 8.7833e-09, 3.5250e-09, 1.0008e-08, 1.7579e-08,\n",
      "        1.3589e-08, 6.9754e-09, 4.8676e-09, 5.6404e-09, 2.2753e-09, 2.0950e-09,\n",
      "        2.2859e-08, 2.3964e-08, 1.1334e-09, 1.5030e-08, 9.4080e-10, 3.0988e-10,\n",
      "        1.3165e-08, 3.1231e-09, 5.7933e-08, 1.7258e-08, 9.2620e-10, 9.4673e-09,\n",
      "        2.6560e-08, 1.9038e-09, 3.8035e-09, 3.4208e-09, 6.8740e-10, 4.6610e-10,\n",
      "        2.7807e-09, 3.6980e-08, 3.0046e-08, 4.6229e-08, 1.4009e-08, 2.0401e-09,\n",
      "        4.6003e-10, 6.2418e-09, 5.8506e-09, 4.0869e-09, 6.3466e-09, 1.9426e-09,\n",
      "        1.3869e-10, 1.3393e-08, 6.1202e-09, 1.5275e-08, 2.9701e-08, 1.6632e-09,\n",
      "        1.2760e-09, 8.1427e-09, 9.8790e-09, 2.5839e-08, 5.3168e-08, 3.9893e-09,\n",
      "        1.0386e-08, 4.4087e-09, 1.0762e-07, 1.6464e-09, 2.3229e-08, 3.9696e-09,\n",
      "        3.0018e-09, 8.4833e-10, 1.3108e-09, 1.2609e-08, 2.1388e-08, 2.9680e-09,\n",
      "        8.2669e-10, 3.7969e-09, 3.5927e-10, 2.2099e-10, 5.9979e-10, 2.8699e-09,\n",
      "        7.8577e-09, 8.7494e-09, 1.2786e-09, 4.4182e-08, 8.9041e-09, 1.9390e-09,\n",
      "        3.8271e-09, 1.5713e-09, 1.4096e-08, 6.2327e-08, 8.2030e-10, 1.2465e-08,\n",
      "        3.5219e-08, 6.2521e-09, 2.1571e-09, 5.4120e-09, 1.0367e-08, 1.6636e-08,\n",
      "        2.6534e-08, 2.5302e-08, 3.8350e-08, 1.4963e-09, 5.6189e-10, 1.2439e-09,\n",
      "        1.5963e-09, 3.3424e-09, 1.4777e-08, 1.6245e-09, 1.1496e-09, 1.4233e-08,\n",
      "        1.0072e-08, 6.3023e-09, 1.0786e-08, 1.6362e-08, 6.8158e-09, 1.8551e-09,\n",
      "        7.8850e-09, 1.6821e-08, 2.7439e-08, 5.2302e-09, 1.5921e-08, 8.1017e-09,\n",
      "        4.0996e-09, 1.9535e-08])}, 27: {'step': tensor(3341.), 'exp_avg': tensor([[ 5.0078e-07, -4.3333e-06, -1.2805e-06,  ...,  1.7998e-06,\n",
      "          1.2764e-06, -6.3854e-06],\n",
      "        [-1.7237e-06, -5.5132e-06, -2.7886e-06,  ...,  2.2544e-06,\n",
      "          2.9755e-06, -1.3849e-05],\n",
      "        [ 6.0024e-06, -7.4875e-06, -3.2576e-06,  ...,  1.4003e-05,\n",
      "          2.2627e-05, -2.3261e-05],\n",
      "        ...,\n",
      "        [ 1.4156e-06,  4.0404e-06, -8.1275e-06,  ...,  4.0266e-06,\n",
      "         -8.6111e-06,  1.4057e-05],\n",
      "        [ 4.0145e-06, -3.0111e-06, -2.6840e-06,  ...,  6.8893e-06,\n",
      "          1.0971e-05, -8.3365e-06],\n",
      "        [ 5.4306e-06, -3.3925e-06, -6.1730e-06,  ...,  1.6187e-05,\n",
      "          8.6864e-06,  3.6366e-06]]), 'exp_avg_sq': tensor([[5.1763e-09, 5.6710e-10, 4.2332e-11,  ..., 6.2975e-10, 6.8447e-10,\n",
      "         5.7311e-10],\n",
      "        [6.3586e-09, 4.9001e-10, 4.0401e-11,  ..., 4.7951e-10, 5.0441e-10,\n",
      "         1.4302e-09],\n",
      "        [7.8385e-09, 2.8780e-09, 2.6532e-10,  ..., 2.6944e-09, 3.3270e-09,\n",
      "         1.6740e-09],\n",
      "        ...,\n",
      "        [1.1364e-08, 2.0236e-09, 2.3889e-10,  ..., 1.8972e-09, 2.4040e-09,\n",
      "         1.4510e-09],\n",
      "        [3.7479e-09, 1.0509e-09, 9.6334e-11,  ..., 1.0147e-09, 1.2648e-09,\n",
      "         7.3564e-10],\n",
      "        [3.5440e-09, 1.1079e-09, 9.6366e-11,  ..., 1.1908e-09, 1.6418e-09,\n",
      "         7.0509e-10]])}, 28: {'step': tensor(3341.), 'exp_avg': tensor([-1.6132e-05,  6.5344e-06, -6.3097e-05, -1.4628e-04,  1.1478e-05,\n",
      "         5.9377e-05, -1.7166e-05,  2.4607e-05, -7.5302e-06, -5.7631e-05,\n",
      "        -4.2489e-05,  1.0257e-05, -6.0932e-05,  2.9085e-05, -8.7322e-05,\n",
      "         3.0617e-05,  2.2566e-05,  3.0045e-07,  1.7316e-05, -2.5654e-05,\n",
      "         6.4715e-05,  1.3625e-04, -1.0389e-04, -3.2607e-05, -1.0428e-04,\n",
      "        -1.9644e-05, -9.9895e-06,  3.7984e-05,  8.7356e-06, -1.3446e-04,\n",
      "         2.9161e-05,  2.7822e-05,  5.0461e-05, -3.5008e-05, -4.3292e-05,\n",
      "         7.4020e-05, -9.7403e-06, -2.6171e-05, -2.1422e-04, -3.0103e-05,\n",
      "         5.2818e-05,  6.7122e-05,  7.5783e-06,  3.9518e-05, -6.8044e-05,\n",
      "         1.6240e-04,  4.2476e-05,  1.8169e-04, -7.2652e-05, -5.8086e-05,\n",
      "         5.7708e-05, -1.0088e-04,  6.2993e-05,  4.6221e-05, -2.0346e-04,\n",
      "        -5.8456e-05, -1.1822e-05,  3.8831e-05,  1.4878e-04,  3.0480e-05,\n",
      "        -2.6134e-05, -8.7771e-05,  6.4706e-05, -9.7414e-06,  9.6550e-05,\n",
      "         1.7243e-04,  8.4395e-05, -1.3420e-04, -5.0941e-05, -5.5112e-05,\n",
      "         1.0163e-04,  9.2691e-05,  1.0692e-04,  5.1949e-05, -5.5381e-05,\n",
      "        -7.2578e-05, -8.5507e-06, -2.1425e-05,  2.4181e-05,  9.1620e-05,\n",
      "        -1.3011e-04,  3.1347e-05,  1.1556e-04,  4.8077e-05,  5.5684e-05,\n",
      "        -1.5594e-04,  1.6202e-04,  5.9566e-05,  4.1287e-06,  5.3317e-05,\n",
      "         3.3576e-05,  2.0039e-05, -4.8993e-05, -4.0152e-05,  1.9499e-05,\n",
      "         7.5624e-06, -2.4259e-05, -1.5285e-05,  9.7551e-05, -4.9532e-06,\n",
      "        -2.6784e-05, -2.9754e-05,  7.6142e-05,  2.5042e-05,  5.3774e-05,\n",
      "        -9.8876e-06,  1.8894e-05, -9.5479e-05,  3.7603e-05, -8.7055e-05,\n",
      "         8.3208e-06,  1.2976e-04,  2.7987e-05,  6.2428e-05,  2.9551e-05,\n",
      "        -8.6032e-05,  2.5114e-05, -1.0213e-05, -5.5886e-05, -8.3235e-05,\n",
      "        -1.6929e-04, -2.3188e-04,  7.1707e-06,  1.1816e-04, -4.2332e-05,\n",
      "        -1.0443e-05, -4.2015e-05, -1.1796e-05]), 'exp_avg_sq': tensor([5.8787e-08, 4.7038e-08, 2.8936e-07, 7.6154e-07, 7.2274e-08, 3.3629e-08,\n",
      "        1.1917e-06, 7.3034e-08, 3.7457e-08, 1.9548e-07, 1.7802e-08, 1.9430e-08,\n",
      "        1.5320e-07, 2.7426e-08, 1.1703e-07, 4.7073e-08, 5.1676e-08, 6.7752e-08,\n",
      "        5.2307e-08, 5.0086e-08, 1.3250e-07, 4.9686e-07, 1.0937e-07, 2.1401e-07,\n",
      "        1.2215e-07, 8.8289e-08, 7.3472e-08, 5.7462e-08, 2.9709e-08, 1.2308e-07,\n",
      "        3.1319e-07, 3.0746e-08, 6.1183e-08, 4.8505e-07, 2.1966e-07, 3.7531e-08,\n",
      "        1.0511e-07, 5.1839e-07, 5.9539e-06, 3.4773e-08, 3.3083e-08, 3.4162e-07,\n",
      "        5.9004e-08, 2.5018e-08, 7.6171e-07, 2.6274e-08, 1.1760e-07, 3.0126e-08,\n",
      "        9.7218e-08, 3.1017e-07, 3.6353e-07, 1.4274e-07, 1.8117e-07, 4.1012e-07,\n",
      "        2.7679e-07, 7.3681e-08, 1.7562e-07, 1.3999e-07, 5.3153e-07, 4.1223e-08,\n",
      "        3.6830e-08, 2.0137e-06, 2.2588e-08, 6.1947e-08, 1.7810e-07, 8.9431e-07,\n",
      "        6.2902e-08, 3.5427e-07, 8.1971e-08, 4.9517e-08, 3.5525e-07, 2.1951e-07,\n",
      "        7.3326e-08, 1.1247e-07, 2.2937e-08, 2.1528e-07, 3.3593e-07, 2.2931e-07,\n",
      "        4.3232e-08, 8.3136e-08, 3.9077e-07, 2.3818e-08, 1.3641e-07, 9.6248e-08,\n",
      "        7.6422e-08, 8.2024e-07, 4.6029e-07, 2.1564e-08, 2.7370e-08, 6.5530e-08,\n",
      "        4.8208e-08, 7.6042e-08, 2.3683e-08, 8.0662e-07, 7.0020e-08, 7.1129e-08,\n",
      "        3.3373e-08, 4.9460e-08, 7.9473e-08, 2.4262e-07, 2.4485e-07, 6.9321e-08,\n",
      "        2.3894e-07, 7.9422e-08, 4.9981e-08, 2.8317e-07, 1.4175e-08, 2.9288e-08,\n",
      "        1.3044e-07, 1.5722e-07, 8.4481e-08, 2.7023e-07, 5.0862e-08, 1.8386e-06,\n",
      "        4.3384e-07, 1.9606e-07, 2.3255e-07, 6.6352e-08, 3.3413e-08, 1.7022e-07,\n",
      "        4.7428e-07, 1.3223e-06, 2.6653e-08, 2.2235e-07, 5.9869e-08, 1.9630e-07,\n",
      "        1.0924e-07, 1.3067e-07])}, 29: {'step': tensor(3341.), 'exp_avg': tensor([-1.1152e-05,  5.2841e-05, -1.6728e-05,  7.8743e-05, -3.3739e-05,\n",
      "        -1.9614e-05,  3.8127e-05, -6.6860e-05, -3.7950e-05,  3.4813e-05,\n",
      "         5.3622e-05,  3.9035e-05,  5.8834e-05,  3.2497e-05,  1.7867e-05,\n",
      "         3.2603e-05, -6.7421e-06, -3.2677e-06,  2.5066e-05,  2.0730e-05,\n",
      "        -8.9494e-05, -3.5966e-05,  9.4470e-06, -2.7622e-05,  1.1045e-05,\n",
      "         1.4604e-05,  2.9307e-05, -1.5604e-05,  1.9871e-05,  3.3394e-05,\n",
      "         1.9062e-05, -8.5721e-06, -7.8799e-05,  3.7170e-05, -3.7450e-06,\n",
      "         2.4812e-05,  2.1327e-05, -5.6399e-05, -8.1956e-05,  1.7237e-05,\n",
      "         1.5415e-05, -3.3042e-05, -3.3385e-05, -1.7099e-05, -2.8219e-05,\n",
      "        -2.6888e-05,  7.0599e-05,  7.4614e-05,  1.0057e-06,  2.2272e-05,\n",
      "        -5.5730e-05, -2.8530e-05, -2.1315e-06, -7.3223e-05,  8.5239e-05,\n",
      "        -6.2757e-05,  1.7847e-05,  4.4652e-05, -2.6244e-05, -2.7318e-05,\n",
      "        -1.6998e-05,  8.3477e-06, -5.9451e-05, -6.9249e-06, -4.1186e-05,\n",
      "        -4.2558e-05,  2.0610e-05, -6.5469e-06, -4.9583e-05,  1.7495e-05,\n",
      "         4.3351e-05,  3.8555e-05, -1.0926e-05,  2.8321e-05,  5.3653e-05,\n",
      "         1.1922e-06, -6.5352e-05, -3.2767e-05,  5.8036e-05, -8.4484e-05,\n",
      "        -2.6332e-06,  1.9258e-05,  1.4346e-05, -3.7743e-05, -7.6161e-06,\n",
      "         8.2755e-05,  1.2423e-05,  5.5724e-07,  2.4533e-05, -6.3205e-06,\n",
      "        -5.9213e-05, -2.1674e-05, -1.8934e-05, -2.6926e-05,  3.8571e-05,\n",
      "        -4.3418e-06,  8.1077e-06,  9.7040e-06,  6.2451e-05,  3.2003e-05,\n",
      "        -3.0169e-05,  8.5847e-05,  9.9001e-05,  3.8599e-05, -4.4833e-05,\n",
      "        -1.2655e-05,  6.3380e-05,  1.7834e-05, -3.5827e-05, -1.1907e-05,\n",
      "        -3.3464e-05, -1.8623e-05, -2.6520e-05, -2.5361e-05,  3.4862e-05,\n",
      "        -7.2676e-05,  3.0155e-05, -1.6516e-05,  1.0295e-05,  9.4803e-06,\n",
      "        -5.7734e-05, -5.4690e-07, -7.9231e-05, -2.0648e-05,  5.8558e-06,\n",
      "        -2.2194e-05,  1.1316e-06, -5.1783e-06]), 'exp_avg_sq': tensor([9.1145e-09, 1.7933e-08, 4.0201e-09, 6.6009e-09, 5.8404e-08, 6.2358e-09,\n",
      "        1.3752e-09, 4.6186e-09, 1.3509e-08, 2.9834e-08, 1.2057e-08, 2.9450e-08,\n",
      "        7.0318e-09, 7.8098e-09, 2.0529e-08, 1.5292e-08, 6.0367e-09, 7.4743e-09,\n",
      "        1.7593e-09, 5.6539e-09, 7.1642e-09, 3.2959e-09, 2.9106e-09, 5.6741e-09,\n",
      "        3.1600e-09, 5.8439e-09, 1.5449e-08, 1.2909e-08, 2.4412e-09, 5.4726e-09,\n",
      "        6.3327e-09, 5.3404e-09, 5.8478e-09, 3.3851e-09, 1.5613e-08, 1.3277e-08,\n",
      "        2.3886e-09, 4.8032e-09, 3.9736e-08, 1.0313e-08, 8.1326e-09, 3.2286e-09,\n",
      "        4.1020e-09, 1.1127e-08, 1.7987e-08, 2.2885e-09, 3.6656e-09, 4.4741e-08,\n",
      "        2.9028e-09, 1.6608e-08, 5.2450e-09, 2.3501e-09, 4.6207e-09, 2.2909e-08,\n",
      "        8.4539e-09, 6.4102e-09, 1.2349e-08, 1.2648e-08, 6.4482e-09, 7.1243e-09,\n",
      "        1.8266e-09, 2.7709e-09, 1.2772e-08, 1.7737e-08, 7.2412e-09, 5.8858e-09,\n",
      "        4.0163e-09, 1.9208e-09, 6.7602e-09, 3.6348e-09, 2.8536e-09, 3.5372e-09,\n",
      "        2.0923e-08, 7.2214e-09, 9.5859e-09, 5.5404e-09, 1.5246e-08, 6.7759e-08,\n",
      "        5.9716e-09, 7.3488e-09, 1.2280e-09, 1.3128e-08, 1.9189e-09, 2.0824e-08,\n",
      "        3.1042e-09, 1.7654e-08, 3.7596e-09, 7.6521e-09, 1.1091e-08, 2.9194e-08,\n",
      "        5.6988e-09, 4.5419e-09, 2.5313e-08, 1.5846e-08, 1.2572e-08, 2.9668e-08,\n",
      "        3.8877e-09, 2.4545e-08, 1.7149e-08, 2.6740e-09, 3.9001e-09, 2.1895e-08,\n",
      "        8.0876e-09, 3.1282e-09, 2.4596e-09, 1.3658e-09, 4.1329e-09, 3.7738e-08,\n",
      "        2.5034e-09, 7.5271e-09, 6.7025e-09, 9.4488e-09, 3.0621e-09, 4.4597e-09,\n",
      "        7.7754e-09, 1.4372e-08, 8.9817e-09, 4.2750e-09, 3.2578e-09, 1.6797e-08,\n",
      "        9.4215e-09, 5.1634e-09, 2.8954e-09, 1.0615e-08, 6.2893e-09, 1.3086e-08,\n",
      "        2.4450e-09, 1.1239e-08])}, 30: {'step': tensor(3341.), 'exp_avg': tensor([ 4.1437e-05,  1.2894e-04,  3.4635e-05,  1.1139e-04, -5.9836e-06,\n",
      "        -1.2512e-04, -6.5465e-06,  4.4214e-05,  3.1844e-05,  8.7720e-06,\n",
      "         9.5772e-05,  7.5277e-05, -3.1948e-05,  1.5630e-05, -1.4431e-05,\n",
      "         2.9581e-05,  8.2146e-05, -1.3322e-05, -4.2644e-05, -3.7224e-05,\n",
      "         1.2460e-04,  1.8200e-05, -3.3716e-05, -2.1102e-05, -3.0612e-05,\n",
      "        -2.4933e-05, -1.9074e-05,  5.6253e-05, -2.2388e-05, -1.7007e-04,\n",
      "         1.5573e-05, -2.6737e-05,  1.2613e-04, -2.5174e-05, -4.6513e-05,\n",
      "         1.8898e-04,  6.8743e-05, -7.4629e-05,  8.8379e-05,  1.9868e-05,\n",
      "         1.8543e-05,  1.6772e-05, -2.3636e-05,  3.7946e-05, -2.3292e-05,\n",
      "         2.1003e-05,  4.4916e-05, -3.6172e-05, -5.4845e-06, -7.0626e-05,\n",
      "         7.8581e-05,  7.8920e-05,  6.5027e-05,  1.5837e-04, -2.5118e-04,\n",
      "        -6.8867e-05,  3.5321e-06,  2.4378e-04,  5.4050e-05, -1.8349e-05,\n",
      "        -4.3612e-05,  1.3750e-05,  1.4721e-04,  9.7277e-05, -9.2654e-06,\n",
      "         4.5950e-05, -1.2448e-05,  3.6963e-05, -1.1488e-05, -5.4151e-06,\n",
      "        -5.9221e-05,  1.1721e-04,  4.3444e-05, -3.8581e-05, -7.1233e-05,\n",
      "        -5.1259e-05, -8.1060e-05, -3.2016e-05,  1.4057e-04,  1.7728e-04,\n",
      "        -2.8254e-05, -1.1526e-05, -3.8259e-06,  1.2501e-04, -7.6497e-05,\n",
      "        -2.3989e-04,  1.8873e-05, -1.9103e-04,  2.0375e-04,  3.9207e-06,\n",
      "         5.3941e-05, -9.9092e-05, -2.0254e-05, -4.3139e-05,  2.5554e-05,\n",
      "        -2.2462e-05,  2.3925e-05, -2.3620e-05,  1.9504e-04, -6.5795e-06,\n",
      "        -1.3362e-04, -1.8414e-04,  4.7360e-05,  7.2333e-05, -1.1926e-06,\n",
      "         5.0754e-05,  7.6356e-05, -4.0723e-06,  4.9937e-05, -9.9418e-05,\n",
      "         9.1010e-06,  1.0888e-04, -3.7274e-05, -2.3158e-05,  7.7975e-06,\n",
      "         1.1624e-04, -9.2884e-06, -1.9087e-05, -8.4666e-05, -4.6790e-05,\n",
      "         1.3477e-05, -3.3753e-05,  6.6749e-05,  3.6253e-05, -1.4168e-04,\n",
      "         3.7666e-07,  4.3171e-05, -1.4073e-04]), 'exp_avg_sq': tensor([1.9091e-08, 1.0499e-07, 1.7630e-08, 6.3029e-08, 2.2035e-08, 6.9775e-08,\n",
      "        2.1525e-08, 2.0152e-08, 2.2897e-08, 4.7356e-08, 1.6025e-07, 1.8766e-08,\n",
      "        2.6691e-08, 8.7752e-09, 3.9362e-08, 3.4634e-08, 4.4220e-08, 1.7024e-08,\n",
      "        1.4104e-08, 3.5732e-08, 1.1839e-07, 1.7974e-08, 1.7420e-08, 3.2910e-08,\n",
      "        2.4276e-08, 1.8146e-08, 3.5508e-08, 2.7509e-08, 7.2021e-09, 1.5402e-07,\n",
      "        1.9349e-08, 4.7923e-08, 1.1349e-07, 9.7954e-09, 3.4554e-08, 1.4130e-07,\n",
      "        3.2780e-08, 2.6453e-08, 9.0236e-08, 3.1533e-08, 1.4949e-08, 6.6154e-09,\n",
      "        2.1948e-08, 2.7181e-08, 4.9225e-08, 4.2529e-08, 1.0454e-08, 2.6007e-08,\n",
      "        1.1134e-08, 4.6680e-08, 8.1020e-08, 2.3708e-08, 8.0363e-09, 1.7585e-07,\n",
      "        2.4666e-07, 1.5761e-08, 1.1950e-08, 2.5516e-07, 6.9807e-08, 2.0701e-08,\n",
      "        6.0755e-09, 3.4712e-08, 1.6857e-07, 8.2335e-08, 8.2873e-09, 6.8686e-08,\n",
      "        1.8564e-08, 2.0034e-08, 4.5899e-08, 2.1453e-08, 2.8744e-08, 5.3136e-08,\n",
      "        1.4617e-08, 1.8606e-08, 4.5635e-08, 3.2333e-08, 8.4635e-09, 2.9889e-08,\n",
      "        9.5914e-08, 1.9028e-07, 6.6146e-09, 4.6009e-08, 1.7381e-08, 6.6758e-08,\n",
      "        2.3596e-08, 2.4218e-07, 4.7077e-08, 1.5875e-07, 2.2558e-07, 3.8432e-08,\n",
      "        9.1163e-09, 2.1597e-08, 4.8328e-08, 3.5652e-08, 2.1612e-08, 1.1869e-08,\n",
      "        6.2768e-08, 2.5523e-08, 1.5252e-07, 1.7177e-08, 3.1384e-08, 2.4110e-07,\n",
      "        7.1274e-08, 1.6538e-08, 7.0904e-09, 9.2723e-09, 5.4835e-08, 1.6267e-08,\n",
      "        1.0911e-08, 2.2666e-08, 1.0249e-08, 6.2501e-08, 1.1198e-08, 4.5504e-08,\n",
      "        3.6174e-08, 6.0666e-08, 1.4814e-08, 1.3924e-08, 3.2510e-08, 3.0445e-08,\n",
      "        1.8555e-08, 3.5053e-08, 8.7424e-09, 2.6642e-08, 6.1875e-08, 2.3984e-08,\n",
      "        1.0234e-08, 6.4861e-08])}, 31: {'step': tensor(3341.), 'exp_avg': tensor([ 3.7499e-05,  8.3622e-06,  4.7618e-05,  1.0736e-04,  1.4496e-05,\n",
      "         6.5245e-05,  1.0804e-05,  9.2270e-06,  1.2270e-05, -1.2595e-05,\n",
      "         1.5769e-05,  2.9762e-05,  3.6321e-05,  7.7581e-07,  4.7584e-05,\n",
      "         2.6994e-05,  4.1780e-05,  7.5934e-06,  1.1852e-05,  8.3843e-06,\n",
      "         3.9343e-05,  6.0937e-05,  9.6020e-06,  5.6899e-06,  1.4213e-05,\n",
      "         1.4455e-05,  7.0650e-05, -6.3829e-06,  4.0023e-06,  1.8078e-04,\n",
      "         3.8917e-06,  2.6328e-05,  7.5248e-05,  2.1665e-05,  2.6867e-05,\n",
      "         1.3199e-05,  5.7238e-06,  5.2020e-05,  1.9141e-04,  3.0697e-05,\n",
      "         4.6236e-06,  3.6252e-05,  3.2280e-06, -4.2869e-07,  1.2641e-04,\n",
      "         1.3008e-04, -4.7428e-07,  5.1685e-05, -1.3004e-05,  8.4477e-05,\n",
      "         7.1842e-05,  3.4986e-05,  1.3785e-05,  7.2862e-05,  1.5611e-04,\n",
      "         2.2255e-07,  2.6227e-05,  9.5831e-05,  4.9649e-05,  1.2167e-05,\n",
      "        -5.6885e-06,  6.4248e-05,  2.7538e-05,  5.9080e-05,  2.7952e-05,\n",
      "         5.5429e-05,  2.3209e-05,  4.6190e-05,  2.0502e-05,  2.1609e-05,\n",
      "         3.3767e-05,  2.4659e-05,  8.7471e-05,  5.2245e-05, -3.4293e-06,\n",
      "        -1.6339e-05,  3.0925e-05,  3.1382e-05,  1.5125e-05,  9.4013e-06,\n",
      "         2.5760e-05,  2.0831e-05,  6.9611e-05,  9.5508e-06,  2.2784e-05,\n",
      "         2.0106e-04,  5.4840e-05,  5.1165e-05,  3.0446e-05,  2.4698e-05,\n",
      "        -5.3147e-06,  2.1936e-05,  3.5205e-06,  6.8792e-05,  4.4850e-06,\n",
      "         1.4708e-05,  2.5867e-05,  2.1107e-05,  5.2660e-05,  2.6357e-05,\n",
      "         1.1652e-05,  3.1298e-05,  4.4577e-05,  3.5416e-06, -1.5895e-05,\n",
      "         2.2043e-05,  1.0281e-05, -3.3883e-05,  1.2051e-06,  2.2692e-05,\n",
      "         1.5086e-05,  3.5262e-05,  2.9162e-06,  4.7473e-05,  2.3174e-05,\n",
      "         3.8081e-05,  2.4430e-05,  2.3517e-05,  2.6875e-05,  3.5947e-05,\n",
      "         3.0353e-05,  8.7368e-05, -6.0839e-06,  7.5215e-05,  2.1737e-06,\n",
      "         2.5251e-05,  1.6372e-05,  1.6735e-05]), 'exp_avg_sq': tensor([1.6206e-08, 1.4552e-08, 3.4378e-08, 2.7481e-07, 8.4294e-09, 5.5145e-08,\n",
      "        1.0296e-08, 4.9759e-09, 1.0004e-09, 1.0158e-08, 3.3149e-08, 2.3010e-08,\n",
      "        1.2838e-08, 1.0998e-08, 1.8919e-08, 5.2553e-09, 1.7360e-08, 8.3019e-09,\n",
      "        1.6852e-08, 1.9009e-09, 3.4710e-08, 3.5830e-08, 1.5953e-09, 1.6508e-08,\n",
      "        2.4607e-09, 1.5204e-08, 9.2269e-08, 3.4728e-09, 1.1731e-08, 7.0832e-08,\n",
      "        7.6081e-09, 1.6749e-08, 5.6974e-08, 7.4746e-09, 1.4826e-07, 8.2361e-09,\n",
      "        3.8840e-09, 7.6870e-08, 6.1586e-07, 4.2422e-08, 2.8645e-08, 5.6205e-09,\n",
      "        2.7365e-09, 7.2223e-09, 1.3157e-07, 1.4889e-07, 9.5300e-10, 2.0387e-07,\n",
      "        1.8190e-08, 4.0092e-08, 5.6343e-08, 2.1419e-08, 8.0006e-09, 1.1249e-07,\n",
      "        1.3313e-07, 8.2088e-09, 4.3317e-09, 1.0751e-07, 6.6311e-08, 8.0568e-09,\n",
      "        1.2449e-08, 5.1764e-08, 6.0332e-09, 4.2728e-08, 1.4633e-08, 6.0820e-08,\n",
      "        3.2747e-09, 3.2121e-08, 9.3280e-09, 5.8178e-09, 2.1714e-08, 1.7021e-08,\n",
      "        1.4731e-08, 2.9665e-08, 8.2219e-10, 2.4863e-08, 6.4932e-08, 2.6988e-08,\n",
      "        8.0836e-09, 8.6636e-09, 9.4848e-09, 2.5545e-08, 1.6942e-08, 7.3337e-09,\n",
      "        1.2283e-08, 5.1858e-07, 3.6499e-08, 4.7207e-08, 1.6301e-08, 8.9645e-09,\n",
      "        1.1471e-08, 6.1047e-09, 1.2818e-08, 1.4837e-07, 3.9804e-09, 2.7915e-08,\n",
      "        1.0724e-08, 5.1908e-09, 1.8769e-08, 2.6831e-08, 8.1805e-09, 3.7702e-08,\n",
      "        9.8402e-09, 8.4274e-09, 4.6182e-09, 3.5311e-08, 6.4456e-09, 3.5745e-08,\n",
      "        5.6127e-09, 1.2650e-08, 5.2513e-09, 1.7639e-08, 1.1039e-08, 2.2596e-08,\n",
      "        4.1361e-08, 7.7957e-09, 2.1082e-08, 3.0358e-08, 5.4016e-09, 3.2754e-08,\n",
      "        2.5595e-08, 1.1245e-07, 3.1897e-09, 1.1032e-07, 2.8515e-09, 6.2710e-08,\n",
      "        1.8633e-08, 2.6841e-09])}, 32: {'step': tensor(3341.), 'exp_avg': tensor([-4.3413e-05,  1.8738e-05, -7.8161e-05,  2.8550e-05,  1.8165e-05,\n",
      "        -2.9488e-05,  2.5090e-05, -6.2624e-06, -2.3035e-05, -1.5136e-05,\n",
      "         9.3898e-06, -1.6665e-05, -6.3796e-05,  2.0106e-05, -7.9429e-05,\n",
      "         3.9726e-05,  4.7141e-05,  5.5724e-06,  5.0453e-06, -2.5001e-05,\n",
      "         6.8217e-05,  1.0824e-04, -8.0337e-05, -9.9328e-06, -7.3979e-05,\n",
      "         1.0172e-05, -4.5569e-05,  5.7529e-06,  4.4430e-06, -1.2807e-04,\n",
      "         2.6089e-05,  3.5310e-05,  7.3218e-05, -1.7398e-05,  9.1139e-06,\n",
      "         6.3574e-05, -1.3601e-05, -7.9129e-06, -1.3556e-04,  2.1412e-05,\n",
      "         2.4183e-06,  5.4973e-05, -9.1063e-06,  1.0514e-05, -7.2947e-05,\n",
      "        -2.5325e-05,  2.3805e-05, -9.8073e-06, -8.7785e-06, -5.1893e-05,\n",
      "         6.4072e-05, -8.0246e-05,  4.4285e-05,  5.2355e-05, -1.6952e-04,\n",
      "        -1.5736e-05, -2.6284e-05,  6.9046e-05,  9.8909e-05,  3.2627e-05,\n",
      "        -4.6373e-06, -6.4865e-05,  5.8296e-05,  3.5145e-05,  6.8698e-05,\n",
      "         1.3069e-04,  7.3133e-05, -1.0689e-04, -3.2398e-05, -6.1818e-05,\n",
      "         8.1341e-05,  8.6085e-05,  1.0822e-04,  5.9939e-05, -4.1974e-05,\n",
      "        -2.8132e-05,  3.8156e-05,  1.2186e-05,  2.3374e-05,  5.5915e-05,\n",
      "        -9.1576e-05, -7.7475e-06,  9.1223e-05,  3.7520e-05,  5.4820e-05,\n",
      "        -1.4262e-04,  9.4837e-05, -2.6642e-05,  3.5443e-05,  3.7110e-05,\n",
      "         2.2661e-05,  2.6845e-05, -3.9901e-07,  6.3392e-07,  1.3290e-05,\n",
      "         7.3910e-06, -3.5727e-05, -1.7898e-05,  9.5510e-05,  2.3749e-05,\n",
      "        -2.4185e-05, -3.4738e-05,  5.9496e-05,  3.5976e-06,  1.9200e-05,\n",
      "        -4.9024e-06,  2.1040e-05, -1.7593e-05,  6.6717e-06, -5.9250e-05,\n",
      "         2.2456e-05,  1.0369e-04,  3.2139e-05,  5.9092e-05,  5.0098e-05,\n",
      "        -6.2818e-05,  3.4031e-06, -3.6052e-05, -6.1677e-05, -5.8489e-05,\n",
      "        -1.0330e-04, -1.1554e-04,  7.1633e-06,  9.4125e-05, -4.2971e-05,\n",
      "         9.2795e-06, -5.4633e-05, -7.5810e-06]), 'exp_avg_sq': tensor([4.5093e-08, 4.1700e-08, 1.6046e-07, 5.3899e-08, 5.4593e-08, 1.3204e-08,\n",
      "        3.5247e-07, 5.0732e-08, 3.2914e-08, 1.0494e-07, 2.1577e-08, 1.4608e-08,\n",
      "        9.1796e-08, 2.4575e-08, 7.0393e-08, 3.2079e-08, 3.4693e-08, 4.6308e-08,\n",
      "        5.6869e-08, 3.5889e-08, 8.8473e-08, 1.8473e-07, 5.8831e-08, 1.2306e-07,\n",
      "        4.7654e-08, 4.7196e-08, 5.5551e-08, 3.3016e-08, 2.7887e-08, 4.5830e-08,\n",
      "        1.1554e-07, 3.2595e-08, 4.7677e-08, 2.2278e-07, 1.3093e-07, 2.0308e-08,\n",
      "        4.5460e-08, 2.5400e-07, 1.6805e-06, 2.4922e-08, 2.8812e-08, 1.3817e-07,\n",
      "        2.9855e-08, 1.9175e-08, 3.7860e-07, 6.6982e-09, 5.1767e-08, 1.1387e-08,\n",
      "        4.9630e-08, 1.3472e-07, 2.0122e-07, 8.8004e-08, 1.1082e-07, 1.5283e-07,\n",
      "        1.0121e-07, 5.0734e-08, 9.1997e-08, 9.4390e-08, 2.4925e-07, 3.1950e-08,\n",
      "        2.3744e-08, 5.3461e-07, 1.4888e-08, 3.4463e-08, 9.2042e-08, 3.4707e-07,\n",
      "        3.4603e-08, 1.3533e-07, 5.3995e-08, 4.1345e-08, 1.6770e-07, 1.0855e-07,\n",
      "        3.8285e-08, 8.4651e-08, 1.0854e-08, 6.9518e-08, 1.9455e-07, 1.3688e-07,\n",
      "        3.4594e-08, 3.7984e-08, 1.2573e-07, 1.8026e-08, 6.9329e-08, 4.4202e-08,\n",
      "        4.3496e-08, 1.7191e-07, 1.8989e-07, 1.3890e-08, 2.6363e-08, 3.4704e-08,\n",
      "        4.2504e-08, 5.1740e-08, 1.4276e-08, 4.4395e-07, 5.5163e-08, 5.9084e-08,\n",
      "        3.4940e-08, 4.6846e-08, 4.2100e-08, 1.2690e-07, 1.1083e-07, 5.7987e-08,\n",
      "        1.0411e-07, 4.6721e-08, 2.2143e-08, 1.7705e-07, 1.6908e-08, 2.3696e-08,\n",
      "        7.5155e-08, 1.0054e-07, 5.5818e-08, 1.1658e-07, 2.5875e-08, 4.7652e-07,\n",
      "        1.8077e-07, 8.4015e-08, 1.4493e-07, 3.5367e-08, 2.9034e-08, 1.3522e-07,\n",
      "        2.2729e-07, 5.4833e-07, 3.0614e-08, 1.1318e-07, 3.5483e-08, 1.2018e-07,\n",
      "        7.4379e-08, 6.3569e-08])}, 33: {'step': tensor(3341.), 'exp_avg': tensor([[ 1.9517e-04,  1.0125e-03,  2.7904e-04, -1.2254e-07, -7.4960e-04],\n",
      "        [-2.6636e-04, -1.1598e-02, -1.2043e-02,  1.6733e-07,  2.0109e-03],\n",
      "        [ 3.1995e-04, -2.6175e-03, -4.1464e-03, -2.0079e-07, -1.0977e-03],\n",
      "        [ 2.6155e-04,  2.4238e-03,  1.5486e-03, -1.6413e-07, -1.1587e-03],\n",
      "        [-1.0257e-04, -5.1583e-04, -8.0478e-05,  6.4371e-08,  2.8070e-04],\n",
      "        [ 4.7072e-05,  7.7224e-04,  6.4707e-04, -2.9557e-08, -2.5640e-04],\n",
      "        [-4.9491e-04, -1.3095e-03,  1.4750e-04,  3.1074e-07,  1.9756e-03],\n",
      "        [-8.9514e-04, -2.5272e-02, -2.5042e-02,  5.6209e-07,  6.0807e-03],\n",
      "        [-1.9250e-04, -1.5844e-03, -1.0200e-03,  1.2077e-07,  8.1862e-04],\n",
      "        [-1.1629e-04, -1.9902e-03, -1.6611e-03,  7.2978e-08,  3.9433e-04],\n",
      "        [ 1.0584e-04,  1.4553e-03,  1.0121e-03, -6.6410e-08, -2.6278e-04],\n",
      "        [ 3.9125e-04,  9.0689e-03,  8.7473e-03, -2.4572e-07, -2.4270e-03],\n",
      "        [ 5.7875e-05,  1.9621e-03,  1.9798e-03, -3.6333e-08, -2.9421e-04],\n",
      "        [-5.8635e-05, -1.0560e-03, -9.0184e-04,  3.6801e-08,  3.3348e-04],\n",
      "        [ 2.0998e-04,  2.5892e-03,  2.0583e-03, -1.3178e-07, -1.1020e-03],\n",
      "        [-3.9566e-05, -1.1633e-03, -9.9994e-04,  2.4874e-08,  2.4017e-04],\n",
      "        [-9.4410e-05,  1.6549e-03,  2.0366e-03,  5.9134e-08,  4.1486e-04],\n",
      "        [-1.9625e-05, -1.4023e-04, -2.6325e-04,  1.2324e-08,  4.1622e-04],\n",
      "        [ 4.7489e-04,  1.3049e-02,  1.3044e-02, -2.9817e-07, -3.2730e-03],\n",
      "        [-2.2474e-05, -4.6024e-04, -4.6708e-04,  1.4088e-08,  1.4289e-04],\n",
      "        [-3.1225e-04, -4.1534e-03, -3.6280e-03,  1.9600e-07,  1.4343e-03],\n",
      "        [ 5.9209e-04,  1.8411e-02,  1.8278e-02, -3.7182e-07, -3.9209e-03],\n",
      "        [ 5.7898e-05, -3.0782e-04, -5.1798e-04, -3.6348e-08, -6.6458e-05],\n",
      "        [ 6.9809e-04,  2.2015e-02,  2.2207e-02, -4.3833e-07, -4.8006e-03],\n",
      "        [ 3.6241e-04, -8.4482e-04, -2.1819e-03, -2.2745e-07, -1.4663e-03],\n",
      "        [-9.0340e-04, -2.4130e-02, -2.3707e-02,  5.6714e-07,  6.0091e-03],\n",
      "        [ 3.2833e-04,  7.8371e-03,  7.6812e-03, -2.0617e-07, -2.1955e-03],\n",
      "        [-1.7551e-04, -6.1374e-03, -6.2226e-03,  1.1024e-07,  1.3121e-03],\n",
      "        [-6.6370e-05, -4.7295e-03, -5.2098e-03,  4.1652e-08,  7.2888e-04],\n",
      "        [ 1.4677e-04,  1.4957e-03,  1.3789e-03, -9.2083e-08, -1.0750e-03],\n",
      "        [-9.3888e-05, -9.7658e-04, -6.8805e-04,  5.8949e-08,  3.6626e-04],\n",
      "        [-1.2286e-05, -1.3616e-03, -1.2967e-03,  7.5827e-09,  1.4901e-05],\n",
      "        [ 2.0277e-04,  3.7484e-03,  3.3672e-03, -1.2723e-07, -1.1400e-03],\n",
      "        [ 3.1698e-05,  3.9442e-04,  2.8842e-04, -1.9930e-08, -2.5116e-06],\n",
      "        [ 8.7895e-05, -1.8339e-03, -2.4510e-03, -5.5149e-08, -1.7216e-04],\n",
      "        [-6.1705e-04, -1.2691e-02, -1.2048e-02,  3.8718e-07,  3.6181e-03],\n",
      "        [ 8.0031e-05,  3.8225e-04,  5.8059e-05, -5.0222e-08, -2.9971e-04],\n",
      "        [ 1.1570e-05,  1.5581e-03,  1.6876e-03, -7.3041e-09, -1.6661e-04],\n",
      "        [-2.0247e-04, -1.8005e-03, -1.2208e-03,  1.2705e-07,  9.1174e-04],\n",
      "        [-1.6543e-04, -1.6377e-03, -1.0504e-03,  1.0382e-07,  6.6687e-04],\n",
      "        [ 1.6752e-04,  1.7000e-03,  1.3103e-03, -1.0520e-07, -8.6799e-04],\n",
      "        [-6.0844e-06,  2.9178e-04,  3.8015e-04,  3.8195e-09,  7.7897e-05],\n",
      "        [-1.4283e-04,  9.7726e-04,  1.4867e-03,  8.9808e-08,  6.8319e-04],\n",
      "        [-3.4129e-05, -1.5089e-03, -1.4995e-03,  2.1415e-08,  3.1656e-04],\n",
      "        [-9.3650e-05,  1.8731e-03,  2.3901e-03,  5.8706e-08,  2.3904e-04],\n",
      "        [-1.0373e-05,  3.2631e-04,  4.7629e-04,  6.5352e-09, -9.8135e-05],\n",
      "        [-3.2692e-07,  2.8575e-05,  8.9569e-05,  2.0672e-10, -8.5709e-05],\n",
      "        [-2.2475e-04,  2.1935e-04,  9.5333e-04,  1.4106e-07,  1.1184e-03],\n",
      "        [-3.7989e-05, -1.0518e-04, -2.4835e-05,  2.3881e-08,  4.3512e-04],\n",
      "        [-8.0839e-05, -5.3809e-04, -2.3981e-04,  5.0720e-08,  3.8533e-04],\n",
      "        [-2.5825e-04, -3.5327e-03, -2.9513e-03,  1.6192e-07,  1.1238e-03],\n",
      "        [-6.5278e-05, -4.3578e-04, -7.9659e-05,  4.0998e-08,  8.9133e-05],\n",
      "        [ 2.2808e-04,  2.0124e-03,  1.2959e-03, -1.4313e-07, -9.6786e-04],\n",
      "        [ 5.4712e-04,  9.2001e-03,  8.3594e-03, -3.4345e-07, -3.0376e-03],\n",
      "        [-5.5153e-06,  5.4998e-04,  4.4719e-04,  3.4721e-09,  1.4180e-04],\n",
      "        [ 2.9650e-05,  1.7943e-03,  1.7925e-03, -1.8678e-08, -3.0168e-05],\n",
      "        [-3.4888e-04, -2.6066e-03, -1.4599e-03,  2.1900e-07,  1.4034e-03],\n",
      "        [-9.2700e-05, -5.5428e-04, -1.6552e-04,  5.8196e-08,  2.5171e-04],\n",
      "        [-3.7009e-04, -3.1118e-03, -2.2949e-03,  2.3245e-07,  1.6294e-03],\n",
      "        [ 2.4385e-04,  8.4054e-03,  8.5100e-03, -1.5313e-07, -1.7223e-03],\n",
      "        [ 1.0117e-04,  1.4921e-03,  1.1834e-03, -6.3501e-08, -3.9169e-04],\n",
      "        [ 9.2021e-04,  3.3162e-03,  2.8428e-04, -5.7778e-07, -4.2621e-03],\n",
      "        [ 1.0696e-04,  1.6875e-03,  1.3144e-03, -6.7150e-08, -5.4527e-04],\n",
      "        [ 1.6897e-05, -5.0579e-04, -6.7864e-04, -1.0621e-08,  1.9437e-04]]), 'exp_avg_sq': tensor([[1.2617e-08, 8.7401e-07, 5.9892e-07, 4.9717e-15, 2.5010e-07],\n",
      "        [3.0457e-06, 2.7216e-03, 2.6208e-03, 1.2000e-12, 1.2132e-04],\n",
      "        [2.8203e-07, 9.5420e-05, 8.9630e-05, 1.1110e-13, 8.5040e-06],\n",
      "        [1.6143e-08, 1.0187e-06, 8.1860e-07, 6.3598e-15, 1.9784e-07],\n",
      "        [1.5741e-08, 8.0898e-07, 9.2413e-07, 6.2021e-15, 1.1949e-07],\n",
      "        [8.7867e-09, 6.3340e-07, 5.0473e-07, 3.4609e-15, 1.3733e-07],\n",
      "        [3.2795e-07, 1.4957e-05, 1.1052e-05, 1.2919e-13, 7.5797e-06],\n",
      "        [1.2712e-05, 1.0999e-02, 1.0533e-02, 5.0084e-12, 4.8280e-04],\n",
      "        [9.0062e-09, 7.0365e-07, 5.2158e-07, 3.5470e-15, 1.5588e-07],\n",
      "        [2.3555e-08, 1.7329e-06, 1.5102e-06, 9.2789e-15, 4.9864e-07],\n",
      "        [2.3209e-08, 1.5944e-06, 1.6376e-06, 9.1426e-15, 3.0127e-07],\n",
      "        [1.5990e-06, 1.4163e-03, 1.3540e-03, 6.2996e-13, 5.7456e-05],\n",
      "        [2.2236e-08, 2.5755e-06, 1.8154e-06, 8.7597e-15, 5.3609e-07],\n",
      "        [1.5173e-08, 1.1352e-06, 8.7203e-07, 5.9773e-15, 1.8909e-07],\n",
      "        [7.4860e-09, 6.4447e-07, 5.5939e-07, 2.9486e-15, 9.2607e-08],\n",
      "        [1.0089e-08, 9.3073e-07, 8.0832e-07, 3.9742e-15, 1.2471e-07],\n",
      "        [9.4491e-08, 9.8384e-06, 8.9966e-06, 3.7228e-14, 1.7172e-06],\n",
      "        [1.3280e-08, 1.4151e-06, 1.4316e-06, 5.2296e-15, 3.8508e-07],\n",
      "        [3.7768e-06, 3.0574e-03, 2.9097e-03, 1.4880e-12, 1.4720e-04],\n",
      "        [3.3351e-08, 7.7030e-07, 9.4475e-07, 1.3136e-14, 1.7816e-07],\n",
      "        [5.1657e-07, 2.3092e-04, 2.1095e-04, 2.0353e-13, 1.7601e-05],\n",
      "        [7.6689e-06, 7.5063e-03, 7.2750e-03, 3.0214e-12, 3.0786e-04],\n",
      "        [1.6367e-08, 9.4723e-07, 8.2213e-07, 6.4483e-15, 1.6396e-07],\n",
      "        [1.2314e-05, 9.9032e-03, 9.4093e-03, 4.8516e-12, 4.5550e-04],\n",
      "        [2.6810e-07, 1.9334e-05, 1.4675e-05, 1.0563e-13, 6.1344e-06],\n",
      "        [1.2405e-05, 1.0465e-02, 9.9949e-03, 4.8876e-12, 4.7282e-04],\n",
      "        [1.0728e-06, 7.7585e-04, 7.3221e-04, 4.2268e-13, 4.1320e-05],\n",
      "        [9.2821e-07, 9.9277e-04, 9.6253e-04, 3.6570e-13, 3.5010e-05],\n",
      "        [5.2638e-07, 1.7326e-04, 1.7072e-04, 2.0738e-13, 1.6209e-05],\n",
      "        [1.2015e-07, 9.5586e-06, 7.2179e-06, 4.7334e-14, 2.2812e-06],\n",
      "        [6.4924e-09, 1.0348e-06, 1.0883e-06, 2.5573e-15, 1.5776e-07],\n",
      "        [9.9022e-08, 7.4275e-06, 5.4074e-06, 3.9011e-14, 2.2116e-06],\n",
      "        [2.3649e-08, 3.7487e-06, 3.3414e-06, 9.3164e-15, 5.3190e-07],\n",
      "        [2.1973e-08, 1.6980e-06, 1.3594e-06, 8.6578e-15, 4.2960e-07],\n",
      "        [5.6560e-08, 1.1772e-05, 1.3157e-05, 2.2285e-14, 1.5798e-06],\n",
      "        [3.1331e-06, 2.4519e-03, 2.3308e-03, 1.2344e-12, 1.1786e-04],\n",
      "        [8.8344e-09, 5.1658e-07, 5.2488e-07, 3.4795e-15, 9.0857e-08],\n",
      "        [4.8998e-08, 8.4180e-06, 7.4540e-06, 1.9307e-14, 1.1709e-06],\n",
      "        [1.8191e-08, 9.0098e-07, 7.5098e-07, 7.1647e-15, 3.2005e-07],\n",
      "        [8.2590e-09, 6.5116e-07, 5.6588e-07, 3.2538e-15, 1.1164e-07],\n",
      "        [1.2903e-07, 5.9141e-05, 5.4978e-05, 5.0828e-14, 3.7203e-06],\n",
      "        [1.2099e-08, 1.0692e-06, 1.0200e-06, 4.7670e-15, 2.3824e-07],\n",
      "        [1.4236e-07, 1.0022e-05, 7.9180e-06, 5.6065e-14, 2.6841e-06],\n",
      "        [1.7514e-08, 1.6562e-06, 1.5003e-06, 6.8998e-15, 1.9612e-07],\n",
      "        [1.3899e-07, 9.4694e-06, 7.3373e-06, 5.4759e-14, 2.3836e-06],\n",
      "        [1.4085e-08, 6.1654e-07, 5.5238e-07, 5.5484e-15, 1.6163e-07],\n",
      "        [4.9968e-09, 3.6420e-07, 3.3508e-07, 1.9684e-15, 7.2016e-08],\n",
      "        [1.3697e-07, 8.9371e-06, 6.2678e-06, 5.3962e-14, 2.6157e-06],\n",
      "        [2.9039e-08, 4.1435e-06, 3.8266e-06, 1.1439e-14, 5.8660e-07],\n",
      "        [4.2396e-08, 8.1094e-07, 7.7893e-07, 1.6707e-14, 2.4435e-07],\n",
      "        [4.4135e-07, 3.0246e-04, 2.8715e-04, 1.7388e-13, 1.6163e-05],\n",
      "        [3.3683e-08, 1.9780e-06, 2.2491e-06, 1.3270e-14, 3.4342e-07],\n",
      "        [1.2184e-08, 5.6406e-07, 4.6947e-07, 4.7994e-15, 1.8173e-07],\n",
      "        [1.1685e-06, 7.5538e-04, 7.1601e-04, 4.6038e-13, 4.5241e-05],\n",
      "        [2.9433e-08, 2.0532e-06, 1.5192e-06, 1.1594e-14, 5.8510e-07],\n",
      "        [3.9906e-08, 3.4539e-06, 3.1107e-06, 1.5724e-14, 9.5699e-07],\n",
      "        [2.0075e-08, 1.0589e-06, 5.3022e-07, 7.9088e-15, 3.5826e-07],\n",
      "        [1.5207e-08, 7.5419e-07, 7.3229e-07, 5.9903e-15, 9.5932e-08],\n",
      "        [3.7765e-07, 1.0514e-04, 9.4104e-05, 1.4881e-13, 1.2284e-05],\n",
      "        [1.5361e-06, 1.2724e-03, 1.2164e-03, 6.0520e-13, 5.9989e-05],\n",
      "        [3.2601e-08, 2.4632e-06, 1.8745e-06, 1.2846e-14, 6.7557e-07],\n",
      "        [4.4499e-07, 2.6778e-05, 1.4858e-05, 1.7534e-13, 1.0730e-05],\n",
      "        [1.2360e-08, 1.3837e-06, 1.1454e-06, 4.8684e-15, 1.6148e-07],\n",
      "        [1.7031e-08, 1.1368e-06, 1.2480e-06, 6.7071e-15, 2.0871e-07]])}, 34: {'step': tensor(3341.), 'exp_avg': tensor([ 1.0125e-03, -1.1598e-02, -2.6152e-03,  2.4238e-03, -5.1583e-04,\n",
      "         7.7224e-04, -1.3095e-03, -2.5272e-02, -1.5844e-03, -1.9902e-03,\n",
      "         1.4553e-03,  9.0689e-03,  1.9621e-03, -1.0560e-03,  2.5892e-03,\n",
      "        -1.1633e-03,  1.6549e-03, -1.4023e-04,  1.3049e-02, -4.6024e-04,\n",
      "        -4.1534e-03,  1.8411e-02, -3.0782e-04,  2.2015e-02, -8.4482e-04,\n",
      "        -2.4130e-02,  7.8371e-03, -6.1374e-03, -4.7295e-03,  1.4957e-03,\n",
      "        -9.7658e-04, -1.3616e-03,  3.7484e-03,  3.9442e-04, -1.8339e-03,\n",
      "        -1.2691e-02,  3.8225e-04,  1.5581e-03, -1.8005e-03, -1.6377e-03,\n",
      "         1.7000e-03,  2.9178e-04,  9.7726e-04, -1.5089e-03,  1.8731e-03,\n",
      "         3.2631e-04,  2.8575e-05,  2.1935e-04, -1.0518e-04, -5.3809e-04,\n",
      "        -3.5327e-03, -4.3578e-04,  2.0124e-03,  9.2001e-03,  5.4998e-04,\n",
      "         1.7943e-03, -2.6066e-03, -5.5428e-04, -3.1118e-03,  8.4054e-03,\n",
      "         1.4921e-03,  3.3162e-03,  1.6875e-03, -5.0579e-04]), 'exp_avg_sq': tensor([8.7401e-07, 2.7216e-03, 9.5417e-05, 1.0187e-06, 8.0899e-07, 6.3340e-07,\n",
      "        1.4957e-05, 1.0999e-02, 7.0365e-07, 1.7329e-06, 1.5944e-06, 1.4163e-03,\n",
      "        2.5755e-06, 1.1352e-06, 6.4447e-07, 9.3073e-07, 9.8384e-06, 1.4151e-06,\n",
      "        3.0574e-03, 7.7031e-07, 2.3092e-04, 7.5064e-03, 9.4723e-07, 9.9032e-03,\n",
      "        1.9334e-05, 1.0465e-02, 7.7585e-04, 9.9277e-04, 1.7326e-04, 9.5586e-06,\n",
      "        1.0348e-06, 7.4275e-06, 3.7487e-06, 1.6980e-06, 1.1772e-05, 2.4519e-03,\n",
      "        5.1658e-07, 8.4180e-06, 9.0099e-07, 6.5116e-07, 5.9141e-05, 1.0692e-06,\n",
      "        1.0022e-05, 1.6562e-06, 9.4694e-06, 6.1654e-07, 3.6420e-07, 8.9371e-06,\n",
      "        4.1435e-06, 8.1094e-07, 3.0246e-04, 1.9780e-06, 5.6406e-07, 7.5538e-04,\n",
      "        2.0532e-06, 3.4539e-06, 1.0589e-06, 7.5419e-07, 1.0514e-04, 1.2724e-03,\n",
      "        2.4633e-06, 2.6778e-05, 1.3837e-06, 1.1368e-06])}, 35: {'step': tensor(3341.), 'exp_avg': tensor([[ 3.7979e-05, -3.6022e-05,  1.4968e-05,  4.4240e-05,  2.0809e-06,\n",
      "          3.1090e-05, -3.5352e-06, -5.0522e-05,  8.1045e-06, -4.2360e-06,\n",
      "          1.4878e-05,  6.9645e-06,  8.8942e-06,  2.4764e-05, -2.2819e-05,\n",
      "          5.2535e-05, -7.4338e-05,  9.5818e-07, -3.9851e-05,  1.3717e-05,\n",
      "         -1.9910e-06, -8.1548e-06, -2.4304e-05, -1.0284e-04,  6.7817e-05,\n",
      "         -5.5123e-05,  9.8226e-06,  3.7186e-06,  2.0016e-05,  6.3663e-05,\n",
      "         -5.1882e-06,  2.6183e-05,  6.4450e-05,  1.9108e-05,  6.0698e-05,\n",
      "         -2.2189e-05,  2.9673e-06,  5.6254e-06,  4.0276e-05, -2.1481e-05,\n",
      "         -5.1756e-06,  4.8326e-06,  2.3544e-05,  8.4139e-06, -4.0243e-04,\n",
      "          7.1058e-05,  2.2805e-06, -1.2329e-05,  4.3494e-06,  7.1677e-05,\n",
      "          3.9500e-06, -1.2794e-05,  5.8056e-06, -3.6688e-06, -4.0108e-06,\n",
      "          1.8722e-05,  1.7300e-05,  5.3834e-06, -4.8917e-06, -1.5268e-05,\n",
      "          3.2388e-06,  4.6358e-05, -1.9249e-07,  2.5656e-05]]), 'exp_avg_sq': tensor([[1.2202e-09, 1.0278e-08, 6.3560e-10, 3.5932e-09, 4.3307e-10, 1.3295e-09,\n",
      "         1.3956e-10, 3.9307e-08, 4.5551e-10, 3.1889e-09, 2.6088e-10, 2.6560e-09,\n",
      "         2.5267e-08, 8.9923e-10, 2.2619e-09, 1.1562e-09, 1.3061e-07, 2.2827e-12,\n",
      "         1.2804e-08, 9.4745e-10, 2.1042e-09, 1.7899e-08, 1.4921e-09, 1.2889e-07,\n",
      "         3.5260e-09, 6.1723e-08, 3.9818e-09, 9.5534e-10, 8.6769e-10, 2.5712e-08,\n",
      "         1.1431e-09, 2.9992e-09, 4.4649e-09, 3.9674e-10, 2.9649e-09, 7.7941e-09,\n",
      "         9.6255e-10, 8.6903e-12, 5.0283e-09, 2.9158e-10, 6.8853e-10, 1.0261e-09,\n",
      "         3.6381e-10, 1.5876e-10, 5.0175e-07, 9.6841e-09, 5.8467e-13, 4.6730e-10,\n",
      "         9.5418e-11, 6.5004e-09, 2.9619e-10, 1.7111e-09, 1.2429e-10, 2.3626e-09,\n",
      "         1.0338e-09, 3.6594e-08, 5.3456e-11, 6.5937e-12, 5.7207e-10, 1.6569e-08,\n",
      "         3.5393e-11, 4.7807e-09, 4.6849e-11, 3.5584e-09]])}, 36: {'step': tensor(3341.), 'exp_avg': tensor([[-1.4247e-04, -4.0775e-04, -8.1708e-05,  ...,  9.5696e-05,\n",
      "         -3.6262e-05, -2.3137e-05],\n",
      "        [-5.7174e-05, -3.2739e-04, -3.3721e-05,  ...,  1.5388e-04,\n",
      "          3.1041e-05, -2.9688e-05],\n",
      "        [ 2.1943e-06, -2.7619e-05,  3.6023e-05,  ...,  1.6956e-04,\n",
      "         -1.1486e-04, -1.1454e-05],\n",
      "        ...,\n",
      "        [ 3.8915e-05,  4.7051e-04,  1.4791e-04,  ..., -5.0687e-05,\n",
      "         -1.7278e-04,  6.8183e-05],\n",
      "        [-3.4280e-05, -1.4189e-04, -2.1660e-05,  ...,  1.3567e-05,\n",
      "         -1.1926e-06, -2.8487e-05],\n",
      "        [ 7.7087e-05,  2.3438e-04,  5.4681e-05,  ..., -4.3418e-05,\n",
      "          4.7092e-05, -2.3604e-06]]), 'exp_avg_sq': tensor([[2.0799e-09, 8.6859e-08, 1.2626e-08,  ..., 2.4717e-08, 3.2577e-09,\n",
      "         1.0924e-09],\n",
      "        [5.7158e-10, 3.0484e-08, 4.0831e-09,  ..., 1.1527e-08, 2.0912e-10,\n",
      "         3.5822e-10],\n",
      "        [1.2555e-08, 1.2563e-06, 3.5941e-08,  ..., 1.7005e-07, 1.4529e-08,\n",
      "         6.0598e-09],\n",
      "        ...,\n",
      "        [3.8555e-09, 2.4206e-07, 2.2401e-08,  ..., 4.5249e-08, 3.3600e-09,\n",
      "         2.7356e-09],\n",
      "        [4.8868e-09, 6.5613e-08, 2.0118e-08,  ..., 1.2743e-08, 1.8926e-09,\n",
      "         5.6536e-09],\n",
      "        [8.9115e-10, 3.3064e-08, 6.9065e-09,  ..., 1.8336e-08, 9.2208e-10,\n",
      "         9.3953e-10]])}, 37: {'step': tensor(3341.), 'exp_avg': tensor([-1.0839e-03, -6.5607e-04,  2.3886e-04,  2.1669e-03,  6.8420e-04,\n",
      "         1.2954e-03,  1.3196e-05, -1.0710e-03,  3.0996e-04, -5.6252e-05,\n",
      "        -7.1098e-04,  8.4636e-04, -1.2143e-03,  1.7670e-03,  1.1925e-03,\n",
      "        -6.7845e-04,  3.9972e-03,  1.4121e-04,  4.4353e-04, -5.8073e-04,\n",
      "        -5.2618e-04,  4.5711e-04, -1.1686e-03,  2.2616e-03, -2.1206e-04,\n",
      "        -1.2172e-04, -1.8812e-04,  4.6035e-04,  3.7884e-04,  4.2804e-04,\n",
      "        -1.2872e-04,  2.0670e-03,  4.3150e-04,  5.0017e-04, -5.9628e-04,\n",
      "         4.3289e-04, -7.5134e-04, -6.2774e-05, -8.8209e-04, -1.5591e-05,\n",
      "        -2.3468e-03, -2.8021e-04,  1.0118e-03,  1.6644e-04, -9.9526e-03,\n",
      "         2.7902e-03, -4.7471e-04, -1.9909e-04, -1.3552e-04,  7.5409e-04,\n",
      "         2.3051e-04, -4.2648e-04,  8.1939e-04,  2.3960e-04,  2.9781e-04,\n",
      "        -1.1981e-03, -1.2793e-03, -7.0591e-04, -1.1003e-03, -3.8746e-04,\n",
      "         2.0619e-04,  6.3629e-04, -5.4700e-04,  6.2123e-04]), 'exp_avg_sq': tensor([1.0274e-06, 2.0790e-07, 3.8023e-05, 4.2317e-05, 1.8009e-07, 5.5969e-07,\n",
      "        2.9013e-07, 3.0569e-07, 1.2552e-07, 8.0709e-07, 1.7031e-06, 9.3699e-06,\n",
      "        3.5009e-05, 3.5051e-05, 5.6400e-06, 7.7718e-06, 3.0637e-04, 3.3841e-07,\n",
      "        5.5249e-07, 3.0467e-07, 9.3923e-07, 2.3278e-07, 1.0528e-05, 3.3076e-05,\n",
      "        9.3876e-07, 5.8273e-07, 3.8314e-06, 1.6405e-05, 2.7183e-07, 1.0531e-04,\n",
      "        8.1650e-06, 5.4299e-05, 2.1709e-07, 9.6814e-07, 5.4743e-07, 2.0036e-07,\n",
      "        3.2207e-07, 5.0794e-07, 2.7020e-07, 3.1627e-07, 3.6328e-05, 3.2848e-07,\n",
      "        2.8885e-07, 1.6088e-06, 6.8310e-04, 4.0960e-05, 1.5084e-07, 1.1466e-07,\n",
      "        5.4575e-07, 2.4388e-07, 1.1028e-07, 1.5055e-05, 2.2967e-06, 1.2181e-06,\n",
      "        5.9469e-07, 1.8679e-04, 1.6361e-06, 2.0941e-06, 1.8297e-07, 1.5951e-07,\n",
      "        6.3858e-07, 1.4643e-06, 2.4857e-06, 2.9596e-07])}, 38: {'step': tensor(3341.), 'exp_avg': tensor([[-1.0623e-04,  1.3953e-04, -5.5670e-04,  ...,  3.0787e-04,\n",
      "          7.9746e-04, -2.3527e-04],\n",
      "        [ 2.5352e-04,  1.4018e-04,  3.9145e-04,  ...,  5.1859e-04,\n",
      "         -4.1025e-04, -3.1454e-04],\n",
      "        [ 3.2197e-05,  2.4827e-06,  8.0785e-05,  ..., -2.8833e-07,\n",
      "         -8.2215e-05, -3.6511e-05],\n",
      "        ...,\n",
      "        [-2.8272e-04, -4.1199e-06, -1.2049e-03,  ..., -1.3406e-04,\n",
      "          1.0378e-03,  8.1138e-05],\n",
      "        [-3.2349e-06, -1.2850e-07, -2.7668e-05,  ...,  1.2352e-05,\n",
      "          3.8614e-05,  5.5488e-06],\n",
      "        [-1.9345e-04, -3.3981e-06, -5.3623e-04,  ..., -5.0773e-05,\n",
      "          4.2274e-04,  9.5252e-05]]), 'exp_avg_sq': tensor([[1.0658e-07, 2.8908e-08, 4.7859e-06,  ..., 3.3862e-07, 2.5584e-06,\n",
      "         1.1455e-07],\n",
      "        [6.3690e-08, 2.1269e-08, 5.1586e-06,  ..., 1.6288e-07, 2.0835e-06,\n",
      "         4.5547e-08],\n",
      "        [2.4817e-09, 3.8797e-10, 3.4852e-08,  ..., 4.2178e-09, 4.5915e-08,\n",
      "         1.1558e-09],\n",
      "        ...,\n",
      "        [1.6606e-07, 2.7586e-08, 3.0272e-06,  ..., 2.8564e-07, 2.4144e-06,\n",
      "         1.2333e-07],\n",
      "        [5.9978e-10, 3.8858e-10, 2.1500e-08,  ..., 1.1273e-09, 3.7651e-08,\n",
      "         3.9897e-10],\n",
      "        [5.7705e-08, 1.3022e-08, 5.6155e-07,  ..., 1.0342e-07, 3.4698e-07,\n",
      "         4.5779e-08]])}, 39: {'step': tensor(3341.), 'exp_avg': tensor([ 1.0088e-03, -2.8597e-04, -1.0021e-04,  4.8112e-04, -4.1594e-04,\n",
      "        -5.6738e-04,  8.7430e-05, -1.0523e-04,  8.7440e-04,  3.1578e-04,\n",
      "         8.3340e-04, -5.5250e-04,  1.7440e-04, -4.9666e-04,  1.1620e-03,\n",
      "         5.2945e-04, -2.6421e-04, -2.4245e-04,  2.0707e-05,  2.0301e-04,\n",
      "         5.3348e-04, -1.4193e-04,  2.4128e-04,  5.3870e-05, -4.2407e-04,\n",
      "        -3.1431e-04,  7.4150e-04,  2.5387e-04, -2.9019e-04,  5.2257e-04,\n",
      "         2.0358e-04,  2.2883e-04,  9.9016e-04, -2.6221e-05, -1.0950e-03,\n",
      "        -5.0658e-04, -5.8834e-04, -1.6153e-04,  5.1123e-04, -4.2851e-04,\n",
      "         4.2708e-05, -2.8030e-04, -4.8901e-04, -4.5678e-05,  1.9751e-03,\n",
      "        -3.8857e-03, -3.1687e-04,  1.0673e-04,  1.3341e-04, -7.3568e-04,\n",
      "        -4.1648e-04, -6.3993e-04,  1.3108e-04,  4.1571e-05, -3.3429e-04,\n",
      "         6.8108e-04, -5.6070e-04, -1.9401e-04,  2.5331e-04, -3.5919e-04,\n",
      "        -2.0563e-05,  8.3398e-04,  5.3730e-05,  3.4374e-04]), 'exp_avg_sq': tensor([2.1496e-06, 1.0862e-06, 1.6906e-07, 1.6466e-06, 3.1241e-07, 4.0600e-06,\n",
      "        3.6661e-07, 6.5782e-08, 1.2842e-06, 7.5579e-06, 4.9300e-07, 1.8893e-06,\n",
      "        6.1729e-07, 1.5526e-06, 1.2285e-06, 1.9648e-06, 2.3332e-06, 7.7216e-08,\n",
      "        6.7852e-07, 3.2619e-07, 7.3571e-07, 7.3850e-07, 9.8970e-08, 5.4726e-07,\n",
      "        4.2247e-06, 5.6701e-07, 5.2664e-06, 3.7583e-07, 1.9127e-07, 2.3796e-07,\n",
      "        3.9828e-06, 6.4949e-07, 1.7758e-06, 3.4650e-07, 1.9970e-06, 8.4156e-07,\n",
      "        6.6317e-07, 6.6014e-08, 3.9999e-06, 4.1450e-07, 4.8478e-07, 1.4976e-06,\n",
      "        5.6905e-07, 3.5532e-07, 1.0621e-06, 3.1948e-06, 6.6491e-08, 3.7359e-07,\n",
      "        1.1260e-07, 5.6080e-06, 1.1331e-06, 7.9160e-07, 6.6874e-08, 2.2655e-07,\n",
      "        1.1505e-06, 3.9307e-07, 1.0400e-07, 8.9481e-08, 1.7774e-07, 1.8941e-06,\n",
      "        8.6117e-08, 3.0489e-06, 7.3785e-08, 6.9190e-07])}, 40: {'step': tensor(3341.), 'exp_avg': tensor([[ 2.3546e-05, -1.2975e-05, -6.0580e-07,  ..., -1.4320e-05,\n",
      "         -2.0405e-07, -1.8850e-05],\n",
      "        [-1.8822e-05,  2.1668e-05,  1.2979e-05,  ..., -5.6579e-05,\n",
      "          1.6533e-05,  4.4418e-05],\n",
      "        [-6.3776e-04,  2.9168e-04, -2.0659e-04,  ..., -4.1363e-04,\n",
      "          1.4445e-04, -4.3322e-04],\n",
      "        ...,\n",
      "        [-3.3529e-05, -2.6693e-05,  7.3083e-05,  ...,  3.8671e-05,\n",
      "         -1.2096e-05,  5.3669e-05],\n",
      "        [ 1.2403e-05,  1.1150e-05, -5.6826e-05,  ...,  4.5025e-05,\n",
      "          4.1970e-06,  4.9790e-05],\n",
      "        [-7.6764e-05, -8.0709e-05, -4.3997e-05,  ...,  1.2031e-04,\n",
      "         -3.7499e-06,  7.3376e-05]]), 'exp_avg_sq': tensor([[8.5308e-10, 7.2831e-10, 1.0396e-09,  ..., 1.4318e-09, 7.8830e-11,\n",
      "         1.9214e-09],\n",
      "        [1.6425e-09, 1.5720e-09, 1.8926e-09,  ..., 3.2868e-09, 1.6298e-10,\n",
      "         3.6705e-09],\n",
      "        [1.5788e-07, 4.0549e-07, 3.0776e-07,  ..., 1.0236e-06, 1.3513e-07,\n",
      "         1.9692e-06],\n",
      "        ...,\n",
      "        [1.9724e-09, 1.8422e-09, 2.7875e-09,  ..., 3.3250e-09, 3.7138e-11,\n",
      "         3.4282e-09],\n",
      "        [2.0561e-09, 1.6837e-09, 3.6983e-09,  ..., 3.3882e-09, 4.6466e-11,\n",
      "         3.6061e-09],\n",
      "        [1.4024e-08, 1.2566e-08, 1.7753e-08,  ..., 1.9187e-08, 2.1439e-10,\n",
      "         2.0125e-08]])}, 41: {'step': tensor(3341.), 'exp_avg': tensor([[-7.6859e-05,  2.1559e-04,  2.6594e-04,  4.8196e-08,  6.7458e-04],\n",
      "        [ 2.8988e-03,  2.9137e-03,  3.4949e-04, -1.8193e-06, -5.5116e-04],\n",
      "        [-3.2640e-04, -4.1730e-04,  1.4239e-03,  2.0474e-07,  6.0946e-04],\n",
      "        [-2.6445e-04, -7.3392e-04, -7.0914e-04,  1.6598e-07,  2.1709e-05],\n",
      "        [ 1.7930e-02,  2.0120e-02,  1.6399e-02, -1.1254e-05,  2.7892e-03],\n",
      "        [ 2.7691e-04, -5.0554e-07,  3.2983e-04, -1.7383e-07,  3.4547e-04],\n",
      "        [-5.0620e-04, -2.1973e-03, -2.1132e-03,  3.1770e-07,  8.4919e-04],\n",
      "        [-2.6190e-04, -8.0415e-04,  4.8139e-04,  1.6432e-07,  8.8123e-04],\n",
      "        [-8.0744e-03, -9.0165e-03, -8.0251e-03,  5.0670e-06, -1.2919e-03],\n",
      "        [ 4.1867e-04,  4.7689e-04, -7.0213e-04, -2.6274e-07, -8.8312e-04],\n",
      "        [ 3.1096e-04,  2.1538e-04,  1.1017e-03, -1.9527e-07,  9.5530e-04],\n",
      "        [-7.4380e-05, -2.1821e-03, -2.3549e-04,  4.6559e-08,  1.9088e-03],\n",
      "        [ 2.0135e-02,  2.1129e-02,  1.9963e-02, -1.2634e-05,  5.1574e-03],\n",
      "        [-3.9257e-05, -4.7949e-04,  4.6861e-04,  2.4479e-08,  1.0498e-03],\n",
      "        [-2.4307e-04,  1.2530e-03, -8.9039e-04,  1.5283e-07, -2.3326e-03],\n",
      "        [ 4.9972e-05, -1.0175e-04, -1.7152e-04, -3.1433e-08,  5.6193e-04],\n",
      "        [-6.5619e-03, -6.2003e-03, -4.3455e-03,  4.1198e-06, -8.4821e-04],\n",
      "        [ 2.9084e-04,  1.2368e-04,  4.8273e-04, -1.8281e-07,  1.3815e-03],\n",
      "        [-3.0461e-02, -3.1576e-02, -2.9888e-02,  1.9122e-05, -7.8227e-03],\n",
      "        [ 9.9236e-05, -7.2482e-04, -7.7407e-04, -6.2301e-08,  4.6521e-04],\n",
      "        [ 6.8109e-04,  2.9692e-04, -5.7598e-04, -4.2756e-07,  1.9503e-04],\n",
      "        [-9.0355e-05,  8.7733e-04,  5.4464e-04,  5.6797e-08, -9.7485e-04],\n",
      "        [ 5.8075e-04,  1.4807e-03,  3.3610e-04, -3.6452e-07, -9.5326e-04],\n",
      "        [ 1.5131e-04,  4.7537e-04,  8.5833e-04, -9.4953e-08,  4.3361e-05],\n",
      "        [ 3.7763e-04,  1.0042e-03,  1.6064e-04, -2.3697e-07, -1.0928e-03],\n",
      "        [-1.5727e-03, -1.7761e-04, -1.1742e-03,  9.8675e-07, -4.4091e-04],\n",
      "        [-5.7062e-05, -4.2731e-05, -1.1748e-03,  3.5944e-08, -8.0289e-04],\n",
      "        [-3.0142e-04,  1.8758e-03,  2.3654e-03,  1.8928e-07, -1.2884e-03],\n",
      "        [-7.8769e-05, -1.3285e-03,  3.4053e-04,  4.9336e-08,  1.0261e-03],\n",
      "        [ 4.7351e-04,  5.1079e-04, -8.0319e-04, -2.9702e-07,  6.8817e-04],\n",
      "        [ 2.4956e-04,  5.6025e-04, -3.0855e-04, -1.5655e-07, -1.3483e-03],\n",
      "        [-4.6198e-04, -1.2105e-03, -8.7933e-04,  2.9008e-07,  1.6367e-04],\n",
      "        [-2.2645e-04, -9.4746e-04,  3.7648e-04,  1.4210e-07,  1.0783e-03],\n",
      "        [ 1.0985e-04,  3.4522e-04, -5.8732e-04, -6.8922e-08,  3.0883e-04],\n",
      "        [-1.4005e-04, -9.2164e-04, -4.9249e-04,  8.7900e-08,  3.8763e-04],\n",
      "        [-8.0486e-03, -5.2698e-03, -4.5188e-03,  5.0536e-06, -2.1778e-03],\n",
      "        [-4.7471e-03, -2.9513e-03, -4.1187e-03,  2.9784e-06, -1.6390e-03],\n",
      "        [ 7.6381e-03,  7.0279e-03,  7.7817e-03, -4.7973e-06,  2.6162e-03],\n",
      "        [ 1.2671e-02,  1.2620e-02,  1.1664e-02, -7.9539e-06,  2.6848e-03],\n",
      "        [-6.5978e-04,  1.9690e-04, -1.3231e-03,  4.1407e-07, -1.5672e-03],\n",
      "        [-1.9743e-02, -1.9695e-02, -1.6638e-02,  1.2394e-05, -3.8099e-03],\n",
      "        [-8.6891e-05,  7.0668e-05,  9.0773e-04,  5.4531e-08, -9.6348e-05],\n",
      "        [ 6.1377e-04,  2.6584e-04,  4.5428e-04, -3.8528e-07, -2.9880e-04],\n",
      "        [ 8.5956e-04, -8.6840e-05, -9.8704e-04, -5.3957e-07,  1.3669e-03],\n",
      "        [ 2.1424e-04,  7.0465e-04, -5.3574e-04, -1.3455e-07, -7.2788e-05],\n",
      "        [ 2.7102e-02,  2.8423e-02,  2.6787e-02, -1.7011e-05,  7.2255e-03],\n",
      "        [-1.6994e-06, -1.5285e-03,  3.4215e-04,  1.0938e-09,  2.5427e-03],\n",
      "        [ 1.0512e-06, -4.8880e-05, -3.5922e-04, -6.4987e-10, -5.6203e-04],\n",
      "        [-1.7895e-02, -1.7808e-02, -1.7545e-02,  1.1234e-05, -4.4503e-03],\n",
      "        [-1.2240e-03, -9.2592e-04,  1.4643e-03,  7.6862e-07,  5.4599e-04],\n",
      "        [-6.2060e-03, -3.1564e-03, -3.5493e-03,  3.8961e-06, -3.2105e-03],\n",
      "        [ 2.0174e-04,  1.2361e-03,  1.9479e-04, -1.2664e-07, -1.2940e-03],\n",
      "        [ 4.6404e-04,  6.5448e-04, -1.5570e-05, -2.9126e-07, -5.1068e-04],\n",
      "        [-1.3747e-04, -1.9506e-03, -2.1404e-03,  8.6286e-08,  1.0721e-03],\n",
      "        [ 4.1997e-02,  4.4205e-02,  3.9783e-02, -2.6357e-05,  9.8582e-03],\n",
      "        [ 5.5950e-04,  8.1974e-04, -1.9619e-04, -3.5106e-07, -1.0853e-03],\n",
      "        [-1.2477e-04,  2.9458e-04,  9.3867e-04,  7.8336e-08,  4.3747e-04],\n",
      "        [ 1.9252e-04, -2.0654e-04, -3.7473e-04, -1.2081e-07,  7.8887e-04],\n",
      "        [ 8.5973e-04, -5.2539e-04,  1.1554e-04, -5.3953e-07,  7.2480e-04],\n",
      "        [ 3.7070e-04,  9.7269e-04, -5.1443e-04, -2.3270e-07, -3.9488e-04],\n",
      "        [ 1.2570e-05,  7.7362e-04,  4.7978e-04, -7.9375e-09, -2.1806e-04],\n",
      "        [-2.4836e-02, -2.6043e-02, -2.3709e-02,  1.5594e-05, -5.7742e-03],\n",
      "        [-1.5781e-03,  3.5904e-04, -9.7858e-04,  9.9049e-07, -8.8194e-04],\n",
      "        [-2.2884e-04,  1.4310e-04, -8.7223e-05,  1.4361e-07,  3.4441e-04]]), 'exp_avg_sq': tensor([[3.3294e-06, 4.5771e-05, 4.0844e-05, 1.3120e-12, 6.0086e-06],\n",
      "        [4.1017e-04, 5.4949e-04, 4.8397e-04, 1.6162e-10, 7.2860e-05],\n",
      "        [2.7168e-05, 5.1383e-05, 4.7206e-05, 1.0706e-11, 8.3053e-06],\n",
      "        [6.9832e-07, 6.7579e-06, 3.7313e-06, 2.7514e-13, 3.6429e-06],\n",
      "        [5.5942e-03, 1.1249e-02, 1.0674e-02, 2.2043e-09, 5.6716e-04],\n",
      "        [9.3942e-07, 1.4578e-05, 1.1435e-05, 3.7020e-13, 3.4324e-06],\n",
      "        [2.2800e-06, 3.1957e-05, 2.9466e-05, 8.9839e-13, 5.9336e-06],\n",
      "        [2.2215e-06, 2.8038e-05, 2.2977e-05, 8.7543e-13, 5.8702e-06],\n",
      "        [1.1826e-03, 2.2257e-03, 2.0924e-03, 4.6597e-10, 1.2800e-04],\n",
      "        [1.7358e-06, 1.7470e-05, 1.4793e-05, 6.8410e-13, 5.8445e-06],\n",
      "        [9.9047e-07, 6.9805e-06, 3.2975e-06, 3.9027e-13, 3.7652e-06],\n",
      "        [1.4482e-06, 1.7619e-05, 1.2958e-05, 5.7069e-13, 5.7506e-06],\n",
      "        [5.4241e-03, 1.2097e-02, 1.1550e-02, 2.1373e-09, 5.5485e-04],\n",
      "        [1.3791e-06, 1.2826e-05, 7.0960e-06, 5.4335e-13, 6.0839e-06],\n",
      "        [1.8246e-06, 1.6676e-05, 1.4440e-05, 7.1901e-13, 3.4166e-06],\n",
      "        [9.8121e-07, 1.9708e-05, 1.6573e-05, 3.8667e-13, 3.3290e-06],\n",
      "        [6.4262e-04, 1.5584e-03, 1.4724e-03, 2.5321e-10, 8.4019e-05],\n",
      "        [1.1885e-06, 1.4775e-05, 8.2064e-06, 4.6824e-13, 5.6772e-06],\n",
      "        [1.1967e-02, 2.6494e-02, 2.5300e-02, 4.7151e-09, 1.2080e-03],\n",
      "        [1.6865e-06, 3.0424e-05, 2.7839e-05, 6.6458e-13, 4.5975e-06],\n",
      "        [4.1038e-06, 2.2370e-05, 1.7179e-05, 1.6172e-12, 6.6996e-06],\n",
      "        [1.3564e-06, 1.8321e-05, 1.6273e-05, 5.3446e-13, 2.8433e-06],\n",
      "        [1.5949e-06, 2.2883e-05, 1.9745e-05, 6.2845e-13, 2.5126e-06],\n",
      "        [7.7631e-07, 1.2149e-05, 9.8770e-06, 3.0585e-13, 3.2771e-06],\n",
      "        [4.9245e-07, 4.1331e-06, 3.0163e-06, 1.9402e-13, 1.2829e-06],\n",
      "        [3.7832e-05, 2.1892e-04, 2.1442e-04, 1.4907e-11, 9.0790e-06],\n",
      "        [1.1368e-06, 1.3724e-05, 9.8615e-06, 4.4792e-13, 4.2922e-06],\n",
      "        [1.5325e-05, 6.0527e-05, 4.9965e-05, 6.0396e-12, 2.0086e-05],\n",
      "        [1.8857e-05, 6.5099e-05, 5.5689e-05, 7.4309e-12, 1.2212e-05],\n",
      "        [5.2091e-06, 1.8167e-05, 1.1776e-05, 2.0527e-12, 5.9560e-06],\n",
      "        [9.6960e-07, 1.6552e-05, 9.6526e-06, 3.8206e-13, 7.1662e-06],\n",
      "        [1.3437e-06, 7.3062e-06, 6.5433e-06, 5.2946e-13, 1.1178e-06],\n",
      "        [8.9444e-07, 9.1000e-06, 4.2116e-06, 3.5241e-13, 5.2026e-06],\n",
      "        [1.8908e-06, 2.0127e-05, 1.6252e-05, 7.4515e-13, 4.8210e-06],\n",
      "        [5.3006e-07, 6.1302e-06, 4.9158e-06, 2.0885e-13, 1.9305e-06],\n",
      "        [9.8085e-04, 2.5830e-03, 2.4792e-03, 3.8647e-10, 1.3040e-04],\n",
      "        [3.3786e-04, 3.1003e-03, 3.0290e-03, 1.3312e-10, 7.9106e-05],\n",
      "        [6.5489e-04, 2.2065e-03, 2.1359e-03, 2.5804e-10, 8.2072e-05],\n",
      "        [2.4947e-03, 7.1480e-03, 6.8313e-03, 9.8295e-10, 3.1265e-04],\n",
      "        [3.6455e-05, 9.1990e-05, 7.0563e-05, 1.4366e-11, 3.2208e-05],\n",
      "        [5.2835e-03, 1.3777e-02, 1.3179e-02, 2.0818e-09, 5.8827e-04],\n",
      "        [3.4610e-06, 1.8744e-05, 1.4881e-05, 1.3639e-12, 6.9390e-06],\n",
      "        [1.6347e-06, 2.2764e-05, 1.9420e-05, 6.4412e-13, 3.9950e-06],\n",
      "        [1.2353e-05, 3.6444e-05, 2.9361e-05, 4.8674e-12, 9.4534e-06],\n",
      "        [6.3680e-07, 1.1935e-05, 7.7969e-06, 2.5096e-13, 4.3704e-06],\n",
      "        [9.3444e-03, 1.9127e-02, 1.8156e-02, 3.6820e-09, 9.3044e-04],\n",
      "        [2.6504e-06, 3.3356e-05, 2.7716e-05, 1.0443e-12, 6.4235e-06],\n",
      "        [1.4404e-06, 8.7492e-06, 6.2878e-06, 5.6761e-13, 3.5946e-06],\n",
      "        [4.2410e-03, 1.1246e-02, 1.0848e-02, 1.6711e-09, 4.5362e-04],\n",
      "        [1.2497e-05, 3.8978e-05, 3.2972e-05, 4.9244e-12, 8.2249e-06],\n",
      "        [6.9135e-04, 1.9012e-03, 1.8089e-03, 2.7242e-10, 9.4467e-05],\n",
      "        [1.2640e-06, 2.8786e-05, 2.4291e-05, 4.9809e-13, 4.8898e-06],\n",
      "        [9.0585e-07, 1.5986e-05, 1.2175e-05, 3.5696e-13, 5.2080e-06],\n",
      "        [2.7295e-05, 5.1534e-05, 4.1561e-05, 1.0756e-11, 1.5831e-05],\n",
      "        [2.2308e-02, 4.7131e-02, 4.4977e-02, 8.7897e-09, 2.2644e-03],\n",
      "        [7.1986e-07, 8.2600e-06, 5.0386e-06, 2.8362e-13, 2.4811e-06],\n",
      "        [1.2900e-06, 1.5366e-05, 1.3933e-05, 5.0831e-13, 2.2519e-06],\n",
      "        [7.5349e-07, 1.0092e-05, 6.7753e-06, 2.9684e-13, 3.4596e-06],\n",
      "        [1.6970e-06, 2.5930e-05, 1.6801e-05, 6.6862e-13, 8.8864e-06],\n",
      "        [1.5827e-06, 1.1769e-05, 9.4412e-06, 6.2375e-13, 2.9081e-06],\n",
      "        [2.4491e-06, 1.8811e-05, 1.5900e-05, 9.6527e-13, 4.0477e-06],\n",
      "        [8.8372e-03, 2.3738e-02, 2.2732e-02, 3.4821e-09, 1.0186e-03],\n",
      "        [2.8703e-05, 4.9043e-05, 4.4124e-05, 1.1310e-11, 8.5620e-06],\n",
      "        [4.7252e-06, 2.7494e-05, 2.2819e-05, 1.8622e-12, 5.1445e-06]])}, 42: {'step': tensor(3341.), 'exp_avg': tensor([ 2.1559e-04,  2.9137e-03, -4.1730e-04, -7.3392e-04,  2.0120e-02,\n",
      "        -5.0554e-07, -2.1973e-03, -8.0415e-04, -9.0165e-03,  4.7689e-04,\n",
      "         2.1538e-04, -2.1821e-03,  2.1129e-02, -4.7949e-04,  1.2531e-03,\n",
      "        -1.0175e-04, -6.2003e-03,  1.2368e-04, -3.1576e-02, -7.2482e-04,\n",
      "         2.9692e-04,  8.7733e-04,  1.4807e-03,  4.7537e-04,  1.0042e-03,\n",
      "        -1.7761e-04, -4.2731e-05,  1.8758e-03, -1.3285e-03,  5.1079e-04,\n",
      "         5.6025e-04, -1.2105e-03, -9.4746e-04,  3.4522e-04, -9.2164e-04,\n",
      "        -5.2709e-03, -2.9513e-03,  7.0279e-03,  1.2620e-02,  1.9690e-04,\n",
      "        -1.9695e-02,  7.0668e-05,  2.6584e-04, -8.6840e-05,  7.0465e-04,\n",
      "         2.8423e-02, -1.5285e-03, -4.8880e-05, -1.7808e-02, -9.2592e-04,\n",
      "        -3.1564e-03,  1.2361e-03,  6.5448e-04, -1.9506e-03,  4.4205e-02,\n",
      "         8.1974e-04,  2.9458e-04, -2.0654e-04, -5.2539e-04,  9.7269e-04,\n",
      "         7.7362e-04, -2.6042e-02,  3.5904e-04,  1.4310e-04]), 'exp_avg_sq': tensor([4.5771e-05, 5.4949e-04, 5.1383e-05, 6.7579e-06, 1.1249e-02, 1.4580e-05,\n",
      "        3.1958e-05, 2.8038e-05, 2.2257e-03, 1.7470e-05, 6.9805e-06, 1.7619e-05,\n",
      "        1.2097e-02, 1.2826e-05, 1.6676e-05, 1.9708e-05, 1.5584e-03, 1.4775e-05,\n",
      "        2.6494e-02, 3.0424e-05, 2.2370e-05, 1.8321e-05, 2.2883e-05, 1.2149e-05,\n",
      "        4.1331e-06, 2.1892e-04, 1.3724e-05, 6.0527e-05, 6.5099e-05, 1.8168e-05,\n",
      "        1.6552e-05, 7.3062e-06, 9.1000e-06, 2.0127e-05, 6.1302e-06, 2.5830e-03,\n",
      "        3.1003e-03, 2.2065e-03, 7.1480e-03, 9.1990e-05, 1.3777e-02, 1.8744e-05,\n",
      "        2.2764e-05, 3.6444e-05, 1.1935e-05, 1.9127e-02, 3.3356e-05, 8.7492e-06,\n",
      "        1.1247e-02, 3.8978e-05, 1.9012e-03, 2.8786e-05, 1.5986e-05, 5.1534e-05,\n",
      "        4.7131e-02, 8.2600e-06, 1.5366e-05, 1.0092e-05, 2.5930e-05, 1.1769e-05,\n",
      "        1.8811e-05, 2.3739e-02, 4.9043e-05, 2.7494e-05])}, 43: {'step': tensor(3341.), 'exp_avg': tensor([[ 1.7115e-06, -1.6011e-05,  3.7157e-05, -3.8192e-06, -5.4664e-05,\n",
      "         -7.5711e-06,  1.3255e-05,  2.2365e-05,  3.3089e-05, -9.4685e-06,\n",
      "          1.0763e-05, -9.5347e-06,  1.8988e-05, -3.3716e-05,  6.6137e-06,\n",
      "          2.3724e-05,  1.1099e-05,  4.3638e-05, -3.3758e-06, -1.0687e-06,\n",
      "         -1.0064e-05, -3.8061e-05, -4.7181e-05, -1.7875e-05, -1.3898e-06,\n",
      "          4.7454e-06, -1.8157e-05,  8.3762e-06,  9.4298e-06,  1.9391e-05,\n",
      "          1.1997e-05, -1.5906e-05, -5.1737e-07, -1.1037e-05, -1.5192e-06,\n",
      "         -7.9413e-06,  4.9413e-05, -3.0107e-05,  3.5295e-05, -6.7627e-06,\n",
      "          2.0018e-05,  2.8957e-05,  1.3791e-05,  1.5243e-05,  3.0025e-06,\n",
      "         -3.6104e-05, -3.7451e-06,  1.3269e-04,  1.1692e-05,  4.5959e-05,\n",
      "         -1.4675e-06,  8.0492e-06,  9.0177e-05, -4.1075e-05, -5.0662e-05,\n",
      "          1.0266e-05, -2.9076e-05,  7.8501e-06,  1.5146e-05,  1.0613e-06,\n",
      "          2.0670e-05,  1.0181e-04,  2.0309e-05,  1.0946e-04]]), 'exp_avg_sq': tensor([[1.3196e-09, 1.4538e-09, 6.7370e-09, 4.5952e-09, 4.8296e-08, 2.6803e-09,\n",
      "         3.4983e-10, 9.7003e-10, 4.2185e-09, 1.5750e-09, 2.5312e-09, 1.0341e-09,\n",
      "         1.3313e-08, 9.9518e-09, 8.4134e-10, 2.9012e-09, 4.4958e-09, 3.0989e-09,\n",
      "         2.8911e-08, 8.0550e-10, 4.1188e-09, 4.2991e-09, 2.9309e-08, 2.0570e-09,\n",
      "         1.3593e-09, 6.2562e-10, 7.5036e-10, 2.8123e-09, 2.6676e-10, 6.9463e-09,\n",
      "         4.6142e-10, 1.3120e-09, 3.2739e-08, 3.1382e-09, 1.4972e-10, 3.9974e-09,\n",
      "         4.6972e-08, 1.1938e-08, 6.8253e-08, 1.7118e-09, 1.4998e-08, 2.4289e-08,\n",
      "         1.4722e-09, 5.6551e-10, 4.1066e-10, 2.0189e-08, 9.2378e-10, 5.8904e-07,\n",
      "         9.1078e-09, 9.8032e-09, 2.6034e-09, 3.9746e-09, 1.0163e-07, 4.9172e-09,\n",
      "         1.5324e-08, 1.3163e-09, 6.8339e-09, 4.9387e-09, 3.6387e-10, 1.9379e-09,\n",
      "         1.4378e-08, 2.3002e-07, 8.8880e-10, 2.3396e-07]])}, 44: {'step': tensor(3341.), 'exp_avg': tensor([[-2.1769e-05, -3.3288e-06,  1.7792e-05,  ..., -1.9471e-04,\n",
      "         -9.2847e-05, -3.9574e-05],\n",
      "        [ 4.4257e-05, -8.9839e-05,  1.3802e-04,  ...,  5.1683e-04,\n",
      "          2.6092e-04, -6.2039e-05],\n",
      "        [ 1.3406e-04,  5.9135e-05, -1.4586e-04,  ...,  8.3378e-04,\n",
      "          2.3196e-04,  1.2416e-04],\n",
      "        ...,\n",
      "        [-7.8691e-05, -6.2094e-05,  4.4380e-04,  ..., -1.1944e-03,\n",
      "         -1.0512e-04, -2.5045e-04],\n",
      "        [-2.7136e-06,  8.0115e-05, -2.4547e-05,  ..., -5.7613e-04,\n",
      "         -1.7232e-04,  2.1709e-05],\n",
      "        [-4.5646e-05, -4.4892e-05, -6.3280e-04,  ...,  1.9301e-03,\n",
      "         -1.0874e-04,  1.9860e-04]]), 'exp_avg_sq': tensor([[2.7573e-08, 4.3335e-08, 3.7622e-08,  ..., 4.8590e-07, 1.0227e-07,\n",
      "         2.2062e-08],\n",
      "        [7.5793e-08, 7.7713e-08, 1.1935e-07,  ..., 1.6502e-06, 1.7482e-07,\n",
      "         6.0017e-08],\n",
      "        [2.0938e-07, 9.9632e-08, 4.6709e-07,  ..., 5.4057e-06, 6.7727e-07,\n",
      "         4.9010e-07],\n",
      "        ...,\n",
      "        [1.7248e-07, 1.4839e-07, 3.0497e-06,  ..., 3.2433e-05, 6.5325e-07,\n",
      "         5.1656e-07],\n",
      "        [4.2605e-08, 4.9677e-08, 1.3128e-07,  ..., 1.7344e-06, 6.3811e-08,\n",
      "         4.3182e-08],\n",
      "        [6.2216e-07, 4.1561e-07, 5.4280e-06,  ..., 5.6676e-05, 1.4764e-06,\n",
      "         1.9699e-06]])}, 45: {'step': tensor(3341.), 'exp_avg': tensor([ 6.1316e-04,  3.9894e-04, -6.2300e-04, -4.5090e-04, -1.8804e-03,\n",
      "         8.1030e-04,  1.2771e-03, -5.3220e-05, -2.1445e-04, -5.9620e-04,\n",
      "         4.0456e-04,  9.2449e-04,  1.7981e-03,  6.8723e-04,  2.0244e-03,\n",
      "         1.1118e-03,  4.2751e-04,  1.3233e-03,  6.0280e-06, -1.0979e-03,\n",
      "         5.9103e-05, -1.6394e-03,  4.2280e-03, -2.9628e-04, -1.0632e-03,\n",
      "         5.1681e-04,  1.1624e-03,  1.4775e-04,  3.8053e-04,  4.1472e-05,\n",
      "        -1.0632e-03, -1.2202e-03, -1.2150e-03, -4.1283e-04,  1.3774e-03,\n",
      "        -7.6273e-05,  6.2143e-04, -2.1751e-03,  3.4760e-04,  5.8630e-04,\n",
      "        -6.2279e-04,  8.8879e-05, -5.5106e-04, -1.1616e-04,  9.7428e-04,\n",
      "        -4.3352e-04,  1.2124e-03, -8.2110e-03, -1.6982e-03,  3.0984e-04,\n",
      "        -3.4603e-04,  1.5427e-03,  2.6009e-03, -8.5556e-04,  1.2274e-03,\n",
      "         8.2875e-04,  6.4169e-04, -5.4927e-05, -3.9114e-04, -1.0785e-03,\n",
      "        -1.7744e-04,  3.4238e-03,  2.4761e-04, -3.6979e-03]), 'exp_avg_sq': tensor([5.3456e-06, 1.5325e-05, 1.1921e-04, 3.5751e-06, 1.1878e-04, 7.5798e-06,\n",
      "        3.0084e-06, 1.8100e-05, 9.3522e-06, 2.0707e-05, 2.5730e-05, 4.1801e-06,\n",
      "        2.2239e-05, 2.0036e-06, 5.3839e-05, 3.2558e-06, 6.0397e-06, 3.5897e-06,\n",
      "        1.2286e-05, 8.1892e-06, 4.2675e-06, 7.9194e-05, 2.9994e-04, 4.0995e-05,\n",
      "        8.3718e-05, 1.2866e-05, 3.9713e-06, 5.6551e-06, 1.2473e-05, 3.1166e-05,\n",
      "        4.3472e-05, 9.7112e-06, 5.1473e-05, 6.1639e-06, 3.0078e-06, 6.2188e-06,\n",
      "        9.8130e-06, 2.0530e-04, 9.7700e-06, 9.8378e-06, 7.9698e-06, 5.9775e-06,\n",
      "        3.4487e-05, 8.7035e-06, 6.2708e-06, 1.0196e-05, 1.9351e-05, 1.3358e-03,\n",
      "        1.1998e-05, 5.2401e-06, 3.2310e-06, 1.0324e-04, 4.4565e-04, 7.9786e-05,\n",
      "        2.6981e-06, 8.0740e-06, 5.0636e-06, 3.4465e-06, 2.3631e-05, 1.0542e-05,\n",
      "        4.4361e-06, 2.0621e-04, 1.4330e-05, 5.3686e-04])}, 46: {'step': tensor(3341.), 'exp_avg': tensor([[-2.9196e-04, -4.5972e-04,  8.0966e-04,  ...,  1.7626e-04,\n",
      "         -4.1897e-04,  2.6388e-04],\n",
      "        [ 3.9747e-04,  2.5268e-04, -6.3066e-04,  ..., -3.9132e-05,\n",
      "          2.9047e-04, -6.8286e-04],\n",
      "        [ 1.5336e-04, -6.4112e-05,  1.7272e-04,  ...,  1.5037e-04,\n",
      "         -1.0041e-04,  3.3866e-04],\n",
      "        ...,\n",
      "        [ 6.8713e-05, -1.1180e-06,  4.6555e-05,  ...,  1.1145e-04,\n",
      "         -3.8959e-05, -5.3579e-04],\n",
      "        [-5.8318e-05, -1.1929e-05, -9.6025e-05,  ..., -2.5720e-05,\n",
      "          6.9076e-05,  1.1784e-04],\n",
      "        [-5.6625e-05,  1.0470e-04, -1.1669e-04,  ..., -6.2265e-05,\n",
      "         -2.0133e-05,  1.8103e-04]]), 'exp_avg_sq': tensor([[2.0443e-07, 4.5992e-07, 5.9188e-07,  ..., 7.8413e-07, 6.0403e-07,\n",
      "         5.2585e-07],\n",
      "        [3.4594e-07, 1.0267e-06, 1.6485e-06,  ..., 1.0249e-06, 1.0985e-06,\n",
      "         1.7873e-06],\n",
      "        [3.9400e-07, 8.9814e-07, 1.0913e-06,  ..., 1.1186e-06, 1.2017e-06,\n",
      "         1.3389e-06],\n",
      "        ...,\n",
      "        [1.3268e-07, 3.8939e-07, 6.7931e-07,  ..., 1.4261e-06, 7.7125e-07,\n",
      "         7.8419e-07],\n",
      "        [1.5605e-08, 1.9101e-08, 6.4520e-08,  ..., 5.5271e-08, 5.2109e-08,\n",
      "         3.0208e-08],\n",
      "        [9.4855e-08, 2.5327e-07, 1.7586e-07,  ..., 1.3684e-06, 2.6633e-07,\n",
      "         5.0531e-07]])}, 47: {'step': tensor(3341.), 'exp_avg': tensor([ 1.1701e-03, -3.8728e-04, -2.2529e-04,  3.5209e-04,  1.0136e-04,\n",
      "         6.8143e-04,  3.3714e-04,  7.2854e-04,  5.6844e-04, -1.9724e-03,\n",
      "         4.2015e-04,  1.0903e-03,  1.1377e-04,  3.1196e-04,  3.1593e-05,\n",
      "        -1.0372e-03,  2.7854e-04, -9.3367e-04,  4.3346e-05,  2.6456e-04,\n",
      "         2.0533e-04, -1.8770e-04,  2.3126e-04,  1.2018e-04, -7.4531e-05,\n",
      "         5.1288e-04,  4.8164e-05,  1.2185e-03,  2.3888e-05,  5.1789e-04,\n",
      "        -8.4002e-05, -5.3581e-05, -2.5016e-04, -2.0166e-04, -2.3429e-04,\n",
      "         4.3037e-05,  2.2139e-04, -2.5867e-04,  3.9809e-04, -3.8450e-04,\n",
      "         3.8138e-04, -3.7177e-04, -9.0130e-05, -2.6408e-04,  4.6416e-05,\n",
      "        -1.1903e-03,  4.4769e-04,  1.4065e-04,  7.7230e-04,  8.2199e-04,\n",
      "        -3.5213e-05,  3.5073e-05,  5.5891e-04, -1.0552e-03, -1.3214e-03,\n",
      "        -2.0747e-04, -2.2062e-04, -4.2715e-04, -4.1075e-04, -4.4652e-05,\n",
      "         3.2268e-04,  1.7452e-04, -9.5001e-05, -2.6439e-04]), 'exp_avg_sq': tensor([2.6331e-06, 3.6011e-06, 2.8122e-06, 4.6268e-06, 2.5331e-07, 6.5366e-06,\n",
      "        1.6531e-06, 1.0761e-06, 2.8088e-06, 1.4765e-05, 1.7599e-06, 5.3248e-06,\n",
      "        1.7131e-06, 2.0918e-06, 6.9061e-06, 1.9852e-06, 2.5615e-06, 2.8441e-06,\n",
      "        2.6188e-07, 7.6407e-06, 6.4971e-07, 9.8118e-07, 2.5047e-06, 3.1457e-07,\n",
      "        5.7480e-06, 1.7880e-06, 5.9284e-07, 6.4256e-06, 9.0008e-07, 4.6459e-06,\n",
      "        1.3401e-06, 1.8852e-06, 1.6227e-06, 2.5338e-06, 3.7202e-07, 1.1702e-06,\n",
      "        1.8328e-06, 6.0535e-06, 2.8035e-06, 1.0081e-06, 8.8370e-07, 2.7776e-07,\n",
      "        2.2171e-06, 5.3905e-07, 1.0008e-06, 5.1562e-07, 3.8896e-06, 3.6437e-06,\n",
      "        4.3265e-06, 1.1211e-05, 9.7226e-06, 6.5999e-06, 1.0728e-06, 5.2315e-06,\n",
      "        5.1072e-06, 1.8599e-06, 1.1660e-05, 4.7538e-06, 1.3871e-06, 2.4906e-07,\n",
      "        2.3519e-06, 3.9782e-06, 2.0686e-07, 2.6408e-06])}, 48: {'step': tensor(3341.), 'exp_avg': tensor([[-3.8311e-04, -9.0785e-04, -1.9504e-03,  ..., -4.8611e-04,\n",
      "          2.7394e-03,  2.7279e-04],\n",
      "        [-5.9912e-07, -1.9354e-05, -2.9206e-05,  ...,  2.2101e-05,\n",
      "          7.3730e-05,  2.0469e-05],\n",
      "        [ 1.3843e-05, -3.5529e-05, -6.2430e-05,  ...,  4.9815e-05,\n",
      "          1.1742e-04, -1.1306e-05],\n",
      "        ...,\n",
      "        [ 7.1916e-06, -1.8589e-06, -1.1261e-05,  ...,  6.4025e-06,\n",
      "          1.6705e-05, -1.3787e-06],\n",
      "        [ 3.9351e-06, -8.8166e-06, -1.7423e-06,  ...,  4.1201e-06,\n",
      "         -8.4893e-06,  4.3770e-06],\n",
      "        [ 7.0632e-06, -1.5562e-05, -1.2659e-05,  ...,  8.4373e-06,\n",
      "          1.7366e-05,  1.6823e-05]]), 'exp_avg_sq': tensor([[8.4525e-06, 8.2061e-06, 8.1700e-06,  ..., 5.7863e-06, 6.2341e-05,\n",
      "         1.7737e-05],\n",
      "        [7.9059e-09, 2.3080e-09, 2.5832e-09,  ..., 2.0424e-09, 1.8581e-08,\n",
      "         3.1558e-09],\n",
      "        [1.6530e-08, 3.7696e-09, 4.1547e-09,  ..., 2.2194e-09, 5.3796e-08,\n",
      "         4.2727e-09],\n",
      "        ...,\n",
      "        [5.0769e-09, 5.3283e-10, 2.7547e-09,  ..., 2.3972e-09, 1.3094e-08,\n",
      "         3.9250e-10],\n",
      "        [1.8444e-10, 9.9884e-11, 1.6203e-10,  ..., 1.1729e-10, 3.5986e-10,\n",
      "         5.3790e-11],\n",
      "        [3.3575e-10, 2.8621e-10, 2.9877e-10,  ..., 2.1411e-10, 5.7983e-10,\n",
      "         2.0713e-10]])}}, 'param_groups': [{'lr': 0.0005262044560943643, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.1, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.0006, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]}, {'lr': 0.0005262044560943643, 'weight_decay': 0.0, 'betas': [0.9, 0.999], 'eps': 1e-08, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.0006, 'params': [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]}]}], 'lr_schedulers': [{'param_group_field': 'lr', '_initial_param_group_field': 'initial_lr', 'base_values': [0.0006, 0.0006], 'metric': None, 't_in_epochs': False, 'noise_range_t': None, 'noise_pct': 0.67, 'noise_type': 'normal', 'noise_std': 1.0, 'noise_seed': 42, 't_initial': 381500, 'lr_min': 5.9999999999999995e-05, 'cycle_mul': 1.0, 'cycle_decay': 1.0, 'cycle_limit': 1, 'warmup_t': 3815.0, 'warmup_lr_init': 1e-06, 'warmup_prefix': False, 'k_decay': 1.0, 'warmup_steps': [1.570117955439056e-07, 1.570117955439056e-07], '_last_epoch': 3345}], 'MixedPrecision': {'scale': 4096.0, 'growth_factor': 2.0, 'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 1695}, 'hparams_name': 'config', 'hyper_parameters': {'train': {'seed': 2222, 'interval': 'step', 'monitor': 'test/loss', 'mode': 'min', 'ema': 0.0, 'test': False, 'debug': False, 'ignore_warnings': False, 'state': {'mode': None, 'n_context': 0, 'n_context_eval': 0}, 'ckpt': None, 'disable_dataset': False, 'validate_at_start': False, 'pretrained_model_path': '/data/leslie/sarthak/hyena/hyena-dna/hyenadna-tiny-1k-seqlen/weights.ckpt', 'pretrained_model_strict_load': True, 'pretrained_model_state_hook': {'_name_': None}, 'post_init_hook': {'_name_': None}, 'layer_decay': {'_name_': None, 'decay': 0.7}, 'gpu_mem': 41, 'global_batch_size': 256}, 'tolerance': {'logdir': './resume', 'id': None}, 'wandb': None, 'trainer': {'_target_': 'pytorch_lightning.Trainer', 'devices': 1, 'accelerator': 'gpu', 'accumulate_grad_batches': 1, 'max_epochs': 100, 'gradient_clip_val': 1.0, 'log_every_n_steps': 10, 'limit_train_batches': 1.0, 'limit_val_batches': 1.0, 'num_nodes': 1, 'precision': 16, 'strategy': 'auto'}, 'loader': {'batch_size': 50, 'num_workers': 4, 'pin_memory': True, 'drop_last': True}, 'dataset': {'_name_': 'cCRE', 'bed_file': None, 'fasta_file': None, 'dataset_name': 'cCRE', 'tokenizer_name': 'char', 'cache_dir': None, 'max_length': 1024, 'add_eos': True, 'batch_size': 256, 'batch_size_eval': 512, 'num_workers': 12, 'shuffle': True, 'pin_memory': True, 'max_length_val': 1024, 'max_length_test': 1024, 'pad_max_length': None, 'rc_aug': True, 'use_fixed_len_val': False, 'replace_N_token': False, 'pad_interval': False}, 'optimizer': {'lr': 0.0006, 'weight_decay': 0.1, 'betas': [0.9, 0.999]}, 'scheduler': {'t_in_epochs': False, 't_initial': 381500, 'lr_min': 5.9999999999999995e-05, 'warmup_lr_init': 1e-06, 'warmup_t': 3815.0, '_name_': 'cosine_warmup_timm'}, 'callbacks': {'learning_rate_monitor': {'logging_interval': 'step'}, 'timer': {'step': True, 'inter_step': False, 'epoch': True, 'val': True, '_name_': 'timer'}, 'params': {'total': True, 'trainable': True, 'fixed': True, '_name_': 'params'}, 'model_checkpoint': {'monitor': 'test/loss', 'mode': 'min', 'save_top_k': 1, 'save_last': True, 'dirpath': 'checkpoints/', 'filename': 'test/loss', 'auto_insert_metric_name': False, 'verbose': True, '_name_': 'model_checkpoint'}}, 'task': {'loss': 'cross_entropy', 'torchmetrics': ['perplexity', 'num_tokens'], '_name_': 'lm'}, 'encoder': None, 'decoder': None, 'model': {'d_model': 128, 'n_layer': 2, 'd_inner': 512, 'vocab_size': 12, 'resid_dropout': 0.0, 'embed_dropout': 0.1, 'fused_mlp': False, 'fused_dropout_add_ln': False, 'checkpoint_mixer': False, 'checkpoint_mlp': False, 'residual_in_fp32': True, 'pad_vocab_size_multiple': 8, 'layer': {'emb_dim': 5, 'filter_order': 64, 'short_filter_order': 3, 'l_max': 1026, 'modulate': True, 'w': 10, 'lr': 0.0006, 'wd': 0.0, 'lr_pos_emb': 0.0, '_name_': 'hyena'}, '_name_': 'lm'}}, 'hparams_type': <class 'omegaconf.dictconfig.DictConfig'>}\n"
     ]
    }
   ],
   "source": [
    "#let's see if we can load the trained model\n",
    "\n",
    "\n",
    "# model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/','hyenadna-tiny-1k-seqlen', device='cpu', use_head=True)\n",
    "# model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/','last', device='cpu', use_head=True)\n",
    "checkpoint = torch.load('/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', map_location=torch.device('cpu'))\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "source": [
    "model = HyenaDNAPreTrainedModel.from_pretrained('/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/','checkpoints', device='cuda', use_head=True)\n",
    "#have to rename the .ckpt file to weights, but it does load if we put the config there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.Size([1, 2])\n",
      "tensor([[0.0496, 0.2254]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def inference(model):\n",
    "    #define some things\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "    max_length = 1024 #has to be less than 1028, which makes sense\n",
    "    use_padding = False\n",
    "    rc_aug = False  # reverse complement augmentation\n",
    "    add_eos = False  # add end of sentence token\n",
    "\n",
    "    # we need these for the decoder head, if using\n",
    "    use_head = False\n",
    "    n_classes = 2  # not used for embeddings only\n",
    "    backbone_cfg = None\n",
    "    # create tokenizer\n",
    "    tokenizer = CharacterTokenizer(\n",
    "        characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters, N is uncertain\n",
    "        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n",
    "        add_special_tokens=False,  # we handle special tokens elsewhere\n",
    "        padding_side='left', # since HyenaDNA is causal, we pad on the left\n",
    "    )\n",
    "\n",
    "    #### Single embedding example ####\n",
    "\n",
    "    # create a sample 450k long, prepare\n",
    "    sequence = 'ACTG' * int(max_length/4)\n",
    "    tok_seq = tokenizer(sequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    # prep model and forward\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    print(embeddings.shape)  # embeddings here!\n",
    "    return embeddings, tok_seq, tokenizer\n",
    "\n",
    "embed,seq,tokenizer = inference(model)\n",
    "print(embed)\n",
    "#so here's the strange thing, why is the output 2 things? shouldn't it be a language modeling procedure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Using Char-level tokenizer**\n",
      "{'[CLS]': 0, '[SEP]': 1, '[BOS]': 2, '[MASK]': 3, '[PAD]': 4, '[RESERVED]': 5, '[UNK]': 6, 'A': 7, 'C': 8, 'G': 9, 'T': 10, 'N': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 115.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CausalLMOutput(logits=tensor([[[-12.7130,  -8.9728, -12.5017,  ..., -12.2773, -12.4860, -12.6194],\n",
      "         [-12.2951,  -8.7940, -12.0502,  ..., -11.8581, -12.0776, -12.1971],\n",
      "         [-12.3912,  -9.0334, -12.2031,  ..., -11.9950, -12.1898, -12.3348],\n",
      "         ...,\n",
      "         [ -9.6103,  -8.6839,  -9.4861,  ...,  -9.2466,  -9.4709,  -9.6077],\n",
      "         [ -7.7952,  -8.2646,  -7.5923,  ...,  -7.5356,  -7.6165,  -7.7128],\n",
      "         [  0.7470,   9.1211,   0.7549,  ...,   0.5151,   0.6694,   0.6526]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>))]\n",
      "torch.Size([1, 73, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#we can also load via the logits\n",
    "\n",
    "import torch \n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import yaml \n",
    "from tqdm import tqdm\n",
    "import json \n",
    "\n",
    "# sys.path.append(os.environ.get(\"SAFARI_PATH\", \".\"))\n",
    "\n",
    "from src.models.sequence.long_conv_lm import ConvLMHeadModel\n",
    "\n",
    "# from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from src.dataloaders.datasets.hg38_char_tokenizer import CharacterTokenizer\n",
    "\n",
    "try:\n",
    "    from tokenizers import Tokenizer  \n",
    "except:\n",
    "    pass\n",
    "\n",
    "# https://github.com/openai/gpt-2/issues/131#issuecomment-492786058\n",
    "# def preprocess(text):\n",
    "#     text = text.replace(\"“\", '\"')\n",
    "#     text = text.replace(\"”\", '\"')\n",
    "#     return '\\n'+text.strip()\n",
    "\n",
    "\n",
    "class HG38Encoder:\n",
    "    \"Encoder inference for HG38 sequences\"\n",
    "    def __init__(self, model_cfg, ckpt_path, max_seq_len):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.model, self.tokenizer = self.load_model(model_cfg, ckpt_path)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def encode(self, seqs):\n",
    "            \n",
    "        results = []\n",
    "\n",
    "        # sample code to loop thru each sample and tokenize first (char level)\n",
    "        for seq in tqdm(seqs):\n",
    "            \n",
    "            if isinstance(self.tokenizer, Tokenizer):\n",
    "                tokenized_seq = self.tokenizer.encode(seq).ids\n",
    "            else:\n",
    "                tokenized_seq = self.tokenizer.encode(seq)\n",
    "            \n",
    "            # can accept a batch, shape [B, seq_len, hidden_dim]\n",
    "            logits, __ = self.model(torch.tensor([tokenized_seq]).to(device=self.device))\n",
    "\n",
    "            # Using head, so just have logits\n",
    "            results.append(logits)\n",
    "\n",
    "        return results\n",
    "        \n",
    "            \n",
    "    def load_model(self, model_cfg, ckpt_path):\n",
    "        config = yaml.load(open(model_cfg, 'r'), Loader=yaml.FullLoader)\n",
    "        model = ConvLMHeadModel(**config['model_config'])\n",
    "        \n",
    "        state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "        # loads model from ddp by removing prexix to single if necessary\n",
    "        torch.nn.modules.utils.consume_prefix_in_state_dict_if_present(\n",
    "            state_dict[\"state_dict\"], \"model.\"\n",
    "        )\n",
    "\n",
    "        model_state_dict = state_dict[\"state_dict\"]\n",
    "\n",
    "        # need to remove torchmetrics. to remove keys, need to convert to list first\n",
    "        for key in list(model_state_dict.keys()):\n",
    "            if \"torchmetrics\" in key:\n",
    "                model_state_dict.pop(key)\n",
    "\n",
    "        model.load_state_dict(state_dict[\"state_dict\"])\n",
    "\n",
    "        # setup tokenizer\n",
    "        if config['tokenizer_name'] == 'char':\n",
    "            print(\"**Using Char-level tokenizer**\")\n",
    "\n",
    "            # add to vocab\n",
    "            tokenizer = CharacterTokenizer(\n",
    "                characters=['A', 'C', 'G', 'T', 'N'],\n",
    "                model_max_length=self.max_seq_len + 2,  # add 2 since default adds eos/eos tokens, crop later\n",
    "                add_special_tokens=False,\n",
    "            )\n",
    "            print(tokenizer._vocab_str_to_int)\n",
    "        else:\n",
    "            raise NotImplementedError(\"You need to provide a custom tokenizer!\")\n",
    "\n",
    "        return model, tokenizer\n",
    "        \n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "# SAFARI_PATH = os.getenv('SAFARI_PATH', '.')\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"--model_cfg\",\n",
    "#     default=f\"{SAFARI_PATH}/configs/evals/hyena_small_150b.yaml\",\n",
    "# )\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"--ckpt_path\",\n",
    "#     default=f\"\",\n",
    "#     help=\"Path to model state dict checkpoint\"\n",
    "# )\n",
    "    \n",
    "# args = parser.parse_args()\n",
    "    \n",
    "# task = HG38Encoder(args.model_cfg, args.ckpt_path, max_seq_len=1024)\n",
    "\n",
    "task = HG38Encoder('/data/leslie/sarthak/hyena/hyena-dna/configs/evals/cCRE.yaml', '/data/leslie/sarthak/hyena/hyena-dna/outputs/2024-01-26/15-03-07-318248/checkpoints/last.ckpt', max_seq_len=1024)\n",
    "\n",
    "# sample sequence, can pass a list of seqs (themselves a list of chars)\n",
    "seqs = [\"ACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGTACGT\"]\n",
    "\n",
    "logits = task.encode(seqs)\n",
    "print(logits)\n",
    "print(logits[0].logits.shape)\n",
    "\n",
    "# breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutput(logits=tensor([[[-12.9311,  -9.0326, -12.7136,  ..., -12.4942, -12.7014, -12.8332],\n",
      "         [-12.3028,  -8.6891, -12.0624,  ..., -11.8627, -12.0896, -12.2054],\n",
      "         [-12.1767,  -8.8591, -11.9926,  ..., -11.7843, -11.9813, -12.1205],\n",
      "         ...,\n",
      "         [ -9.4851,  -8.6454,  -9.3673,  ...,  -9.1197,  -9.3482,  -9.4840],\n",
      "         [ -7.7929,  -8.1901,  -7.5875,  ...,  -7.5260,  -7.6086,  -7.7100],\n",
      "         [  1.2594,   9.4085,   1.2475,  ...,   1.0240,   1.1690,   1.1695]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>))\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(logits[0]) #list with 1 index, and when we index in we get the logits, but the shape makes no sense\n",
    "print(len(seqs[0]))\n",
    "#so our logits are 1,73,16. That part makes sense, since for every input it then predicts, and so predicts to the end\n",
    "#now why there are 16, I have absolutely no clue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[CLS]': 0, '[SEP]': 1, '[BOS]': 2, '[MASK]': 3, '[PAD]': 4, '[RESERVED]': 5, '[UNK]': 6, 'A': 7, 'C': 8, 'G': 9, 'T': 10, 'N': 11}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=1026,  # add 2 since default adds eos/eos tokens, crop later\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "print(tokenizer._vocab_str_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer)) #see 12, but why are the logits outputting 16??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
