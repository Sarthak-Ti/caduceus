{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we are doing this masking procedure and wrote a loss, let's test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataloaders.datasets.general_dataset import GeneralDataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = GeneralDataset(\n",
    "    split='train',\n",
    "    preprocess=False,\n",
    "    data_path='/data1/lesliec/sarthak/data/DK_zarr/zarr_arrays/cell_type_arrays/GM12878_DNase.npz',\n",
    "    data_is_zarr=False,\n",
    "    sequences_bed_file='/data1/lesliec/sarthak/data/DK_zarr/sequences_enformer.bed',\n",
    "    length=524288,\n",
    "    load_in=False\n",
    ")\n",
    "\n",
    "dataset_mask_cont = GeneralDataset(\n",
    "    split='train',\n",
    "    preprocess=False,\n",
    "    data_path='/data1/lesliec/sarthak/data/DK_zarr/zarr_arrays/cell_type_arrays/GM12878_DNase.npz',\n",
    "    data_is_zarr=False,\n",
    "    sequences_bed_file='/data1/lesliec/sarthak/data/DK_zarr/sequences_enformer.bed',\n",
    "    length=524288,\n",
    "    load_in=False,\n",
    "    mlm=0.25,  # increased masking percentage\n",
    "    acc_mask=0.25,  # increased accessibility masking percentage\n",
    "    weight_peaks=True  # weight peaks more (this is the new parameter we added to the dataset class\n",
    ")\n",
    "\n",
    "dataset_mask_cat = GeneralDataset(\n",
    "    split='train',\n",
    "    preprocess=False,\n",
    "    data_path='/data1/lesliec/sarthak/data/DK_zarr/zarr_arrays/cell_type_arrays/GM12878_DNase.npz',\n",
    "    data_is_zarr=False,\n",
    "    sequences_bed_file='/data1/lesliec/sarthak/data/DK_zarr/sequences_enformer.bed',\n",
    "    length=524288,\n",
    "    load_in=False,\n",
    "    mlm=0.25,  # increased masking percentage\n",
    "    acc_mask=0.25,  # increased accessibility masking percentage\n",
    "    acc_type='category',  # categorical accessibility values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: torch.Size([6, 524288])\n",
      "seq_unmask: torch.Size([524288, 6])\n",
      "acc: torch.Size([2, 524288])\n",
      "acc_unmask: torch.Size([524288, 2])\n"
     ]
    }
   ],
   "source": [
    "seq, acc, seq_unmask, acc_unmask = dataset_mask_cont[0]\n",
    "print('seq:', seq.shape)\n",
    "print('seq_unmask:', seq_unmask.shape)\n",
    "print('acc:', acc.shape)\n",
    "print('acc_unmask:', acc_unmask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 524288, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = torch.rand(1, 524288, 5)\n",
    "seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 524288, 6])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_unmask = seq_unmask.unsqueeze(0)\n",
    "seq_unmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 524288, 5]) torch.Size([2, 524288, 6])\n"
     ]
    }
   ],
   "source": [
    "seq = torch.rand(2, 524288, 5) #now simulate batch size of 2\n",
    "#duplicate seq_unmask to match batch size\n",
    "seq_unmask = torch.cat([seq_unmask, seq_unmask], dim=0)\n",
    "print(seq.shape, seq_unmask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([261766, 5]) torch.Size([261766, 6])\n",
      "loss: tensor(1.6420)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def ce_loss_mask_seq(seq,seq_unmask,acc,acc_unmask):\n",
    "    '''cross entropy loss function for sequence and accessibility classification\n",
    "    seq: (batch_size, seq_len, vocab_size)\n",
    "    seq_unmask: (batch_size, seq_len, vocab_size+1) #the last one is the mask\n",
    "    acc: Not used\n",
    "    acc_unmask: Not used\n",
    "    '''\n",
    "    \n",
    "    #mask out useless elements, note this will collapse the batch dimension but that's ok\n",
    "    mask = seq_unmask[:,:,-1] == 1\n",
    "    seq = seq[mask]\n",
    "    seq_unmask = seq_unmask[mask]\n",
    "    \n",
    "    # print(seq.shape, seq_unmask.shape)\n",
    "    \n",
    "    seq_unmask = seq_unmask[:,:-1] #remove the mask dim\n",
    "    \n",
    "    #now compute the loss\n",
    "    loss = F.cross_entropy(seq, seq_unmask)\n",
    "    return loss\n",
    "\n",
    "loss = ce_loss_mask_seq(seq,seq_unmask,acc,acc_unmask)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5911, 0.8155, 0.3790, 0.4603, 0.2671],\n",
       "         [0.4750, 0.2600, 0.8024, 0.9460, 0.8289],\n",
       "         [0.1431, 0.6778, 0.2760, 0.9449, 0.1958],\n",
       "         ...,\n",
       "         [0.9885, 0.6013, 0.0174, 0.9407, 0.4589],\n",
       "         [0.7305, 0.9701, 0.9287, 0.1451, 0.9505],\n",
       "         [0.5324, 0.3756, 0.8453, 0.3226, 0.3585]],\n",
       "\n",
       "        [[0.4535, 0.3415, 0.2855, 0.9170, 0.7606],\n",
       "         [0.9668, 0.6466, 0.3859, 0.1298, 0.8456],\n",
       "         [0.6013, 0.5249, 0.2071, 0.4356, 0.7695],\n",
       "         ...,\n",
       "         [0.4311, 0.7603, 0.1298, 0.6016, 0.8848],\n",
       "         [0.2243, 0.7400, 0.8467, 0.0974, 0.5645],\n",
       "         [0.9781, 0.8408, 0.5260, 0.0680, 0.8351]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 1.],\n",
       "        [0., 0., 1., 0., 0., 1.],\n",
       "        [0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_unmask[0,:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([1., 0., 0., 0., 0.])\n",
      "tensor(0)\n",
      "1 tensor([1., 0., 0., 0., 0.])\n",
      "tensor(0)\n",
      "5 tensor([0., 0., 1., 0., 0.])\n",
      "tensor(2)\n",
      "0 tensor([1., 0., 0., 0., 0.])\n",
      "tensor(0)\n",
      "1 tensor([1., 0., 0., 0., 0.])\n",
      "tensor(0)\n",
      "5 tensor([0., 0., 1., 0., 0.])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "#let's manually calculate this to make sure it works like we would expect!\n",
    "\n",
    "loss_list = []\n",
    "for b in range(seq.shape[0]):\n",
    "    tempseq = seq[b]\n",
    "    tempseq_unmask = seq_unmask[b]\n",
    "    for i in range(seq.shape[1]):\n",
    "        if tempseq_unmask[i,-1] == 0: #only looks at masked values!\n",
    "            continue\n",
    "        #now manually calculate the loss\n",
    "        #raise to the power of exp\n",
    "        pred = tempseq[i]\n",
    "        target = tempseq_unmask[i,:-1]\n",
    "        # print(pred.shape, target.shape) #both are 5 as you would expect\n",
    "        # break\n",
    "        print(i,target)\n",
    "        true = torch.argmax(target)\n",
    "        print(true)\n",
    "        if i > 4:\n",
    "            break\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524288/524288 [00:09<00:00, 56930.46it/s]\n",
      "100%|██████████| 524288/524288 [00:08<00:00, 58837.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "loss_list = []\n",
    "for b in range(seq.shape[0]):\n",
    "    tempseq = seq[b]\n",
    "    tempseq_unmask = seq_unmask[b]\n",
    "    for i in range(seq.shape[1]):\n",
    "        if tempseq_unmask[i,-1] == 0: #only looks at masked values!\n",
    "            continue\n",
    "        #now manually calculate the loss\n",
    "        #raise to the power of exp\n",
    "        pred = tempseq[i]\n",
    "        target = tempseq_unmask[i,:-1]\n",
    "        # print(pred.shape, target.shape) #both are 5 as you would expect\n",
    "        # break\n",
    "        # print(i,target)\n",
    "        true = torch.argmax(target)\n",
    "        # print(true)\n",
    "        exp = torch.exp(pred)\n",
    "        exp_sum = torch.sum(exp)\n",
    "        s = exp[true]/exp_sum\n",
    "        L = -torch.log(s)\n",
    "        loss_list.append(L)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6420)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss_list)/len(loss_list) #bro it's literally exactly accurate lmao!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(1.6420)\n"
     ]
    }
   ],
   "source": [
    "#technicallyy we should be providing class indices not one hot encoded values\n",
    "def ce_loss_mask_seq(seq,seq_unmask,acc,acc_unmask):\n",
    "    '''cross entropy loss function for sequence and accessibility classification\n",
    "    seq: (batch_size, seq_len, vocab_size)\n",
    "    seq_unmask: (batch_size, seq_len, vocab_size+1) #the last one is the mask\n",
    "    acc: Not used\n",
    "    acc_unmask: Not used\n",
    "    '''\n",
    "    \n",
    "    #mask out useless elements, note this will collapse the batch dimension but that's ok\n",
    "    mask = seq_unmask[:,:,-1] == 1\n",
    "    seq = seq[mask]\n",
    "    seq_unmask = seq_unmask[mask]\n",
    "    \n",
    "    # print(seq.shape, seq_unmask.shape)\n",
    "    \n",
    "    seq_unmask = seq_unmask[:,:-1] #remove the mask dim\n",
    "    #now convert to class indices\n",
    "    seq_unmask = torch.argmax(seq_unmask, dim=-1)\n",
    "    \n",
    "    #now compute the loss\n",
    "    loss = F.cross_entropy(seq, seq_unmask)\n",
    "    return loss\n",
    "\n",
    "loss = ce_loss_mask_seq(seq,seq_unmask,acc,acc_unmask)\n",
    "print('loss:', loss) #literallyy the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6419885158538818"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6419991254806519"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(loss_list)/len(loss_list)).item()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0610e-05)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(loss_list)/len(loss_list)) - loss #so tiny it doesn't even matter lmao!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 524288]) torch.Size([524288, 2])\n"
     ]
    }
   ],
   "source": [
    "#now let's look at accessibility\n",
    "print(acc.shape, acc_unmask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, acc, seq_unmask, acc_unmask = dataset_mask_cont[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 524288, 1]) torch.Size([2, 524288, 2])\n"
     ]
    }
   ],
   "source": [
    "acc = torch.rand(2, 524288, 1)\n",
    "acc_unmask = acc_unmask.unsqueeze(0)\n",
    "acc_unmask = torch.cat([acc_unmask, acc_unmask], dim=0)\n",
    "print(acc.shape, acc_unmask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([279000]) torch.Size([279000])\n",
      "loss: tensor(0.9876)\n"
     ]
    }
   ],
   "source": [
    "def poisson_loss_mask(seq,seq_unmask,acc,acc_unmask):\n",
    "    '''poisson loss function for sequence and accessibility regression\n",
    "    seq: Not used\n",
    "    seq_unmask: Not used\n",
    "    acc: (batch_size, seq_len, 1)\n",
    "    acc_unmask: (batch_size, seq_len, 2)\n",
    "    '''\n",
    "    #subset it to the values that are beign evaluated\n",
    "    acc = acc.squeeze(-1)\n",
    "    mask = acc_unmask[:,:,1] == 1\n",
    "    acc = acc[mask]\n",
    "    acc_unmask = acc_unmask[mask][:,0] #remove the mask dim\n",
    "    acc = F.softplus(acc)\n",
    "    print(acc.shape, acc_unmask.shape)\n",
    "    \n",
    "    #and now compute the loss\n",
    "    loss = F.poisson_nll_loss(acc, acc_unmask, log_input=False, full=False)\n",
    "    return loss\n",
    "loss = poisson_loss_mask(seq,seq_unmask,acc,acc_unmask)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524288/524288 [00:09<00:00, 53249.06it/s]\n",
      "100%|██████████| 524288/524288 [00:10<00:00, 52059.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#let's manually calculate the nll loss too\n",
    "loss_list = []\n",
    "for b in range(acc.shape[0]):\n",
    "    tempseq = acc[b]\n",
    "    tempseq_unmask = acc_unmask[b]\n",
    "    for i in tqdm(range(acc.shape[1])):\n",
    "        if tempseq_unmask[i,-1] == 0: #only looks at masked values!\n",
    "            continue\n",
    "        #now manually calculate the loss\n",
    "        #raise to the power of exp\n",
    "        pred = tempseq[i]\n",
    "        target = tempseq_unmask[i,:-1]\n",
    "        # print(pred.shape, target.shape) #both are 1 as we exxpect here since it's elementwise!\n",
    "        # break\n",
    "        # print(i,target)\n",
    "        #now do softplus on the prediction\n",
    "        pred = torch.log(1+torch.exp(pred))\n",
    "        L = pred - target*torch.log(pred+1e-8)\n",
    "        loss_list.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9876])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss_list)/len(loss_list) #hmmm, why is it different here... OH, it's because we forgot the softplus lmao!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss_list) #so same number of elements, why is it different..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.119510650634766e-06"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(loss_list)/len(loss_list)).item() - loss.item() #so it's the same, just need to add the softplus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 524288]) torch.Size([3, 524288]) torch.Size([524288, 6]) torch.Size([524288, 3])\n"
     ]
    }
   ],
   "source": [
    "#final one is the weirdedst which is for cross entropy\n",
    "seq, acc, seq_unmask, acc_unmask = dataset_mask_cat[0]\n",
    "print(seq.shape, acc.shape, seq_unmask.shape, acc_unmask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 524288, 1]) torch.Size([2, 524288, 3])\n"
     ]
    }
   ],
   "source": [
    "acc = torch.rand(2, 524288, 1)\n",
    "acc_unmask = acc_unmask.unsqueeze(0)\n",
    "acc_unmask = torch.cat([acc_unmask, acc_unmask], dim=0)\n",
    "print(acc.shape, acc_unmask.shape) #matches what we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.9825)\n"
     ]
    }
   ],
   "source": [
    "def ce_loss_mask_acc(seq,seq_unmask,acc,acc_unmask): #separate so we can profile them separately, also, we have a single value, so use binary cross entropy\n",
    "    '''cross entropy loss function for sequence and accessibility classification\n",
    "    seq: Not used\n",
    "    seq_unmask: Not used\n",
    "    acc: (batch_size, seq_len, 1)\n",
    "    acc_unmask: (batch_size, seq_len, 3) #the last one is the mask\n",
    "    '''    \n",
    "    #mask out useless elements\n",
    "    acc = acc.squeeze(-1)\n",
    "    mask = acc_unmask[:,:,2] == 1\n",
    "    acc = acc[mask]\n",
    "    acc_unmask = acc_unmask[mask]\n",
    "    \n",
    "    acc = acc.squeeze(0)\n",
    "    acc_unmask = acc_unmask[:,1] #removes mask dim and just gets the values where it is accessible!\n",
    "    \n",
    "    #now compute the loss\n",
    "    loss = F.binary_cross_entropy_with_logits(acc, acc_unmask)\n",
    "    return loss\n",
    "\n",
    "loss = ce_loss_mask_acc(seq,seq_unmask,acc,acc_unmask)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 524288/524288 [00:09<00:00, 52945.08it/s]\n",
      "100%|██████████| 524288/524288 [00:10<00:00, 52075.25it/s]\n"
     ]
    }
   ],
   "source": [
    "#now let's manually calculate this!\n",
    "\n",
    "loss_list = []\n",
    "for b in range(acc.shape[0]):\n",
    "    tempseq = acc[b]\n",
    "    tempseq_unmask = acc_unmask[b]\n",
    "    for i in tqdm(range(acc.shape[1])):\n",
    "        if tempseq_unmask[i,-1] == 0: #only looks at masked values!\n",
    "            continue\n",
    "        \n",
    "        #can think of BCE as CE where one class prob is sigmoid(pred) and the other is 1-sigmoid(pred)\n",
    "        pred = tempseq[i]\n",
    "        target = tempseq_unmask[i,:-1]\n",
    "        # print(pred.shape, target.shape) #1 and 2 as we expect\n",
    "        # break\n",
    "        prob = torch.sigmoid(pred) #sigmoid is basically the probability of 1, already normalized too!\n",
    "        prob2 = 1-prob\n",
    "        true = torch.argmax(target)\n",
    "        probs = torch.stack([prob2, prob]) #importantly we have to assign probabilityy of 1 as the sigmoid as higher means more likely 1\n",
    "        # exp = torch.exp(probs)\n",
    "        # exp_sum = torch.sum(exp)\n",
    "        # s = exp[true]/exp_sum\n",
    "        s = probs[true]\n",
    "        L = -torch.log(s)\n",
    "        loss_list.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9825])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(loss_list)/len(loss_list) #so it's the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.47713851928711e-06"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(loss_list)/len(loss_list)).item() - loss.item() #so it's the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if we calculate the traditional way? eh not worth testing, it's fine!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([524288, 6])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's see what perfect loss is like\n",
    "\n",
    "def ce_loss_mask_seq(x, y):\n",
    "    \"\"\"\n",
    "    Cross entropy loss for sequence classification.\n",
    "    \n",
    "    x: tuple (seq, dummy)\n",
    "         - seq: (batch_size, seq_len, vocab_size)\n",
    "    y: tuple (seq_unmask, dummy)\n",
    "         - seq_unmask: (batch_size, seq_len, vocab_size+1)  (last channel is the mask)\n",
    "    \"\"\"\n",
    "    seq = x[0]\n",
    "    seq_unmask = y[0]\n",
    "    \n",
    "    # Create mask from last column of seq_unmask\n",
    "    mask = seq_unmask[:, :, -1] == 1\n",
    "    seq_pred = seq[mask]\n",
    "    # Remove mask channel from target; resulting shape is (N, vocab_size)\n",
    "    seq_target = seq_unmask[mask][:, :-1]\n",
    "    \n",
    "    loss = F.cross_entropy(seq_pred, seq_target)\n",
    "    return loss\n",
    "\n",
    "seq_unmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 524288, 6])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_unmask = seq_unmask.unsqueeze(0)\n",
    "seq_unmask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9048)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now input for perfect loss\n",
    "ce_loss_mask_seq((seq_unmask[:,:,:-1], None), (seq_unmask, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = seq_unmask[:,:,-1] == 1\n",
    "s1 = seq_unmask[mask][:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([131039, 5])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9048)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(s1, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([131039])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's define s1 in terms of indices\n",
    "s1_idx = torch.argmax(s1, dim=-1)\n",
    "s1_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9048)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(s1, s1_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.sum(1).max() #they're all 1s because it's 1 hot true values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([131039, 5]) torch.Size([131039])\n"
     ]
    }
   ],
   "source": [
    "rand_idx = torch.randint(0, 5, (131039,))\n",
    "rand_onehot = torch.zeros(131039, 5)\n",
    "rand_onehot[torch.arange(131039), rand_idx] = 1\n",
    "print(rand_onehot.shape, rand_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9048)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(rand_onehot, rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oh that isn't perfect, here's what perfect is\n",
    "#logits aren't already softmaxed, so can keep going down\n",
    "F.cross_entropy(rand_onehot*1e9, rand_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(rand_onehot*1000, rand_onehot) #so rand idx or rand onehot is fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
