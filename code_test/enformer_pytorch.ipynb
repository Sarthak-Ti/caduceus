{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have a custom class, let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "from typing import ForwardRef\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna')\n",
    "\n",
    "import src.models.nn.utils as U\n",
    "import src.utils as utils\n",
    "import src.utils.config\n",
    "from src.models.sequence.block import SequenceResidualBlock\n",
    "from src.models.nn.components import Normalization\n",
    "from src.utils.enformer_pytorch import exponential_linspace_int, MaxPool, AttentionPool, ConvBlock, Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder abstraction\n",
    "    Accepts a tensor and optional kwargs. Outside of the main tensor, all other arguments should be kwargs.\n",
    "    Returns a tensor and optional kwargs.\n",
    "    Encoders are combined via U.PassthroughSequential which passes these kwargs through in a pipeline. The resulting kwargs are accumulated and passed into the model backbone.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        x: input tensor\n",
    "        *args: additional info from the dataset (e.g. sequence lengths)\n",
    "\n",
    "        Returns:\n",
    "        y: output tensor\n",
    "        *args: other arguments to pass into the model backbone\n",
    "        \"\"\"\n",
    "        return x, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnformerEncoder(Encoder):\n",
    "    def __init__(self, d_input, d_model, filter_sizes, flat=False,\n",
    "                dim = 1536,\n",
    "                num_downsamples = 7,    # genetic sequence is downsampled 2 ** 7 == 128x in default Enformer - can be changed for higher resolution\n",
    "                dim_divisible_by = 128,\n",
    "                use_tf_gamma = False,\n",
    "                pool_type = 'max',\n",
    "                conv_tower = False,\n",
    "                **kwargs,\n",
    "             ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_downsamples = num_downsamples\n",
    "        self.dim_divisible_by = dim_divisible_by\n",
    "        self.pool_type = pool_type\n",
    "        self.use_conv_tower = conv_tower\n",
    "        \n",
    "        Pool = MaxPool if self.pool_type == 'max' else AttentionPool\n",
    "        half_dim = self.dim // 2\n",
    "        twice_dim = self.dim * 2\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(4, half_dim, 15, padding = 7),\n",
    "            Residual(ConvBlock(half_dim)),\n",
    "            Pool(half_dim, pool_size = 2)\n",
    "        )\n",
    "\n",
    "        filter_list = exponential_linspace_int(half_dim, self.dim, num = (self.num_downsamples - 1), divisible_by = self.dim_divisible_by)\n",
    "        filter_list = [half_dim, *filter_list]\n",
    "\n",
    "        conv_layers = []\n",
    "        for dim_in, dim_out in zip(filter_list[:-1], filter_list[1:]):\n",
    "            conv_layers.append(nn.Sequential(\n",
    "                ConvBlock(dim_in, dim_out, kernel_size = 5),\n",
    "                Residual(ConvBlock(dim_out, dim_out, 1)),\n",
    "                Pool(dim_out, pool_size = 2)\n",
    "            ))\n",
    "\n",
    "        self.conv_tower = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        if self.use_conv_tower:\n",
    "            x = self.conv_tower(x)\n",
    "        return x, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnformerEncoder(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv1d(4, 768, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (1): Residual(\n",
       "      (fn): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "    )\n",
       "    (2): MaxPool()\n",
       "  )\n",
       "  (conv_tower): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 768, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(768, 768, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(768, 896, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(896, 896, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(896, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1024, 1152, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1152, 1152, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1152, 1280, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1280, 1280, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): GELU()\n",
       "        (2): Conv1d(1280, 1536, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (fn): Sequential(\n",
       "          (0): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): GELU()\n",
       "          (2): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,))\n",
       "        )\n",
       "      )\n",
       "      (2): MaxPool()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obviously need to add a self.forward but let's just test it directly here! Then we can see the output of both and determine what's needed\n",
    "#literally just self.stem and self.conv_tower see the outputs!!\n",
    "\n",
    "#let's define it\n",
    "encoder = EnformerEncoder(1,1,1,output_heads = dict(human=1), pool_type = 'max', conv_tower = False)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's make a simple one hot encoded sequence\n",
    "pattern = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "sequence = torch.tensor([pattern*224]).float()\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 896, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Need to test this, particularly to see what the shape of x is. X should be b, d, n. Test this but probably fine?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#let's input it\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/leslie/sarthak/environments/hyena_backup2/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/leslie/sarthak/hyena/hyena-dna/src/utils/enformer_pytorch.py:32\u001b[0m, in \u001b[0;36mMaxPool.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(x, (pad_left, pad_right))\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#let's raise an error now to make sure this isn't used because it isn't implemented\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed to test this, particularly to see what the shape of x is. X should be b, d, n. Test this but probably fine?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmax_pool1d(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_size)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Need to test this, particularly to see what the shape of x is. X should be b, d, n. Test this but probably fine?"
     ]
    }
   ],
   "source": [
    "#let's input it\n",
    "out = encoder.stem(sequence.transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 896])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I added that error in max pool, let's test it and ensure\n",
    "out1 = encoder.stem[0](sequence.transpose(1,2))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 896])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turns this into 768 channels which is good, and keeps it the same length with its padding\n",
    "out2 = encoder.stem[1](out1)\n",
    "out2.shape #this is just gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and now the next step\n",
    "class MaxPool(torch.nn.Module):\n",
    "    def __init__(self, dim = None, pool_size=2, padding='same'): #note dim is taken as a useless argument since only used for compatibility with AttentionPool\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.padding == 'same':\n",
    "            pad_total = self.pool_size - 1\n",
    "            pad_left = pad_total // 2\n",
    "            pad_right = pad_total - pad_left\n",
    "            x = F.pad(x, (pad_left, pad_right))\n",
    "            #let's raise an error now to make sure this isn't used because it isn't implemented\n",
    "            # raise NotImplementedError(\"Need to test this, particularly to see what the shape of x is. X should be b, d, n. Test this but probably fine?\")\n",
    "        return F.max_pool1d(x, self.pool_size, stride=self.pool_size)\n",
    "out3 = MaxPool()(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 448])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3.shape #yup reduces the length dimension not the channel, this is good!!\n",
    "#input should be 1 x 4 x length where 4 is the one hot encoding and length is the length of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 448])\n"
     ]
    }
   ],
   "source": [
    "#now we have the output of the model here, what if we look at the conv tower\n",
    "#fixed the max pool so we can just do the stem\n",
    "out1 = encoder.stem(sequence.transpose(1,2))\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1536, 7])\n"
     ]
    }
   ],
   "source": [
    "#now check the conv tower\n",
    "out2 = encoder.conv_tower(out1)\n",
    "print(out2.shape) #so length 896 turns to 7... so 128x downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#but the initial stem is only 2k downsample. So we can try both or maybe stick to 1? we have options!\n",
    "#wait one of the benefits of 128x downsample is that when we do 128 bp we can just do 1x1 conv to get the output...\n",
    "#so there is a point, maybe we'll try both?\n",
    "encoder.use_conv_tower = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1536, 7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = encoder(sequence.transpose(1,2))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test a new model\n",
    "encoder = EnformerEncoder(1,1,1, dim = 256,pool_type = 'max', conv_tower = True) #we'll make sure it's the right model dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = encoder(sequence.transpose(1,2))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test it with attention pool\n",
    "encoder = EnformerEncoder(1,1,1, dim = 256,pool_type = 'attention', conv_tower = True) #we'll make sure it's the right model dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = encoder(sequence.transpose(1,2))\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 448])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also let's see what the output of the stem is!\n",
    "out1 = encoder.stem(sequence.transpose(1,2))\n",
    "out1.shape\n",
    "#so it's half, meaning if we just want to use the tem we do 2* the model dimension we want\n",
    "#but since we're downsamplign can increase model dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the decoder as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so after the main moel part my model is batch x seqlen x dmodel\n",
    "import sys\n",
    "sys.path.append('/data/leslie/sarthak/hyena/hyena-dna')\n",
    "from einops.layers.torch import Rearrange\n",
    "from src.utils.enformer_pytorch import exponential_linspace_int, MaxPool, AttentionPool, ConvBlock, Residual, GELU\n",
    "import torch.nn as nn\n",
    "dropout_rate = 0.1\n",
    "d_model = 256\n",
    "half_dim = d_model//2\n",
    "twice_dim = d_model*2\n",
    "filter_list = exponential_linspace_int(half_dim, d_model, num = (7 - 1), divisible_by = 128)\n",
    "filter_list = [half_dim, *filter_list]\n",
    "final_pointwise = nn.Sequential(\n",
    "    Rearrange('b n d -> b d n'),\n",
    "    ConvBlock(filter_list[-1], twice_dim, 1),\n",
    "    Rearrange('b d n -> b n d'),\n",
    "    nn.Dropout(dropout_rate / 4),\n",
    "    GELU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's make a test samplee\n",
    "import torch\n",
    "seq = torch.randn(1,1024,256)\n",
    "#now let's see what happens if we pass it through the final pointwise\n",
    "out = final_pointwise(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 512])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actually before that we have this \n",
    "class TargetLengthCrop(nn.Module):\n",
    "    def __init__(self, target_length):\n",
    "        super().__init__()\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len, target_len = x.shape[-2], self.target_length\n",
    "\n",
    "        if target_len == -1:\n",
    "            return x\n",
    "\n",
    "        if seq_len < target_len:\n",
    "            raise ValueError(f'sequence length {seq_len} is less than target length {target_len}')\n",
    "\n",
    "        trim = (target_len - seq_len) // 2\n",
    "\n",
    "        if trim == 0:\n",
    "            return x\n",
    "\n",
    "        return x[:, -trim:trim]\n",
    "\n",
    "target_length = 896\n",
    "crop = TargetLengthCrop(target_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 896, 256])\n",
      "torch.Size([1, 896, 512])\n"
     ]
    }
   ],
   "source": [
    "seq = torch.randn(1,1024,256)\n",
    "out = crop(seq)\n",
    "print(out.shape)\n",
    "out = final_pointwise(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then after that convolution it does a linear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
