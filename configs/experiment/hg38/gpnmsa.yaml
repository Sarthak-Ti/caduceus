# @package _global_
defaults:
  - /pipeline: enformer_pipeline
  - /model: ???
  - override /dataset: GPNMSA_dataset
  # - default model/layer: mha
  # - override /scheduler: constant

#for mamba we don't do this rather we override it, so here we put arguments if we use the dna embedding model like it's model size and num layers, but don't care about that now
# model: # alot of this is based on the config.json file that came along with it
  # _name_: dna_embedding
  # d_model: 256
  # n_layer: 16 #was 2
  # d_inner: ${eval:4 * ${.d_model}} #is 512, need to keep it that way
  # vocab_size: 16 #to be consistent, but adding embeddings will fix this
  # resid_dropout: 0.0
  # embed_dropout: 0.1
  # fused_mlp: False # figure out how to use fused MLP, maybe only with bf16 + a100
  # fused_dropout_add_ln: False #false for short or if we don't have package installed!
  # checkpoint_mixer: False  # set true for memory reduction, might cause issues?
  # checkpoint_mlp: False  # set true for memory reduction
  # residual_in_fp32: True
  # pad_vocab_size_multiple: 1
  # # load_old_embedding: 180 #used in lmbackbone to say the size of the new one, and how many elements to make it
  # layer:
  #   _name_: hyena
  #   emb_dim: 5
  #   filter_order: 64
  #   short_filter_order: 3
  #   l_max: ${eval:${dataset.max_length}+2}
  #   modulate: True
  #   w: 10
  #   lr: ${optimizer.lr}
  #   wd: 0.0
  #   lr_pos_emb: 0.0

task:
  metrics:
    - poisson_loss

decoder:
  d_model: ${model.config.d_model}
  d_output: ${dataset.d_output}
  kmer_len: ${dataset.kmer_len}
  bin_size: ${dataset.pool}

trainer:
  accelerator: gpu
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${dataset.batch_size} * ${trainer.num_nodes}}}
  max_epochs: 1000
  precision: 16  # bf16 only a100
  gradient_clip_val: 1.0
  strategy: auto
  limit_train_batches: 1.0   # train on full dataset, can be used to toggle quick run
  limit_val_batches: 1.0     # validate on full dataset, can be used to toggle quick run
  num_sanity_val_steps: 0

dataset:
  # batch_size: 32  # Per GPU
  batch_size: 2
  max_length: 131072 # 262144, 524288
  # optional, default is max_length
  max_length_val: ${dataset.max_length}
  max_length_test: ${dataset.max_length}
  tokenizer_name: char
  pad_max_length: null  # needed for bpe tokenizer
  rc_aug: true #was false
  d_output: 674 #have to manually define this
  num_workers: 1
  use_fixed_len_val: false  # placing a fixed length val here, but it's really the test
  replace_N_token: false  # replace N (uncertain token) with pad tokens in dataloader
  pad_interval: false  # handle uncertain tokens within the FastaInteral class  
  return_CAGE: false  # return cage data in dataloader
  kmer_len: null  # kmer length for kmer tokenization
  cell_type: DNase
  one_hot: true
  pool: 128
  msa_path: null


# scheduler:
#   t_in_epochs: False
#   t_initial: ${eval:${div_up:${dataset.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs}}  # num steps for 1 cycle
#   warmup_lr_init: 1e-6  # starting point
#   warmup_t: ${eval:${div_up:${dataset.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs} * 0.01}  # time for ramp up
#   lr_min: ${eval:0.1 * ${optimizer.lr}}  # flatlines with this

optimizer:
  lr: 8e-5  # peak
  weight_decay: 0.1

train:
  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i 0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode()) / 1000)"}
  seed: 2222
  global_batch_size: ${dataset.batch_size}
  # pretrained_model_path: /data/leslie/sarthak/caduceus/caduceus-ps_seqlen-131k_d_model-256_n_layer-16/weights.ckpt #the path for the fine tuned model
  # pretrained_safetensors_model_path: /data/leslie/sarthak/caduceus/caduceus-ps_seqlen-131k_d_model-256_n_layer-16/model.safetensors #the path for the fine tuned model
  pretrained_model_strict_load: False  # false allows encoder/decoder to be used if new model uses it, which downstream fineituning does
  pretrained_model_state_hook: #note all of these are passed to the load_backbone function
    _name_: load_backbone
    freeze_backbone: false  # first try we just use this
    ignore_head: true #in our case we want to make sure this is true
    add_embeddings: false #initially was 4
    ignore_embeddings: false #tells it to reset embeddings if true, completely reinitializes them
    # load_old_embedding: 12 #tells it to load the old embeddings from the pretrained model, then to add new ones for new tokens.
    # keeps the number of embeddings that you tell it to keep, and adds new ones for new tokens, the new amount is the difference.
    #between this number and the number in the config.dataset
  #bias model chrombpnet_model_1000/models/bias_model_scaled.h5