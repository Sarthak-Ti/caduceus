# @package _global_
defaults:
  - /pipeline: kmer_pretrain
  - /model: ???
  - override /scheduler: cosine_warmup_timm
  # - default model/layer: mha
  # - override /scheduler: constant

#for mamba we don't do this rather we override it, so here we put arguments if we use the dna embedding model like it's model size and num layers, but don't care about that now
# model: # alot of this is based on the config.json file that came along with it
  # _name_: dna_embedding
  # d_model: 256
  # n_layer: 16 #was 2
  # d_inner: ${eval:4 * ${.d_model}} #is 512, need to keep it that way
  # vocab_size: 16 #to be consistent, but adding embeddings will fix this
  # resid_dropout: 0.0
  # embed_dropout: 0.1
  # fused_mlp: False # figure out how to use fused MLP, maybe only with bf16 + a100
  # fused_dropout_add_ln: False #false for short or if we don't have package installed!
  # checkpoint_mixer: False  # set true for memory reduction, might cause issues?
  # checkpoint_mlp: False  # set true for memory reduction
  # residual_in_fp32: True
  # pad_vocab_size_multiple: 1
  # # load_old_embedding: 180 #used in lmbackbone to say the size of the new one, and how many elements to make it
  # layer:
  #   _name_: hyena
  #   emb_dim: 5
  #   filter_order: 64
  #   short_filter_order: 3
  #   l_max: ${eval:${dataset.max_length}+2}
  #   modulate: True
  #   w: 10
  #   lr: ${optimizer.lr}
  #   wd: 0.0
  #   lr_pos_emb: 0.0

task:
  _name_: lm
  loss:
    _name_: cross_entropy

trainer:
  accelerator: gpu
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${dataset.batch_size} * ${trainer.num_nodes}}}
  max_epochs: 1000
  precision: 16  # bf16 only a100
  gradient_clip_val: 1.0
  strategy: auto
  limit_train_batches: 1.0   # train on full dataset, can be used to toggle quick run
  limit_val_batches: 1.0     # validate on full dataset, can be used to toggle quick run
  num_sanity_val_steps: 0

dataset:
  # batch_size: 32  # Per GPU
  batch_size: 2
  max_length: 196608 # 262144, 524288
  # optional, default is max_length
  max_length_val: ${dataset.max_length}
  max_length_test: ${dataset.max_length}
  tokenizer_name: char
  pad_max_length: null  # needed for bpe tokenizer
  add_eos: false
  rc_aug: false #was false
  num_workers: 1
  use_fixed_len_val: false  # placing a fixed length val here, but it's really the test
  replace_N_token: false  # replace N (uncertain token) with pad tokens in dataloader
  pad_interval: false  # handle uncertain tokens within the FastaInteral class  
  kmer_len: 6  # kmer length for kmer tokenization


# scheduler:
#   t_in_epochs: False
#   t_initial: ${eval:${div_up:${dataset.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs}}  # num steps for 1 cycle
#   warmup_lr_init: 1e-6  # starting point
#   warmup_t: ${eval:${div_up:${dataset.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs} * 0.01}  # time for ramp up
#   lr_min: ${eval:0.1 * ${optimizer.lr}}  # flatlines with this

optimizer:
  lr: 6e-4
  weight_decay: 0.1
  betas: [0.9, 0.95]

train:
  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i 0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode()) / 1000)"}
  seed: 2222
  global_batch_size: ${dataset.batch_size}
  # pretrained_model_path: /data/leslie/sarthak/caduceus/caduceus-ps_seqlen-131k_d_model-256_n_layer-16/weights.ckpt #the path for the fine tuned model