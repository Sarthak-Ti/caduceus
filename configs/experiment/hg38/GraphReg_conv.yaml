# @package _global_
defaults:
  - /pipeline: enformer_pipeline
  - /model: graph_reg_conv
  - override /dataset: graphreg_dataset


task:
  _name_: basic
  metrics:
    - poisson_loss

trainer:
  accelerator: gpu
  devices: 1
  num_nodes: 1
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${dataset.batch_size} * ${trainer.num_nodes}}}
  max_epochs: 200
  precision: 16  # bf16 only a100
  gradient_clip_val: 1.0
  strategy: auto
  limit_train_batches: 1.0   # train on full dataset, can be used to toggle quick run
  limit_val_batches: 1.0     # validate on full dataset, can be used to toggle quick run
  num_sanity_val_steps: 0

dataset:
  # batch_size: 32  # Per GPU
  batch_size: 2
  max_length: 100000
  # optional, default is max_length
  max_length_val: ${dataset.max_length}
  max_length_test: ${dataset.max_length}
  tokenizer_name: char
  pad_max_length: null  # needed for bpe tokenizer
  add_eos: false
  rc_aug: false #was false
  num_workers: 1
  use_fixed_len_val: false  # placing a fixed length val here, but it's really the test
  replace_N_token: false  # replace N (uncertain token) with pad tokens in dataloader
  pad_interval: false  # handle uncertain tokens within the FastaInteral class  
  kmer_len: null  # kmer length for kmer tokenization
  cell_type: GM12878
  vocab_size: ${model.config.vocab_size}
  one_hot: true

decoder: id

optimizer:
  lr: 8e-5  # peak
  weight_decay: 0.1

train:
  gpu_mem: ${eval:"round(float(__import__('subprocess').check_output('nvidia-smi -i 0 --query-gpu=memory.total --format=csv,noheader,nounits', shell=True).strip().decode()) / 1000)"}
  seed: 2222
  global_batch_size: ${dataset.batch_size}